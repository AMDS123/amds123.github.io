---
layout: post
title: "Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves"
date: 2016-04-08 05:45:44
categories: arXiv_CL
tags: arXiv_CL RNN
author: Fei Tian, Bin Gao, Di He, Tie-Yan Liu
mathjax: true
---

* content
{:toc}

##### Abstract
We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks (RNN) based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic (i.e., topics to sentences), which is a key technology for real world applications such as personalized short text conversation.

##### Abstract (translated by Google)
我们提出了句子级复发主题模型（SLRTM），这是一个假设句子中每个单词的生成都依赖于句子的主题和句子中前面单词的整个历史的新主题模型。与传统的话题模型不同，这些模型在很大程度上忽略了单词的连续顺序或者话题连贯性，SLRTM通过使用基于递归神经网络（RNN）的框架给出了完整的表征。实验结果表明，SLRTM在各种任务上胜过了几个强大的基线。此外，SLRTM可以自动生成给定主题（即主题到句子）的句子，这是现实世界应用（例如个性化短文本会话）的关键技术。

##### URL
[https://arxiv.org/abs/1604.02038](https://arxiv.org/abs/1604.02038)

##### PDF
[https://arxiv.org/pdf/1604.02038](https://arxiv.org/pdf/1604.02038)

