---
layout: post
title: "Learning Concept Embeddings for Efficient Bag-of-Concepts Densification"
date: 2017-02-10 22:44:59
categories: arXiv_SD
tags: arXiv_SD Sparse Embedding Classification
author: Walid Shalaby, Wlodek Zadrozny
mathjax: true
---

* content
{:toc}

##### Abstract
Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose two neural embedding models in order to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to generate fully dense bag-of-concepts representations. Empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other concept embedding models. In addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness of the densified vector representation over the typical sparse representations for dataless classification where we can achieve at least same or better accuracy with much less dimensions.

##### Abstract (translated by Google)
明确的概念空间模型已被证明在许多自然语言和文本挖掘应用程序中用于文本表示的功效。这个想法是将文本结构嵌入概念的语义空间，这些概念捕捉这些结构的主要主题。由于低概念重叠，所谓的概念包表示遭受数据稀疏，导致类似文本之间的相似性分数低。在本文中，我们提出了两个神经嵌入模型，以学习连续的概念向量。一旦学习，我们提出一种有效的向量聚合方法来生成完全密集的概念表示。用于度量实体语义相关性的基准数据集的实验结果表现出优于其他概念嵌入模型的性能。此外，通过利用我们的高效聚合方法，我们证明了对无数据分类的典型稀疏表示的致密矢量表示的有效性，其中我们可以实现至少相同或更好的精度和更少的维度。

##### URL
[https://arxiv.org/abs/1702.03342](https://arxiv.org/abs/1702.03342)

##### PDF
[https://arxiv.org/pdf/1702.03342](https://arxiv.org/pdf/1702.03342)

