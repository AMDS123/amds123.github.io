---
layout: post
title: "Essentially No Barriers in Neural Network Energy Landscape"
date: 2018-03-02 15:22:10
categories: arXiv_AI
tags: arXiv_AI Sparse Knowledge
author: Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred A. Hamprecht
mathjax: true
---

* content
{:toc}

##### Abstract
Training neural networks involves finding minima of a high-dimensional non-convex loss function. Knowledge of the structure of this energy landscape is sparse. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that neural networks have enough capacity for structural changes, or that these changes are small between minima. Also, each minimum has at least one vanishing Hessian eigenvalue in addition to those resulting from trivial invariance.

##### Abstract (translated by Google)
训练神经网络涉及找到高维非凸损失函数的最小值。这种能源景观的结构知识很稀少。从线性插值放宽，我们在CIFAR10和CIFAR100上构建最近的神经网络结构的最小值之间的连续路径。令人惊讶的是，在培训和测试环境中，路径基本上是平坦的。这意味着神经网络具有足够的结构变化能力，或者这些变化在最小值之间很小。另外，每个最小值除了由平凡的不变性产生的最小值外，还至少有一个消失的Hessian特征值。

##### URL
[http://arxiv.org/abs/1803.00885](http://arxiv.org/abs/1803.00885)

##### PDF
[http://arxiv.org/pdf/1803.00885](http://arxiv.org/pdf/1803.00885)

