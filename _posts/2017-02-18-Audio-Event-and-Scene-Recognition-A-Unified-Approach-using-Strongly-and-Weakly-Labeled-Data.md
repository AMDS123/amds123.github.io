---
layout: post
title: "Audio Event and Scene Recognition: A Unified Approach using Strongly and Weakly Labeled Data"
date: 2017-02-18 07:18:32
categories: arXiv_CV
tags: arXiv_CV Regularization Weakly_Supervised Optimization Detection Recognition
author: Anurag Kumar, Bhiksha Raj
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we propose a novel learning framework called Supervised and Weakly Supervised Learning where the goal is to learn simultaneously from weakly and strongly labeled data. Strongly labeled data can be simply understood as fully supervised data where all labeled instances are available. In weakly supervised learning only data is weakly labeled which prevents one from directly applying supervised learning methods. Our proposed framework is motivated by the fact that a small amount of strongly labeled data can give considerable improvement over only weakly supervised learning. The primary problem domain focus of this paper is acoustic event and scene detection in audio recordings. We first propose a naive formulation for leveraging labeled data in both forms. We then propose a more general framework for Supervised and Weakly Supervised Learning (SWSL). Based on this general framework, we propose a graph based approach for SWSL. Our main method is based on manifold regularization on graphs in which we show that the unified learning can be formulated as a constraint optimization problem which can be solved by iterative concave-convex procedure (CCCP). Our experiments show that our proposed framework can address several concerns of audio content analysis using weakly labeled data.

##### Abstract (translated by Google)
在本文中，我们提出了一个新的学习框架，称为监督和弱监督学习，其目标是同时学习弱和强烈标记的数据。强标记的数据可以简单地理解为完全监督的数据，其中所有标记的实例都可用。在弱监督学习中，只有数据被弱标记，不能直接应用监督学习方法。我们提出的框架的动机是这样一个事实，即少量强标记的数据可以相对于只有弱监督的学习产生相当大的改进。本文主要关注的问题是录音中的声音事件和场景检测。我们首先提出一个天真的公式来利用两种形式的标签数据。然后，我们提出了一个更一般的监督和弱监督学习（SWSL）的框架。基于这个通用框架，我们提出了一个基于图的SWSL方法。我们的主要方法是基于图上的流形正则化，其中我们表明统一学习可以被形成一个约束优化问题，可以通过迭代凹凸程序（CCCP）来解决。我们的实验表明，我们提出的框架可以解决使用弱标记数据的音频内容分析的几个问题。

##### URL
[https://arxiv.org/abs/1611.04871](https://arxiv.org/abs/1611.04871)

##### PDF
[https://arxiv.org/pdf/1611.04871](https://arxiv.org/pdf/1611.04871)

