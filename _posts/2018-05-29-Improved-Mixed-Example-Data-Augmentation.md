---
layout: post
title: "Improved Mixed-Example Data Augmentation"
date: 2018-05-29 07:06:58
categories: arXiv_CV
tags: arXiv_CV
author: Cecilia Summers, Michael J. Dinneen
mathjax: true
---

* content
{:toc}

##### Abstract
In order to reduce overfitting, neural networks are typically trained with data augmentation, the practice of artificially generating additional training data via label-preserving transformations of existing training examples. Recent work has demonstrated a surprisingly effective type of non-label-preserving data augmentation, in which pairs of training examples are averaged together. In this work, we generalize this "mixed-example data augmentation", which allows us to find methods that improve upon previous work. This generalization also reveals that linearity is not necessary as an inductive bias in order for mixed-example data augmentation to be effective, providing evidence against the primary theoretical hypothesis from prior work.

##### Abstract (translated by Google)
为了减少过拟合，神经网络通常使用数据增强进行训练，即通过对现有训练样例进行标记保留转换来人工生成额外的训练数据。最近的工作已经证明了令人惊讶的有效类型的非标签保存数据增强，其中将成对的训练样例平均在一起。在这项工作中，我们概括了这个“混合示例数据增强”，它使我们能够找到改进以前工作的方法。这种一般化还表明，为了使混合示例数据增加有效，线性并不是必要的，这为归纳偏差提供了证据，反对之前工作的主要理论假设。

##### URL
[http://arxiv.org/abs/1805.11272](http://arxiv.org/abs/1805.11272)

##### PDF
[http://arxiv.org/pdf/1805.11272](http://arxiv.org/pdf/1805.11272)

