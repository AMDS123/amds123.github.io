---
layout: post
title: "Deep Learning with Data Dependent Implicit Activation Function"
date: 2018-02-01 06:25:39
categories: arXiv_CV
tags: arXiv_CV Deep_Learning Recognition
author: Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley J. Osher
mathjax: true
---

* content
{:toc}

##### Abstract
Though deep neural networks (DNNs) achieve remarkable performances in many artificial intelligence tasks, the lack of training instances remains a notorious challenge. As the network goes deeper, the generalization accuracy decays rapidly in the situation of lacking massive amounts of training data. In this paper, we propose novel deep neural network structures that can be inherited from all existing DNNs with almost the same level of complexity, and develop simple training algorithms. We show our paradigm successfully resolves the lack of data issue. Tests on the CIFAR10 and CIFAR100 image recognition datasets show that the new paradigm leads to 20$\%$ to $30\%$ relative error rate reduction compared to their base DNNs. The intuition of our algorithms for deep residual network stems from theories of the partial differential equation (PDE) control problems. Code will be made available.

##### Abstract (translated by Google)
虽然深度神经网络（DNNs）在许多人工智能任务中取得了显着的成绩，但缺乏训练实例仍然是一个臭名昭着的挑战。随着网络的深入，在缺乏大量训练数据的情况下泛化精度迅速衰减。在本文中，我们提出了新的深度神经网络结构，可以从所有现有的DNN中继承，具有几乎相同的复杂度水平，并且开发简单的训练算法。我们展示了我们的范例，成功解决了数据缺乏问题。 CIFAR10和CIFAR100图像识别数据集上的测试表明，与基本DNN相比，新范例导致相对误差率降低了20％到30％。我们的深度残余网络算法的直觉源于偏微分方程（PDE）控制问题的理论。代码将被提供。

##### URL
[http://arxiv.org/abs/1802.00168](http://arxiv.org/abs/1802.00168)

##### PDF
[http://arxiv.org/pdf/1802.00168](http://arxiv.org/pdf/1802.00168)

