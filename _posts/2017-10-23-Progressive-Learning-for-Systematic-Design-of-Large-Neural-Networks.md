---
layout: post
title: "Progressive Learning for Systematic Design of Large Neural Networks"
date: 2017-10-23 10:06:15
categories: arXiv_CV
tags: arXiv_CV Regularization Optimization Classification
author: Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra, Mikael Skoglund
mathjax: true
---

* content
{:toc}

##### Abstract
We develop an algorithm for systematic design of a large artificial neural network using a progression property. We find that some non-linear functions, such as the rectifier linear unit and its derivatives, hold the property. The systematic design addresses the choice of network size and regularization of parameters. The number of nodes and layers in network increases in progression with the objective of consistently reducing an appropriate cost. Each layer is optimized at a time, where appropriate parameters are learned using convex optimization. Regularization parameters for convex optimization do not need a significant manual effort for tuning. We also use random instances for some weight matrices, and that helps to reduce the number of parameters we learn. The developed network is expected to show good generalization power due to appropriate regularization and use of random weights in the layers. This expectation is verified by extensive experiments for classification and regression problems, using standard databases.

##### Abstract (translated by Google)
我们开发了一个使用渐进属性的大型人工神经网络系统设计算法。我们发现一些非线性函数，例如整流器线性单元及其派生物，具有这个性质。系统的设计解决了网络规模的选择和参数的正则化问题。网络中的节点和层次的数量随着持续降低适当成本的目标而增加。每一层都进行优化，在适当的参数下使用凸优化学习。凸优化的正则化参数不需要大量的手动调整。我们也使用一些权矩阵的随机实例，这有助于减少我们学习的参数的数量。由于适当的正则化和各层中随机权重的使用，预计发达的网络将显示出良好的泛化能力。通过使用标准数据库的分类和回归问题的广泛实验来证实这种期望。

##### URL
[https://arxiv.org/abs/1710.08177](https://arxiv.org/abs/1710.08177)

##### PDF
[https://arxiv.org/pdf/1710.08177](https://arxiv.org/pdf/1710.08177)

