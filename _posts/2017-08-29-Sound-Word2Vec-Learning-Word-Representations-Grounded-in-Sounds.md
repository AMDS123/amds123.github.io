---
layout: post
title: "Sound-Word2Vec: Learning Word Representations Grounded in Sounds"
date: 2017-08-29 15:54:31
categories: arXiv_SD
tags: arXiv_SD Embedding Language_Model
author: Ashwin K Vijayakumar, Ramakrishna Vedantam, Devi Parikh
mathjax: true
---

* content
{:toc}

##### Abstract
To be able to interact better with humans, it is crucial for machines to understand sound - a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic textual similarity assessment. In this work, we treat sound as a first-class citizen, studying downstream textual tasks which require aural grounding. To this end, we propose sound-word2vec - a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.

##### Abstract (translated by Google)
为了能够更好地与人类互动，机器理解声音至关重要 - 人类感知的主要形式。以前的作品使用声音来学习嵌入，以改善通用的文本相似性评估。在这项工作中，我们把声音视为一流的公民，研究下游需要听觉依据的文本任务。为此，我们提出了sound-word2vec  - 一种新的嵌入方案，可以学习基于声音的专业词汇嵌入。例如，我们知道两个表面上（语义上）不相关的概念，比如叶子和纸是相似的，因为它们所产生的相似的沙沙声。我们的嵌入在文本任务中被证明是有用的，这些任务需要基于文本的声音检索和发现福利音效（在电影中使用）的声音推理。此外，我们的嵌入空间捕捉单词和拟声词之间的有趣的依赖关系，并且胜过以前在与听觉相关的单词相关性数据集如AMEN和ASLex上的工作。

##### URL
[https://arxiv.org/abs/1703.01720](https://arxiv.org/abs/1703.01720)

##### PDF
[https://arxiv.org/pdf/1703.01720](https://arxiv.org/pdf/1703.01720)

