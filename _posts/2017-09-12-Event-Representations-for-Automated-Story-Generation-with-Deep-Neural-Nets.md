---
layout: post
title: "Event Representations for Automated Story Generation with Deep Neural Nets"
date: 2017-09-12 14:45:22
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock, Shruti Singh, Brent Harrison, Mark O. Riedl
mathjax: true
---

* content
{:toc}

##### Abstract
Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.

##### Abstract (translated by Google)
自动故事生成是自动选择一系列可以作为故事讲述的事件，动作或单词的问题。我们试图开发一个系统，可以通过学习从文本故事语料库中知道的所有东西来生成故事。到目前为止，在字符，单词或句子级别学习语言模型的递归神经网络几乎没有成功地产生连贯的故事。我们探讨事件表示的问题，它提供了单词和句子之间的抽象中间级别，以保留原始数据的语义信息，同时最大限度地减少事件的稀疏性。我们提出了一种将文本故事数据预处理成事件序列的技术。然后，我们提出一种自动故事生成技术，将问题分解为连续事件（event2event）的生成和事件（event2sentence）中自然语言句子的生成。我们给出了实证结果，比较了不同的事件表征及其对事件后继生成和事件翻译为自然语言的影响。

##### URL
[https://arxiv.org/abs/1706.01331](https://arxiv.org/abs/1706.01331)

##### PDF
[https://arxiv.org/pdf/1706.01331](https://arxiv.org/pdf/1706.01331)

