---
layout: post
title: "Towards Coherent and Cohesive Long-form Text Generation"
date: 2019-05-29 15:56:31
categories: arXiv_CL
tags: arXiv_CL Attention Text_Generation Language_Model
author: Woon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley, Chris Brockett, Mengdi Wang, Jianfeng Gao
mathjax: true
---

* content
{:toc}

##### Abstract
Generating coherent and cohesive long-form texts is a challenging task. Previous works relied on large amounts of human-generated texts to train neural language models. However, few attempted to explicitly improve neural language models from the perspectives of coherence and cohesion. In this work, we propose a new neural language model that is equipped with two neural discriminators which provide feedback signals at the levels of sentence (cohesion) and paragraph (coherence). Our model is trained using a simple yet efficient variant of policy gradient, called negative-critical sequence training, which is proposed to eliminate the need of training a separate critic for estimating baseline. Results demonstrate the effectiveness of our approach, showing improvements over the strong baseline -- recurrent attention-based bidirectional MLE-trained neural language model.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.00511](http://arxiv.org/abs/1811.00511)

##### PDF
[http://arxiv.org/pdf/1811.00511](http://arxiv.org/pdf/1811.00511)

