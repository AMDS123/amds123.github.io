---
layout: post
title: "Adversarial Multi-task Learning for Text Classification"
date: 2017-04-19 14:17:25
categories: arXiv_CL
tags: arXiv_CL Adversarial Knowledge Text_Classification Classification
author: Pengfei Liu, Xipeng Qiu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at \url{this http URL}

##### Abstract (translated by Google)
神经网络模型已经显示了其多任务学习的有前途的机会，其重点在于学习共享层以提取常见的和任务不变的特征。然而，在大多数现有的方法中，提取的共享特征容易受到任务特定的特征或其他任务带来的噪声的污染。在本文中，我们提出了一个对抗性的多任务学习框架，减轻了共享和私有潜在特征空间之间的相互干扰。我们对16个不同的文本分类任务进行了广泛的实验，这证明了我们的方法的好处。此外，我们表明，我们提出的模型学到的共享知识可以被看作是现成的知识，很容易转移到新的任务上。所有16个任务的数据集都可以在\ url {this http URL}

##### URL
[https://arxiv.org/abs/1704.05742](https://arxiv.org/abs/1704.05742)

##### PDF
[https://arxiv.org/pdf/1704.05742](https://arxiv.org/pdf/1704.05742)

