---
layout: post
title: "Bayesian Sparsification of Recurrent Neural Networks"
date: 2017-07-31 21:33:42
categories: arXiv_CL
tags: arXiv_CL Sentiment Sparse RNN Language_Model
author: Ekaterina Lobacheva, Nadezhda Chirkova, Dmitry Vetrov
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights. Recently proposed Sparse Variational Dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality. We apply this technique to sparsify recurrent neural networks. To account for recurrent specifics we also rely on Binary Variational Dropout for RNN. We report 99.5% sparsity level on sentiment analysis task without a quality drop and up to 87% sparsity level on language modeling task with slight loss of accuracy.

##### Abstract (translated by Google)
递归神经网络在许多文本分析任务中显示了最新的结果，但是通常需要大量的内存来存储它们的权重。最近提出的稀疏变分压差消除了前馈神经网络中的大部分权重，而没有显着的质量损失。我们应用这种技术来稀疏周期性神经网络。为了说明经常性的细节，我们还依赖于RNN的二元变分压差（Binary Variational Dropout for RNN）。我们在情感分析任务上报告了99.5％的稀疏度，没有质量下降，在语言建模任务上的稀疏度高达87％，精度略有下降。

##### URL
[https://arxiv.org/abs/1708.00077](https://arxiv.org/abs/1708.00077)

##### PDF
[https://arxiv.org/pdf/1708.00077](https://arxiv.org/pdf/1708.00077)

