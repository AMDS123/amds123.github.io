---
layout: post
title: "Sentiment Analysis on Bangla and Romanized Bangla Text using Deep Recurrent models"
date: 2016-11-24 02:13:05
categories: arXiv_CL
tags: arXiv_CL Sentiment Review RNN
author: A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad
mathjax: true
---

* content
{:toc}

##### Abstract
Sentiment Analysis (SA) is an action research area in the digital age. With rapid and constant growth of online social media sites and services, and the increasing amount of textual data such as - statuses, comments, reviews etc. available in them, application of automatic SA is on the rise. However, most of the research works on SA in natural language processing (NLP) are based on English language. Despite being the sixth most widely spoken language in the world, Bangla still does not have a large and standard dataset. Because of this, recent research works in Bangla have failed to produce results that can be both comparable to works done by others and reusable as stepping stones for future researchers to progress in this field. Therefore, we first tried to provide a textual dataset - that includes not just Bangla, but Romanized Bangla texts as well, is substantial, post-processed and multiple validated, ready to be used in SA experiments. We tested this dataset in Deep Recurrent model, specifically, Long Short Term Memory (LSTM), using two types of loss functions - binary crossentropy and categorical crossentropy, and also did some experimental pre-training by using data from one validation to pre-train the other and vice versa. Lastly, we documented the results along with some analysis on them, which were promising.

##### Abstract (translated by Google)
情感分析（SA）是数字时代的行动研究领域。随着在线社交媒体网站和服务的快速持续增长，以及其中可用的状态，评论，评论等文本数据量的不断增加，自动SA的应用也在不断增加。然而，自然语言处理（NLP）中的大多数关于SA的研究都是基于英语的。尽管是世界上使用最广泛的第六种语言，孟加拉仍然没有一个庞大而标准的数据集。正因为如此，孟加拉最近的研究工作未能产生可与其他人所做的工作相媲美的结果，并可作为未来研究人员在这一领域取得进展的垫脚石。因此，我们首先试图提供一个文本数据集 - 不仅包括孟加拉语，还包括罗马化的孟加拉语文本，这些文本都是实质性的，后处理的和多重验证的，准备用于SA实验。我们在Deep Recurrent模型中测试了这个数据集，特别是长期短期记忆（LSTM），使用了两种类型的损失函数 - 二进制交叉熵和分类交叉熵，并且使用从验证到预训练的数据做了一些实验性的预训练另一个，反之亦然。最后，我们记录了这些结果，并对其进行了一些分析，结果令人满意。

##### URL
[https://arxiv.org/abs/1610.00369](https://arxiv.org/abs/1610.00369)

##### PDF
[https://arxiv.org/pdf/1610.00369](https://arxiv.org/pdf/1610.00369)

