---
layout: post
title: "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks"
date: 2018-11-21 18:58:46
categories: arXiv_AI
tags: arXiv_AI Optimization Classification Deep_Learning Gradient_Descent
author: Difan Zou, Yuan Cao, Dongruo Zhou, Quanquan Gu
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of training deep neural networks with Rectified Linear Unit (ReLU) activiation function using gradient descent and stochastic gradient descent. In particular, we study the binary classification problem and show that for a broad family of loss functions, with proper random weight initialization, both gradient descent and stochastic gradient descent can find the global minima of the training loss for an over-parameterized deep ReLU network, under mild assumption on the training data. The key idea of our proof is that Gaussian random initialization followed by (stochastic) gradient descent produces a sequence of iterates that stay inside a small perturbation region centering around the initial weights, in which the empirical loss function of deep ReLU networks enjoys nice local curvature properties that ensure the global convergence of (stochastic) gradient descent. Our theoretical results shed light on understanding the optimization of deep learning, and pave the way to study the optimization dynamics of training modern deep neural networks.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.08888](http://arxiv.org/abs/1811.08888)

##### PDF
[http://arxiv.org/pdf/1811.08888](http://arxiv.org/pdf/1811.08888)

