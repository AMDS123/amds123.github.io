---
layout: post
title: "Syntax-based Attention Model for Natural Language Inference"
date: 2016-07-22 04:21:54
categories: arXiv_CL
tags: arXiv_CL Attention Inference
author: PengFei Liu, Xipeng Qiu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Introducing attentional mechanism in neural network is a powerful concept, and has achieved impressive results in many natural language processing tasks. However, most of the existing models impose attentional distribution on a flat topology, namely the entire input representation sequence. Clearly, any well-formed sentence has its accompanying syntactic tree structure, which is a much rich topology. Applying attention to such topology not only exploits the underlying syntax, but also makes attention more interpretable. In this paper, we explore this direction in the context of natural language inference. The results demonstrate its efficacy. We also perform extensive qualitative analysis, deriving insights and intuitions of why and how our model works.

##### Abstract (translated by Google)
在神经网络中引入注意力机制是一个强大的概念，在许多自然语言处理任务中取得了令人瞩目的成果。然而，现有的大多数模型都是在平面拓扑上进行注意分配，即整个输入表示序列。显然，任何格式正确的句子都有其伴随的句法树结构，这是一个非常丰富的拓扑结构。应用这种拓扑结构不仅可以利用底层的语法，还可以使注意力更易于理解。在本文中，我们在自然语言推理的背景下探索这个方向。结果证明了它的功效。我们还进行了大量的定性分析，为我们的模型的工作原理和模式提供见解和直觉。

##### URL
[https://arxiv.org/abs/1607.06556](https://arxiv.org/abs/1607.06556)

##### PDF
[https://arxiv.org/pdf/1607.06556](https://arxiv.org/pdf/1607.06556)

