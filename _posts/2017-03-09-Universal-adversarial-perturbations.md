---
layout: post
title: "Universal adversarial perturbations"
date: 2017-03-09 17:01:25
categories: arXiv_CV
tags: arXiv_CV Adversarial Relation
author: Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard
mathjax: true
---

* content
{:toc}

##### Abstract
Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.

##### Abstract (translated by Google)
给定一个最先进的深度神经网络分类器，我们展示了一个普遍的（图像不可知的）和非常小的扰动矢量的存在，导致自然图像被错误分类的概率很高。我们提出了一个计算全局扰动的系统算法，并表明最先进的深度神经网络非常容易受到这种扰动的影响，尽管它是人眼不可察觉的。我们进一步经验地分析这些普遍的扰动，并且特别地显示它们在神经网络中非常好地推广。通用扰动的惊人存在揭示了分类器的高维决策边界之间的重要几何关系。它进一步概述了潜在的安全漏洞，在输入空间中存在单一方向，对手可能利用这些方向来破坏大多数自然图像上的分类器。

##### URL
[https://arxiv.org/abs/1610.08401](https://arxiv.org/abs/1610.08401)

##### PDF
[https://arxiv.org/pdf/1610.08401](https://arxiv.org/pdf/1610.08401)

