---
layout: post
title: "Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts"
date: 2017-12-04 21:40:10
categories: arXiv_CV
tags: arXiv_CV Regularization Adversarial GAN Classification
author: Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Ahmed Elgammal
mathjax: true
---

* content
{:toc}

##### Abstract
Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.

##### Abstract (translated by Google)
大多数现有的零点学习方法都把这个问题看作是一个视觉语义嵌入问题。考虑到生成敌对网络（GAN）生成图像的能力，我们改为利用GAN来想象文本描述中看不见的类别，从而识别没有任何示例的新类。具体来说，我们提出了一个简单而有效的生成模型，输入有关看不见的类（如维基百科文章）的噪音文本描述，并为这个类生成合成的视觉特征。通过添加伪数据，零点学习自然地转换为传统的分类问题。此外，为了保持产生的特征的类间歧视，提出了视觉枢轴正则化作为明确的监督。与以前使用复杂工程正则化器的方法不同，我们的方法可以很好地抑制噪声，无需额外的正则化。从经验上讲，我们表明，我们的方法在基于文本的零点学习的最大可用基准上始终优于最先进的技术。

##### URL
[http://arxiv.org/abs/1712.01381](http://arxiv.org/abs/1712.01381)

##### PDF
[http://arxiv.org/pdf/1712.01381](http://arxiv.org/pdf/1712.01381)

