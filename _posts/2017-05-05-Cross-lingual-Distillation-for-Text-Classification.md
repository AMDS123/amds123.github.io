---
layout: post
title: "Cross-lingual Distillation for Text Classification"
date: 2017-05-05 03:36:11
categories: arXiv_CL
tags: arXiv_CL Adversarial Text_Classification Classification Prediction
author: Ruochen Xu, Yiming Yang
mathjax: true
---

* content
{:toc}

##### Abstract
Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.

##### Abstract (translated by Google)
跨语言文本分类（CLTC）是将用不同语言编写的文档分类到相同的分类分类中的任务。本文提出了一种新型的CLTC方法，建立在模型蒸馏的基础上，适应并扩展了最初提出的用于模型压缩的框架。对标签丰富的语言中的文档使用软概率预测作为文档平行语料库中的（诱导的）监督标签，我们成功地训练了分类器，用于标记的训练数据不可用的新语言。在模型训练期间也应用对抗特征适应技术以减少分配不匹配。我们对两个基准CLTC数据集进行了实验，将英语作为源语言，将德语，法语，日语和中文作为未标记的目标语言。所提出的方法具有其他现有技术方法的有利或可比较的性能。

##### URL
[https://arxiv.org/abs/1705.02073](https://arxiv.org/abs/1705.02073)

##### PDF
[https://arxiv.org/pdf/1705.02073](https://arxiv.org/pdf/1705.02073)

