---
layout: post
title: "Factorized Bilinear Models for Image Recognition"
date: 2017-09-04 08:14:01
categories: arXiv_CV
tags: arXiv_CV CNN Recognition
author: Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou
mathjax: true
---

* content
{:toc}

##### Abstract
Although Deep Convolutional Neural Networks (CNNs) have liberated their power in various computer vision tasks, the most important components of CNN, convolutional layers and fully connected layers, are still limited to linear transformations. In this paper, we propose a novel Factorized Bilinear (FB) layer to model the pairwise feature interactions by considering the quadratic terms in the transformations. Compared with existing methods that tried to incorporate complex non-linearity structures into CNNs, the factorized parameterization makes our FB layer only require a linear increase of parameters and affordable computational cost. To further reduce the risk of overfitting of the FB layer, a specific remedy called DropFactor is devised during the training process. We also analyze the connection between FB layer and some existing models, and show FB layer is a generalization to them. Finally, we validate the effectiveness of FB layer on several widely adopted datasets including CIFAR-10, CIFAR-100 and ImageNet, and demonstrate superior results compared with various state-of-the-art deep models.

##### Abstract (translated by Google)
尽管深度卷积神经网络（CNN）已经在各种计算机视觉任务中解放了他们的力量，但CNN的最重要的组成部分，卷积层和完全连接的层仍然局限于线性变换。在本文中，我们提出了一个新的分解双线性（FB）层来模拟成对的特征相互作用，通过考虑转换中的二次项。与现有的试图将复杂的非线性结构纳入CNN的方法相比，分解参数化使得我们的FB层只需要参数的线性增加和可承受的计算成本。为了进一步降低FB层过度拟合的风险，在训练过程中设计了一个名为DropFactor的特殊补救方法。我们还分析了FB层与现有模型之间的联系，并展示FB层是对它们的推广。最后，我们验证了FB层在包括CIFAR-10，CIFAR-100和ImageNet在内的几个广泛采用的数据集上的有效性，并且与各种最先进的深度模型相比，证明了优越的结果。

##### URL
[https://arxiv.org/abs/1611.05709](https://arxiv.org/abs/1611.05709)

##### PDF
[https://arxiv.org/pdf/1611.05709](https://arxiv.org/pdf/1611.05709)

