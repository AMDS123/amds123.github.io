---
layout: post
title: "Selective Encoding for Abstractive Sentence Summarization"
date: 2017-04-24 07:57:37
categories: arXiv_CL
tags: arXiv_CL Attention Summarization RNN
author: Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.

##### Abstract (translated by Google)
我们提出了一个选择性编码模型来扩展抽象句汇总的序列框架。它由一个句子编码器，一个选择性的门网络和一个配备有注意力的解码器组成。句子编码器和解码器是用递归神经网络构建的。选择性门控网络通过控制从编码器到解码器的信息流来构造第二级句子表示。第二级表示是针对句子摘要任务量身定做的，从而导致更好的表现。我们在英文Gigaword，DUC 2004和MSR抽象句总结数据集上评估我们的模型。实验结果表明，所提出的选择性编码模型优于现有的基线模型。

##### URL
[https://arxiv.org/abs/1704.07073](https://arxiv.org/abs/1704.07073)

##### PDF
[https://arxiv.org/pdf/1704.07073](https://arxiv.org/pdf/1704.07073)

