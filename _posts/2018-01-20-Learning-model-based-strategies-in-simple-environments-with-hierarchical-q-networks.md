---
layout: post
title: "Learning model-based strategies in simple environments with hierarchical q-networks"
date: 2018-01-20 15:31:35
categories: arXiv_AI
tags: arXiv_AI GAN Reinforcement_Learning Deep_Learning
author: Necati Alp Muyesser, Kyle Dunovan, Timothy Verstynen
mathjax: true
---

* content
{:toc}

##### Abstract
Recent advances in deep learning have allowed artificial agents to rival human-level performance on a wide range of complex tasks; however, the ability of these networks to learn generalizable strategies remains a pressing challenge. This critical limitation is due in part to two factors: the opaque information representation in deep neural networks and the complexity of the task environments in which they are typically deployed. Here we propose a novel Hierarchical Q-Network (HQN) motivated by theories of the hierarchical organization of the human prefrontal cortex, that attempts to identify lower dimensional patterns in the value landscape that can be exploited to construct an internal model of rules in simple environments. We draw on combinatorial games, where there exists a single optimal strategy for winning that generalizes across other features of the game, to probe the strategy generalization of the HQN and other reinforcement learning (RL) agents using variations of Wythoff's game. Traditional RL approaches failed to reach satisfactory performance on variants of Wythoff's Game; however, the HQN learned heuristic-like strategies that generalized across changes in board configuration. More importantly, the HQN allowed for transparent inspection of the agent's internal model of the game following training. Our results show how a biologically inspired hierarchical learner can facilitate learning abstract rules to promote robust and flexible action policies in simplified training environments with clearly delineated optimal strategies.

##### Abstract (translated by Google)
深度学习方面的最新进展使得人造代理人可以在广泛的复杂任务上与人类表现相媲美。然而，这些网络学习普遍化策略的能力仍然是一个紧迫的挑战。这种严重的局限性部分是由于两个因素：深层神经网络中的不透明信息表示以及它们通常部署的任务环境的复杂性。在这里，我们提出了一个新的分层Q网络（HQN）的动机，由人类前额叶皮层的分层组织理论，试图识别价值景观中的低维模式，可以利用在简单的环境中构建规则的内部模型。我们利用组合博弈，其中存在一个单一的最优策略，通过游戏的其他特征进行概括，利用Wythoff游戏的变化来探索HQN和其他强化学习（RL）代理的策略泛化。传统的RL方法未能在Wythoff游戏的变体上达到令人满意的性能;然而，HQN学习了启发式的策略，通过改变董事会配置。更重要的是，HQN允许在训练之后对代理人的内部模型进行透明的检查。我们的研究结果表明，生物学启发的等级学习者如何能够在简化的培训环境中促进学习抽象规则，以促进健全和灵活的行动政策，并明确界定最佳策略。

##### URL
[https://arxiv.org/abs/1801.06689](https://arxiv.org/abs/1801.06689)

##### PDF
[https://arxiv.org/pdf/1801.06689](https://arxiv.org/pdf/1801.06689)

