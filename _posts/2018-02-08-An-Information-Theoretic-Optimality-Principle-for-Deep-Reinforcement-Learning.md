---
layout: post
title: "An Information-Theoretic Optimality Principle for Deep Reinforcement Learning"
date: 2018-02-08 14:07:53
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Felix Leibfried, Jordi Grau-Moya, Haitham Bou-Ammar
mathjax: true
---

* content
{:toc}

##### Abstract
We methodologically address the problem of Q-value overestimation in deep reinforcement learning to handle high-dimensional state spaces efficiently. By adapting concepts from information theory, we introduce an intrinsic penalty signal encouraging reduced Q-value estimates. The resultant algorithm encompasses a wide range of learning outcomes containing deep Q-networks as a special case. Different learning outcomes can be demonstrated by tuning a Lagrange multiplier accordingly. We furthermore propose a novel scheduling scheme for this Lagrange multiplier to ensure efficient and robust learning. In experiments on Atari games, our algorithm outperforms other algorithms (e.g. deep and double deep Q-networks) in terms of both game-play performance and sample complexity.

##### Abstract (translated by Google)
我们从方法上解决深度强化学习中Q值高估的问题，以有效处理高维状态空间。通过适应信息论的概念，我们引入一个内在的惩罚信号，鼓励减少的Q值估计。由此产生的算法包含了广泛的包含深度Q网络的学习成果作为特例。不同的学习成果可以通过相应地调整一个拉格朗日乘数来证明。我们还提出了一个新的拉格朗日乘子的调度方案，以确保有效和鲁棒的学习。在Atari游戏的实验中，我们的算法在游戏性能和样本复杂性方面优于其他算法（例如深度和双深度的Q网络）。

##### URL
[http://arxiv.org/abs/1708.01867](http://arxiv.org/abs/1708.01867)

##### PDF
[http://arxiv.org/pdf/1708.01867](http://arxiv.org/pdf/1708.01867)

