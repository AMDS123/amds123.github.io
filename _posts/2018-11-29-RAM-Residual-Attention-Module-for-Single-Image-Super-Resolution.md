---
layout: post
title: "RAM: Residual Attention Module for Single Image Super-Resolution"
date: 2018-11-29 09:59:31
categories: arXiv_CV
tags: arXiv_CV Super_Resolution Attention
author: Jun-Hyuk Kim, Jun-Ho Choi, Manri Cheon, Jong-Seok Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Attention mechanisms are a design trend of deep neural networks that stands out in various computer vision tasks. Recently, some works have attempted to apply attention mechanisms to single image super-resolution (SR) tasks. However, they apply the mechanisms to SR in the same or similar ways used for high-level computer vision problems without much consideration of the different nature between SR and other problems. In this paper, we propose a new attention method, which is composed of new channel-wise and spatial attention mechanisms optimized for SR and a new fused attention to combine them. Based on this, we propose a new residual attention module (RAM) and a SR network using RAM (SRRAM). We provide in-depth experimental analysis of different attention mechanisms in SR. It is shown that the proposed method can construct both deep and lightweight SR networks showing improved performance in comparison to existing state-of-the-art methods.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.12043](http://arxiv.org/abs/1811.12043)

##### PDF
[http://arxiv.org/pdf/1811.12043](http://arxiv.org/pdf/1811.12043)

