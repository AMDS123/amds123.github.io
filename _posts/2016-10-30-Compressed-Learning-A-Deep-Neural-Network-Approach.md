---
layout: post
title: "Compressed Learning: A Deep Neural Network Approach"
date: 2016-10-30 07:54:19
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Inference Classification Deep_Learning
author: Amir Adler, Michael Elad, Michael Zibulevsky
mathjax: true
---

* content
{:toc}

##### Abstract
Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46% compared to 41.06% with state-of-the-art.

##### Abstract (translated by Google)
压缩学习（CL）是信号推理的联合信号处理和机器学习框架，使用通过信号的线性投影获得的少量测量。在本文中，我们提出了一个端到端深度学习的方法，其中由完全连接的层组成的网络，然后是卷积层，执行线性传感和非线性推理阶段。在训练阶段，感知矩阵和非线性推理算子是联合优化的，所提出的方法在图像分类任务方面优于现有技术。例如，在检测率为1％（28×28像素的图像只有8个测量值）的情况下，MNIST手写数字数据集的分类错误率为6.46％，而最先进的是41.06％。

##### URL
[https://arxiv.org/abs/1610.09615](https://arxiv.org/abs/1610.09615)

##### PDF
[https://arxiv.org/pdf/1610.09615](https://arxiv.org/pdf/1610.09615)

