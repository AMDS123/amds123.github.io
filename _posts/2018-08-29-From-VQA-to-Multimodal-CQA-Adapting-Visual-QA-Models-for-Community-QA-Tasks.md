---
layout: post
title: "From VQA to Multimodal CQA: Adapting Visual QA Models for Community QA Tasks"
date: 2018-08-29 05:53:17
categories: arXiv_AI
tags: arXiv_AI Knowledge QA Classification VQA
author: Avikalp Srivastava, Hsin Wen Liu, Sumio Fujita
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we present novel methods to adapt visual QA models for community QA tasks of practical significance - automated question category classification and finding experts for question answering - on questions containing both text and image. To the best of our knowledge, this is the first work to tackle the multimodality challenge in CQA, and is an enabling step towards basic question-answering on image-based CQA. First, we analyze the differences between visual QA and community QA datasets, discussing the limitations of applying VQA models directly to CQA tasks, and then we propose novel augmentations to VQA-based models to best address those limitations. Our model, with the augmentations of an image-text combination method tailored for CQA and use of auxiliary tasks for learning better grounding features, significantly outperforms the text-only and VQA model baselines for both tasks on real-world CQA data from Yahoo! Chiebukuro, a Japanese counterpart of Yahoo! Answers.

##### Abstract (translated by Google)
在这项工作中，我们提出了新的方法，以适应具有实际意义的社区QA任务的视觉QA模型 - 自动问题类别分类和寻找问答的专家 - 包含文本和图像的问题。据我们所知，这是解决CQA中多模式挑战的第一项工作，也是实现基于图像的CQA基本问答的一个有利步骤。首先，我们分析了视觉质量保证和社区质量保证数据集之间的差异，讨论了将VQA模型直接应用于CQA任务的局限性，然后我们提出了对基于VQA的模型的新颖增强，以最好地解决这些限制。我们的模型，增加了为CQA量身定制的图像 - 文本组合方法以及使用辅助任务来学习更好的接地功能，显着优于Yahoo!的真实世界CQA数据上的任务的纯文本和VQA模型基线。日本雅虎的Chiebukuro！解答。

##### URL
[http://arxiv.org/abs/1808.09648](http://arxiv.org/abs/1808.09648)

##### PDF
[http://arxiv.org/pdf/1808.09648](http://arxiv.org/pdf/1808.09648)

