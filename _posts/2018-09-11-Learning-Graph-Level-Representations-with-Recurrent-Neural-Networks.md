---
layout: post
title: "Learning Graph-Level Representations with Recurrent Neural Networks"
date: 2018-09-11 18:19:09
categories: arXiv_AI
tags: arXiv_AI Embedding RNN Classification
author: Yu Jin, Joseph F. JaJa
mathjax: true
---

* content
{:toc}

##### Abstract
Recently a variety of methods have been developed to encode graphs into low-dimensional vectors that can be easily exploited by machine learning algorithms. The majority of these methods start by embedding the graph nodes into a low-dimensional vector space, followed by using some scheme to aggregate the node embeddings. In this work, we develop a new approach to learn graph-level representations, which includes a combination of unsupervised and supervised learning components. We start by learning a set of node representations in an unsupervised fashion. Graph nodes are mapped into node sequences sampled from random walk approaches approximated by the Gumbel-Softmax distribution. Recurrent neural network (RNN) units are modified to accommodate both the node representations as well as their neighborhood information. Experiments on standard graph classification benchmarks demonstrate that our proposed approach achieves superior or comparable performance relative to the state-of-the-art algorithms in terms of convergence speed and classification accuracy. We further illustrate the effectiveness of the different components used by our approach.

##### Abstract (translated by Google)
最近，已经开发了各种方法来将图形编码成可以通过机器学习算法容易地利用的低维矢量。这些方法中的大多数首先将图节点嵌入到低维向量空间中，然后使用一些方案来聚合节点嵌入。在这项工作中，我们开发了一种新的方法来学习图形级表示，其中包括无监督和监督学习组件的组合。我们首先以无人监督的方式学习一组节点表示。图节点被映射到从Gumbel-Softmax分布近似的随机游走方法中采样的节点序列。修改递归神经网络（RNN）单元以适应节点表示以及它们的邻域信息。标准图分类基准测试的实验表明，我们提出的方法在收敛速度和分类精度方面相对于最先进的算法实现了优越或相当的性能。我们进一步说明了我们的方法所使用的不同组件的有效性。

##### URL
[http://arxiv.org/abs/1805.07683](http://arxiv.org/abs/1805.07683)

##### PDF
[http://arxiv.org/pdf/1805.07683](http://arxiv.org/pdf/1805.07683)

