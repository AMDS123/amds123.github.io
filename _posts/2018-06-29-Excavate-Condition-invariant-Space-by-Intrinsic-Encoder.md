---
layout: post
title: "Excavate Condition-invariant Space by Intrinsic Encoder"
date: 2018-06-29 08:58:54
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Recognition
author: Jian Xu
mathjax: true
---

* content
{:toc}

##### Abstract
As the human, we can recognize the places across a wide range of changing environmental conditions such as those caused by weathers, seasons, and day-night cycles. We can excavate and memorize the stable semantic structure of different places and scenes. For example, we can recognize tree whether the bare tree in winter or lush tree in summer. Therefore, the intrinsic features that are corresponding to specific semantic contents and condition-invariant of appearance changes can improve the performance of long-term place recognition significantly. 
 In this paper, we propose a novel intrinsic encoder that excavates the condition-invariant latent space of different places under drastic appearance changes. Our method excavates the space of intrinsic structure features by self-supervised cycle loss designed based on Generative Adversarial Network (GAN). Different from previous learning based place recognition methods that need paired training data of each place with appearance changes, we employ the weakly-supervised strategy to utilize unpaired set-based training data of different environmental conditions. 
 We conduct comprehensive experiments on the standard datasets and show that our semi-supervised intrinsic encoder achieves excellent performance for place recognition under drastic appearance changes.

##### Abstract (translated by Google)
作为人类，我们可以识别各种不断变化的环境条件下的地方，例如由天气，季节和昼夜循环引起的环境条件。我们可以挖掘和记忆不同地点和场景的稳定语义结构。例如，我们可以识别树木是冬天的光秃秃的树木还是夏天的郁郁葱葱的树木。因此，与特定语义内容相对应的内在特征和外观变化的条件不变可以显着提高长期地点识别的性能。
 在本文中，我们提出了一种新的内在编码器，它在剧烈的外观变化下挖掘不同地方的条件不变的潜在空间。我们的方法通过基于生成对抗网络（GAN）设计的自监督循环损失挖掘内在结构特征的空间。与以往基于学习的场所识别方法不同，这些方法需要每个场所的配对训练数据和外观变化，我们采用弱监督策略来利用不同环境条件下的不成对的基于集合的训练数据。
 我们对标准数据集进行了全面的实验，结果表明，我们的半监督内在编码器在剧烈的外观变化下实现了优异的位置识别性能。

##### URL
[http://arxiv.org/abs/1806.11306](http://arxiv.org/abs/1806.11306)

##### PDF
[http://arxiv.org/pdf/1806.11306](http://arxiv.org/pdf/1806.11306)

