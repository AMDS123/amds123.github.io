---
layout: post
title: "Direct Output Connection for a High-Rank Language Model"
date: 2018-08-30 07:03:51
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Sho Takase, Jun Suzuki, Masaaki Nagata
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: https://github.com/nttcslab- nlp/doc_lm.

##### Abstract (translated by Google)
本文提出了一种最先进的递归神经网络（RNN）语言模型，该模型结合了不仅从最终RNN层而且从中间层计算的概率分布。我们提出的方法基于Yang等人引入的语言建模的矩阵分解解释，提高了语言模型的表达能力。 （2018）。所提出的方法改进了当前最先进的语言模型，并在Penn Treebank和WikiText-2上获得了最佳分数，这是标准的基准数据集。此外，我们指出我们提出的方法有助于两个应用任务：机器翻译和标题生成。我们的代码可在以下网址公开获取：https：//github.com/nttcslab-nlp / doc_lm。

##### URL
[http://arxiv.org/abs/1808.10143](http://arxiv.org/abs/1808.10143)

##### PDF
[http://arxiv.org/pdf/1808.10143](http://arxiv.org/pdf/1808.10143)

