---
layout: post
title: "LSTM vs. GRU vs. Bidirectional RNN for script generation"
date: 2019-08-12 18:39:10
categories: arXiv_CL
tags: arXiv_CL RNN Deep_Learning
author: Sanidhya Mangal, Poorva Joshi, Rahul Modak
mathjax: true
---

* content
{:toc}

##### Abstract
Scripts are an important part of any TV series. They narrate movements, actions and expressions of characters. In this paper, a case study is presented on how different sequence to sequence deep learning models perform in the task of generating new conversations between characters as well as new scenarios on the basis of a script (previous conversations). A comprehensive comparison between these models, namely, LSTM, GRU and Bidirectional RNN is presented. All the models are designed to learn the sequence of recurring characters from the input sequence. Each input sequence will contain, say "n" characters, and the corresponding targets will contain the same number of characters, except, they will be shifted one character to the right. In this manner, input and output sequences are generated and used to train the models. A closer analysis of explored models performance and efficiency is delineated with the help of graph plots and generated texts by taking some input string. These graphs describe both, intraneural performance and interneural model performance for each model.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.04332](http://arxiv.org/abs/1908.04332)

##### PDF
[http://arxiv.org/pdf/1908.04332](http://arxiv.org/pdf/1908.04332)

