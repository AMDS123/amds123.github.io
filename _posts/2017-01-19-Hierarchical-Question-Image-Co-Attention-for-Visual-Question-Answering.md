---
layout: post
title: "Hierarchical Question-Image Co-Attention for Visual Question Answering"
date: 2017-01-19 05:03:33
categories: arXiv_CV
tags: arXiv_CV CNN VQA
author: Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh
mathjax: true
---

* content
{:toc}

##### Abstract
A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling "where to look" or visual attention, it is equally important to model "what words to listen to" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.

##### Abstract (translated by Google)
最近的一些作品已经提出了视觉问题解答（VQA）的注意模型，这些模型生成的空间地图突出显示与回答问题相关的图像区域。在本文中，我们认为，除了建模“看什么”或视觉注意力之外，“听什么单词”或提问注意力的模型也同样重要。我们提出了一个新的VQA的共同关注模型，共同的形象和问题关注的原因。此外，我们的模型通过一种新颖的一维卷积神经网络（CNN）以分层的方式来解决这个问题的原因（以及因此通过共同关注机制的图像）。我们的模型在COCO-QA数据集上将VQA数据集的最新状态从60.3％提高到了60.5％，从61.6％提高到了63.3％。通过使用ResNet，性能进一步提高到VQA的62.1％和COCO-QA的65.4％。

##### URL
[https://arxiv.org/abs/1606.00061](https://arxiv.org/abs/1606.00061)

##### PDF
[https://arxiv.org/pdf/1606.00061](https://arxiv.org/pdf/1606.00061)

