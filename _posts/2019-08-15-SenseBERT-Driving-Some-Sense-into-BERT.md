---
layout: post
title: "SenseBERT: Driving Some Sense into BERT"
date: 2019-08-15 17:20:20
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, Yoav Shoham
mathjax: true
---

* content
{:toc}

##### Abstract
Self-supervision techniques have allowed neural language models to advance the frontier in Natural Language Understanding. However, existing self-supervision techniques operate at the word-form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ self-supervision directly at the word-sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a lexical-semantic level language model, without the use of human annotation. SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval, and by attaining a state of the art result on the Word in Context (WiC) task. Our approach is extendable to other linguistic signals, which can be similarly integrated into the pre-training process, leading to increasingly semantically informed language models.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.05646](http://arxiv.org/abs/1908.05646)

##### PDF
[http://arxiv.org/pdf/1908.05646](http://arxiv.org/pdf/1908.05646)

