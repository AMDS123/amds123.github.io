---
layout: post
title: "Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"
date: 2017-08-25 07:05:49
categories: arXiv_CV
tags: arXiv_CV Action_Recognition CNN Recognition
author: Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural networks with spatio-temporal 3D kernels (3D CNNs) have an ability to directly extract spatio-temporal features from videos for action recognition. Although the 3D kernels tend to overfit because of a large number of their parameters, the 3D CNNs are greatly improved by using recent huge video databases. However, the architecture of 3D CNNs is relatively shallow against to the success of very deep neural networks in 2D-based CNNs, such as residual networks (ResNets). In this paper, we propose a 3D CNNs based on ResNets toward a better action representation. We describe the training procedure of our 3D ResNets in details. We experimentally evaluate the 3D ResNets on the ActivityNet and Kinetics datasets. The 3D ResNets trained on the Kinetics did not suffer from overfitting despite the large number of parameters of the model, and achieved better performance than relatively shallow networks, such as C3D. Our code and pretrained models (e.g. Kinetics and ActivityNet) are publicly available at this https URL

##### Abstract (translated by Google)
具有时空三维内核的卷积神经网络（3D CNN）具有直接从视频中提取时空特征以用于动作识别的能力。尽管3D内核由于其大量参数而倾向于过度配置，但是通过使用最近的巨大视频数据库，3D CNN得到了极大的改善。然而，3D CNN的体系结构相对于基于二维的CNN（如残余网络（ResNets））中非常深的神经网络的成功而言是相对较浅的。在本文中，我们提出了一个基于ResNets的3D CNN，以更好的行动表示。我们详细介绍了我们的3D ResNets的培训程序。我们通过实验评估ActivityNet和Kinetics数据集上的3D ResNets。在动力学上训练的3D ResNets尽管有大量的模型参数，但并没有受到过拟合的影响，并且取得了比C3D等相对较浅的网络更好的性能。我们的代码和预训练模型（例如Kinetics和ActivityNet）可在此https URL上公开获得

##### URL
[https://arxiv.org/abs/1708.07632](https://arxiv.org/abs/1708.07632)

##### PDF
[https://arxiv.org/pdf/1708.07632](https://arxiv.org/pdf/1708.07632)

