---
layout: post
title: "Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples"
date: 2017-08-18 03:01:35
categories: arXiv_CV
tags: arXiv_CV Adversarial Optimization Prediction
author: Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks (DNNs) have demonstrated impressive performance on a wide array of tasks, but they are usually considered opaque since internal structure and learned parameters are not interpretable. In this paper, we re-examine the internal representations of DNNs using adversarial images, which are generated by an ensemble-optimization algorithm. We find that: (1) the neurons in DNNs do not truly detect semantic objects/parts, but respond to objects/parts only as recurrent discriminative patches; (2) deep visual representations are not robust distributed codes of visual concepts because the representations of adversarial images are largely not consistent with those of real images, although they have similar visual appearance, both of which are different from previous findings. To further improve the interpretability of DNNs, we propose an adversarial training scheme with a consistent loss such that the neurons are endowed with human-interpretable concepts. The induced interpretable representations enable us to trace eventual outcomes back to influential neurons. Therefore, human users can know how the models make predictions, as well as when and why they make errors.

##### Abstract (translated by Google)
深度神经网络（DNN）已经在各种任务中表现出令人印象深刻的性能，但是由于内部结构和学习参数是不可解释的，所以它们通常被认为是不透明的。在本文中，我们重新审视DNN的内部表示，使用敌对图像，由集合优化算法生成。我们发现：（1）DNN中的神经元没有真正检测到语义对象/部分，而只是将对象/部分作为反复出现的判别性修补; （2）深度视觉表征不是视觉概念的鲁棒分布式编码，因为对抗性图像的表征虽然具有相似的视觉外观，但与以前的研究结果不同，它们在很大程度上与实际图像的表现不一致。为了进一步提高DNN的可解释性，我们提出了一个持续的损失的对抗性训练方案，以使神经元具有人类可解释的概念。诱导的解释表示使我们能够追踪最终的结果回到有影响力的神经元。因此，用户可以知道这些模型如何进行预测，以及何时和为何出现错误。

##### URL
[https://arxiv.org/abs/1708.05493](https://arxiv.org/abs/1708.05493)

##### PDF
[https://arxiv.org/pdf/1708.05493](https://arxiv.org/pdf/1708.05493)

