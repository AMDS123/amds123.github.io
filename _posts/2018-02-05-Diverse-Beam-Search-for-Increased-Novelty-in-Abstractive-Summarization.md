---
layout: post
title: "Diverse Beam Search for Increased Novelty in Abstractive Summarization"
date: 2018-02-05 15:17:01
categories: arXiv_CL
tags: arXiv_CL Summarization
author: Andr&#xe9; Cibils, Claudiu Musat, Andreea Hossman, Michael Baeriswyl
mathjax: true
---

* content
{:toc}

##### Abstract
Text summarization condenses a text to a shorter version while retaining the important informations. Abstractive summarization is a recent development that generates new phrases, rather than simply copying or rephrasing sentences within the original text. Recently neural sequence-to-sequence models have achieved good results in the field of abstractive summarization, which opens new possibilities and applications for industrial purposes. However, most practitioners observe that these models still use large parts of the original text in the output summaries, making them often similar to extractive frameworks. To address this drawback, we first introduce a new metric to measure how much of a summary is extracted from the input text. Secondly, we present a novel method, that relies on a diversity factor in computing the neural network loss, to improve the diversity of the summaries generated by any neural abstractive model implementing beam search. Finally, we show that this method not only makes the system less extractive, but also improves the overall rouge score of state-of-the-art methods by at least 2 points.

##### Abstract (translated by Google)
文本摘要将文本浓缩为较短的版本，同时保留重要的信息。抽象概括是最近发展起来的一种新的短语，而不是简单地在原文中复制或改写句子。最近，神经序列 - 序列模型在抽象概括领域取得了良好的结果，为工业目的开辟了新的可能性和应用。然而，大多数从业人员观察到，这些模型在输出摘要中仍然使用大部分原始文本，使得它们经常与抽取框架类似。为了解决这个缺点，我们首先引入了一个新的度量标准来衡量从输入文本中提取了多少摘要。其次，我们提出了一种新的方法，它依赖于多样性因素来计算神经网络损失，以改善任何实施波束搜索的神经抽象模型生成的摘要的多样性。最后，我们证明这种方法不仅使得系统的提取效率更低，而且使得最先进的方法的整体胭脂评分至少提高了2分。

##### URL
[http://arxiv.org/abs/1802.01457](http://arxiv.org/abs/1802.01457)

##### PDF
[http://arxiv.org/pdf/1802.01457](http://arxiv.org/pdf/1802.01457)

