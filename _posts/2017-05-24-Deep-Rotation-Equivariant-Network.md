---
layout: post
title: "Deep Rotation Equivariant Network"
date: 2017-05-24 06:22:09
categories: arXiv_CV
tags: arXiv_CV Attention
author: Junying Li, Zichen Yang, Haifeng Liu, Deng Cai
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, learning equivariant representations has attracted considerable research attention. Dieleman et al. introduce four operations which can be inserted to CNN to learn deep representations equivariant to rotation. However, feature maps should be copied and rotated four times in each layer in their approach, which causes much running time and memory overhead. In order to address this problem, we propose Deep Rotation Equivariant Network(DREN) consisting of cycle layers, isotonic layers and decycle layers.Our proposed layers apply rotation transformation on filters rather than feature maps, achieving a speed up of more than 2 times with even less memory overhead. We evaluate DRENs on Rotated MNIST and CIFAR-10 datasets and demonstrate that it can improve the performance of state-of-the-art architectures. Our codes are released on GitHub.

##### Abstract (translated by Google)
最近，学习equivariant交涉已经吸引了相当多的研究注意力。 Dieleman等人引入四个可以插入CNN的操作来学习等价于旋转的深度表示。然而，特征映射应该在它们的方法中在每一层被复制和旋转四次，这导致很多运行时间和内存开销。为了解决这个问题，我们提出了由循环层，等渗层和循环层组成的深度旋转等变网络（DREN）。我们提出的层在滤波器上应用旋转变换而不是在特征映射上，加速了2倍以上甚至更少的内存开销。我们评估旋转MNIST和CIFAR-10数据集上的DREN，并证明它可以提高最先进的体系结构的性能。我们的代码在GitHub上发布。

##### URL
[https://arxiv.org/abs/1705.08623](https://arxiv.org/abs/1705.08623)

##### PDF
[https://arxiv.org/pdf/1705.08623](https://arxiv.org/pdf/1705.08623)

