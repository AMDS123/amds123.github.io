---
layout: post
title: "Imagination improves Multimodal Translation"
date: 2017-07-07 09:18:55
categories: arXiv_CL
tags: arXiv_CL Image_Caption Attention Prediction
author: Desmond Elliott, Ákos Kádár
mathjax: true
---

* content
{:toc}

##### Abstract
We decompose multimodal translation into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.

##### Abstract (translated by Google)
我们将多模式翻译分解为两个子任务：学习翻译和学习视觉基础的表示。在多任务学习框架中，在基于注意力的编码器 - 解码器中学习翻译，并且通过图像表示预测学习地面表示。与Multi30K数据集的最新技术相比，我们的方法提高了翻译性能。此外，如果我们在外部MS COCO数据集上训练图像预测任务，同样有效，如果我们在外部新闻评论平行文本上训练翻​​译模型，我们会发现有改进。

##### URL
[https://arxiv.org/abs/1705.04350](https://arxiv.org/abs/1705.04350)

##### PDF
[https://arxiv.org/pdf/1705.04350](https://arxiv.org/pdf/1705.04350)

