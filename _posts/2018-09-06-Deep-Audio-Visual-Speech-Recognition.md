---
layout: post
title: "Deep Audio-Visual Speech Recognition"
date: 2018-09-06 17:34:27
categories: arXiv_CV
tags: arXiv_CV Attention Face Speech_Recognition Recognition
author: Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman
mathjax: true
---

* content
{:toc}

##### Abstract
The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) we compare two models for lip reading, one using a CTC loss, and the other using a sequence-to-sequence loss. Both models are built on top of the transformer self-attention architecture; (2) we investigate to what extent lip reading is complementary to audio speech recognition, especially when the audio signal is noisy; (3) we introduce and publicly release a new dataset for audio-visual speech recognition, LRS2-BBC, consisting of thousands of natural sentences from British television. The models that we train surpass the performance of all previous work on a lip reading benchmark dataset by a significant margin.

##### Abstract (translated by Google)
这项工作的目标是识别说话的面部所说的短语和句子，有或没有音频。与之前专注于识别有限数量的单词或短语的作品不同，我们将唇读作为一个开放世界的问题 - 无约束的自然语言句子，以及野外视频。我们的主要贡献是：（1）我们比较两种唇读模型，一种使用CTC损失，另一种使用序列到序列丢失。两种型号都建立在变压器自注意架构之上; （2）我们研究唇读与音频语音识别的互补程度，特别是当音频信号有噪声时; （3）我们引入并公开发布了一个新的视听语音识别数据集LRS2-BBC，由英国电视台的数千个自然句子组成。我们训练的模型超过了唇读基准数据集上所有先前工作的表现。

##### URL
[http://arxiv.org/abs/1809.02108](http://arxiv.org/abs/1809.02108)

##### PDF
[http://arxiv.org/pdf/1809.02108](http://arxiv.org/pdf/1809.02108)

