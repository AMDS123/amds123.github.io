---
layout: post
title: "Triplet-based Deep Similarity Learning for Person Re-Identification"
date: 2018-02-09 13:40:59
categories: arXiv_CV
tags: arXiv_CV Re-identification Attention Person_Re-identification CNN
author: Wentong Liao, Michael Ying Yang, Ni Zhan, Bodo Rosenhahn
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, person re-identification (re-id) catches great attention in both computer vision community and industry. In this paper, we propose a new framework for person re-identification with a triplet-based deep similarity learning using convolutional neural networks (CNNs). The network is trained with triplet input: two of them have the same class labels and the other one is different. It aims to learn the deep feature representation, with which the distance within the same class is decreased, while the distance between the different classes is increased as much as possible. Moreover, we trained the model jointly on six different datasets, which differs from common practice - one model is just trained on one dataset and tested also on the same one. However, the enormous number of possible triplet data among the large number of training samples makes the training impossible. To address this challenge, a double-sampling scheme is proposed to generate triplets of images as effective as possible. The proposed framework is evaluated on several benchmark datasets. The experimental results show that, our method is effective for the task of person re-identification and it is comparable or even outperforms the state-of-the-art methods.

##### Abstract (translated by Google)
近年来，人们重新认同（re-id）引起了计算机视觉界和工业界的高度重视。在本文中，我们提出了一个新的人脸识别框架，使用卷积神经网络（CNNs）进行基于三元组的深度相似性学习。网络训练有三重输入：其中两个具有相同的类别标签，另一个不同。它旨在学习深度特征表示，使同类中的距离减小，同时尽可能地增加不同类别之间的距离。此外，我们在六个不同的数据集上共同训练模型，这与一般惯例不同 - 一个模型只是在一个数据集上进行训练，并在同一个数据集上进行测试。然而，大量训练样本中可能的三元组数据数量庞大，使得训练变得不可能。为了解决这一挑战，提出了双采样方案来尽可能有效地产生三联图像。所提出的框架在几个基准数据集上进行评估。实验结果表明，我们的方法对于人员重新识别任务是有效的，并且其可比较甚至优于最先进的方法。

##### URL
[http://arxiv.org/abs/1802.03254](http://arxiv.org/abs/1802.03254)

##### PDF
[http://arxiv.org/pdf/1802.03254](http://arxiv.org/pdf/1802.03254)

