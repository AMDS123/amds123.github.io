---
layout: post
title: "Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation"
date: 2018-08-09 21:22:37
categories: arXiv_CV
tags: arXiv_CV Face
author: Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, Michael Rubinstein
mathjax: true
---

* content
{:toc}

##### Abstract
We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).

##### Abstract (translated by Google)
我们提出了一种联合视听模型，用于从诸如其他扬声器和背景噪声之类的声音混合中隔离单个语音信号。仅使用音频作为输入来解决该任务是极具挑战性的，并且不提供分离的语音信号与视频中的扬声器的关联。在本文中，我们提出了一个基于网络的深度模型，它结合了视觉和听觉信号来解决这个任务。视觉特征用于将音频“聚焦”在场景中的所需扬声器上并改善语音分离质量。为了训练我们的联合视听模型，我们引入了AVSpeech，这是一个由来自网络的数千小时视频片段组成的新数据集。我们展示了我们的方法对经典语音分离任务的适用性，以及涉及激烈访谈，嘈杂酒吧和尖叫儿童的真实场景，只要求用户在视频中指定他们想要的语音的人脸。隔离。在混合语音的情况下，我们的方法显示出优于现有技术的仅音频语音分离的明显优势。此外，我们的模型与扬声器无关（训练一次，适用于任何扬声器），比最近依赖于说话者的视听语音分离方法产生更好的结果（需要为每个感兴趣的说话者训练单独的模型）。

##### URL
[http://arxiv.org/abs/1804.03619](http://arxiv.org/abs/1804.03619)

##### PDF
[http://arxiv.org/pdf/1804.03619](http://arxiv.org/pdf/1804.03619)

