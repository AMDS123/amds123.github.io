---
layout: post
title: "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems"
date: 2015-08-26 17:16:25
categories: arXiv_CL
tags: arXiv_CL Face RNN
author: Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young
mathjax: true
---

* content
{:toc}

##### Abstract
Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.

##### Abstract (translated by Google)
自然语言生成（NLG）是口语对话的重要组成部分，对可用性和感知质量都有重大影响。大多数常用的NLG系统采用规则和启发式方法，倾向于在没有人类语言的自然变化的情况下产生严格和程式化的响应。他们也不容易扩展到覆盖多个领域和语言的系统。本文提出了一种基于语义控制的长期短期记忆（LSTM）结构的统计语言生成器。 LSTM生成器可以通过使用简单的交叉熵训练准则联合优化句子规划和表面实现来从未对齐的数据中学习，并且通过从输出候选者中抽样可以容易地实现语言变化。采用较少的启发式方法，在两个不同测试域中进行的客观评估显示，与以前的方法相比，所提出的方法提高了性能。人类法官在信息性和自然性方面评分较高的LSTM系统，总体上优于其他系统。

##### URL
[https://arxiv.org/abs/1508.01745](https://arxiv.org/abs/1508.01745)

##### PDF
[https://arxiv.org/pdf/1508.01745](https://arxiv.org/pdf/1508.01745)

