---
layout: post
title: "Differentiable Product Quantization for End-to-End Embedding Compression"
date: 2019-08-26 15:56:10
categories: arXiv_AI
tags: arXiv_AI Embedding
author: Ting Chen, Yizhou Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Embedding layer is commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. As the number of symbols increase, the number of embedding parameter, as well as their size, increase linearly and become problematically large. In this work, we aim to reduce the size of embedding layer via learning discrete codes and composing embedding vectors from the codes. More specifically, we propose a differentiable product quantization framework with two instantiations, which can serve as an efficient drop-in replacement for existing embedding layer. Empirically, we evaluate the proposed method on three different language tasks, and show that the proposed method enables end-to-end training of embedding compression that achieves significant compression ratios (14-238$\times$) at almost no performance cost (sometimes even better).

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.09756](http://arxiv.org/abs/1908.09756)

##### PDF
[http://arxiv.org/pdf/1908.09756](http://arxiv.org/pdf/1908.09756)

