---
layout: post
title: "Contrastive Explanations with Local Foil Trees"
date: 2018-06-19 21:12:37
categories: arXiv_AI
tags: arXiv_AI Classification
author: Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu Brinkhuis, Mark Neerincx
mathjax: true
---

* content
{:toc}

##### Abstract
Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a high-dimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like "Why this output (the fact) instead of that output (the foil)?" to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versus-all decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.

##### Abstract (translated by Google)
可解释的机器学习（iML）和可解释的AI（XAI）的最新进展基于分类任务中特征的重要性构造解释。然而，在高维特征空间中，这种方法可能变得不可行，而不限制一组重要特征。我们建议利用人的倾向来问问题“为什么这个输出（事实）而不是输出（箔）？”以减少那些在问题对比中起主要作用的特征的数量。我们提出的方法利用本地训练的一对所有决策树来识别导致树将数据点分类为箔而不是事实的不相交规则集。在这项研究中，我们用三个基准分类任务来说明这种方法。

##### URL
[http://arxiv.org/abs/1806.07470](http://arxiv.org/abs/1806.07470)

##### PDF
[http://arxiv.org/pdf/1806.07470](http://arxiv.org/pdf/1806.07470)

