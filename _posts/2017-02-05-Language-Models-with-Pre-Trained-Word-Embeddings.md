---
layout: post
title: "Language Models with Pre-Trained Word Embeddings"
date: 2017-02-05 11:24:05
categories: arXiv_CL
tags: arXiv_CL Embedding RNN Language_Model
author: Victor Makarenkov, Bracha Shapira, Lior Rokach
mathjax: true
---

* content
{:toc}

##### Abstract
In this work we implement a training of a Language Model (LM), using Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2], but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].

##### Abstract (translated by Google)
在这项工作中，我们使用Pennigton等人介绍的递归神经网络（RNN）和GloVe词语嵌入来实现对语言模型（LM）的训练。在[1]中。实现遵循[2]中介绍的LM任务训练RNN的总体思路，而是使用门控重复单元（GRU）[3]作为存储单元，而不是更常用的LSTM [4]。

##### URL
[https://arxiv.org/abs/1610.03759](https://arxiv.org/abs/1610.03759)

##### PDF
[https://arxiv.org/pdf/1610.03759](https://arxiv.org/pdf/1610.03759)

