---
layout: post
title: "Multilingual Twitter Sentiment Classification: The Role of Human Annotators"
date: 2016-05-05 07:05:52
categories: arXiv_SD
tags: arXiv_SD Sentiment Sentiment_Classification Classification
author: Igor Mozetic, Miha Grcar, Jasmina Smailovic
mathjax: true
---

* content
{:toc}

##### Abstract
What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered.

##### Abstract (translated by Google)
自动Twitter的情绪分类有什么限制？我们分析了大量不同语言的人工标记推文，将其作为训练数据，构建自动分类模型。事实证明，分类模型的质量更多取决于训练数据的质量和规模，而不取决于训练模型的类型。实验结果表明，顶级分类模型的性能没有统计学上的显着差异。我们通过应用各种注释器协议测量来量化训练数据的质量，并识别不同数据集的最弱点。当训练集的大小足够大时，我们表明模型性能接近内部注释器协议。然而，定期监测自签名者和签名者之间的协议是至关重要的，因为这改进了训练数据集，从而改善了模型的性能。最后，我们表明，有强有力的证据表明，人类按照有序的方式来感知情感类别（消极的，中性的和积极的）。

##### URL
[https://arxiv.org/abs/1602.07563](https://arxiv.org/abs/1602.07563)

##### PDF
[https://arxiv.org/pdf/1602.07563](https://arxiv.org/pdf/1602.07563)

