---
layout: post
title: "Discovering Discrete Latent Topics with Neural Variational Inference"
date: 2017-06-01 15:55:42
categories: arXiv_CL
tags: arXiv_CL Inference
author: Yishu Miao, Edward Grefenstette, Phil Blunsom
mathjax: true
---

* content
{:toc}

##### Abstract
Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.

##### Abstract (translated by Google)
主题模型已被广泛探讨为概率生成模型的文档。传统推理方法已经寻求用于更新模型的闭式推导，但是随着这些模型的表达性增长，对其参数执行快速和准确推断的困难也越来越大。本文提出了替代神经方法进行话题建模，通过提供参数化分布的主题，允许通过反向传播在神经变量推理的框架下进行训练。另外，在一个坚持不懈的构建的帮助下，我们提出了一个循环网络，能够发现一个概念上无限的话题，类似于贝叶斯非参数主题模型。 MXM Song Lyrics，20NewsGroups和Reuters News数据集的实验结果证明了这些神经主题模型的有效性和有效性。

##### URL
[https://arxiv.org/abs/1706.00359](https://arxiv.org/abs/1706.00359)

##### PDF
[https://arxiv.org/pdf/1706.00359](https://arxiv.org/pdf/1706.00359)

