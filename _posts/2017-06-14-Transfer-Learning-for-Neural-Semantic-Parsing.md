---
layout: post
title: "Transfer Learning for Neural Semantic Parsing"
date: 2017-06-14 05:53:51
categories: arXiv_CL
tags: arXiv_CL Transfer_Learning Deep_Learning
author: Xing Fan, Emilio Monti, Lambert Mathias, Markus Dreyer
mathjax: true
---

* content
{:toc}

##### Abstract
The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with a focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence modeling and compare their performance with an independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to a target task with smaller labeled data. We see absolute accuracy gains ranging from 1.0% to 4.4% in our in- house data set, and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.

##### Abstract (translated by Google)
语义分析的目标是将自然语言映射到机器可解释的含义表示语言（MRL）。限制深度学习技术进行语义分析的一个限制是缺乏足够的注释训练数据。在本文中，我们提出使用序列到序列的多任务设置进行语义分析，重点是转移学习。我们探索三个多任务体系结构的序列到序列建模，并将其性能与独立训练的模型进行比较。我们的实验表明，多任务设置帮助将具有大标签数据的辅助任务的学习转移到具有较小标签数据的目标任务。在我们的内部数据集中，我们看到绝对准确的增长率从1.0％到4.4％不等，而且在语义和语义辅助任务的ATIS语义分析任务中，我们也看到了从2.5％到7.0％的良好收益。

##### URL
[https://arxiv.org/abs/1706.04326](https://arxiv.org/abs/1706.04326)

##### PDF
[https://arxiv.org/pdf/1706.04326](https://arxiv.org/pdf/1706.04326)

