---
layout: post
title: "Manifold Learning & Stacked Sparse Autoencoder for Robust Breast Cancer Classification from Histopathological Images"
date: 2018-06-18 18:24:16
categories: arXiv_CV
tags: arXiv_CV Sparse Face Embedding Classification Deep_Learning Detection
author: Sawon Pratiher, Subhankar Chattoraj
mathjax: true
---

* content
{:toc}

##### Abstract
Computer aided diagnosis (CAD) of histopathological images (HI) requires efficient structural representation of the underlying surface tissue convolutions as manifested by the diverse breast cancerous (BC) tissue morphology. In this contribution, HI are modelled as spatially-progressive lower dimensional dynamical patterns embedded in the higher dimensional HI space. Manifold learning on these HI point-cloud is envisaged by LandMark ISOMAP (L-ISOMAP) for isometric feature mapping. The dimensionality reduced L-ISOMAP descriptors are cascaded with stacked sparse autoencoder (SSAE) for learning deep textural feature and tumor malignancy detection thereof. Classification accuracy of 99.4% obtained on publicly available BreaKHis dataset outperforms the state-of-the-art methods and validates it's adequacy as an adjunct tool to clinicians in confirming their diagnosis. Further, employing L-Isomap based manifold embedding, the dimensionality of HI are reduced drastically without significant loss in its discriminating competency. These relieves the GPU requirement for SSAE aided deep learning. Experimental results are discussed in detail.

##### Abstract (translated by Google)
组织病理学图像（HI）的计算机辅助诊断（CAD）需要有效的结构表示下面的表面组织卷积，表现为多种乳腺癌（BC）组织形态学。在这个贡献中，HI被建模为嵌入在更高维HI空间中的空间渐进的低维动态模式。 LandMark ISOMAP（L-ISOMAP）设想了用于这些HI点云的流形学习，用于等距特征映射。维度降低的L-ISOMAP描述符与堆叠稀疏自动编码器（SSAE）级联以用于学习其深层纹理特征和肿瘤恶性肿瘤检测。在公开可用的BreaKHis数据集上获得的99.4％的分类准确性优于最先进的方法，并验证其作为临床医生确认其诊断的辅助工具的充分性。此外，采用基于L-Isomap的流形嵌入，HI的维度大大降低，而其歧视能力没有显着的损失。这减轻了对SSAE辅助深度学习的GPU要求。实验结果进行了详细讨论。

##### URL
[http://arxiv.org/abs/1806.06876](http://arxiv.org/abs/1806.06876)

##### PDF
[http://arxiv.org/pdf/1806.06876](http://arxiv.org/pdf/1806.06876)

