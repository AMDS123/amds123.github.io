---
layout: post
title: "Learning Context-Aware Convolutional Filters for Text Processing"
date: 2018-08-29 04:15:40
categories: arXiv_CL
tags: arXiv_CL Sentiment Attention Ontology CNN Classification
author: Dinghan Shen, Martin Renqiang Min, Yitong Li, Lawrence Carin
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-aware convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-aware filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-aware filters, we further validate and rationalize the effectiveness of proposed framework.

##### Abstract (translated by Google)
卷积神经网络（CNN）最近已成为自然语言处理（NLP）的流行构建块。尽管取得了成功，但NLP中使用的大多数现有CNN模型与所有输入句子共享相同的学习（和静态）过滤器集。在本文中，我们考虑使用小元网络来学习用于文本处理的上下文感知卷积滤波器的方法。元网络的作用是将句子或文档的上下文信息抽象为一组输入感知过滤器。我们进一步概括了这个框架来建模句子对，其中引入了双向过滤器生成机制来封装相互依赖的句子表示。在我们对四个不同任务的基准测试中，包括本体分类，情感分析，答案句选择和复述识别，我们提出的模型，具有上下文感知过滤器的修改后的CNN，始终优于标准CNN和基于注意力的CNN基线。通过可视化学习的上下文感知过滤器，我们进一步验证并合理化所提出的框架的有效性。

##### URL
[http://arxiv.org/abs/1709.08294](http://arxiv.org/abs/1709.08294)

##### PDF
[http://arxiv.org/pdf/1709.08294](http://arxiv.org/pdf/1709.08294)

