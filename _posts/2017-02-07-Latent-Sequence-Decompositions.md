---
layout: post
title: "Latent Sequence Decompositions"
date: 2017-02-07 15:52:27
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition CNN Recognition
author: William Chan, Yu Zhang, Quoc Le, Navdeep Jaitly
mathjax: true
---

* content
{:toc}

##### Abstract
We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.6% WER.

##### Abstract (translated by Google)
我们提出潜在序列分解（LSD）框架。 LSD根据输入序列和输出序列分解具有可变长输出单元的序列。我们提出一个训练算法，采样有效的扩展和一个近似的解码算法。我们试验了华尔街日报的语音识别任务。我们的LSD模型WER达到了12.9％，而WER为14.8％。当与编码器上的卷积网络相结合时，我们实现了9.6％的WER。

##### URL
[https://arxiv.org/abs/1610.03035](https://arxiv.org/abs/1610.03035)

##### PDF
[https://arxiv.org/pdf/1610.03035](https://arxiv.org/pdf/1610.03035)

