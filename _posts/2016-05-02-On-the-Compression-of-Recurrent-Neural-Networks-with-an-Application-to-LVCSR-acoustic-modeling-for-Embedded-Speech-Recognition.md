---
layout: post
title: "On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition"
date: 2016-05-02 15:19:30
categories: arXiv_SD
tags: arXiv_SD Speech_Recognition RNN Recognition
author: Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, Ian McGraw
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.

##### Abstract (translated by Google)
我们研究压缩递归神经网络（RNN）的问题。尤其是，我们专注于RNN声学模型的压缩，这是由于建立紧凑和精确的语音识别系统的目标，该系统可以在移动设备上高效地运行。在这项工作中，我们提出了一种共同压缩复发和非复发层间权重矩阵的一般复发模型压缩技术。我们发现所提出的技术使我们能够将我们的长期短期记忆（LSTM）声学模型的尺寸减小到其原始尺寸的三分之一，而精度损失可以忽略不计。

##### URL
[https://arxiv.org/abs/1603.08042](https://arxiv.org/abs/1603.08042)

##### PDF
[https://arxiv.org/pdf/1603.08042](https://arxiv.org/pdf/1603.08042)

