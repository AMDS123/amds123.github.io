---
layout: post
title: "Learning Action Concept Trees and Semantic Alignment Networks from Image-Description Data"
date: 2016-09-08 05:53:31
categories: arXiv_CV
tags: arXiv_CV Classification
author: Jiyang Gao, Ram Nevatia
mathjax: true
---

* content
{:toc}

##### Abstract
Action classification in still images has been a popular research topic in computer vision. Labelling large scale datasets for action classification requires tremendous manual work, which is hard to scale up. Besides, the action categories in such datasets are pre-defined and vocabularies are fixed. However humans may describe the same action with different phrases, which leads to the difficulty of vocabulary expansion for traditional fully-supervised methods. We observe that large amounts of images with sentence descriptions are readily available on the Internet. The sentence descriptions can be regarded as weak labels for the images, which contain rich information and could be used to learn flexible expressions of action categories. We propose a method to learn an Action Concept Tree (ACT) and an Action Semantic Alignment (ASA) model for classification from image-description data via a two-stage learning process. A new dataset for the task of learning actions from descriptions is built. Experimental results show that our method outperforms several baseline methods significantly.

##### Abstract (translated by Google)
静止图像中的行为分类一直是计算机视觉领域的热门研究课题。为行动分类标记大规模数据集需要大量的人工工作，这是难以扩大的。此外，这些数据集中的动作类别是预先定义的，词汇表是固定的。然而人类可能会用不同的短语来描述相同的行为，这就导致了传统的完全监督方法的词汇扩展的困难。我们观察到大量的带有句子描述的图像在互联网上很容易找到。句子描述可以看作是图片的弱标签，它包含丰富的信息，可以用来学习动作类别的灵活表达。我们提出了一种方法来学习行动概念树（ACT）和行动语义对齐（ASA）模型的分类从图像描述数据通过两阶段的学习过程。建立了一个从描述中学习动作的新数据集。实验结果表明，我们的方法显着优于几个基准方法。

##### URL
[https://arxiv.org/abs/1609.02284](https://arxiv.org/abs/1609.02284)

##### PDF
[https://arxiv.org/pdf/1609.02284](https://arxiv.org/pdf/1609.02284)

