---
layout: post
title: "Why Do Neural Response Generation Models Prefer Universal Replies?"
date: 2018-08-28 09:11:49
categories: arXiv_CL
tags: arXiv_CL Regularization Optimization
author: Bowen Wu, Nan Jiang, Zhifeng Gao, Suke Li, Wenge Rong, Baoxun Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Recent advances in sequence-to-sequence learning reveal a purely data-driven approach to the response generation task. Despite its diverse applications, existing neural models are prone to producing short and generic replies, making it infeasible to tackle open-domain challenges. In this research, we analyze this critical issue in light of the model's optimization goal and the specific characteristics of the human-to-human dialog corpus. By decomposing the black box into parts, a detailed analysis of the probability limit was conducted to reveal the reason behind these universal replies. Based on these analyses, we propose a max-margin ranking regularization term to avoid the models leaning to these replies. Finally, empirical experiments on case studies and benchmarks with several metrics validate this approach.

##### Abstract (translated by Google)
序列到序列学习的最新进展揭示了响应生成任务的纯粹数据驱动方法。尽管其应用多种多样，但现有的神经模型很容易产生简短而通用的回复，因此无法应对开放领域的挑战。在这项研究中，我们根据模型的优化目标和人与人对话语料库的具体特征来分析这一关键问题。通过将黑匣子分解成部分，对概率限制进行了详细分析，以揭示这些普遍回复背后的原因。基于这些分析，我们提出了最大边际排名正则化项，以避免模型倾向于这些回复。最后，案例研究和基准测试的实证实验验证了这种方法。

##### URL
[http://arxiv.org/abs/1808.09187](http://arxiv.org/abs/1808.09187)

##### PDF
[http://arxiv.org/pdf/1808.09187](http://arxiv.org/pdf/1808.09187)

