---
layout: post
title: "Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image Generation"
date: 2017-05-19 21:51:30
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Lei Cai, Hongyang Gao, Shuiwang Ji
mathjax: true
---

* content
{:toc}

##### Abstract
Variational auto-encoder (VAE) is a powerful unsupervised learning framework for image generation. One drawback of VAE is that it generates blurry images due to its Gaussianity assumption and thus L2 loss. To allow the generation of high quality images by VAE, we increase the capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. To overcome the limitation of L2 loss, we propose to generate images in a multi-stage manner from coarse to fine. In the simplest case, the proposed multi-stage VAE divides the decoder into two components in which the second component generates refined images based on the course images generated by the first component. Since the second component is independent of the VAE model, it can employ other loss functions beyond the L2 loss and different model architectures. The proposed framework can be easily generalized to contain more than two components. Experiment results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can generate sharper images as compared to those from the original VAE.

##### Abstract (translated by Google)
变分自动编码器（VAE）是一个功能强大的无监督学习框架，用于图像生成。 VAE的一个缺点是由于它的高斯性假设和L2损失，它会产生模糊的图像。为了能够通过VAE生成高质量的图像，我们通过使用残余块和跳过连接来增加解码器网络的容量，这也实现了高效的优化。为了克服L2损失的局限性，我们提出从粗到细的多阶段生成图像。在最简单的情况下，所提出的多级VAE将解码器分成两个分量，其中第二分量基于由第一分量生成的路线图像生成细化的图像。由于第二部分独立于VAE模型，因此它可以使用超过L2损失和不同模型体系结构的其他损失函数。提出的框架可以很容易地推广到包含两个以上的组件。 MNIST和CelebA数据集上的实验结果表明，与原始VAE相比，所提出的多级VAE可以生成更清晰的图像。

##### URL
[https://arxiv.org/abs/1705.07202](https://arxiv.org/abs/1705.07202)

##### PDF
[https://arxiv.org/pdf/1705.07202](https://arxiv.org/pdf/1705.07202)

