---
layout: post
title: "Understanding training and generalization in deep learning by Fourier analysis"
date: 2018-08-21 21:01:43
categories: arXiv_AI
tags: arXiv_AI Deep_Learning
author: Zhiqin John Xu
mathjax: true
---

* content
{:toc}

##### Abstract
Background: It is still an open research area to theoretically understand why Deep Neural Networks (DNNs)---equipped with many more parameters than training data and trained by (stochastic) gradient-based methods---often achieve remarkably low generalization error. Contribution: We study DNN training by Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic) gradient-based methods endows low-frequency components of the target function with a higher priority during the training; ii) Small initialization leads to good generalization ability of DNN while preserving the DNN's ability of fitting any function. These results are further confirmed by experiments of DNNs fitting the following datasets, i.e., natural images, one-dimensional functions and MNIST dataset.

##### Abstract (translated by Google)
背景：理论上仍然是一个开放的研究领域，理论上理解为什么深度神经网络（DNN）配备了比训练数据更多的参数并且通过（随机）基于梯度的方法训练 - 通常会实现非常低的泛化误差。贡献：我们通过傅里叶分析研究DNN培训。我们的理论框架解释了：i）具有（随机）基于梯度的方法的DNN在训练期间赋予目标函数的低频分量更高的优先级; ii）小的初始化导致DNN的良好泛化能力，同时保持DNN适合任何功能的能力。通过适合以下数据集的DNN的实验进一步证实了这些结果，即自然图像，一维函数和MNIST数据集。

##### URL
[http://arxiv.org/abs/1808.04295](http://arxiv.org/abs/1808.04295)

##### PDF
[http://arxiv.org/pdf/1808.04295](http://arxiv.org/pdf/1808.04295)

