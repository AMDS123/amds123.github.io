---
layout: post
title: "Optimization on Submanifolds of Convolution Kernels in CNNs"
date: 2016-10-22 07:21:26
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Optimization Classification Gradient_Descent
author: Mete Ozay, Takayuki Okatani
mathjax: true
---

* content
{:toc}

##### Abstract
Kernel normalization methods have been employed to improve robustness of optimization methods to reparametrization of convolution kernels, covariate shift, and to accelerate training of Convolutional Neural Networks (CNNs). However, our understanding of theoretical properties of these methods has lagged behind their success in applications. We develop a geometric framework to elucidate underlying mechanisms of a diverse range of kernel normalization methods. Our framework enables us to expound and identify geometry of space of normalized kernels. We analyze and delineate how state-of-the-art kernel normalization methods affect the geometry of search spaces of the stochastic gradient descent (SGD) algorithms in CNNs. Following our theoretical results, we propose a SGD algorithm with assurance of almost sure convergence of the methods to a solution at single minimum of classification loss of CNNs. Experimental results show that the proposed method achieves state-of-the-art performance for major image classification benchmarks with CNNs.

##### Abstract (translated by Google)
内核归一化方法已被用来提高优化方法的鲁棒性，以重新卷积核卷积，协变量和加速卷积神经网络（CNN）的训练。但是，我们对这些方法的理论性质的理解已经落后于它们在应用中的成功。我们开发了一个几何框架来阐明各种内核规范化方法的基本机制。我们的框架使我们能够阐述和识别标准化内核的空间几何。我们分析和描述了最新的内核规范化方法如何影响CNN中随机梯度下降（SGD）算法的搜索空间的几何。根据我们的理论结果，我们提出了一种保证在CNNs的单个最小分类损失下方法几乎可靠收敛的SGD算法。实验结果表明，所提出的方法实现了与CNN的主要图像分类基准的最新性能。

##### URL
[https://arxiv.org/abs/1610.07008](https://arxiv.org/abs/1610.07008)

##### PDF
[https://arxiv.org/pdf/1610.07008](https://arxiv.org/pdf/1610.07008)

