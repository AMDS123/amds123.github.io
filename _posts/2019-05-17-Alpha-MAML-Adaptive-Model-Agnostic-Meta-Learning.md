---
layout: post
title: "Alpha MAML: Adaptive Model-Agnostic Meta-Learning"
date: 2019-05-17 18:45:25
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Classification
author: Harkirat Singh Behl, At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin, Philip H.S. Torr
mathjax: true
---

* content
{:toc}

##### Abstract
Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.07435](http://arxiv.org/abs/1905.07435)

##### PDF
[http://arxiv.org/pdf/1905.07435](http://arxiv.org/pdf/1905.07435)

