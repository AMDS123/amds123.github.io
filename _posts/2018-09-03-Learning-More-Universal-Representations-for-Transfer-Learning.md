---
layout: post
title: "Learning More Universal Representations for Transfer-Learning"
date: 2018-09-03 01:03:21
categories: arXiv_CV
tags: arXiv_CV Attention Classification
author: Youssef Tamaazousti, Herv&#xe9; Le Borgne, C&#xe9;line Hudelot, Mohamed El Amine Seddik, Mohamed Tamaazousti
mathjax: true
---

* content
{:toc}

##### Abstract
A representation is supposed universal if it encodes any element of the visual world (e.g., objects, scenes) in any configuration (e.g., scale, context). While not expecting pure universal representations, the goal in the literature is to improve the universality level, starting from a representation with a certain level. To do so, the state-of-the-art consists in learning CNN-based representations on a diversified training problem (e.g., ImageNet modified by adding annotated data). While it effectively increases universality, such approach still requires a large amount of efforts to satisfy the needs in annotated data. In this work, we propose two methods to improve universality, but pay special attention to limit the need of annotated data. We also propose a unified framework of the methods based on the diversifying of the training problem. Finally, to better match Atkinson's cognitive study about universal human representations, we proposed to rely on the transfer-learning scheme as well as a new metric to evaluate universality. This latter, aims us to demonstrates the interest of our methods on 10 target-problems, relating to the classification task and a variety of visual domains.

##### Abstract (translated by Google)
如果表示在任何配置（例如，比例，上下文）中编码视觉世界的任何元素（例如，对象，场景），则该表示被认为是通用的。虽然不期待纯粹的普遍表征，但文献中的目标是从具有一定水平的表示开始提高普遍性水平。为此，现有技术包括在多样化的训练问题上学习基于CNN的表示（例如，通过添加注释数据修改的ImageNet）。虽然它有效地增加了普遍性，但是这种方法仍然需要大量努力来满足注释数据的需要。在这项工作中，我们提出了两种提高普遍性的方法，但要特别注意限制注释数据的需要。我们还提出了基于培训问题多样化的方法的统一框架。最后，为了更好地匹配阿特金森关于普遍人类表征的认知研究，我们提出依靠转移学习方案以及评估普遍性的新指标。后者旨在表明我们的方法对10个目标问题的兴趣，涉及分类任务和各种视觉领域。

##### URL
[http://arxiv.org/abs/1712.09708](http://arxiv.org/abs/1712.09708)

##### PDF
[http://arxiv.org/pdf/1712.09708](http://arxiv.org/pdf/1712.09708)

