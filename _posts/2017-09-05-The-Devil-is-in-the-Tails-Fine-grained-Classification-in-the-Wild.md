---
layout: post
title: "The Devil is in the Tails: Fine-grained Classification in the Wild"
date: 2017-09-05 15:26:47
categories: arXiv_CV
tags: arXiv_CV Transfer_Learning Classification Recognition
author: Grant Van Horn, Pietro Perona
mathjax: true
---

* content
{:toc}

##### Abstract
The world is long-tailed. What does this mean for computer vision and visual recognition? The main two implications are (1) the number of categories we need to consider in applications can be very large, and (2) the number of training examples for most categories can be very small. Current visual recognition algorithms have achieved excellent classification accuracy. However, they require many training examples to reach peak performance, which suggests that long-tailed distributions will not be dealt with well. We analyze this question in the context of eBird, a large fine-grained classification dataset, and a state-of-the-art deep network classification algorithm. We find that (a) peak classification performance on well-represented categories is excellent, (b) given enough data, classification performance suffers only minimally from an increase in the number of classes, (c) classification performance decays precipitously as the number of training examples decreases, (d) surprisingly, transfer learning is virtually absent in current methods. Our findings suggest that our community should come to grips with the question of long tails.

##### Abstract (translated by Google)
世界是长尾的这对计算机视觉和视觉识别意味着什么？主要的两个影响是（1）我们在应用中需要考虑的类别数量可能非常大，（2）大多数类别的训练样例的数量可能非常少。目前的视觉识别算法已经取得了很好的分类精度然而，他们需要很多训练实例才能达到最佳表现，这表明长尾分布处理不好。我们在eBird（一个大的细粒度分类数据集）和一个最先进的深度网络分类算法的背景下分析这个问题。我们发现：（a）表现良好的类别中的最高分类性能是优异的，（b）给出足够的数据，分类性能仅受类别数量增加的影响最小;（c）分类性能随着训练次数（d）令人惊讶的是，目前的方法几乎没有转移学习。我们的研究结果表明，我们的社区应该处理长尾巴的问题。

##### URL
[https://arxiv.org/abs/1709.01450](https://arxiv.org/abs/1709.01450)

##### PDF
[https://arxiv.org/pdf/1709.01450](https://arxiv.org/pdf/1709.01450)

