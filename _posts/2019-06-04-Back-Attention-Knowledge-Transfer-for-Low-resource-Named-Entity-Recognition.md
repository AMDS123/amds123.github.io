---
layout: post
title: "Back Attention Knowledge Transfer for Low-resource Named Entity Recognition"
date: 2019-06-04 03:33:51
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention Recognition
author: Linghao Sun, Huixiong Yi, Huanhuan Chen
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, great success has been achieved in the field of natural language processing (NLP), thanks in part to the considerable amount of annotated resources. For named entity recognition (NER), most languages do not have such an abundance of labeled data, so the performances of those languages are comparatively lower. To improve the performance, we propose a general approach called Back Attention Network (BAN). BAN uses translation system to translate other language sentences into English and utilizes the pre-trained English NER model to get task-specific information. After that, BAN applies a new mechanism named back attention knowledge transfer to improve the semantic representation, which aids in generation of the result. Experiments on three different language datasets indicate that our approach outperforms other state-of-the-art methods.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.01183](http://arxiv.org/abs/1906.01183)

##### PDF
[http://arxiv.org/pdf/1906.01183](http://arxiv.org/pdf/1906.01183)

