---
layout: post
title: "Paying Attention to Descriptions Generated by Image Captioning Models"
date: 2017-08-04 11:24:45
categories: arXiv_CV
tags: arXiv_CV Image_Caption Salient Attention Caption Language_Model
author: Hamed R. Tavakoli, Rakshith Shetty, Ali Borji, Jorma Laaksonen
mathjax: true
---

* content
{:toc}

##### Abstract
To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1704.07434](https://arxiv.org/abs/1704.07434)

##### PDF
[https://arxiv.org/pdf/1704.07434](https://arxiv.org/pdf/1704.07434)

