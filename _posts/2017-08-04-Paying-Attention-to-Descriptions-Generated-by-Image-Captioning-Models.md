---
layout: post
title: "Paying Attention to Descriptions Generated by Image Captioning Models"
date: 2017-08-04 11:24:45
categories: arXiv_CV
tags: arXiv_CV Image_Caption Salient Attention Caption Language_Model
author: Hamed R. Tavakoli, Rakshith Shetty, Ali Borji, Jorma Laaksonen
mathjax: true
---

* content
{:toc}

##### Abstract
To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.

##### Abstract (translated by Google)
为了在图像理解和描述中弥合人与机器之间的差距，我们需要进一步了解人们如何描述感知场景。在本文中，我们研究了自下而上基于显着性的视觉注意与场景描述构造中的对象引用之间的一致性。我们研究人类描述和机器生成的描述的属性。然后，我们提出了一个显着提升的图像字幕模型，以便研究语言模型中低级线索的好处。我们了解到：（1）人类在描述中提及的内容比不那么突出的人更早，（2）字幕模型的表现越好，与人类描述的关注度越高，（3）提出的显着提升模型，与其基线形式相比，在MS COCO数据库上没有显着改善，表明明确的自下而上的提升对于在数据上很好地学习和调整任务没有帮助。（4）然而，观察到更好的概括。关于未见数据的显着性提升模型。

##### URL
[https://arxiv.org/abs/1704.07434](https://arxiv.org/abs/1704.07434)

##### PDF
[https://arxiv.org/pdf/1704.07434](https://arxiv.org/pdf/1704.07434)

