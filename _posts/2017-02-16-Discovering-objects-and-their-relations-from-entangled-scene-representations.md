---
layout: post
title: "Discovering objects and their relations from entangled scene representations"
date: 2017-02-16 18:08:27
categories: arXiv_CV
tags: arXiv_CV Salient Knowledge Relation
author: David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, Peter Battaglia
mathjax: true
---

* content
{:toc}

##### Abstract
Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) - a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning.

##### Abstract (translated by Google)
我们的世界可以简洁而紧凑地描述为对象和关系的结构化场景。一个典型的房间，例如，包含显着的对象，如桌子，椅子和书籍，这些对象通常是由其根本原因和语义相互关联。这产生相关的特征，例如位置，功能和形状。人类利用物体及其关系的知识来学习广泛的任务，更一般地，当学习观察数据的基础结构时。在这项工作中，我们引入了关系网络（RNs） - 一种用于对象关系推理的通用神经网络架构。我们显示RN能够从场景描述数据中学习对象关系。此外，我们还表明，RN可以作为一个瓶颈，诱使纠缠场景描述输入物体的分解，并从变分自动编码器提供的场景图像的分布深度表示。该模型还可以与一次性学习任务中隐式关系发现的可区分记忆机制结合使用。我们的研究结果表明，关系网络是解决各种需要对象关系推理的问题的潜在强大的体系结构。

##### URL
[https://arxiv.org/abs/1702.05068](https://arxiv.org/abs/1702.05068)

##### PDF
[https://arxiv.org/pdf/1702.05068](https://arxiv.org/pdf/1702.05068)

