---
layout: post
title: "Playing for Benchmarks"
date: 2017-09-21 13:44:47
categories: arXiv_CV
tags: arXiv_CV Semantic_Instance_Segmentation Object_Detection Segmentation Tracking Detection
author: Stephan R. Richter, Zeeshan Hayder, Vladlen Koltun
mathjax: true
---

* content
{:toc}

##### Abstract
We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research. The supplementary video can be viewed at this https URL

##### Abstract (translated by Google)
我们提出了一个视觉感知的基准套件。该基准基于超过250K的高分辨率视频帧，所有视频帧都包含低层和高层视觉任务的地面实况数据，包括光流，语义实例分割，对象检测和跟踪，对象级3D场景布局和视觉测距。所有任务的地面实况数据可用于每一帧。在现实的虚拟世界中驾驶，骑行和在不同的环境条件下步行184公里，收集数据。为了创建基准，我们开发了一种新的方法来收集来自模拟世界的地面实况数据，而无需访问其源代码或内容。我们进行的统计分析表明，基准场景的组成与相应物理环境的组成非常匹配。收集到的数据的真实性通过感知实验进一步验证。我们分析多个任务的最新方法的性能，为未来的研究提供参考基准和突出的挑战。补充视频可以在这个https URL查看

##### URL
[https://arxiv.org/abs/1709.07322](https://arxiv.org/abs/1709.07322)

##### PDF
[https://arxiv.org/pdf/1709.07322](https://arxiv.org/pdf/1709.07322)

