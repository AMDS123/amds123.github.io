---
layout: post
title: "Dialogue Learning With Human-In-The-Loop"
date: 2017-01-13 21:12:38
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, Jason Weston
mathjax: true
---

* content
{:toc}

##### Abstract
An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.

##### Abstract (translated by Google)
开发会话代理的一个重要方面是让机器人能够通过与人交流来改进，并从错误中学习。大多数研究集中于从标记数据的固定训练集中学习，而不是以在线方式与对话伙伴进行交互。在本文中，我们探讨了强化学习环境中的这个方向，其中机器人根据其生成的响应提高了教师提供的反馈问题回答能力。我们建立了一个模拟器，在综合环境下测试这种学习的各个方面，并介绍在这个体系中工作的模型。最后，通过Mechanical Turk的真实实验验证了方法。

##### URL
[https://arxiv.org/abs/1611.09823](https://arxiv.org/abs/1611.09823)

##### PDF
[https://arxiv.org/pdf/1611.09823](https://arxiv.org/pdf/1611.09823)

