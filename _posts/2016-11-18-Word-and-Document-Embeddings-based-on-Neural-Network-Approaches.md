---
layout: post
title: "Word and Document Embeddings based on Neural Network Approaches"
date: 2016-11-18 03:21:28
categories: arXiv_CL
tags: arXiv_CL Embedding CNN Represenation_Learning Deep_Learning Relation
author: Siwei Lai
mathjax: true
---

* content
{:toc}

##### Abstract
Data representation is a fundamental task in machine learning. The representation of data affects the performance of the whole machine learning system. In a long history, the representation of data is done by feature engineering, and researchers aim at designing better features for specific tasks. Recently, the rapid development of deep learning and representation learning has brought new inspiration to various domains. In natural language processing, the most widely used feature representation is the Bag-of-Words model. This model has the data sparsity problem and cannot keep the word order information. Other features such as part-of-speech tagging or more complex syntax features can only fit for specific tasks in most cases. This thesis focuses on word representation and document representation. We compare the existing systems and present our new model. First, for generating word embeddings, we make comprehensive comparisons among existing word embedding models. In terms of theory, we figure out the relationship between the two most important models, i.e., Skip-gram and GloVe. In our experiments, we analyze three key points in generating word embeddings, including the model construction, the training corpus and parameter design. We evaluate word embeddings with three types of tasks, and we argue that they cover the existing use of word embeddings. Through theory and practical experiments, we present some guidelines for how to generate a good word embedding. Second, in Chinese character or word representation. We introduce the joint training of Chinese character and word. ... Third, for document representation, we analyze the existing document representation models, including recursive NNs, recurrent NNs and convolutional NNs. We point out the drawbacks of these models and present our new model, the recurrent convolutional neural networks. ...

##### Abstract (translated by Google)
数据表示是机器学习的一项基本任务。数据的表示会影响整个机器学习系统的性能。长期以来，数据的表示是通过特征工程完成的，研究人员的目标是为特定任务设计更好的特征。近年来，深度学习与表征学习的快速发展给各个领域带来了新的启发。在自然语言处理中，最广泛使用的特征表示是Bag-of-Words模型。该模型存在数据稀疏性问题，无法保留词序信息。其他功能（如词性标注或更复杂的语法功能）只能适用于大多数情况下的特定任务。本文着重于文字表示和文档表示。我们比较现有的系统，并提出我们的新模型。首先，为了生成字嵌入，我们对现有的字嵌入模型进行综合比较。在理论上，我们找出两个最重要的模型，即Skip-gram和GloVe之间的关系。在我们的实验中，我们分析了生成字嵌入的三个关键点，包括模型构建，训练语料库和参数设计。我们用三种类型的任务评估词嵌入，我们认为它们涵盖了现有的词嵌入使用。通过理论和实践的实验，我们提出了一些如何生成一个好的词嵌入的指导方针。其次，用中文字或词表示。介绍汉字和文字的联合训练。第三，对于文档表示，我们分析现有的文档表示模型，包括递归神经网络，递归神经网络和卷积神经网络。我们指出了这些模型的缺点，并提出了我们的新模型，即经常性的卷积神经网络。 ...

##### URL
[https://arxiv.org/abs/1611.05962](https://arxiv.org/abs/1611.05962)

##### PDF
[https://arxiv.org/pdf/1611.05962](https://arxiv.org/pdf/1611.05962)

