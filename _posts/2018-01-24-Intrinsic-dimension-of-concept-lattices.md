---
layout: post
title: "Intrinsic dimension of concept lattices"
date: 2018-01-24 13:49:37
categories: arXiv_AI
tags: arXiv_AI Knowledge
author: Tom Hanika, Friedrich Martin Schneider, Gerd Stumme
mathjax: true
---

* content
{:toc}

##### Abstract
Geometric analysis is a very capable theory to understand the influence of the high dimensionality of the input data in machine learning (ML) and knowledge discovery (KD). With our approach we can assess how far the application of a specific KD/ML-algorithm to a concrete data set is prone to the curse of dimensionality. To this end we extend V.~Pestov's axiomatic approach to the instrinsic dimension of data sets, based on the seminal work by M.~Gromov on concentration phenomena, and provide an adaptable and computationally feasible model for studying observable geometric invariants associated to features that are natural to both the data and the learning procedure. In detail, we investigate data represented by formal contexts and give first theoretical as well as experimental insights into the intrinsic dimension of a concept lattice. Because of the correspondence between formal concepts and maximal cliques in graphs, applications to social network analysis are at hand.

##### Abstract (translated by Google)
几何分析是理解输入数据在机器学习（ML）和知识发现（KD）中的高维度的影响的理论。通过我们的方法，我们可以评估特定的KD / ML算法在具体数据集上的应用程度是否会受到维数灾难的影响。为此，基于M.〜Gromov关于浓缩现象的开创性工作，对V.〜Pestov的数据集的内在维度的公理化方法进行了扩展，并提供了一个适应性和计算可行的模型，用于研究与特征相关的可观测几何不变量对数据和学习过程都是自然的。详细地说，我们调查由形式上下文所代表的数据，并对概念格的固有维度给出第一个理论和实验的见解。由于图形中形式概念和极大派系之间的对应关系，社交网络分析的应用即将到来。

##### URL
[http://arxiv.org/abs/1801.07985](http://arxiv.org/abs/1801.07985)

##### PDF
[http://arxiv.org/pdf/1801.07985](http://arxiv.org/pdf/1801.07985)

