---
layout: post
title: "Eye Tracking for Everyone"
date: 2016-06-18 23:53:54
categories: arXiv_CV
tags: arXiv_CV Tracking CNN Prediction
author: Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra Bhandarkar, Wojciech Matusik, Antonio Torralba
mathjax: true
---

* content
{:toc}

##### Abstract
From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at this http URL

##### Abstract (translated by Google)
从科学研究到商业应用，眼动追踪是许多领域的重要工具。尽管其应用范围广泛，但眼动追踪尚未成为普遍的技术。我们相信，通过构建适用于移动电话和平板电脑等商用硬件的眼动追踪软件，无需额外的传感器或设备，我们就可以将眼动追踪的力量放在每个人的手掌上。我们通过引入第一个用于眼动追踪的大规模数据集GazeCapture来解决这个问题，该数据集包含来自超过1450人的近250万帧的数据。使用GazeCapture，我们训练iTracker，一个用于眼动追踪的卷积神经网络，在现代移动设备上实时运行（10-15fps），比以前的方法显着减少了错误。我们的模型在手机和平​​板电脑上分别实现了1.71cm和2.53cm的预测误差。通过校准，这可以减少到1.34厘米和2.12厘米。此外，我们证明了iTracker学到的功能很好地概括了其他数据集，实现了最先进的结果。代码，数据和模型可以在这个http URL中找到

##### URL
[https://arxiv.org/abs/1606.05814](https://arxiv.org/abs/1606.05814)

##### PDF
[https://arxiv.org/pdf/1606.05814](https://arxiv.org/pdf/1606.05814)

