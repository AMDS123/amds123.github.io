---
layout: post
title: "On the Learning Dynamics of Deep Neural Networks"
date: 2018-09-18 17:58:49
categories: arXiv_AI
tags: arXiv_AI Adversarial GAN Classification
author: Remi Tachet des Combes, Mohammad Pezeshki, Samira Shabanian, Aaron Courville, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.

##### Abstract (translated by Google)
虽然近年来取得了很多进展，但深度非线性神经网络的学习动力至今仍然存在很大的误解。在这项工作中，我们研究了二元分类的情况，并在强大的假设（如数据的线性可分性）下证明了这种网络中学习的各种属性。从线性情况扩展现有结果，我们通过证明分类误差在非线性体系结构中也遵循S形形状来证实经验观察。我们表明，给定适当的初始化，学习阐述了并行独立模式，并且参数空间的某些区域可能导致训练失败。我们还证明了数据集中的输入范数和特征频率导致了明显的收敛速度，这可能会对深度神经网络的泛化能力有所启发。我们提供了学习动态与交叉熵和铰链损失之间的比较，这可能有助于理解生成对抗网络训练的最新进展。最后，我们确定了一种我们为渐变饥饿施洗的现象，其中数据集中最常见的特征阻止了其他不太频繁但信息量相同的特征的学习。

##### URL
[https://arxiv.org/abs/1809.06848](https://arxiv.org/abs/1809.06848)

##### PDF
[https://arxiv.org/pdf/1809.06848](https://arxiv.org/pdf/1809.06848)

