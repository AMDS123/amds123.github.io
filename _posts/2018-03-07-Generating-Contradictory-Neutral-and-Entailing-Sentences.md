---
layout: post
title: "Generating Contradictory, Neutral, and Entailing Sentences"
date: 2018-03-07 15:18:03
categories: arXiv_AI
tags: arXiv_AI Adversarial Inference
author: Yikang Shen, Shawn Tan, Chin-Wei Huang, Aaron Courville
mathjax: true
---

* content
{:toc}

##### Abstract
Learning distributed sentence representations remains an interesting problem in the field of Natural Language Processing (NLP). We want to learn a model that approximates the conditional latent space over the representations of a logical antecedent of the given statement. In our paper, we propose an approach to generating sentences, conditioned on an input sentence and a logical inference label. We do this by modeling the different possibilities for the output sentence as a distribution over the latent representation, which we train using an adversarial objective. We evaluate the model using two state-of-the-art models for the Recognizing Textual Entailment (RTE) task, and measure the BLEU scores against the actual sentences as a probe for the diversity of sentences produced by our model. The experiment results show that, given our framework, we have clear ways to improve the quality and diversity of generated sentences.

##### Abstract (translated by Google)
学习分布式句子表示仍然是自然语言处理（NLP）领域中一个有趣的问题。我们想要学习一个模型来逼近给定语句的逻辑先行词的表示上的条件潜在空间。在我们的论文中，我们提出了一种生成句子的方法，以输入句子和逻辑推理标签为条件。我们通过将输出句子的不同可能性建模为潜在表示的分布来建模，我们使用敌对目标进行训练。我们使用两种最先进的识别文本（RTE）任务模型来评估模型，并根据实际句子测量BLEU分数，作为我们模型生成的句子多样性的探索。实验结果表明，在我们的框架下，我们有明确的方法来提高生成句子的质量和多样性。

##### URL
[http://arxiv.org/abs/1803.02710](http://arxiv.org/abs/1803.02710)

##### PDF
[http://arxiv.org/pdf/1803.02710](http://arxiv.org/pdf/1803.02710)

