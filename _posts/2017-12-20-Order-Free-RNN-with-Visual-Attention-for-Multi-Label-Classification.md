---
layout: post
title: "Order-Free RNN with Visual Attention for Multi-Label Classification"
date: 2017-12-20 06:07:28
categories: arXiv_CV
tags: arXiv_CV Image_Caption Knowledge Attention Caption Inference RNN Classification Prediction
author: Shang-Fu Chen, Yi-Chen Chen, Chih-Kuan Yeh, Yu-Chiang Frank Wang
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose the joint learning attention and recurrent neural network (RNN) models for multi-label classification. While approaches based on the use of either model exist (e.g., for the task of image captioning), training such existing network architectures typically require pre-defined label sequences. For multi-label classification, it would be desirable to have a robust inference process, so that the prediction error would not propagate and thus affect the performance. Our proposed model uniquely integrates attention and Long Short Term Memory (LSTM) models, which not only addresses the above problem but also allows one to identify visual objects of interests with varying sizes without the prior knowledge of particular label ordering. More importantly, label co-occurrence information can be jointly exploited by our LSTM model. Finally, by advancing the technique of beam search, prediction of multiple labels can be efficiently achieved by our proposed network model.

##### Abstract (translated by Google)
在本文中，我们提出了联合学习注意和循环神经网络（RNN）模型的多标签分类。虽然存在基于使用任一模型的方法（例如，用于图像字幕的任务），但是训练这样的现有网络架构通常需要预定义的标签序列。对于多标签分类，期望具有鲁棒的推理过程，使得预测误差不会传播并因此影响性能。我们提出的模型独特地集成了注意力和长期短期记忆（LSTM）模型，其不仅解决了上述问题，而且允许人们在不具有特定标签排序的先验知识的情况下识别具有不同尺寸的感兴趣的视觉对象。更重要的是，我们的LSTM模型可以共同利用标签共现信息。最后，通过推进光束搜索技术，我们提出的网络模型可以有效地实现多个标签的预测。

##### URL
[https://arxiv.org/abs/1707.05495](https://arxiv.org/abs/1707.05495)

##### PDF
[https://arxiv.org/pdf/1707.05495](https://arxiv.org/pdf/1707.05495)

