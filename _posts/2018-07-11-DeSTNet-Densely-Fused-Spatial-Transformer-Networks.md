---
layout: post
title: "DeSTNet: Densely Fused Spatial Transformer Networks"
date: 2018-07-11 10:06:32
categories: arXiv_CV
tags: arXiv_CV Knowledge CNN
author: Roberto Annunziata, Christos Sagonas, Jacques Cal&#xec;
mathjax: true
---

* content
{:toc}

##### Abstract
Modern Convolutional Neural Networks (CNN) are extremely powerful on a range of computer vision tasks. However, their performance may degrade when the data is characterised by large intra-class variability caused by spatial transformations. The Spatial Transformer Network (STN) is currently the method of choice for providing CNNs the ability to remove those transformations and improve performance in an end-to-end learning framework. In this paper, we propose Densely Fused Spatial Transformer Network (DeSTNet), which, to the best of our knowledge, is the first dense fusion pattern for combining multiple STNs. Specifically, we show how changing the connectivity pattern of multiple STNs from sequential to dense leads to more powerful alignment modules. Extensive experiments on three benchmarks namely, MNIST, GTSRB, and IDocDB show that the proposed technique outperforms related state-of-the-art methods (i.e., STNs and CSTNs) both in terms of accuracy and robustness.

##### Abstract (translated by Google)
现代卷积神经网络（CNN）在一系列计算机视觉任务中非常强大。然而，当数据的特征在于由空间变换引起的大的类内可变性时，它们的性能可能降低。空间变换器网络（STN）目前是提供CNN能够在端到端学习框架中移除这些变换并提高性能的首选方法。在本文中，我们提出了密集融合空间变换器网络（DeSTNet），据我们所知，它是第一个用于组合多个STN的密集融合模式。具体来说，我们展示了如何将多个STN的连接模式从顺序更改为密集导致更强大的对齐模块。在三个基准上进行的广泛实验，即MNIST，GTSRB和IDocDB，表明所提出的技术在准确性和鲁棒性方面都优于相关的现有技术方法（即STN和CSTN）。

##### URL
[http://arxiv.org/abs/1807.04050](http://arxiv.org/abs/1807.04050)

##### PDF
[http://arxiv.org/pdf/1807.04050](http://arxiv.org/pdf/1807.04050)

