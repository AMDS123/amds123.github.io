---
layout: post
title: "Generative Exploration and Exploitation"
date: 2019-04-21 14:15:24
categories: arXiv_AI
tags: arXiv_AI Sparse Knowledge Reinforcement_Learning
author: Jiechuan Jiang, Zongqing Lu
mathjax: true
---

* content
{:toc}

##### Abstract
Sparse reward is one of the biggest challenges in reinforcement learning (RL). In this paper, we propose a novel method called Generative Exploration and Exploitation (GENE) to overcome sparse reward. GENE dynamically changes the start state of agent to the generated novel state to encourage the agent to explore the environment or to the generated rewarding state to boost the agent to exploit the received reward signal. GENE relies on no prior knowledge about the environment and can be combined with any RL algorithm, no matter on-policy or off-policy, single-agent or multi-agent. Empirically, we demonstrate that GENE significantly outperforms existing methods in four challenging tasks with only binary rewards indicating whether or not the task is completed, including Maze, Goal Ant, Pushing, and Cooperative Navigation. The ablation studies verify that GENE can adaptively tradeoff between exploration and exploitation as the learning progresses by automatically adjusting the proportion between generated novel states and rewarding states, which is the key for GENE to solving these challenging tasks effectively and efficiently.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.09605](http://arxiv.org/abs/1904.09605)

##### PDF
[http://arxiv.org/pdf/1904.09605](http://arxiv.org/pdf/1904.09605)

