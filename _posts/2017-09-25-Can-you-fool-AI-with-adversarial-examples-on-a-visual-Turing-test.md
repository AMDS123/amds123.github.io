---
layout: post
title: "Can you fool AI with adversarial examples on a visual Turing test?"
date: 2017-09-25 19:32:49
categories: arXiv_CV
tags: arXiv_CV Adversarial Deep_Learning VQA
author: Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darell, Dawn Song
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning has achieved impressive results in many areas of Computer Vision and Natural Language Pro- cessing. Among others, Visual Question Answering (VQA), also referred to a visual Turing test, is considered one of the most compelling problems, and recent deep learning models have reported significant progress in vision and language modeling. Although Artificial Intelligence (AI) is getting closer to passing the visual Turing test, at the same time the existence of adversarial examples to deep learning systems may hinder the practical application of such systems. In this work, we conduct the first extensive study on adversarial examples for VQA systems. In particular, we focus on generating targeted adversarial examples for a VQA system while the target is considered to be a question-answer pair. Our evaluation shows that the success rate of whether a targeted adversarial example can be generated is mostly dependent on the choice of the target question-answer pair, and less on the choice of images to which the question refers. We also report the language prior phenomenon of a VQA model, which can explain why targeted adversarial examples are hard to generate for some question-answer targets. We also demonstrate that a compositional VQA architecture is slightly more resilient to adversarial attacks than a non-compositional one. Our study sheds new light on how to build deep vision and language resilient models robust against adversarial examples.

##### Abstract (translated by Google)
深度学习在计算机视觉和自然语言处理的许多领域取得了令人瞩目的成果。其中，视觉问答（VQA），也被称为视觉图灵测试，被认为是最引人注目的问题之一，最近的深度学习模型已经报告了视觉和语言建模方面的重大进展。尽管人工智能（AI）越来越接近通过视觉图灵测试，同时对于深度学习系统存在敌对的例子可能会阻碍这种系统的实际应用。在这项工作中，我们对VQA系统的敌对案例进行了第一次广泛的研究。特别是，我们专注于为VQA系统生成有针对性的对抗性例子，而目标被认为是问答对。我们的评估表明，能否产生有针对性的对抗性例子的成功率主要取决于目标问题 - 答案对的选择，而较少取决于问题提及的图像的选择。我们也报道了VQA模型的语言以前的现象，这可以解释为什么有针对性的对抗的例子很难产生一些问答的目标。我们还证明，构成VQA体系结构比非构成性体系更能抵御对手攻击。我们的研究揭示了如何建立强有力的对抗案例的深层视觉和语言复原模型。

##### URL
[https://arxiv.org/abs/1709.08693](https://arxiv.org/abs/1709.08693)

