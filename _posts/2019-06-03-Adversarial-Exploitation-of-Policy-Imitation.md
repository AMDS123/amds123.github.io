---
layout: post
title: "Adversarial Exploitation of Policy Imitation"
date: 2019-06-03 23:38:33
categories: arXiv_AI
tags: arXiv_AI Adversarial Reinforcement_Learning
author: Vahid Behzadan, William Hsu
mathjax: true
---

* content
{:toc}

##### Abstract
This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.01121](http://arxiv.org/abs/1906.01121)

##### PDF
[http://arxiv.org/pdf/1906.01121](http://arxiv.org/pdf/1906.01121)

