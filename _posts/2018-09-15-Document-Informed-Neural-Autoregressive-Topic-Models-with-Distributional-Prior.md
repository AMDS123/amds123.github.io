---
layout: post
title: "Document Informed Neural Autoregressive Topic Models with Distributional Prior"
date: 2018-09-15 12:48:16
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Classification Language_Model
author: Pankaj Gupta, Yatin Chaudhary, Florian Buettner, Hinrich Sch&#xfc;tze
mathjax: true
---

* content
{:toc}

##### Abstract
We address two challenges in topic models: (1) Context information around words helps in determining their actual meaning, e.g., "networks" used in the contexts artificial neural networks vs. biological neuron networks. Generative topic models infer topic-word distributions, taking no or only little context into account. Here, we extend a neural autoregressive topic model to exploit the full context information around words in a document in a language modeling fashion. The proposed model is named as iDocNADE. (2) Due to the small number of word occurrences (i.e., lack of context) in short text and data sparsity in a corpus of few documents, the application of topic models is challenging on such texts. Therefore, we propose a simple and efficient way of incorporating external knowledge into neural autoregressive topic models: we use embeddings as a distributional prior. The proposed variants are named as DocNADE2 and iDocNADE2. We present novel neural autoregressive topic model variants that consistently outperform state-of-the-art generative topic models in terms of generalization, interpretability (topic coherence) and applicability (retrieval and classification) over 6 long-text and 8 short-text datasets from diverse domains.

##### Abstract (translated by Google)
我们在主题模型中解决了两个挑战：（1）围绕单词的上下文信息有助于确定它们的实际含义，例如在上下文中使用的“网络”人工神经网络与生物神经网络。生成主题模型推断主题 - 单词分布，不考虑或仅考虑很少的上下文。在这里，我们扩展神经自回归主题模型，以语言建模方式利用文档中单词周围的完整上下文信息。提出的模型被命名为iDocNADE。 （2）由于短文本中的单词出现次数（即缺少上下文）和少数文档的语料库中的数据稀疏性，主题模型的应用对这些文本具有挑战性。因此，我们提出了一种将外部知识纳入神经自回归主题模型的简单有效方法：我们使用嵌入作为分布式先验。建议的变体命名为DocNADE2和iDocNADE2。我们提出了新的神经自回归主题模型变体，它们在泛化，可解释性（主题一致性）和适用性（检索和分类）方面始终优于最先进的生成主题模型，包括6个长文本和8个短文本数据集。多元化领域。

##### URL
[http://arxiv.org/abs/1809.06709](http://arxiv.org/abs/1809.06709)

##### PDF
[http://arxiv.org/pdf/1809.06709](http://arxiv.org/pdf/1809.06709)

