---
layout: post
title: "Compressing Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks"
date: 2017-10-10 03:53:33
categories: arXiv_CV
tags: arXiv_CV Regularization Sparse Inference
author: Julian Faraone, Nicholas Fraser, Giulio Gambardella, Michaela Blott, Philip H.W. Leong
mathjax: true
---

* content
{:toc}

##### Abstract
A low precision deep neural network training technique for producing sparse, ternary neural networks is presented. The technique incorporates hard- ware implementation costs during training to achieve significant model compression for inference. Training involves three stages: network training using L2 regularization and a quantization threshold regularizer, quantization pruning, and finally retraining. Resulting networks achieve improved accuracy, reduced memory footprint and reduced computational complexity compared with conventional methods, on MNIST and CIFAR10 datasets. Our networks are up to 98% sparse and 5 & 11 times smaller than equivalent binary and ternary models, translating to significant resource and speed benefits for hardware implementations.

##### Abstract (translated by Google)
提出了一种生成稀疏三元神经网络的低精度深度神经网络训练方法。该技术在训练过程中包含硬件实现成本，以实现重要的推理模型压缩。训练涉及三个阶段：使用L2正则化的网络训练和量化门限正规化器，量化修剪以及最后再训练。在MNIST和CIFAR10数据集上，与传统方法相比，所得到的网络实现了更高的准确性，减少了内存占用和计算复杂度。我们的网络比相应的二进制和三进制模型高达98％的稀疏性和5和11倍，为硬件实现转化为显着的资源和速度优势。

##### URL
[https://arxiv.org/abs/1709.06262](https://arxiv.org/abs/1709.06262)

##### PDF
[https://arxiv.org/pdf/1709.06262](https://arxiv.org/pdf/1709.06262)

