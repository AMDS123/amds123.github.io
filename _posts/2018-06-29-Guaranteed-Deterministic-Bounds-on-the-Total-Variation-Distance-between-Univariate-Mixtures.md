---
layout: post
title: "Guaranteed Deterministic Bounds on the Total Variation Distance between Univariate Mixtures"
date: 2018-06-29 09:22:41
categories: arXiv_CV
tags: arXiv_CV
author: Frank Nielsen, Ke Sun
mathjax: true
---

* content
{:toc}

##### Abstract
The total variation distance is a core statistical distance between probability measures that satisfies the metric axioms, with value always falling in $[0,1]$. This distance plays a fundamental role in machine learning and signal processing: It is a member of the broader class of $f$-divergences, and it is related to the probability of error in Bayesian hypothesis testing. Since the total variation distance does not admit closed-form expressions for statistical mixtures (like Gaussian mixture models), one often has to rely in practice on costly numerical integrations or on fast Monte Carlo approximations that however do not guarantee deterministic lower and upper bounds. In this work, we consider two methods for bounding the total variation of univariate mixture models: The first method is based on the information monotonicity property of the total variation to design guaranteed nested deterministic lower bounds. The second method relies on computing the geometric lower and upper envelopes of weighted mixture components to derive deterministic bounds based on density ratio. We demonstrate the tightness of our bounds in a series of experiments on Gaussian, Gamma and Rayleigh mixture models.

##### Abstract (translated by Google)
总变差距离是满足公制公理的概率测度之间的核心统计距离，其值始终在$ [0,1] $中。这个距离在机器学习和信号处理中起着重要的作用：它是更广泛的$ f $ -divergences类的成员，它与贝叶斯假设检验中的错误概率有关。由于总变差距离不允许统计混合的封闭形式表达式（如高斯混合模型），人们经常不得不依赖实际昂贵的数值积分或快速蒙特卡罗近似，但不能保证确定性的下限和上限。在这项工作中，我们考虑两种限制单变量混合模型总变化的方法：第一种方法是基于总变差的信息单调性来设计保证的嵌套确定性下界。第二种方法依赖于计算加权混合分量的几何下限和上限，从而根据密度比导出确定性边界。我们在高斯，伽马和瑞利混合模型的一系列实验中证明了我们的界限的紧密性。

##### URL
[http://arxiv.org/abs/1806.11311](http://arxiv.org/abs/1806.11311)

##### PDF
[http://arxiv.org/pdf/1806.11311](http://arxiv.org/pdf/1806.11311)

