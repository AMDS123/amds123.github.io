---
layout: post
title: "Improving Deep Neural Network with Multiple Parametric Exponential Linear Units"
date: 2017-01-17 08:44:56
categories: arXiv_CV
tags: arXiv_CV Classification
author: Yang Li, Chunxiao Fan, Yong Li, Qiong Wu, Yue Ming
mathjax: true
---

* content
{:toc}

##### Abstract
Activation function is crucial to the recent successes of deep neural networks. In this paper, we first propose a new activation function, Multiple Parametric Exponential Linear Units (MPELU), aiming to generalize and unify the rectified and exponential linear units. As the generalized form, MPELU shares the advantages of Parametric Rectified Linear Unit (PReLU) and Exponential Linear Unit (ELU), leading to better classification performance and convergence property. In addition, weight initialization is very important to train very deep networks. The existing methods laid a solid foundation for networks using rectified linear units but not for exponential linear units. This paper complements the current theory and extends it to the wider range. Specifically, we put forward a way of initialization, enabling training of very deep networks using exponential linear units. Experiments demonstrate that the proposed initialization not only helps the training process but leads to better generalization performance. Finally, utilizing the proposed activation function and initialization, we present a deep MPELU residual architecture that achieves state-of-the-art performance on the CIFAR-10/100 datasets. The code is available at this https URL

##### Abstract (translated by Google)
激活功能对深度神经网络最近的成功至关重要。在本文中，我们首先提出了一种新的激活函数，多参数指数线性单位（MPELU），旨在将整数和指数线性单位进行概括和统一。作为广义形式，MPELU分享了参数整流线性单元（PReLU）和指数线性单元（ELU）的优点，从而导致更好的分类性能和收敛性。另外，权重的初始化对培养非常深的网络非常重要。现有的方法为使用整流线性单元的网络奠定了坚实的基础，而不是指数线性单元。本文补充了现有的理论，并将其扩展到更广泛的范围。具体来说，我们提出了一种初始化的方法，使用指数线性单位训练非常深的网络。实验表明，所提出的初始化不仅有助于训练过程，而且导致更好的泛化性能。最后，利用提出的激活函数和初始化，我们提出了一个深度MPELU残余体系结构，在CIFAR-10/100数据集上实现了最先进的性能。该代码可在此https网址获得

##### URL
[https://arxiv.org/abs/1606.00305](https://arxiv.org/abs/1606.00305)

##### PDF
[https://arxiv.org/pdf/1606.00305](https://arxiv.org/pdf/1606.00305)

