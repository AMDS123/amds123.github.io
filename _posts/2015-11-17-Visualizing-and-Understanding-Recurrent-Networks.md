---
layout: post
title: "Visualizing and Understanding Recurrent Networks"
date: 2015-11-17 02:42:24
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model Prediction
author: Andrej Karpathy, Justin Johnson, Li Fei-Fei
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.

##### Abstract (translated by Google)
递归神经网络（RNN），尤其是具有长期短期记忆（LSTM）的变体，由于在涉及顺序数据的广泛的机器学习问题中的成功应用，正在重新获得兴趣。然而，尽管LSTM在实践中提供了卓越的成果，但是它们的表现来源和局限性仍然不甚了解。使用字符级语言模型作为可解释的测试平台，我们的目标是通过提供对它们的表示，预测和错误类型的分析来填补这个空白。特别是，我们的实验揭示了可解释单元的存在，这些单元可以跟踪长度依赖性，例如行长，引号和括号。此外，我们与有限时域n-gram模型的比较分析，追溯了长程结构依赖关系的LSTM改进的来源。最后，我们提供剩余错误分析，并提出需要进一步研究的领域。

##### URL
[https://arxiv.org/abs/1506.02078](https://arxiv.org/abs/1506.02078)

##### PDF
[https://arxiv.org/pdf/1506.02078](https://arxiv.org/pdf/1506.02078)

