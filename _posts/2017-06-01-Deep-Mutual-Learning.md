---
layout: post
title: "Deep Mutual Learning"
date: 2017-06-01 16:57:15
categories: arXiv_CV
tags: arXiv_CV Re-identification Knowledge Person_Re-identification Recognition
author: Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu
mathjax: true
---

* content
{:toc}

##### Abstract
Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, that is better suited to low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy where, rather than one way transfer between a static pre-defined teacher and a student, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on CIFAR-100 recognition and Market-1501 person re-identification benchmarks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.

##### Abstract (translated by Google)
模型蒸馏是将知识从教师传递给学生网络的有效和广泛使用的技术。典型的应用是从一个强大的大型网络或整体转移到一个小型网络，这更适合于低内存或快速执行的要求。在本文中，我们提出了一个深度相互学习（DML）策略，而不是静态的预先定义的老师和学生之间的一种方式转移，学生的合奏在整个培训过程中协同学习和相互教导。我们的实验表明，各种网络架构从互相学习中受益，并在CIFAR-100识别和Market-1501人员重新识别基准方面取得令人瞩目的结果。令人惊讶的是，它揭示了没有先有的强大的教师网络是必要的 - 一个简单的学生网络集合的相互学习是有效的，而且胜过一个更强大而静止的老师的蒸馏。

##### URL
[https://arxiv.org/abs/1706.00384](https://arxiv.org/abs/1706.00384)

##### PDF
[https://arxiv.org/pdf/1706.00384](https://arxiv.org/pdf/1706.00384)

