---
layout: post
title: "Learning Perceptually-Aligned Representations via Adversarial Robustness"
date: 2019-06-03 17:55:20
categories: arXiv_CV
tags: arXiv_CV Adversarial Optimization
author: Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Aleksander Madry
mathjax: true
---

* content
{:toc}

##### Abstract
Many applications of machine learning require models that are human-aligned, i.e., that make decisions based on human-meaningful information about the input. We identify the pervasive brittleness of deep networks' learned representations as a fundamental barrier to attaining this goal. We then re-cast robust optimization as a tool for enforcing human priors on the features learned by deep neural networks. The resulting robust feature representations turn out to be significantly more aligned with human perception. We leverage these representations to perform input interpolation, feature manipulation, and sensitivity mapping, without any post-processing or human intervention after model training. Our code and models for reproducing these results is available at https://git.io/robust-reps.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.00945](http://arxiv.org/abs/1906.00945)

##### PDF
[http://arxiv.org/pdf/1906.00945](http://arxiv.org/pdf/1906.00945)

