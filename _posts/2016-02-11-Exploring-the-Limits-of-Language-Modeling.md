---
layout: post
title: "Exploring the Limits of Language Modeling"
date: 2016-02-11 23:01:48
categories: arXiv_SD
tags: arXiv_SD CNN RNN Language_Model
author: Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu
mathjax: true
---

* content
{:toc}

##### Abstract
In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.

##### Abstract (translated by Google)
在这项工作中，我们探讨了递归神经网络在大规模语言建模方面的最新进展，这是语言理解的核心任务。我们扩展现有的模型来处理这个任务中存在的两个关键挑战：语料库和词汇量，以及复杂的，长期的语言结构。我们对十亿字基准测试技术进行了详尽的研究，如字符卷积神经网络或长期短期记忆。我们最好的单一模型显着地将最新的困惑从51.3降低到了30.0（同时将参数数量减少了20倍），而模型集合通过将困惑从41.0降低到23.7，创造了新的纪录。我们也为NLP和ML社区发布这些模型来学习和改进。

##### URL
[https://arxiv.org/abs/1602.02410](https://arxiv.org/abs/1602.02410)

##### PDF
[https://arxiv.org/pdf/1602.02410](https://arxiv.org/pdf/1602.02410)

