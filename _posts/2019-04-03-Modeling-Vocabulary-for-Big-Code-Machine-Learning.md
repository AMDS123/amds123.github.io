---
layout: post
title: "Modeling Vocabulary for Big Code Machine Learning"
date: 2019-04-03 09:27:57
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Hlib Babii, Andrea Janes, Romain Robbes
mathjax: true
---

* content
{:toc}

##### Abstract
When building machine learning models that operate on source code, several decisions have to be made to model source-code vocabulary. These decisions can have a large impact: some can lead to not being able to train models at all, others significantly affect performance, particularly for Neural Language Models. Yet, these decisions are not often fully described. This paper lists important modeling choices for source code vocabulary, and explores their impact on the resulting vocabulary on a large-scale corpus of 14,436 projects. We show that a subset of decisions have decisive characteristics, allowing to train accurate Neural Language Models quickly on a large corpus of 10,106 projects.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1904.01873](https://arxiv.org/abs/1904.01873)

##### PDF
[https://arxiv.org/pdf/1904.01873](https://arxiv.org/pdf/1904.01873)

