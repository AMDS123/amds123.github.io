---
layout: post
title: "Towards Neural Machine Translation with Latent Tree Attention"
date: 2017-09-06 17:44:53
categories: arXiv_CL
tags: arXiv_CL Segmentation Attention Reinforcement_Learning RNN
author: James Bradbury, Richard Socher
mathjax: true
---

* content
{:toc}

##### Abstract
Building models that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in natural language processing. We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.

##### Abstract (translated by Google)
在没有先验注释的情况下构建利用语言层级结构的模型是自然语言处理中的一个长期目标。我们引入这样的机器翻译任务模型，将递归神经网络语法编码器与新颖的注意力RNNG解码器配对，并应用策略梯度强化学习来诱导源和目标上的无监督树结构。当在没有明确分割或解析注释的字符级数据集上进行训练时，该模型学习合理的分割和浅层解析，获得接近注意基线的性能。

##### URL
[https://arxiv.org/abs/1709.01915](https://arxiv.org/abs/1709.01915)

##### PDF
[https://arxiv.org/pdf/1709.01915](https://arxiv.org/pdf/1709.01915)

