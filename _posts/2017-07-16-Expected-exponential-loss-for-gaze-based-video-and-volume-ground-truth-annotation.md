---
layout: post
title: "Expected exponential loss for gaze-based video and volume ground truth annotation"
date: 2017-07-16 16:22:12
categories: arXiv_CV
tags: arXiv_CV Segmentation
author: Laurent Lejeune, Mario Christoudias, Raphael Sznitman
mathjax: true
---

* content
{:toc}

##### Abstract
Many recent machine learning approaches used in medical imaging are highly reliant on large amounts of image and ground truth data. In the context of object segmentation, pixel-wise annotations are extremely expensive to collect, especially in video and 3D volumes. To reduce this annotation burden, we propose a novel framework to allow annotators to simply observe the object to segment and record where they have looked at with a \$200 eye gaze tracker. Our method then estimates pixel-wise probabilities for the presence of the object throughout the sequence from which we train a classifier in semi-supervised setting using a novel Expected Exponential loss function. We show that our framework provides superior performances on a wide range of medical image settings compared to existing strategies and that our method can be combined with current crowd-sourcing paradigms as well.

##### Abstract (translated by Google)
医学成像中使用的许多最近的机器学习方法高度依赖于大量的图像和地面真实数据。在对象分割的情况下，以像素为单位的注释收集起来非常昂贵，尤其是在视频和3D卷中。为了减轻这个注解的负担，我们提出了一个新的框架，允许注释者只需观察对象就可以用200美元的眼睛注视跟踪器来分割和记录他们所看到的位置。我们的方法然后估计整个序列中的对象存在的像素方式的概率，我们使用新的预期指数损失函数在半监督设置下训练分类器。我们证明，与现有策略相比，我们的框架在广泛的医学图像设置上提供了卓越的性能，并且我们的方法也可以与当前的众包范例相结合。

##### URL
[https://arxiv.org/abs/1707.04905](https://arxiv.org/abs/1707.04905)

##### PDF
[https://arxiv.org/pdf/1707.04905](https://arxiv.org/pdf/1707.04905)

