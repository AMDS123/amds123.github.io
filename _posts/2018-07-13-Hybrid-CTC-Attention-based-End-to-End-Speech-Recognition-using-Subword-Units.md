---
layout: post
title: "Hybrid CTC-Attention based End-to-End Speech Recognition using Subword Units"
date: 2018-07-13 09:06:07
categories: arXiv_CL
tags: arXiv_CL Attention Speech_Recognition Language_Model Recognition
author: Zhangyu Xiao, Zhijian Ou, Wei Chu, Hui Lin
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we present an end-to-end automatic speech recognition system, which successfully employs subword units in a hybrid CTC-Attention based system. The subword units are obtained by the byte-pair encoding (BPE) compression algorithm. Compared to using words as modeling units, using characters or subword units does not suffer from the out-of-vocabulary (OOV) problem. Furthermore, using subword units further offers a capability in modeling longer context than using characters. We evaluate different systems over the LibriSpeech 1000h dataset. The subword-based hybrid CTC-Attention system obtains 6.8% word error rate (WER) on the test_clean subset without any dictionary or external language model. This represents a significant improvement (a 12.8% WER relative reduction) over the character-based hybrid CTC-Attention system.

##### Abstract (translated by Google)
在本文中，我们提出了一种端到端的自动语音识别系统，该系统在基于混合CTC-Attention的系统中成功地使用了子字单元。子字单元通过字节对编码（BPE）压缩算法获得。与使用单词作为建模单元相比，使用字符或子单元不会出现词汇外（OOV）问题。此外，使用子字单元还提供了比使用字符建模更长上下文的能力。我们通过LibriSpeech 1000h数据集评估不同的系统。基于子字的混合CTC-Attention系统在test_clean子集上获得6.8％的字错误率（WER），而没有任何字典或外部语言模型。与基于字符的混合CTC-Attention系统相比，这代表了显着的改进（相对减少了12.8％WER）。

##### URL
[http://arxiv.org/abs/1807.04978](http://arxiv.org/abs/1807.04978)

##### PDF
[http://arxiv.org/pdf/1807.04978](http://arxiv.org/pdf/1807.04978)

