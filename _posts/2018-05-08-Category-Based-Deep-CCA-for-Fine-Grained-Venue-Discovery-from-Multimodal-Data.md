---
layout: post
title: "Category-Based Deep CCA for Fine-Grained Venue Discovery from Multimodal Data"
date: 2018-05-08 13:17:57
categories: arXiv_CV
tags: arXiv_CV Attention Deep_Learning Relation
author: Yi Yu, Suhua Tang, Kiyoharu Aizawa, Akiko Aizawa
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, travel destination and business location are taken as venues. Discovering a venue by a photo is very important for context-aware applications. Unfortunately, few efforts paid attention to complicated real images such as venue photos generated by users. Our goal is fine-grained venue discovery from heterogeneous social multimodal data. To this end, we propose a novel deep learning model, Category-based Deep Canonical Correlation Analysis (C-DCCA). Given a photo as input, this model performs (i) exact venue search (find the venue where the photo was taken), and (ii) group venue search (find relevant venues with the same category as that of the photo), by the cross-modal correlation between the input photo and textual description of venues. In this model, data in different modalities are projected to a same space via deep networks. Pairwise correlation (between different modal data from the same venue) for exact venue search and category-based correlation (between different modal data from different venues with the same category) for group venue search are jointly optimized. Because a photo cannot fully reflect rich text description of a venue, the number of photos per venue in the training phase is increased to capture more aspects of a venue. We build a new venue-aware multimodal dataset by integrating Wikipedia featured articles and Foursquare venue photos. Experimental results on this dataset confirm the feasibility of the proposed method. Moreover, the evaluation over another publicly available dataset confirms that the proposed method outperforms state-of-the-arts for cross-modal retrieval between image and text.

##### Abstract (translated by Google)
在这项工作中，旅游目的地和商业地点被视为场地。通过照片发现场地对于上下文感知应用程序非常重要。不幸的是，很少有人注意复杂的真实图像，例如用户生成的场地照片。我们的目标是利用异构社会多模式数据进行细粒度的场地发现。为此，我们提出了一种新型的深度学习模型，即基于分类的深典型相关分析（C-DCCA）。给定一张照片作为输入，该模型执行（i）确切的场地搜索（找到拍摄照片的地点），和（ii）小组场地搜索（找到与照片相同类别的相关场地）输入照片和场地文字描述之间的跨模式关联。在这个模型中，不同形式的数据通过深度网络投影到同一个空间。用于团体场地搜索的精确场地搜索和基于类别的相关性（来自同一类别的不同场地的不同模态数据）之间的成对关联（来自同一场地的不同模态数据）被共同优化。由于照片无法充分反映场地的丰富文字描述，因此在训练阶段每个场地的照片数量会增加，以捕捉场地的更多方面。我们通过整合维基百科特色文章和Foursquare场地照片，建立一个新的场地感知多模态数据集。该数据集的实验结果证实了该方法的可行性。此外，对另一个公开可用数据集的评估证实，所提出的方法在图像和文本之间的跨模式检索方面优于现有技术。

##### URL
[https://arxiv.org/abs/1805.02997](https://arxiv.org/abs/1805.02997)

##### PDF
[https://arxiv.org/pdf/1805.02997](https://arxiv.org/pdf/1805.02997)

