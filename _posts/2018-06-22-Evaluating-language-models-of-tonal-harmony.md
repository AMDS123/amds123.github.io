---
layout: post
title: "Evaluating language models of tonal harmony"
date: 2018-06-22 15:18:54
categories: arXiv_SD
tags: arXiv_SD RNN Language_Model Prediction
author: David R. W. Sears, Filip Korzeniowski, Gerhard Widmer
mathjax: true
---

* content
{:toc}

##### Abstract
This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most state-of-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. 
 Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types.

##### Abstract (translated by Google)
本研究从自然语言处理中借鉴并扩展了概率语言模型，以发现音调和声的句法特征。语言模型的形式和大小各不相同，但其中心目的总是相同的：用字母，单词，音符或和弦预测下一个事件。然而，使用这种模型的研究很少使用西方音调音乐的大规模语料库对最先进的架构进行评估，而是倾向于使用包含爵士，流行音乐和摇滚等当代流派的和弦注释的相对较小的数据集。
 本研究采用灵活的数据驱动编码方案，以（1）评估和弦预测中的有限上下文（或n-gram）模型和递归神经网络（RNN）的符号表示，任务; （2）比较来自每个所选数据集的和弦起始点的最佳执行模型的预测准确度;和（3）在回归分析中解释两种模型体系结构之间的差异。我们发现使用部分匹配预测（PPM）算法的有限上下文模型优于RNN，尤其是对于钢琴数据集，其回归模型表明RNN与特别罕见的和弦类型相互作用。

##### URL
[http://arxiv.org/abs/1806.08724](http://arxiv.org/abs/1806.08724)

##### PDF
[http://arxiv.org/pdf/1806.08724](http://arxiv.org/pdf/1806.08724)

