---
layout: post
title: "Dual Attention Networks for Multimodal Reasoning and Matching"
date: 2017-03-21 05:12:02
categories: arXiv_CV
tags: arXiv_CV QA Attention Inference VQA
author: Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim
mathjax: true
---

* content
{:toc}

##### Abstract
We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.

##### Abstract (translated by Google)
我们提出双重注意力网络（DANs），它共同利用视觉和文本注意机制来捕捉视觉和语言之间细微的相互作用。 DAN通过多个步骤处理图像中的特定区域和文本中的单词，并从两种模式中收集基本信息。基于此框架，我们分别介绍了两种类型的DAN，用于多模态推理和匹配。推理模型允许视觉和文本注意在协作推理期间相互引导，这对于诸如视觉问题回答（VQA）之类的任务是有用的。此外，匹配模型利用两种注意机制，通过关注它们的共享语义来估计图像和句子之间的相似性。我们广泛的实验验证了DAN在视觉和语言相结合方面的有效性，在VQA和图像文本匹配的公共基准上实现了最先进的性能。

##### URL
[https://arxiv.org/abs/1611.00471](https://arxiv.org/abs/1611.00471)

##### PDF
[https://arxiv.org/pdf/1611.00471](https://arxiv.org/pdf/1611.00471)

