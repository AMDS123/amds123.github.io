---
layout: post
title: "Learning Word Vectors for 157 Languages"
date: 2018-02-19 22:32:47
categories: arXiv_CL
tags: arXiv_CL
author: Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, Tomas Mikolov
mathjax: true
---

* content
{:toc}

##### Abstract
Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.

##### Abstract (translated by Google)
最近，分布式词表示或词向量已应用于自然语言处理中的许多任务，从而实现了最先进的性能。成功应用这些表示的关键因素是在非常大的语料库上训练它们，并将这些预先训练好的模型用于下游任务。在本文中，我们描述了我们如何为157种语言培训如此高质量的词汇表达。我们使用两种数据来训练这些模型：免费的在线百科全书维基百科和来自普通爬行项目的数据。我们还介绍了三个新的单词比喻数据集来评估这些单词向量，法语，印地语和波兰语。最后，我们评估预先训练的10个语言的词汇向量，其中存在评估数据集，与以前的模型相比，表现出非常强的性能。

##### URL
[http://arxiv.org/abs/1802.06893](http://arxiv.org/abs/1802.06893)

##### PDF
[http://arxiv.org/pdf/1802.06893](http://arxiv.org/pdf/1802.06893)

