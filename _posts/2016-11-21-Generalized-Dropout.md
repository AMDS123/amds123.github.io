---
layout: post
title: "Generalized Dropout"
date: 2016-11-21 14:06:48
categories: arXiv_CV
tags: arXiv_CV Inference Deep_Learning
author: Suraj Srinivas, R. Venkatesh Babu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.

##### Abstract (translated by Google)
深度神经网络通常需要良好的正则化器来推广。辍学就是这样一个在深度学习者中广泛使用的正规化者。最近的工作表明，丢失也可以被视为在网络参数上执行近似贝叶斯推理。在这项工作中，我们概括了这个概念，并且介绍了一个我们称之为广义辍学的正规化家族。本系列中的一组方法称为Dropout ++，它是一个带有可训练参数的Dropout版本。古典辍学是这种方法的特例。这个家族的另一个成员选择神经网络层的宽度。实验表明，这些方法有助于提高Dropout的泛化性能。

##### URL
[https://arxiv.org/abs/1611.06791](https://arxiv.org/abs/1611.06791)

##### PDF
[https://arxiv.org/pdf/1611.06791](https://arxiv.org/pdf/1611.06791)

