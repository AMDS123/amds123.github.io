---
layout: post
title: "Building an Evaluation Scale using Item Response Theory"
date: 2016-09-23 16:35:16
categories: arXiv_CL
tags: arXiv_CL
author: John P. Lalor, Hao Wu, Hong Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (accuracy/precision/recall/F1). The current assumption is that all items in a given test set are equal with regards to difficulty and discriminating power. We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation. IRT is able to describe characteristics of individual items - their difficulty and discriminating power - and can account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment. By collecting a large number of human responses and fitting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics. We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern.

##### Abstract (translated by Google)
对NLP方法的评估需要根据以前审查过的黄金标准测试集和报告标准度量（准确度/精确度/召回率/ F1）进行测试。目前的假设是，给定测试集中的所有项目在难度和区分能力方面是相等的。我们从心理测量学提出项目反应理论（IRT）作为黄金标准测试集生成和NLP系统评估的替代手段。 IRT能够描述单个项目的特征 - 它们的难度和区分能力 - 并且可以在估算人类智能或NLP任务能力时考虑到这些特征。在本文中，我们通过生成一个用于识别语境的黄金标准测试集来演示IRT。通过收集大量的人类反应并拟合我们的IRT模型，我们展示了我们的IRT模型将NLP系统与人群中的表现进行比较，并且能够提供比标准评估指标更多的系统性能洞察。我们表明，高准确性分数并不总是意味着高IRT分数，这取决于项目的特点和响应模式。

##### URL
[https://arxiv.org/abs/1605.08889](https://arxiv.org/abs/1605.08889)

##### PDF
[https://arxiv.org/pdf/1605.08889](https://arxiv.org/pdf/1605.08889)

