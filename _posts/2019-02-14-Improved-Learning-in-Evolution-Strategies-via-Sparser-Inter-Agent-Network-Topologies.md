---
layout: post
title: "Improved Learning in Evolution Strategies via Sparser Inter-Agent Network Topologies"
date: 2019-02-14 21:34:40
categories: arXiv_AI
tags: arXiv_AI Sparse Reinforcement_Learning Deep_Learning
author: Dhaval Adjodah, Dan Calacci, Yan Leng, Peter Krafft, Esteban Moro, Alex Pentland
mathjax: true
---

* content
{:toc}

##### Abstract
We draw upon a previously largely untapped literature on human collective intelligence as a source of inspiration for improving deep learning. Implicit in many algorithms that attempt to solve Deep Reinforcement Learning (DRL) tasks is the network of processors along which parameter values are shared. So far, existing approaches have implicitly utilized fully-connected networks, in which all processors are connected. However, the scientific literature on human collective intelligence suggests that complete networks may not always be the most effective information network structures for distributed search through complex spaces. Here we show that alternative topologies can improve deep neural network training: we find that sparser networks learn higher rewards faster, leading to learning improvements at lower communication costs.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1711.11180](http://arxiv.org/abs/1711.11180)

##### PDF
[http://arxiv.org/e-print/1711.11180](http://arxiv.org/e-print/1711.11180)

