---
layout: post
title: "Multiple-Step Greedy Policies in Online and Approximate Reinforcement Learning"
date: 2018-05-21 09:17:09
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor
mathjax: true
---

* content
{:toc}

##### Abstract
Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work \cite{efroni2018beyond}, multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator.

##### Abstract (translated by Google)
通过使用蒙特卡洛树搜索或模型预测控制，多步先行策略在强化学习中表现出高度的经验能力。在最近的工作中，提出并分析了多步贪婪策略及其在香草策略迭代算法中的应用。在这项工作中，我们在更实际的设置中研究多步贪心算法。我们首先强调一下与软政策更新相反的直观困难：即使在没有近似值的情况下，与单步贪婪案例相反，除非更新步长足够大，否则不会保证单调政策的改进。特别关注这个困难，我们制定和分析使用这种多步贪婪算子的在线和近似算法。

##### URL
[https://arxiv.org/abs/1805.07956](https://arxiv.org/abs/1805.07956)

##### PDF
[https://arxiv.org/pdf/1805.07956](https://arxiv.org/pdf/1805.07956)

