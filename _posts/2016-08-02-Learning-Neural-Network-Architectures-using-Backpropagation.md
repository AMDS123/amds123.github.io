---
layout: post
title: "Learning Neural Network Architectures using Backpropagation"
date: 2016-08-02 11:46:48
categories: arXiv_CV
tags: arXiv_CV Prediction
author: Suraj Srinivas, R. Venkatesh Babu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.

##### Abstract (translated by Google)
具有数百万参数的深度神经网络是当今许多最先进的机器学习模型的核心。然而，最近的研究表明，参数数量少得多的模型也可以执行。在这项工作中，我们介绍了建筑学习的问题，学习神经网络的体系结构和权重。我们引入一个新的可训练参数，称为三态ReLU，它有助于消除不必要的神经元。我们还提出了一个平滑的正规化器，鼓励消除后的神经元总数很少。由此产生的目标是可微和简单的优化。我们在小型和大型网络上实验验证了我们的方法，并证明它可以学习具有相当少量参数的模型，而不会影响预测精度。

##### URL
[https://arxiv.org/abs/1511.05497](https://arxiv.org/abs/1511.05497)

##### PDF
[https://arxiv.org/pdf/1511.05497](https://arxiv.org/pdf/1511.05497)

