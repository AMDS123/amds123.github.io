---
layout: post
title: "Sparse Code Formation with Linear Inhibition"
date: 2015-03-13 15:45:11
categories: arXiv_CV
tags: arXiv_CV Sparse Recognition
author: Nam Do-Hoang Le
mathjax: true
---

* content
{:toc}

##### Abstract
Sparse code formation in the primary visual cortex (V1) has been inspiration for many state-of-the-art visual recognition systems. To stimulate this behavior, networks are trained networks under mathematical constraint of sparsity or selectivity. In this paper, the authors exploit another approach which uses lateral interconnections in feature learning networks. However, instead of adding direct lateral interconnections among neurons, we introduce an inhibitory layer placed right after normal encoding layer. This idea overcomes the challenge of computational cost and complexity on lateral networks while preserving crucial objective of sparse code formation. To demonstrate this idea, we use sparse autoencoder as normal encoding layer and apply inhibitory layer. Early experiments in visual recognition show relative improvements over traditional approach on CIFAR-10 dataset. Moreover, simple installment and training process using Hebbian rule allow inhibitory layer to be integrated into existing networks, which enables further analysis in the future.

##### Abstract (translated by Google)
初级视觉皮层（V1）中的稀疏编码形成了许多最新的视觉识别系统的灵感。为了刺激这种行为，网络是在稀疏性或选择性的数学约束下训练的网络。在本文中，作者利用另一种在特征学习网络中使用横向互连的方法。然而，我们不是增加神经元之间的直接横向互连，而是在正常编码层之后引入一个抑制层。这个想法克服了横向网络的计算成本和复杂性的挑战，同时保留了稀疏编码形成的关键目标。为了证明这个想法，我们使用稀疏的自编码器作为正常的编码层并应用抑制层。在CIFAR-10数据集上，早期的视觉识别实验显示出相对于传统方法的相对改进。此外，使用Hebbian规则的简单安装和培训过程允许将抑制层集成到现有网络中，这使得将来能够进一步分析。

##### URL
[https://arxiv.org/abs/1503.04115](https://arxiv.org/abs/1503.04115)

##### PDF
[https://arxiv.org/pdf/1503.04115](https://arxiv.org/pdf/1503.04115)

