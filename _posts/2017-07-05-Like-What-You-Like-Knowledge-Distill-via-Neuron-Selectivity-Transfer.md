---
layout: post
title: "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer"
date: 2017-07-05 05:44:02
categories: arXiv_CV
tags: arXiv_CV Knowledge Attention
author: Zehao Huang, Naiyan Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expenses of high storage and computational cost. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results.

##### Abstract (translated by Google)
尽管深度神经网络在各种应用中表现出了非凡的力量，但其优越的性能是以高存储和计算成本为代价的。因此，神经网络的加速和压缩最近引起了很多关注。知识转移（KT）是一种流行的解决方案，旨在通过从较大的教师模式中传授知识来培训较小的学生网络。在本文中，我们提出了一种新的知识转移方法，将其视为分配匹配问题。特别是，我们匹配老师和学生网络之间神经元选择性模式的分布。为了实现这个目标，我们通过最小化这些分布之间的最大平均差异（MMD）度量来设计新的KT损失函数。结合原有的损失函数，我们的方法可以显着提高学生网络的性能。我们验证了我们的方法在多个数据集上的有效性，并进一步将其与其他KT方法结合起来，以探索最佳的结果。

##### URL
[https://arxiv.org/abs/1707.01219](https://arxiv.org/abs/1707.01219)

##### PDF
[https://arxiv.org/pdf/1707.01219](https://arxiv.org/pdf/1707.01219)

