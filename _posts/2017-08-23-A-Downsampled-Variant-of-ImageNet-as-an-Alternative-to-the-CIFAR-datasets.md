---
layout: post
title: "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets"
date: 2017-08-23 16:06:20
categories: arXiv_CV
tags: arXiv_CV
author: Patryk Chrabaszcz, Ilya Loshchilov, Frank Hutter
mathjax: true
---

* content
{:toc}

##### Abstract
The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32$\times$32 (and its variants ImageNet64$\times$64 and ImageNet16$\times$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32$\times$32 pixels per image (64$\times$64 and 16$\times$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at this http URL and this https URL

##### Abstract (translated by Google)
原始的ImageNet数据集是训练深度神经网络的一个流行的大规模基准。由于对原始数据集进行实验（如算法设计，体系结构搜索和超参数调整）的成本可能会过高，因此我们建议考虑采用下采样版本的ImageNet。与CIFAR数据集和较早的下采样版本的ImageNet相比，我们提出的ImageNet32 $ \ times $ 32（及其变体ImageNet64 $ \ times $ 64和ImageNet16 $ \ times $ 16）包含与ImageNet完全相同的类和图像数量，只不同的是，图像下采样为每个图像32 $ \ times 32像素（分别为64 $ \ times $ 64和16 $ \ times $ 16像素）。这些下采样变体的实验比原始ImageNet上的实验快得多，下采样数据集相对于最优超参数的特性似乎保持相似。建议的数据集和脚本来重现我们的结果可以在这个http URL和这个https URL上找到

##### URL
[https://arxiv.org/abs/1707.08819](https://arxiv.org/abs/1707.08819)

##### PDF
[https://arxiv.org/pdf/1707.08819](https://arxiv.org/pdf/1707.08819)

