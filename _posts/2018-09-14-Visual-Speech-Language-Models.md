---
layout: post
title: "Visual Speech Language Models"
date: 2018-09-14 11:07:32
categories: arXiv_SD
tags: arXiv_SD Classification Language_Model
author: Helen L Bear
mathjax: true
---

* content
{:toc}

##### Abstract
Language models (LM) are very powerful in lipreading systems. Language models built upon the ground truth utterances of datasets learn grammar and structure rules of words and sentences (the latter in the case of continuous speech). However, visual co-articulation effects in visual speech signals damage the performance of visual speech LM's as visually, people do not utter what the language model expects. These models are commonplace but while higher-order N-gram LM's may improve classification rates, the cost of this model is disproportionate to the common goal of developing more accurate classifiers. So we compare which unit would best optimize a lipreading (visual speech) LM to observe their limitations. We compare three units; visemes (visual speech units) \cite{lan2010improving}, phonemes (audible speech units), and words.

##### Abstract (translated by Google)
语言模型（LM）在唇读系统中非常强大。建立在数据集的基本真实话语基础上的语言模型学习单词和句子的语法和结构规则（后者在连续语音的情况下）。然而，视觉语音信号中的视觉共同清晰度效果在视觉上损害了视觉语音LM的性能，人们不会说出语言模型所期望的。这些模型很常见，但是高阶N-gram LM可以提高分类率，但这种模型的成本与开发更准确的分类器的共同目标不成比例。因此，我们比较哪个单元最能优化唇读（视觉语音）LM来观察它们的局限性。我们比较三个单位; visemes（视觉语音单元）\ cite {lan2010improving}，音素（可听语音单元）和单词。

##### URL
[http://arxiv.org/abs/1809.06800](http://arxiv.org/abs/1809.06800)

##### PDF
[http://arxiv.org/pdf/1809.06800](http://arxiv.org/pdf/1809.06800)

