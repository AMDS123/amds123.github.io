---
layout: post
title: "Multi-Hypothesis Visual-Inertial Flow"
date: 2018-03-08 12:18:06
categories: arXiv_RO
tags: arXiv_RO Quantitative Detection SLAM
author: E. Jared Shamwell, William D. Nothwang, Donald Perlis
mathjax: true
---

* content
{:toc}

##### Abstract
Estimating the correspondences between pixels in sequences of images is a critical first step for a myriad of tasks including vision-aided navigation (e.g., visual odometry (VO), visual-inertial odometry (VIO), and visual simultaneous localization and mapping (VSLAM)) and anomaly detection. We introduce a new unsupervised deep neural network architecture called the Visual Inertial Flow (VIFlow) network and demonstrate image correspondence and optical flow estimation by an unsupervised multi-hypothesis deep neural network receiving grayscale imagery and extra-visual inertial measurements. VIFlow learns to combine heterogeneous sensor streams and sample from an unknown, un-parametrized noise distribution to generate several (4 or 8 in this work) probable hypotheses on the pixel-level correspondence mappings between a source image and a target image . We quantitatively benchmark VIFlow against several leading vision-only dense correspondence and flow methods and show a substantial decrease in runtime and increase in efficiency compared to all methods with similar performance to state-of-the-art (SOA) dense correspondence matching approaches. We also present qualitative results showing how VIFlow can be used for detecting anomalous independent motion.

##### Abstract (translated by Google)
估计图像序列中像素之间的对应关系是包括视觉辅助导航（例如，视觉测距（VO），视觉惯性测距（VIO）和视觉同时定位和映射（VSLAM））在内的众多任务的关键的第一步。 ）和异常检测。我们引入了一种称为视觉惯性流（VIFlow）网络的新型无监督深度神经网络架构，并通过接受灰度图像和非视觉惯性测量的无监督多假设深度神经网络来演示图像对应和光流估计。 VIFlow学习将异质传感器流与来自未知非参数化噪声分布的样本进行组合，以在源图像与目标图像之间的像素级对应映射中生成若干（本文中为4或8）可能的假设。我们将VIFlow与几种主要的仅用于视觉的密集通信和流量方法进行了定量比较，并且与具有与最先进（SOA）密集通信匹配方法类似性能的所有方法相比，显示运行时间显着减少并且效率提高。我们还提供了定性结果，说明VIFlow如何用于检测异常独立运动。

##### URL
[https://arxiv.org/abs/1803.05727](https://arxiv.org/abs/1803.05727)

##### PDF
[https://arxiv.org/pdf/1803.05727](https://arxiv.org/pdf/1803.05727)

