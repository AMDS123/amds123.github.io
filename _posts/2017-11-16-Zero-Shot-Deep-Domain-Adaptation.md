---
layout: post
title: "Zero-Shot Deep Domain Adaptation"
date: 2017-11-16 15:06:43
categories: arXiv_CV
tags: arXiv_CV Knowledge Classification
author: Kuan-Chuan Peng, Ziyan Wu, Jan Ernst
mathjax: true
---

* content
{:toc}

##### Abstract
The existing methods of domain adaptation (DA) work under the assumption that the task-relevant target-domain training data is given. However, such assumption can be violated, which is often ignored by the prior works. To tackle this issue, we propose zero-shot deep domain adaptation (ZDDA), which uses the privileged information from the task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation which is not only tailored for the task of interest (TOI) but also close to the target-domain representation. Therefore, the source-domain TOI solution (e.g. the classifier for classification tasks) which is jointly trained with the source-domain representation can be applicable to both the source and target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN RGB-D datasets, we show that ZDDA can perform DA in classification tasks without the access to the task-relevant target-domain training data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classification task by simulating the task-relevant target-domain representations with the task-relevant source-domain data. To the best of our knowledge, ZDDA is the first DA and sensor fusion method which needs no task-relevant target-domain data.

##### Abstract (translated by Google)
现有的领域适应方法（DA）是在假设任务相关的目标领域训练数据给出的情况下工作的。但是，这样的假设是可以被违背的，这在以前的作品中往往被忽略。为了解决这个问题，我们提出了零镜深度域适配（ZDDA），它使用来自与任务无关的双域对的特权信息。 ZDDA学习源域表示，它不仅针对感兴趣的任务（TOI）量身定做，还接近目标域表示。因此，与源域表示共同训练的源域TOI解决方案（例如用于分类任务的分类器）可适用于源表示和目标表示。使用MNIST，Fashion-MNIST，NIST，EMNIST和SUN RGB-D数据集，我们显示ZDDA可以在分类任务中执行DA，而无需访问任务相关的目标域训练数据。通过模拟与任务相关的目标域表示和任务相关的源域数据，我们还扩展了ZDDA在SUN RGB-D场景分类任务中执行传感器融合。据我们所知，ZDDA是第一个DA和传感器融合方法，它不需要任务相关的目标域数据。

##### URL
[https://arxiv.org/abs/1707.01922](https://arxiv.org/abs/1707.01922)

##### PDF
[https://arxiv.org/pdf/1707.01922](https://arxiv.org/pdf/1707.01922)

