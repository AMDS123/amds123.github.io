---
layout: post
title: "Accelerating Deep Learning with Shrinkage and Recall"
date: 2016-09-19 19:27:39
categories: arXiv_CV
tags: arXiv_CV CNN Classification Deep_Learning
author: Shuai Zheng, Abhinav Vishnu, Chris Ding
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.

##### Abstract (translated by Google)
深度学习是一个非常强大的机器学习模型。深度学习训练了多层的大量参数，当数据规模大，架构规模大时，速度非常慢。受加速计算支持向量机（SVM）算法和LASSO中使用的筛选技术的启发，我们提出了一种缩减深度学习与召回（sDLr）的方法来加速深度学习计算。我们使用深度神经网络（DNN），深度信任网络（DBN）和卷积神经网络（CNN）在4个数据集上实验缩减深度学习与召回（sDLr）。结果表明，使用深度学习与召回（sDLr）缩小的加速可以达到2.0以上，同时仍然具有有竞争力的分类性能。

##### URL
[https://arxiv.org/abs/1605.01369](https://arxiv.org/abs/1605.01369)

##### PDF
[https://arxiv.org/pdf/1605.01369](https://arxiv.org/pdf/1605.01369)

