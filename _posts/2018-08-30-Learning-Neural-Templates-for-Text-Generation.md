---
layout: post
title: "Learning Neural Templates for Text Generation"
date: 2018-08-30 05:15:42
categories: arXiv_CL
tags: arXiv_CL Text_Generation
author: Sam Wiseman, Stuart M. Shieber, Alexander M. Rush
mathjax: true
---

* content
{:toc}

##### Abstract
While neural, encoder-decoder models have had significant empirical success in text generation, there remain several unaddressed problems with this style of generation. Encoder-decoder models are largely (a) uninterpretable, and (b) difficult to control in terms of their phrasing or content. This work proposes a neural generation system using a hidden semi-markov model (HSMM) decoder, which learns latent, discrete templates jointly with learning to generate. We show that this model learns useful templates, and that these templates make generation both more interpretable and controllable. Furthermore, we show that this approach scales to real data sets and achieves strong performance nearing that of encoder-decoder text generation models.

##### Abstract (translated by Google)
虽然神经，编码器 - 解码器模型在文本生成方面取得了重大的经验成功，但这种生成方式仍存在一些未解决的问题。编码器 - 解码器模型在很大程度上（a）无法解释，并且（b）难以控制其措辞或内容。这项工作提出了一个神经生成系统，使用隐藏的半马尔可夫模型（HSMM）解码器，它学习潜在的，离散的模板与学习生成。我们展示了该模型学习了有用的模板，并且这些模板使得生成更具可解释性和可控性。此外，我们表明这种方法可以扩展到实际数据集，并实现接近编码器 - 解码器文本生成模型的强大性能。

##### URL
[http://arxiv.org/abs/1808.10122](http://arxiv.org/abs/1808.10122)

##### PDF
[http://arxiv.org/pdf/1808.10122](http://arxiv.org/pdf/1808.10122)

