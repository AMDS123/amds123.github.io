---
layout: post
title: "Person Depth ReID: Robust Person Re-identification with Commodity Depth Sensors"
date: 2017-05-28 01:22:38
categories: arXiv_CV
tags: arXiv_CV Regularization Re-identification Attention Person_Re-identification
author: Nikolaos Karianakis, Zicheng Liu, Yinpeng Chen, Stefano Soatto
mathjax: true
---

* content
{:toc}

##### Abstract
This work targets person re-identification (ReID) from depth sensors such as Kinect. Since depth is invariant to illumination and less sensitive than color to day-by-day appearance changes, a natural question is whether depth is an effective modality for Person ReID, especially in scenarios where individuals wear different colored clothes or over a period of several months. We explore the use of recurrent Deep Neural Networks for learning high-level shape information from low-resolution depth images. In order to tackle the small sample size problem, we introduce regularization and a hard temporal attention unit. The whole model can be trained end to end with a hybrid supervised loss. We carry out a thorough experimental evaluation of the proposed method on three person re-identification datasets, which include side views, views from the top and sequences with varying degree of partial occlusion, pose and viewpoint variations. To that end, we introduce a new dataset with RGB-D and skeleton data. In a scenario where subjects are recorded after three months with new clothes, we demonstrate large performance gains attained using Depth ReID compared to a state-of-the-art Color ReID. Finally, we show further improvements using the temporal attention unit in multi-shot setting.

##### Abstract (translated by Google)
这项工作的目标是从深度传感器（如Kinect）重新识别人（ReID）。由于深度对光照不敏感，对日常外观变化的敏感性不如颜色，所以自然而然的问题是深度是否是ReID的有效模式，特别是在个人穿着不同颜色的衣服或几个月的时间。我们探索使用经常性的深度神经网络从低分辨率深度图像学习高级形状信息。为了解决小样本问题，我们引入正则化和一个困难的时间关注单元。整个模型可以通过混合监督损失进行端对端训练。对三种人体识别数据集进行了全面的实验评估，包括侧面视图，顶部视图和不同程度部分遮挡，姿态和视点变化的序列。为此，我们引入一个带有RGB-D和骨架数据的新数据集。在三个月后用新衣服拍摄主题的情况下，我们展示了使用Depth ReID与最先进的Color ReID相比获得的大幅性能提升。最后，我们在多拍设置中使用时间关注单元展示了进一步的改进。

##### URL
[https://arxiv.org/abs/1705.09882](https://arxiv.org/abs/1705.09882)

##### PDF
[https://arxiv.org/pdf/1705.09882](https://arxiv.org/pdf/1705.09882)

