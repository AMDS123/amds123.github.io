---
layout: post
title: "A Comparative Study on Regularization Strategies for Embedding-based Neural Networks"
date: 2015-08-15 11:16:39
categories: arXiv_CL
tags: arXiv_CL Regularization Embedding
author: Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin
mathjax: true
---

* content
{:toc}

##### Abstract
This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, re-embedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.

##### Abstract (translated by Google)
本文旨在比较不同的正则化策略，以解决基于嵌入的NLP神经网络中的一个普遍现象，即过度拟合。我们选择了两个广泛研究的神经模型和任务作为我们的试验台。我们尝试了几个经常使用或新提出的正则化策略，包括惩罚权重（不包括嵌入），惩罚嵌入，重新嵌入单词和辍学。我们还强调了增量式超参数调整，并结合了不同的正则化。结果提供了关于神经NLP模型的超参数调整的图片。

##### URL
[https://arxiv.org/abs/1508.03721](https://arxiv.org/abs/1508.03721)

##### PDF
[https://arxiv.org/pdf/1508.03721](https://arxiv.org/pdf/1508.03721)

