---
layout: post
title: "AJILE Movement Prediction: Multimodal Deep Learning for Natural Human Neural Recordings and Video"
date: 2017-09-13 01:28:44
categories: arXiv_CV
tags: arXiv_CV Face CNN RNN Deep_Learning Prediction
author: Xin Ru Nancy Wang, Ali Farhadi, Rajesh Rao, Bingni Brunton
mathjax: true
---

* content
{:toc}

##### Abstract
Developing useful interfaces between brains and machines is a grand challenge of neuroengineering. An effective interface has the capacity to not only interpret neural signals, but predict the intentions of the human to perform an action in the near future; prediction is made even more challenging outside well-controlled laboratory experiments. This paper describes our approach to detect and to predict natural human arm movements in the future, a key challenge in brain computer interfacing that has never before been attempted. We introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset; AJILE includes automatically annotated poses of 7 upper body joints for four human subjects over 670 total hours (more than 72 million frames), along with the corresponding simultaneously acquired intracranial neural recordings. The size and scope of AJILE greatly exceeds all previous datasets with movements and electrocorticography (ECoG), making it possible to take a deep learning approach to movement prediction. We propose a multimodal model that combines deep convolutional neural networks (CNN) with long short-term memory (LSTM) blocks, leveraging both ECoG and video modalities. We demonstrate that our models are able to detect movements and predict future movements up to 800 msec before movement initiation. Further, our multimodal movement prediction models exhibit resilience to simulated ablation of input neural signals. We believe a multimodal approach to natural neural decoding that takes context into account is critical in advancing bioelectronic technologies and human neuroscience.

##### Abstract (translated by Google)
开发大脑和机器之间有用的界面是神经工程的一个巨大挑战。一个有效的界面不仅能够解释神经信号，而且能够预测人类在不久的将来执行行动的意图;在良好控制的实验室实验之外，预测变得更加困难。本文描述了我们的方法来检测和预测未来的自然人体手臂运动，这是以前从未尝试过的脑电脑接口方面的关键挑战。我们在长期ECoG（AJILE）数据集中引入了新颖的注释关节; AJILE包括7个上肢关节的自动注释姿势，用于总共670小时（超过7200万画面）的四个人体对象，以及相应的同时采集的颅内神经记录。 AJILE的大小和范围大大超过了所有以前的运动和脑电图（ECoG）数据集，因此可以采取深入的学习方法进行运动预测。我们提出了一种多模式模型，将深度卷积神经网络（CNN）与长期短期记忆（LSTM）模块相结合，利用ECoG和视频模式。我们证明，我们的模型能够检测运动并预测运动开始之前的高达800毫秒的未来运动。此外，我们的多模态运动预测模型展现出对输入神经信号的模拟消融的适应能力。我们相信，在推进生物电子技术和人类神经科学的过程中，考虑背景的自然神经解码多模式方法至关重要。

##### URL
[https://arxiv.org/abs/1709.05939](https://arxiv.org/abs/1709.05939)

##### PDF
[https://arxiv.org/pdf/1709.05939](https://arxiv.org/pdf/1709.05939)

