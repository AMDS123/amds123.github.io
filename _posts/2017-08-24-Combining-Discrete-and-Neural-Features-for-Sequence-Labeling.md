---
layout: post
title: "Combining Discrete and Neural Features for Sequence Labeling"
date: 2017-08-24 05:24:26
categories: arXiv_CL
tags: arXiv_CL Segmentation Attention Embedding Recognition
author: Jie Yang, Zhiyang Teng, Meishan Zhang, Yue Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network models have recently received heated research attention in the natural language processing community. Compared with traditional models with discrete features, neural models have two main advantages. First, they take low-dimensional, real-valued embedding vectors as inputs, which can be trained over large raw data, thereby addressing the issue of feature sparsity in discrete models. Second, deep neural networks can be used to automatically combine input features, and including non-local features that capture semantic patterns that cannot be expressed using discrete indicator features. As a result, neural network models have achieved competitive accuracies compared with the best discrete models for a range of NLP tasks. On the other hand, manual feature templates have been carefully investigated for most NLP tasks over decades and typically cover the most useful indicator pattern for solving the problems. Such information can be complementary the features automatically induced from neural networks, and therefore combining discrete and neural features can potentially lead to better accuracy compared with models that leverage discrete or neural features only. In this paper, we systematically investigate the effect of discrete and neural feature combination for a range of fundamental NLP tasks based on sequence labeling, including word segmentation, POS tagging and named entity recognition for Chinese and English, respectively. Our results on standard benchmarks show that state-of-the-art neural models can give accuracies comparable to the best discrete models in the literature for most tasks and combing discrete and neural features unanimously yield better results.

##### Abstract (translated by Google)
神经网络模型最近在自然语言处理领域受到了热烈的研究关注。与具有离散特征的传统模型相比，神经模型具有两个主要优点。首先将低维的实值嵌入向量作为输入，在大量的原始数据上进行训练，从而解决离散模型中的特征稀疏问题。其次，可以使用深度神经网络来自动组合输入特征，并且包括捕捉不能使用离散指示符特征来表达的语义模式的非局部特征。因此，与一系列NLP任务的最佳离散模型相比，神经网络模型已经实现了竞争精度。另一方面，几十年来人工特征模板已经被大多数NLP任务仔细研究过了，通常涵盖了最有用的指标模式来解决问题。这些信息可以与神经网络自动产生的特征互补，因此，与仅利用离散或神经特征的模型相比，组合离散特征和神经特征可能会导致更好的准确性。在本文中，我们系统地研究了基于序列标注的一系列基本NLP任务的离散和神经特征组合的效果，包括分词，词性标注和命名实体识别。我们在标准基准测试中的结果表明，最先进的神经模型可以为大多数任务提供与文献中最好的离散模型相当的精度，并且将离散和神经特征结合在一起可以产生更好的结果。

##### URL
[https://arxiv.org/abs/1708.07279](https://arxiv.org/abs/1708.07279)

##### PDF
[https://arxiv.org/pdf/1708.07279](https://arxiv.org/pdf/1708.07279)

