---
layout: post
title: "Learning to Embed Sentences Using Attentive Recursive Trees"
date: 2018-11-06 13:12:22
categories: arXiv_CL
tags: arXiv_CL Embedding Deep_Learning
author: Jiaxin Shi, Lei Hou, Juanzi Li, Zhiyuan Liu, Hanwang Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Sentence embedding is an effective feature representation for most deep learning-based NLP tasks. One prevailing line of methods is using recursive latent tree-structured networks to embed sentences with task-specific structures. However, existing models have no explicit mechanism to emphasize task-informative words in the tree structure. To this end, we propose an Attentive Recursive Tree model (AR-Tree), where the words are dynamically located according to their importance in the task. Specifically, we construct the latent tree for a sentence in a proposed important-first strategy, and place more attentive words nearer to the root; thus, AR-Tree can inherently emphasize important words during the bottom-up composition of the sentence embedding. We propose an end-to-end reinforced training strategy for AR-Tree, which is demonstrated to consistently outperform, or be at least comparable to, the state-of-the-art sentence embedding methods on three sentence understanding tasks.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.02338](http://arxiv.org/abs/1811.02338)

##### PDF
[http://arxiv.org/pdf/1811.02338](http://arxiv.org/pdf/1811.02338)

