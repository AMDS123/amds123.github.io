---
layout: post
title: "Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval"
date: 2018-08-23 15:07:52
categories: arXiv_CV
tags: arXiv_CV Embedding
author: Niluthpol Chowdhury Mithun, Rameswar Panda, Evangelos E. Papalexakis, Amit K. Roy-Chowdhury
mathjax: true
---

* content
{:toc}

##### Abstract
Cross-modal retrieval between visual data and natural language description remains a long-standing challenge in multimedia. While recent image-text retrieval methods offer great promise by learning deep representations aligned across modalities, most of these methods are plagued by the issue of training with small-scale datasets covering a limited number of images with ground-truth sentences. Moreover, it is extremely expensive to create a larger dataset by annotating millions of images with sentences and may lead to a biased model. Inspired by the recent success of webly supervised learning in deep neural networks, we capitalize on readily-available web images with noisy annotations to learn robust image-text joint representation. Specifically, our main idea is to leverage web images and corresponding tags, along with fully annotated datasets, in training for learning the visual-semantic joint embedding. We propose a two-stage approach for the task that can augment a typical supervised pair-wise ranking loss based formulation with weakly-annotated web images to learn a more robust visual-semantic embedding. Experiments on two standard benchmark datasets demonstrate that our method achieves a significant performance gain in image-text retrieval compared to state-of-the-art approaches.

##### Abstract (translated by Google)
视觉数据和自然语言描述之间的跨模式检索仍然是多媒体中长期存在的挑战。虽然最近的图像文本检索方法通过学习跨模态的深度表示提供了很大的希望，但是大多数这些方法都受到小规模数据集的训练问题的困扰，这些数据集覆盖了具有地面真实句子的有限数量的图像。此外，通过用句子注释数百万个图像来创建更大的数据集是非常昂贵的，并且可能导致偏向模型。受近期深度神经网络中网络监督学习的成功启发，我们利用具有嘈杂注释的易于获取的网络图像来学习强大的图像 - 文本联合表示。具体来说，我们的主要想法是在学习视觉语义联合嵌入的培训中利用Web图像和相应的标签以及完全注释的数据集。我们提出了一个两阶段的方法来完成这个任务，可以增加一个典型的监督成对排序损失的配方与弱注释的网络图像，以学习更健壮的视觉语义嵌入。在两个标准基准数据集上的实验表明，与最先进的方法相比，我们的方法在图像文本检索中实现了显着的性能提升。

##### URL
[http://arxiv.org/abs/1808.07793](http://arxiv.org/abs/1808.07793)

##### PDF
[http://arxiv.org/pdf/1808.07793](http://arxiv.org/pdf/1808.07793)

