---
layout: post
title: "Perturbation Analysis of Learning Algorithms: A Unifying Perspective on Generation of Adversarial Examples"
date: 2018-12-15 21:48:46
categories: arXiv_AI
tags: arXiv_AI Adversarial Classification Prediction
author: Emilio Rafael Balda, Arash Behboodi, Rudolf Mathar
mathjax: true
---

* content
{:toc}

##### Abstract
Despite the tremendous success of deep neural networks in various learning problems, it has been observed that adding an intentionally designed adversarial perturbation to inputs of these architectures leads to erroneous classification with high confidence in the prediction. In this work, we propose a general framework based on the perturbation analysis of learning algorithms which consists of convex programming and is able to recover many current adversarial attacks as special cases. The framework can be used to propose novel attacks against learning algorithms for classification and regression tasks under various new constraints with closed form solutions in many instances. In particular we derive new attacks against classification algorithms which are shown to achieve comparable performances to notable existing attacks. The framework is then used to generate adversarial perturbations for regression tasks which include single pixel and single subset attacks. By applying this method to autoencoding and image colorization tasks, it is shown that adversarial perturbations can effectively perturb the output of regression tasks as well.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.07385](http://arxiv.org/abs/1812.07385)

##### PDF
[http://arxiv.org/pdf/1812.07385](http://arxiv.org/pdf/1812.07385)

