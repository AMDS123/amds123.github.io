---
layout: post
title: "Dynamic Integration of Background Knowledge in Neural NLU Systems"
date: 2017-10-25 14:54:53
categories: arXiv_CL
tags: arXiv_CL Knowledge QA
author: Dirk Weissenborn, Tomáš Kočiský, Chris Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.

##### Abstract (translated by Google)
理解自然语言需要常识或背景知识，但在大多数神经自然语言理解（NLU）系统中，必要的背景知识是从静态语料库间接获取的。我们为NLU模型中显式背景知识的动态集成开发了一种新的阅读架构。新的与任务无关的读取模块通过以自由文本语句的形式处理背景知识以及特定于任务的输入，为任务特定的NLU体系结构提供细化的词表示。在文件问答（DQA）和识别文本包含（RTE）方面的强大表现证明了我们的方法的有效性和灵活性。分析表明，我们的模型学习有选择性地使用语义上适当的方式来开发知识。

##### URL
[https://arxiv.org/abs/1706.02596](https://arxiv.org/abs/1706.02596)

##### PDF
[https://arxiv.org/pdf/1706.02596](https://arxiv.org/pdf/1706.02596)

