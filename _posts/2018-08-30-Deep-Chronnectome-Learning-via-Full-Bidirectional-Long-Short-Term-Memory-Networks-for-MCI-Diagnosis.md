---
layout: post
title: "Deep Chronnectome Learning via Full Bidirectional Long Short-Term Memory Networks for MCI Diagnosis"
date: 2018-08-30 16:24:11
categories: arXiv_CV
tags: arXiv_CV RNN Classification Memory_Networks
author: Weizheng Yan, Han Zhang, Jing Sui, Dinggang Shen
mathjax: true
---

* content
{:toc}

##### Abstract
Brain functional connectivity (FC) extracted from resting-state fMRI (RS-fMRI) has become a popular approach for disease diagnosis, where discriminating subjects with mild cognitive impairment (MCI) from normal controls (NC) is still one of the most challenging problems. Dynamic functional connectivity (dFC), consisting of time-varying spatiotemporal dynamics, may characterize "chronnectome" diagnostic information for improving MCI classification. However, most of the current dFC studies are based on detecting discrete major brain status via spatial clustering, which ignores rich spatiotemporal dynamics contained in such chronnectome. We propose Deep Chronnectome Learning for exhaustively mining the comprehensive information, especially the hidden higher-level features, i.e., the dFC time series that may add critical diagnostic power for MCI classification. To this end, we devise a new Fully-connected Bidirectional Long Short-Term Memory Network (Full-BiLSTM) to effectively learn the periodic brain status changes using both past and future information for each brief time segment and then fuse them to form the final output. We have applied our method to a rigorously built large-scale multi-site database (i.e., with 164 data from NCs and 330 from MCIs, which can be further augmented by 25 folds). Our method outperforms other state-of-the-art approaches with an accuracy of 73.6% under solid cross-validations. We also made extensive comparisons among multiple variants of LSTM models. The results suggest high feasibility of our method with promising value also for other brain disorder diagnoses.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1808.10383](https://arxiv.org/abs/1808.10383)

##### PDF
[https://arxiv.org/pdf/1808.10383](https://arxiv.org/pdf/1808.10383)

