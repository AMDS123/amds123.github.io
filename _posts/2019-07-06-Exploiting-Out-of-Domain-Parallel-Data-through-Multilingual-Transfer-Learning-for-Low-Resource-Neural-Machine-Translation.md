---
layout: post
title: "Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation"
date: 2019-07-06 02:14:30
categories: arXiv_CL
tags: arXiv_CL Transfer_Learning NMT
author: Aizhan Imankulova, Raj Dabre, Atsushi Fujita, Kenji Imamura
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a novel multilingual multistage fine-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese--Russian pair for benchmarking. Although there are many solutions for low-resource scenarios, such as multilingual NMT and back-translation, we have empirically confirmed their limited success when restricted to in-domain data. We therefore propose to exploit out-of-domain data through transfer learning, by using it to first train a multilingual NMT model followed by multistage fine-tuning on in-domain parallel and back-translated pseudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.03060](http://arxiv.org/abs/1907.03060)

##### PDF
[http://arxiv.org/pdf/1907.03060](http://arxiv.org/pdf/1907.03060)

