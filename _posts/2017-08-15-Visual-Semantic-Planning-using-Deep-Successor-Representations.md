---
layout: post
title: "Visual Semantic Planning using Deep Successor Representations"
date: 2017-08-15 21:13:49
categories: arXiv_CV
tags: arXiv_CV Knowledge Reinforcement_Learning
author: Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi
mathjax: true
---

* content
{:toc}

##### Abstract
A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.

##### Abstract (translated by Google)
现实世界智能代理的关键能力是他们能够规划一系列的行动来实现他们在视觉世界中的目标。在这项工作中，我们解决了视觉语义规划的问题：从视觉观察预测一系列动作的任务，将动态环境从初始状态转换为目标状态。这样做需要关于物体及其可供性的知识，以及行动及其先决条件和效果。我们建议通过与视觉和动态环境交互来学习。我们提出的解决方案涉及引导强化学习与模仿学习。为了确保交叉任务泛化，我们开发了一个基于后继表示的深度预测模型。我们的实验结果表明，在挑战性THOR环境中，广泛的任务都能达到最佳效果。

##### URL
[https://arxiv.org/abs/1705.08080](https://arxiv.org/abs/1705.08080)

##### PDF
[https://arxiv.org/pdf/1705.08080](https://arxiv.org/pdf/1705.08080)

