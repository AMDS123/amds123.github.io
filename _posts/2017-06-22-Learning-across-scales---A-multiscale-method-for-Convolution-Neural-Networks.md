---
layout: post
title: "Learning across scales - A multiscale method for Convolution Neural Networks"
date: 2017-06-22 16:39:14
categories: arXiv_CV
tags: arXiv_CV CNN Relation
author: Eldad Haber, Lars Ruthotto, Elliot Holtham, Seong-Hwan Jun
mathjax: true
---

* content
{:toc}

##### Abstract
In this work we establish the relation between optimal control and training deep Convolution Neural Networks (CNNs). We show that the forward propagation in CNNs can be interpreted as a time-dependent nonlinear differential equation and learning as controlling the parameters of the differential equation such that the network approximates the data-label relation for given training data. Using this continuous interpretation we derive two new methods to scale CNNs with respect to two different dimensions. The first class of multiscale methods connects low-resolution and high-resolution data through prolongation and restriction of CNN parameters. We demonstrate that this enables classifying high-resolution images using CNNs trained with low-resolution images and vice versa and warm-starting the learning process. The second class of multiscale methods connects shallow and deep networks and leads to new training strategies that gradually increase the depths of the CNN while re-using parameters for initializations.

##### Abstract (translated by Google)
在这项工作中，我们建立了最优控制和深度卷积神经网络（CNNs）之间的关系。我们证明了CNNs中的正向传播可以解释为一个时间相关的非线性微分方程，并学习如何控制微分方程的参数，使网络逼近给定训练数据的数据标签关系。使用这种连续的解释，我们推导出两种新的方法来根据两个不同的维度来扩展CNN。第一类多尺度方法通过延长和限制CNN参数来连接低分辨率和高分辨率的数据。我们证明，这使得能够使用用低分辨率图像训练的CNN对高分辨率图像进行分类，反之亦然，并对学习过程进行热启动。第二类多尺度方法连接浅层和深层网络，并导致新的培训策略，逐渐增加CNN的深度，同时重新使用参数进行初始化。

##### URL
[https://arxiv.org/abs/1703.02009](https://arxiv.org/abs/1703.02009)

##### PDF
[https://arxiv.org/pdf/1703.02009](https://arxiv.org/pdf/1703.02009)

