---
layout: post
title: "Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference"
date: 2018-09-10 14:38:47
categories: arXiv_AI
tags: arXiv_AI Represenation_Learning Inference Deep_Learning
author: Yi Tay, Luu Anh Tuan, Siu Cheung Hui
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a $\approx 3$ times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.

##### Abstract (translated by Google)
本文提出了一种新的自然语言推理（NLI）深度学习架构。首先，我们介绍一种新的体系结构，其中比对对被比较，压缩然后传播到上层以增强表示学习。其次，我们采用分解层将对齐矢量有效且富有表现力地压缩为标量特征，然后将其用于增强基本词表示。我们的方法设计旨在从概念上简单，紧凑而强大。我们对三个流行的基准测试进行了实验，SNLI，MultiNLI和SciTail，在所有基准上实现了竞争性能。与现有的最先进型号（例如ESIM和DIIN）相比，我们模型的轻量级参数化还可以减少约3美元的参数尺寸，同时保持竞争性能。此外，视觉分析表明我们的传播特征具有高度可解释性。

##### URL
[http://arxiv.org/abs/1801.00102](http://arxiv.org/abs/1801.00102)

##### PDF
[http://arxiv.org/pdf/1801.00102](http://arxiv.org/pdf/1801.00102)

