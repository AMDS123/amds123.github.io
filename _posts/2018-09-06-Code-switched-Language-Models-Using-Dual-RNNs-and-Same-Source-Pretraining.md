---
layout: post
title: "Code-switched Language Models Using Dual RNNs and Same-Source Pretraining"
date: 2018-09-06 13:12:27
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Saurabh Garg, Tanmay Parekh, Preethi Jyothi
mathjax: true
---

* content
{:toc}

##### Abstract
This work focuses on building language models (LMs) for code-switched text. We propose two techniques that significantly improve these LMs: 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data. We demonstrate the effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English task and derive significant reductions in perplexity.

##### Abstract (translated by Google)
这项工作的重点是为代码转换文本构建语言模型（LM）。我们提出了两种显着改善这些LM的技术：1）一种新的递归神经网络单元，其双重组件分别关注代码转换文本中的每种语言2）使用来自使用训练数据估计的生成模型的合成文本预训练LM 。我们通过报告普通话 - 英语任务的困惑来证明我们提出的技术的有效性，并且可以显着减少困惑。

##### URL
[http://arxiv.org/abs/1809.01962](http://arxiv.org/abs/1809.01962)

##### PDF
[http://arxiv.org/pdf/1809.01962](http://arxiv.org/pdf/1809.01962)

