---
layout: post
title: "Exploring Automated Essay Scoring for Nonnative English Speakers"
date: 2017-09-29 07:08:50
categories: arXiv_CL
tags: arXiv_CL Relation
author: Amber Nigam
mathjax: true
---

* content
{:toc}

##### Abstract
Automated Essay Scoring (AES) has been quite popular and is being widely used. However, lack of appropriate methodology for rating nonnative English speakers' essays has meant a lopsided advancement in this field. In this paper, we report initial results of our experiments with nonnative AES that learns from manual evaluation of nonnative essays. For this purpose, we conducted an exercise in which essays written by nonnative English speakers in test environment were rated both manually and by the automated system designed for the experiment. In the process, we experimented with a few features to learn about nuances linked to nonnative evaluation. The proposed methodology of automated essay evaluation has yielded a correlation coefficient of 0.750 with the manual evaluation.

##### Abstract (translated by Google)
自动化文章评分（AES）已经相当流行，正在被广泛使用。然而，缺乏适当的方法来评估非本族英语人士的散文，这意味着在这个领域的一个不平衡的进步。在本文中，我们报告的初始结果我们的实验与非本能的AES，从人工评价的本体散文学习。为此，我们进行了一个练习，在这个练习中，由测试环境中的非本族英语人士撰写的散文被手工评定，并且通过为实验设计的自动化系统评价。在这个过程中，我们尝试了一些功能来了解与非本体评估有关的细微差别。所提出的自动评价方法与人工评价相关系数为0.750。

##### URL
[https://arxiv.org/abs/1706.03335](https://arxiv.org/abs/1706.03335)

##### PDF
[https://arxiv.org/pdf/1706.03335](https://arxiv.org/pdf/1706.03335)

