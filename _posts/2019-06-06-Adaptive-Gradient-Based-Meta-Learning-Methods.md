---
layout: post
title: "Adaptive Gradient-Based Meta-Learning Methods"
date: 2019-06-06 17:36:34
categories: arXiv_AI
tags: arXiv_AI Optimization Deep_Learning Prediction
author: Mikhail Khodak, Maria Florina-Balcan, Ameet Talwalkar
mathjax: true
---

* content
{:toc}

##### Abstract
We build a theoretical framework for understanding practical meta-learning methods that enables the integration of sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their training and meta-test-time performance on standard problems in few-shot and federated deep learning.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.02717](http://arxiv.org/abs/1906.02717)

##### PDF
[http://arxiv.org/pdf/1906.02717](http://arxiv.org/pdf/1906.02717)

