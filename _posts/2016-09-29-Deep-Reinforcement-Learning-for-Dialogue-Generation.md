---
layout: post
title: "Deep Reinforcement Learning for Dialogue Generation"
date: 2016-09-29 15:02:32
categories: arXiv_CL
tags: arXiv_CL QA Reinforcement_Learning
author: Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky
mathjax: true
---

* content
{:toc}

##### Abstract
Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.

##### Abstract (translated by Google)
最近的对话生成的神经模型提供了很大的希望为会话代理人产生响应，但是往往是短视的，一次一个地预测话语，而忽略它们对未来结果的影响。建模对话的未来方向对于产生连贯而有趣的对话至关重要，这种对话导致传统的NLP对话模型借鉴强化学习。在本文中，我们展示了如何整合这些目标，应用深入的强化学习来为chatbot对话中的未来奖励建模。该模型模拟两个虚拟代理之间的对话，使用策略梯度方法来奖励显示三个有用会话属性的序列：信息性（非重复性转向），连贯性和易于回答（与前瞻性函数有关）。我们评估我们的模型的多样性，长度以及与人类评委，表明提出的算法产生更多的互动反应，并设法促进对话模拟更持久的对话。这项工作标志着学习基于对话长期成功的神经对话模型的第一步。

##### URL
[https://arxiv.org/abs/1606.01541](https://arxiv.org/abs/1606.01541)

##### PDF
[https://arxiv.org/pdf/1606.01541](https://arxiv.org/pdf/1606.01541)

