---
layout: post
title: "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning"
date: 2018-02-05 18:36:19
categories: arXiv_AI
tags: arXiv_AI Knowledge Detection
author: Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, Sergey Levine
mathjax: true
---

* content
{:toc}

##### Abstract
Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.

##### Abstract (translated by Google)
人类和动物能够通过观察他人执行一次技能来学习新的行为。我们考虑允许机器人做同样的事情 - 即使在机器人和被观察的人之间的视角，环境和实施例中存在实质性的区域移位的情况下，也可以从人的原始视频像素学习。这个问题的先前的方法已经手动指定人类和机器人动作如何对应，并经常依赖于明确的人体姿态检测系统。在这项工作中，我们通过使用来自以前各种任务的人类和机器人演示数据，通过元学习来构建先前的知识，从而从人的视频中提出了一次性学习的方法。然后，结合这些先前的知识和只有一个人的视频演示，机器人可以执行人类演示的任务。我们在PR2手臂和Sawyer手臂上展示了实验，证明了在元学习之后，机器人可以学习使用一个执行操纵的人的视频来放置，推动和拾取新的对象。

##### URL
[http://arxiv.org/abs/1802.01557](http://arxiv.org/abs/1802.01557)

##### PDF
[http://arxiv.org/pdf/1802.01557](http://arxiv.org/pdf/1802.01557)

