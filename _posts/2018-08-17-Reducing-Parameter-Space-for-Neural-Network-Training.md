---
layout: post
title: "Reducing Parameter Space for Neural Network Training"
date: 2018-08-17 17:13:01
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Tong Qin, Ling Zhou, Dongbin Xiu
mathjax: true
---

* content
{:toc}

##### Abstract
For neural networks (NNs) with rectified linear unit (ReLU) or binary activation functions, we show that their training can be accomplished in a reduced parameter space. Specifically, the weights in each neuron can be trained on the unit sphere, as opposed to the entire space, and the threshold can be trained in a bounded interval, as opposed to the real line. We show that the NNs in the reduced parameter space are mathematically equivalent to the standard NNs with parameters in the whole space. The reduced parameter space shall facilitate the optimization procedure for the network training, as the search space becomes (much) smaller. We demonstrate the improved training performance using numerical examples.

##### Abstract (translated by Google)
对于具有整流线性单元（ReLU）或二元激活函数的神经网络（NN），我们表明它们的训练可以在减小的参数空间中完成。具体地，每个神经元中的权重可以在单位球上训练，而不是整个空间，并且阈值可以在有界区间中训练，而不是实线。我们证明了简化参数空间中的NN在数学上等同于在整个空间中具有参数的标准NN。随着搜索空间变得（更小），减小的参数空间将促进网络训练的优化过程。我们使用数值例子证明了改进的训练表现。

##### URL
[http://arxiv.org/abs/1805.08340](http://arxiv.org/abs/1805.08340)

##### PDF
[http://arxiv.org/pdf/1805.08340](http://arxiv.org/pdf/1805.08340)

