---
layout: post
title: "Grounded Textual Entailment"
date: 2018-06-14 16:56:44
categories: arXiv_CV
tags: arXiv_CV Relation
author: Hoa Trong Vu, Claudio Greco, Aliia Erofeeva, Somayeh Jafaritazehjan, Guido Linders, Marc Tanti, Alberto Testoni, Raffaella Bernardi, Albert Gatt
mathjax: true
---

* content
{:toc}

##### Abstract
Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant "world" or "situation"). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare "blind" and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing "grounding" in an optimal fashion.

##### Abstract (translated by Google)
捕获语句之间的语义关系，如蕴含，对计算语义来说是一个长期存在的挑战。基于逻辑的模型根据可能的世界（解释或情况）来分析蕴涵，其中前提P需要假设H iff在P为真的所有世界中，H也是如此。统计模型以概率论的方式来看待这种关系，并根据人类是否可能从P推断出H来解决这个问题。在本文中，我们希望通过争论视觉接近版本的文本完成任务来弥合这两种观点。具体来说，如果除了P和H之外还有一个图像（对应于相关的“世界”或“情境”），我们会问模型是否可以更好地发挥作用。我们使用SNLI数据集的多模式版本（Bowman et al。，2015），我们比较了文本蕴涵的“盲”和视觉增强模型。我们展示视觉信息是有益的，但我们也进行深入的错误分析，揭示当前的多模式模型没有以最佳方式进行“接地”。

##### URL
[http://arxiv.org/abs/1806.05645](http://arxiv.org/abs/1806.05645)

##### PDF
[http://arxiv.org/pdf/1806.05645](http://arxiv.org/pdf/1806.05645)

