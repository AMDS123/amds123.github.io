---
layout: post
title: "Symbolic Tensor Neural Networks for Digital Media - from Tensor Processing via BNF Graph Rules to CREAMS Applications"
date: 2018-09-18 08:25:01
categories: arXiv_CV
tags: arXiv_CV Embedding CNN Recognition
author: Wladyslaw Skarbek
mathjax: true
---

* content
{:toc}

##### Abstract
This tutorial material on Convolutional Neural Networks (CNN) and its applications in digital media research is based on the concept of Symbolic Tensor Neural Networks. The set of STNN expressions is specified in Backus-Naur Form (BNF) which is annotated by constraints typical for labeled acyclic directed graphs (DAG). The BNF induction begins from a collection of neural unit symbols with extra (up to five) decoration fields (including tensor depth and sharing fields). The inductive rules provide not only the general graph structure but also the specific shortcuts for residual blocks of units. A syntactic mechanism for network fragments modularization is introduced via user defined units and their instances. Moreover, the dual BNF rules are specified in order to generate the Dual Symbolic Tensor Neural Network (DSTNN). The joined interpretation of STNN and DSTNN provides the correct flow of gradient tensors, back propagated at the training stage. The proposed symbolic representation of CNNs is illustrated for six generic digital media applications (CREAMS): Compression, Recognition, Embedding, Annotation, 3D Modeling for human-computer interfacing, and data Security based on digital media objects. In order to make the CNN description and its gradient flow complete, for all presented applications, the symbolic representations of mathematically defined loss/gain functions and gradient flow equations for all used core units, are given. The tutorial is to convince the reader that STNN is not only a convenient symbolic notation for public presentations of CNN based solutions for CREAMS problems but also that it is a design blueprint with a potential for automatic generation of application source code.

##### Abstract (translated by Google)
关于卷积神经网络（CNN）及其在数字媒体研究中的应用的本教程材料基于符号张量神经网络的概念。该组STNN表达式在Backus-Naur形式（BNF）中指定，其通过标记的非循环有向图（DAG）的典型约束来注释。 BNF归纳从具有额外（最多五个）装饰场（包括张量深度和共享场）的神经单元符号的集合开始。归纳规则不仅提供了一般的图形结构，还提供了残余单元块的特定快捷方式。通过用户定义的单元及其实例引入网络片段模块化的句法机制。此外，指定双BNF规则以生成双符号张量神经网络（DSTNN）。 STNN和DSTNN的联合解释提供了正确的梯度张量流，在训练阶段反向传播。所提出的CNN的符号表示用于六种通用数字媒体应用（CREAMS）：压缩，识别，嵌入，注释，用于人机接口的3D建模，以及基于数字媒体对象的数据安全性。为了使CNN描述及其梯度流完整，对于所有呈现的应用，给出了所有使用的核心单元的数学定义的损失/增益函数和梯度流动方程的符号表示。本教程旨在说服读者，STNN不仅是基于CNN的CREAMS问题解决方案公开演示的方便符号表示法，而且还是一个可以自动生成应用程序源代码的设计蓝图。

##### URL
[https://arxiv.org/abs/1809.06582](https://arxiv.org/abs/1809.06582)

##### PDF
[https://arxiv.org/pdf/1809.06582](https://arxiv.org/pdf/1809.06582)

