---
layout: post
title: "Learning in Memristive Neural Network Architectures using Analog Backpropagation Circuits"
date: 2018-08-31 08:31:15
categories: arXiv_AI
tags: arXiv_AI Face RNN Gradient_Descent
author: Olga Krestinskaya, Khaled Nabil Salama, Alex Pappachen James
mathjax: true
---

* content
{:toc}

##### Abstract
The on-chip implementation of learning algorithms would speed-up the training of neural networks in crossbar arrays. The circuit level design and implementation of backpropagation algorithm using gradient descent operation for neural network architectures is an open problem. In this paper, we proposed the analog backpropagation learning circuits for various memristive learning architectures, such as Deep Neural Network (DNN), Binary Neural Network (BNN), Multiple Neural Network (MNN), Hierarchical Temporal Memory (HTM) and Long-Short Term Memory (LSTM). The circuit design and verification is done using TSMC 180nm CMOS process models, and TiO2 based memristor models. The application level validations of the system are done using XOR problem, MNIST character and Yale face image databases

##### Abstract (translated by Google)
学习算法的片上实现将加速交叉阵列中神经网络的训练。使用梯度下降操作的神经网络架构的反向传播算法的电路级设计和实现是一个开放的问题。在本文中，我们提出了各种忆阻学习架构的模拟反向传播学习电路，如深度神经网络（DNN），二进制神经网络（BNN），多神经网络（MNN），分层时间记忆（HTM）和长短路术语记忆（LSTM）。电路设计和验证使用TSMC 180nm CMOS工艺模型和基于TiO2的忆阻器模型完成。使用XOR问题，MNIST字符和耶鲁人脸图像数据库完成系统的应用程序级别验证

##### URL
[http://arxiv.org/abs/1808.10631](http://arxiv.org/abs/1808.10631)

##### PDF
[http://arxiv.org/pdf/1808.10631](http://arxiv.org/pdf/1808.10631)

