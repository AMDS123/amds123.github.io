---
layout: post
title: "Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis"
date: 2018-03-23 23:56:49
categories: arXiv_CL
tags: arXiv_CL Style_Transfer Embedding
author: Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Fei Ren, Ye Jia, Rif A. Saurous
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we propose "global style tokens" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable "labels" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.

##### Abstract (translated by Google)
在这项工作中，我们提出了“全球风格标记”（GST），这是一个在Tacotron中共同培训的嵌入库，这是一个最先进的端到端语音合成系统。嵌入训练没有明确的标签，但学会建模大范围的声学表现力。消费税导致了一系列重要结果。他们生成的软解释“标签”可用于以新颖的方式控制合成，如变化速度和讲话风格 - 独立于文本内容。它们也可以用于样式转换，在整个长形文本语料库中复制单个音频剪辑的发言风格。在嘈杂的，未标记的发现数据上进行训练时，GST学会将噪音和说话人身份因数分解，为高度可扩展但强大的语音合成提供了一条途径。

##### URL
[https://arxiv.org/abs/1803.09017](https://arxiv.org/abs/1803.09017)

##### PDF
[https://arxiv.org/pdf/1803.09017](https://arxiv.org/pdf/1803.09017)

