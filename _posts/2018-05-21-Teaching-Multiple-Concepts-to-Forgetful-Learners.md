---
layout: post
title: "Teaching Multiple Concepts to Forgetful Learners"
date: 2018-05-21 23:34:11
categories: arXiv_AI
tags: arXiv_AI Optimization
author: Anette Hunziker, Yuxin Chen, Oisin Mac Aodha, Manuel Gomez Rodriguez, Andreas Krause, Pietro Perona, Yisong Yue, Adish Singla
mathjax: true
---

* content
{:toc}

##### Abstract
How can we help a forgetful learner learn multiple concepts within a limited time frame? For long-term learning, it is crucial to devise teaching strategies that leverage the underlying forgetting mechanisms of the learners. In this paper, we cast the problem of adaptively teaching a forgetful learner as a novel discrete optimization problem, where we seek to optimize a natural objective function that characterizes the learner's expected performance throughout the teaching session. We then propose a simple greedy teaching strategy and derive strong performance guarantees based on two intuitive data-dependent parameters, which characterize the degree of diminishing returns of teaching each concept. We show that, given some assumptions of the learner's memory model, one can efficiently compute the performance bounds. Furthermore, we identify parameter settings of our memory models where greedy is guaranteed to achieve high performance. We have deployed our approach in two concrete applications, namely (1) an educational app for online vocabulary teaching and (2) an app for teaching novices how to recognize bird species. We demonstrate the effectiveness of our algorithm using simulations along with user studies.

##### Abstract (translated by Google)
我们如何才能帮助健忘的学习者在有限的时间内学习多种概念？对于长期学习，制定教学策略来利用学习者的基本遗忘机制至关重要。在本文中，我们将自适应地教导一个健忘的学习者作为一个新颖的离散优化问题，我们试图优化一个自然的目标函数，这个自然目标函数描述了整个教学过程中学习者期望的表现。然后，我们提出一个简单的贪婪教学策略，并基于两个直观的数据相关参数获得强大的性能保证，这些参数描述了教授每个概念的收益递减程度。我们证明，考虑到学习者记忆模型的一些假设，可以有效地计算性能范围。此外，我们确定我们的存储器模型的参数设置，其中贪心保证实现高性能。我们已经在两个具体应用中部署了我们的方法，即（1）在线词汇教学的教育应用程序和（2）教新手如何识别鸟类的应用程序。我们证明了我们的算法使用仿真和用户研究的有效性。

##### URL
[https://arxiv.org/abs/1805.08322](https://arxiv.org/abs/1805.08322)

##### PDF
[https://arxiv.org/pdf/1805.08322](https://arxiv.org/pdf/1805.08322)

