---
layout: post
title: "Grammar Induction with Neural Language Models: An Unusual Replication"
date: 2018-08-29 18:21:50
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Phu Mon Htut, Kyunghyun Cho, Samuel R. Bowman
mathjax: true
---

* content
{:toc}

##### Abstract
A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.

##### Abstract (translated by Google)
最近关于潜在树学习的工作的一个重要主题是尝试开发具有解析值潜在变量的神经网络模型，并在非解析任务上训练它们，以期让它们发现可解释的树结构。在最近的一篇论文中，Shen等人。 （2018）引入这样的模型并报告关于语言建模的目标任务的近乎最先进的结果，以及关于选区解析的第一个强潜在树学习结果。为了重现这些结果，我们发现了使原始结果难以信任的问题，包括调整甚至培训有效的测试集。在这里，我们尝试在公平的实验中重现这些结果，并将它们扩展到两个新的数据集。我们发现这项工作的结果是稳健的：所研究的模型的所有变体都优于所有潜在的树学习基线，并且与符号语法诱导系统竞争性地执行。我们发现这个模型代表了潜在树学习的第一个经验成功，并且神经网络语言建模需要进一步研究作为语法归纳的设置。

##### URL
[http://arxiv.org/abs/1808.10000](http://arxiv.org/abs/1808.10000)

##### PDF
[http://arxiv.org/pdf/1808.10000](http://arxiv.org/pdf/1808.10000)

