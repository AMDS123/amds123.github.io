---
layout: post
title: "Compositionality for Recursive Neural Networks"
date: 2019-01-30 09:32:51
categories: arXiv_AI
tags: arXiv_AI
author: Martha Lewis
mathjax: true
---

* content
{:toc}

##### Abstract
Modelling compositionality has been a longstanding area of research in the field of vector space semantics. The categorical approach to compositionality maps grammar onto vector spaces in a principled way, but comes under fire for requiring the formation of very high-dimensional matrices and tensors, and therefore being computationally infeasible. In this paper I show how a linear simplification of recursive neural tensor network models can be mapped directly onto the categorical approach, giving a way of computing the required matrices and tensors. This mapping suggests a number of lines of research for both categorical compositional vector space models of meaning and for recursive neural network models of compositionality.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1901.10723](http://arxiv.org/abs/1901.10723)

##### PDF
[http://arxiv.org/pdf/1901.10723](http://arxiv.org/pdf/1901.10723)

