---
layout: post
title: "Memory and Information Processing in Recurrent Neural Networks"
date: 2016-04-23 17:36:12
categories: arXiv_CV
tags: arXiv_CV RNN Relation
author: Alireza Goudarzi, Sarah Marzen, Peter Banda, Guy Feldman, Christof Teuscher, Darko Stefanovic
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNN) are simple dynamical systems whose computational power has been attributed to their short-term memory. Short-term memory of RNNs has been previously studied analytically only for the case of orthogonal networks, and only under annealed approximation, and uncorrelated input. Here for the first time, we present an exact solution to the memory capacity and the task-solving performance as a function of the structure of a given network instance, enabling direct determination of the function--structure relation in RNNs. We calculate the memory capacity for arbitrary networks with exponentially correlated input and further related it to the performance of the system on signal processing tasks in a supervised learning setup. We compute the expected error and the worst-case error bound as a function of the spectra of the network and the correlation structure of its inputs and outputs. Our results give an explanation for learning and generalization of task solving using short-term memory, which is crucial for building alternative computer architectures using physical phenomena based on the short-term memory principle.

##### Abstract (translated by Google)
递归神经网络（RNN）是简单的动力系统，其计算能力归功于其短期记忆。 RNNs的短期记忆先前已经被分析地研究过，仅用于正交网络的情况，并且仅在退火近似和不相关的输入下。在这里，我们首次提出了一个精确的解决方案，将存储器容量和任务解决性能作为一个给定网络实例结构的函数，从而能够直接确定RNN中的函数 - 结构关系。我们计算指数相关输入的任意网络的存储容量，并进一步将其与系统在监督学习设置中的信号处理任务的性能相关联。我们计算预期误差和最坏情况的误差界限作为网络频谱和其输入和输出的相关结构的函数。我们的研究结果给出了解释使用短期记忆的任务解决方案的学习和推广，这对于使用基于短期记忆原理的物理现象构建替代计算机体系结构是至关重要的。

##### URL
[https://arxiv.org/abs/1604.06929](https://arxiv.org/abs/1604.06929)

##### PDF
[https://arxiv.org/pdf/1604.06929](https://arxiv.org/pdf/1604.06929)

