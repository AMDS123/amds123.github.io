---
layout: post
title: "Graded Entailment for Compositional Distributional Semantics"
date: 2016-01-25 20:10:27
categories: arXiv_SD
tags: arXiv_SD Knowledge
author: Desislava Bankova, Bob Coecke, Martha Lewis, Daniel Marsden
mathjax: true
---

* content
{:toc}

##### Abstract
The categorical compositional distributional model of natural language provides a conceptually motivated procedure to compute the meaning of sentences, given grammatical structure and the meanings of its words. This approach has outperformed other models in mainstream empirical language processing tasks. However, until recently it has lacked the crucial feature of lexical entailment -- as do other distributional models of meaning. In this paper we solve the problem of entailment for categorical compositional distributional semantics. Taking advantage of the abstract categorical framework allows us to vary our choice of model. This enables the introduction of a notion of entailment, exploiting ideas from the categorical semantics of partial knowledge in quantum computation. The new model of language uses density matrices, on which we introduce a novel robust graded order capturing the entailment strength between concepts. This graded measure emerges from a general framework for approximate entailment, induced by any commutative monoid. Quantum logic embeds in our graded order. Our main theorem shows that entailment strength lifts compositionally to the sentence level, giving a lower bound on sentence entailment. We describe the essential properties of graded entailment such as continuity, and provide a procedure for calculating entailment strength.

##### Abstract (translated by Google)
自然语言的分类组合分布模型提供了一个概念上有动机的过程来计算句子的意义，给定了语法结构及其词的意义。这种方法在主流经验语言处理任务中胜过了其他模型。然而，直到最近，它还缺乏词汇蕴涵的关键特征 - 其他分布的意义模型也是如此。在本文中，我们解决了分类成分分布语义所蕴含的问题。利用抽象的分类框架，我们可以改变我们对模型的选择。这使得引入包含的概念，从量子计算中部分知识的分类语义中挖掘出思想。新的语言模型使用密度矩阵，在这个矩阵中我们引入了一个新的强大的分级顺序来捕获概念之间的蕴含强度。这种分级衡量标准是由任何交换性猿人所引发的一种近似含量的一般框架所形成的。量子逻辑嵌入我们的分级顺序。我们的主要定理表明，蕴涵力在构成上升到句子层面，给句子蕴涵带来下界。我们描述分级蕴涵的基本属性，如连续性，并提供计算蕴涵强度的过程。

##### URL
[https://arxiv.org/abs/1601.04908](https://arxiv.org/abs/1601.04908)

##### PDF
[https://arxiv.org/pdf/1601.04908](https://arxiv.org/pdf/1601.04908)

