---
layout: post
title: "Select-Additive Learning: Improving Generalization in Multimodal Sentiment Analysis"
date: 2017-04-12 21:38:40
categories: arXiv_CL
tags: arXiv_CL Sentiment Review Attention Sentiment_Classification Classification Prediction
author: Haohan Wang, Aaksha Meghawat, Louis-Philippe Morency, Eric P. Xing
mathjax: true
---

* content
{:toc}

##### Abstract
Multimodal sentiment analysis is drawing an increasing amount of attention these days. It enables mining of opinions in video reviews which are now available aplenty on online platforms. However, multimodal sentiment analysis has only a few high-quality data sets annotated for training machine learning algorithms. These limited resources restrict the generalizability of models, where, for example, the unique characteristics of a few speakers (e.g., wearing glasses) may become a confounding factor for the sentiment classification task. In this paper, we propose a Select-Additive Learning (SAL) procedure that improves the generalizability of trained neural networks for multimodal sentiment analysis. In our experiments, we show that our SAL approach improves prediction accuracy significantly in all three modalities (verbal, acoustic, visual), as well as in their fusion. Our results show that SAL, even when trained on one dataset, achieves good generalization across two new test datasets.

##### Abstract (translated by Google)
多模态情绪分析正在引起越来越多的关注。它能够在视频评论中挖掘意见，现在在线平台上可以使用这些意见。然而，多模态情感分析只有少数高质量的数据集被注释用于训练机器学习算法。这些有限的资源限制了模型的普遍性，例如，少数演讲者（例如佩戴眼镜）的独特特征可能成为情感分类任务的混杂因素。在本文中，我们提出一个选择加性学习（SAL）程序，提高了训练神经网络在多模态情感分析中的普遍性。在我们的实验中，我们展示了我们的SAL方法在所有三种模式（口头的，声学的，视觉的）以及融合中显着提高了预测精度。我们的研究结果表明，即使在训练一个数据集的情况下，SAL也能在两个新的测试数据集上实现良好的泛化。

##### URL
[https://arxiv.org/abs/1609.05244](https://arxiv.org/abs/1609.05244)

##### PDF
[https://arxiv.org/pdf/1609.05244](https://arxiv.org/pdf/1609.05244)

