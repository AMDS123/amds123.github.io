---
layout: post
title: "On Training Recurrent Neural Networks for Lifelong Learning"
date: 2018-11-16 20:13:23
categories: arXiv_AI
tags: arXiv_AI RNN
author: Shagun Sodhani, Sarath Chandar, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
Capacity saturation and catastrophic forgetting are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with emphasis on recurrent neural networks. To evaluate the models in life-long learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on a task with increasing levels of difficulty. As a step towards developing true lifelong learning systems, we unify Gradient Episodic Memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.07017](http://arxiv.org/abs/1811.07017)

##### PDF
[http://arxiv.org/pdf/1811.07017](http://arxiv.org/pdf/1811.07017)

