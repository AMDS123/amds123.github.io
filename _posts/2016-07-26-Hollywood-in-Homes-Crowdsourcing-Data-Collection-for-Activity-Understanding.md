---
layout: post
title: "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding"
date: 2016-07-26 22:49:22
categories: arXiv_CV
tags: arXiv_CV Video_Caption Action_Recognition Recognition
author: Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta
mathjax: true
---

* content
{:toc}

##### Abstract
Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 seconds, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and 41,104 labels for 46 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community.

##### Abstract (translated by Google)
计算机视觉有很大的潜力，通过寻找丢失的钥匙，浇花或提醒我们吃药，来帮助我们的日常生活。为了成功完成这些任务，计算机视觉方法需要从我们日常动态场景的真实多样化的例子中进行培训。尽管大多数这样的场景并不特别令人兴奋，但是它们通常不会出现在YouTube，电影或电视广播中。那么我们如何收集代表我们生活的多样而无聊的样本呢？我们提出了一个新的好莱坞家园的方法来收集这样的数据。我们不是在实验室拍摄视频，而是通过分发和众包视频创作的全过程，从脚本编写到视频记录和注释，确保多样性。按照这个程序，我们收集了一个新的数据集，Charades，有数百人在自己的家中录制视频，从事日常休闲活动。该数据集由9848个带注释的视频组成，平均时长为30秒，显示来自三大洲的267人的活动。每个视频都有多个自由文本描述，动作标签，动作间隔和交互对象的类别进行注释。 Charades总共提供了27,847个视频描述，66,500个临时定位的157个动作类别的区间和41,104个46个对象类别的标签。使用这些丰富的数据，我们评估并提供几项任务的基准结果，包括动作识别和自动描述生成。我们相信，这个数据集的现实性，多样性和随意性将为计算机视觉社区带来独特的挑战和新的机遇。

##### URL
[https://arxiv.org/abs/1604.01753](https://arxiv.org/abs/1604.01753)

##### PDF
[https://arxiv.org/pdf/1604.01753](https://arxiv.org/pdf/1604.01753)

