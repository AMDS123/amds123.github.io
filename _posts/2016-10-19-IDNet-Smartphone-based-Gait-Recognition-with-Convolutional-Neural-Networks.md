---
layout: post
title: "IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks"
date: 2016-10-19 12:11:57
categories: arXiv_CV
tags: arXiv_CV CNN Classification Deep_Learning Recognition
author: Matteo Gadaleta, Michele Rossi
mathjax: true
---

* content
{:toc}

##### Abstract
Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user's trousers. IDNet features several innovations including: i) a robust and smartphone-orientation-independent walking cycle extraction block, ii) a novel feature extractor based on convolutional neural networks, iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance.

##### Abstract (translated by Google)
在这里，我们介绍智能手机获取的运动信号的用户认证框架IDNet。其目标是使用由用户裤子的前袋中佩戴的商业智能电话提供的加速度计和陀螺仪（惯性）信号从他们的行走方式识别目标用户。 IDNet具有以下几项创新点：i）强大的智能手机定向无关步行循环提取模块; ii）基于卷积神经网络的新颖特征提取器; iii）用于分类步行循环的一类支持向量机;将这些集成到iv）多阶段认证技术中。 IDNet是第一个将深度学习方法作为步态识别的通用特征提取器的系统，它将后续步行周期的分类结果组合到多阶段决策框架中。实验结果表明，我们的方法优于最先进的技术，导致误分率（假阴性或阳性）小于0.15％，少于五个步行周期。设计选择在整个过程中进行讨论和动机，评估它们对用户认证性能的影响。

##### URL
[https://arxiv.org/abs/1606.03238](https://arxiv.org/abs/1606.03238)

##### PDF
[https://arxiv.org/pdf/1606.03238](https://arxiv.org/pdf/1606.03238)

