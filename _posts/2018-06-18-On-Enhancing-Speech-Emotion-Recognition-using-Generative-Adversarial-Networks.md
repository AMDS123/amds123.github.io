---
layout: post
title: "On Enhancing Speech Emotion Recognition using Generative Adversarial Networks"
date: 2018-06-18 12:21:18
categories: arXiv_CL
tags: arXiv_CL Adversarial Attention GAN Recognition
author: Saurabh Sahu, Rahul Gupta, Carol Espy-Wilson
mathjax: true
---

* content
{:toc}

##### Abstract
Generative Adversarial Networks (GANs) have gained a lot of attention from machine learning community due to their ability to learn and mimic an input data distribution. GANs consist of a discriminator and a generator working in tandem playing a min-max game to learn a target underlying data distribution; when fed with data-points sampled from a simpler distribution (like uniform or Gaussian distribution). Once trained, they allow synthetic generation of examples sampled from the target distribution. We investigate the application of GANs to generate synthetic feature vectors used for speech emotion recognition. Specifically, we investigate two set ups: (i) a vanilla GAN that learns the distribution of a lower dimensional representation of the actual higher dimensional feature vector and, (ii) a conditional GAN that learns the distribution of the higher dimensional feature vectors conditioned on the labels or the emotional class to which it belongs. As a potential practical application of these synthetically generated samples, we measure any improvement in a classifier's performance when the synthetic data is used along with real data for training. We perform cross-validation analyses followed by a cross-corpus study.

##### Abstract (translated by Google)
生成对抗网络（GAN）由于其学习和模拟输入数据分布的能力而受到机器学习社区的广泛关注。 GAN由一个鉴别器和一个发生器串联工作，发挥最小最大游戏来学习目标数据分布;当提供从更简单的分布（如统一或高斯分布）采样的数据点时。一旦训练完成，它们允许合成生成从目标分布采样的示例。我们研究了GAN在生成用于语音情感识别的合成特征向量中的应用。具体而言，我们研究了两个设置：（i）学习实际高维特征向量的较低维表示的分布的一个香草GAN，和（ii）一个条件GAN，其学习高维特征向量的分布，标签或其所属的情感类别。作为这些合成生成的样本的潜在实际应用，我们测量分类器性能的任何改善，当合成数据与真实数据一起使用以用于训练时。我们进行交叉验证分析，然后进行跨语料库研究。

##### URL
[http://arxiv.org/abs/1806.06626](http://arxiv.org/abs/1806.06626)

##### PDF
[http://arxiv.org/pdf/1806.06626](http://arxiv.org/pdf/1806.06626)

