---
layout: post
title: "Natural Adversarial Examples"
date: 2019-07-16 17:56:30
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce natural adversarial examples -- real-world, unmodified, and naturally occurring examples that cause classifier accuracy to significantly degrade. We curate 7,500 natural adversarial examples and release them in an ImageNet classifier test set that we call ImageNet-A. This dataset serves as a new way to measure classifier robustness. Like l_p adversarial examples, ImageNet-A examples successfully transfer to unseen or black-box classifiers. For example, on ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%. Recovering this accuracy is not simple because ImageNet-A examples exploit deep flaws in current classifiers including their over-reliance on color, texture, and background cues. We observe that popular training techniques for improving robustness have little effect, but we show that some architectural changes can enhance robustness to natural adversarial examples. Future research is required to enable robust generalization to this hard ImageNet test set.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.07174](http://arxiv.org/abs/1907.07174)

##### PDF
[http://arxiv.org/pdf/1907.07174](http://arxiv.org/pdf/1907.07174)

