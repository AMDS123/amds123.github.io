---
layout: post
title: "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics"
date: 2017-06-22 07:55:47
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Phong Le, Ivan Titov
mathjax: true
---

* content
{:toc}

##### Abstract
Coreference evaluation metrics are hard to optimize directly as they are non-differentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.

##### Abstract (translated by Google)
相关性评估指标很难直接优化，因为它们是不可区分的功能，不易分解为基本的决策。因此，大多数方法只优化与最终目标间接相关的目标，导致性能不理想。相反，我们提出了一种可区分的放松，它可以用于基于梯度的优化，从而绕过了强化学习或启发式修改交叉熵的需要。我们表明，通过修改竞争神经系统的训练目标，我们获得了实质性的收益。这表明我们的方法可以被认为是一个可行的替代使用强化学习或更昂贵的仿真学习计算。

##### URL
[https://arxiv.org/abs/1704.04451](https://arxiv.org/abs/1704.04451)

##### PDF
[https://arxiv.org/pdf/1704.04451](https://arxiv.org/pdf/1704.04451)

