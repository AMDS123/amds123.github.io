---
layout: post
title: "A concatenating framework of shortcut convolutional neural networks"
date: 2017-10-03 03:56:33
categories: arXiv_CV
tags: arXiv_CV Face CNN Image_Classification Optimization Classification Recognition
author: Yujian Li, Ting Zhang, Zhaoying Liu, Haihe Hu
mathjax: true
---

* content
{:toc}

##### Abstract
It is well accepted that convolutional neural networks play an important role in learning excellent features for image classification and recognition. However, in tradition they only allow adjacent layers connected, limiting integration of multi-scale information. To further improve their performance, we present a concatenating framework of shortcut convolutional neural networks. This framework can concatenate multi-scale features by shortcut connections to the fully-connected layer that is directly fed to the output layer. We do a large number of experiments to investigate performance of the shortcut convolutional neural networks on many benchmark visual datasets for different tasks. The datasets include AR, FERET, FaceScrub, CelebA for gender classification, CUReT for texture classification, MNIST for digit recognition, and CIFAR-10 for object recognition. Experimental results show that the shortcut convolutional neural networks can achieve better results than the traditional ones on these tasks, with more stability in different settings of pooling schemes, activation functions, optimizations, initializations, kernel numbers and kernel sizes.

##### Abstract (translated by Google)
卷积神经网络在学习图像分类和识别的优秀特征方面发挥着重要的作用。但是，传统上它们只允许相邻的层连接，限制了多尺度信息的集成。为了进一步提高它们的性能，我们提出了捷径卷积神经网络的连接框架。这个框架可以通过连接到直接馈送到输出层的完全连接层的快捷连接来连接多尺度特征。我们进行了大量的实验来研究快捷卷积神经网络在许多不同任务的基准视觉数据集上的性能。数据集包括AR，FERET，FaceScrub，性别分类的CelebA，纹理分类的CUReT，数字识别的MNIST以及物体识别的CIFAR-10。实验结果表明，快捷卷积神经网络在这些任务上可以取得比传统卷积神经网络更好的结果，在不同的池化方案设置，激活函数，优化，初始化，内核数目和内核大小方面具有更高的稳定性。

##### URL
[https://arxiv.org/abs/1710.00974](https://arxiv.org/abs/1710.00974)

##### PDF
[https://arxiv.org/pdf/1710.00974](https://arxiv.org/pdf/1710.00974)

