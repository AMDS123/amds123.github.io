---
layout: post
title: "Residual-CNDS for Grand Challenge Scene Dataset"
date: 2019-01-13 23:00:11
categories: arXiv_CV
tags: arXiv_CV CNN Classification
author: Hussein A. Al-Barazanchi, Hussam Qassim, David Feinzimer, Abhishek Verma
mathjax: true
---

* content
{:toc}

##### Abstract
Increasing depth of convolutional neural networks (CNNs) is a highly promising method of increasing the accuracy of the (CNNs). Increased CNN depth will also result in increased layer count (parameters), leading to a slow backpropagation convergence prone to overfitting. We trained our model (Residual-CNDS) to classify very large-scale scene datasets MIT Places 205, and MIT Places 365-Standard. The outcome result from the two datasets proved our proposed model (Residual-CNDS) effectively handled the slow convergence, overfitting, and degradation. CNNs that include deep supervision (CNDS) add supplementary branches to the deep convolutional neural network in specified layers by calculating vanishing, effectively addressing delayed convergence and overfitting. Nevertheless, (CNDS) does not resolve degradation; hence, we add residual learning to the (CNDS) in certain layers after studying the best place in which to add it. With this approach we overcome degradation in the very deep network. We have built two models (Residual-CNDS 8), and (Residual-CNDS 10). Moreover, we tested our models on two large-scale datasets, and we compared our results with other recently introduced cutting-edge networks in the domain of top-1 and top-5 classification accuracy. As a result, both of models have shown good improvement, which supports the assertion that the addition of residual connections enhances network CNDS accuracy without adding any computation complexity.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.10030](http://arxiv.org/abs/1902.10030)

##### PDF
[http://arxiv.org/pdf/1902.10030](http://arxiv.org/pdf/1902.10030)

