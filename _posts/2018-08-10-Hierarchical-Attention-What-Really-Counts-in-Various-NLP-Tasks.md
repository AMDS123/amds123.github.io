---
layout: post
title: "Hierarchical Attention: What Really Counts in Various NLP Tasks"
date: 2018-08-10 23:28:33
categories: arXiv_CL
tags: arXiv_CL Attention Text_Generation Embedding RNN
author: Zehao Dou, Zhihua Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Attention mechanisms in sequence to sequence models have shown great ability and wonderful performance in various natural language processing (NLP) tasks, such as sentence embedding, text generation, machine translation, machine reading comprehension, etc. Unfortunately, existing attention mechanisms only learn either high-level or low-level features. In this paper, we think that the lack of hierarchical mechanisms is a bottleneck in improving the performance of the attention mechanisms, and propose a novel Hierarchical Attention Mechanism (Ham) based on the weighted sum of different layers of a multi-level attention. Ham achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation task and a nearly 6.5% averaged improvement compared with the existing machine reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our experiments and theorems reveal that Ham has greater generalization and representation ability than existing attention mechanisms.

##### Abstract (translated by Google)
顺序模型序列中的注意机制在各种自然语言处理（NLP）任务中表现出很强的能力和出色的表现，例如句子嵌入，文本生成，机器翻译，机器阅读理解等。不幸的是，现有的注意机制只能学习高级别或低级功能。在本文中，我们认为缺乏层次机制是提高注意机制性能的瓶颈，并提出了一种基于多层次关注不同层次加权和的新型层次关注机制（Ham）。与现有的机器阅读理解模型如BIDAF和Match-LSTM相比，汉姆在中国诗歌创作任务中取得了0.26的最新BLEU成绩，平均得分提高了近6.5％。此外，我们的实验和定理表明，汉姆比现有的注意机制具有更强的泛化和表达能力。

##### URL
[http://arxiv.org/abs/1808.03728](http://arxiv.org/abs/1808.03728)

##### PDF
[http://arxiv.org/pdf/1808.03728](http://arxiv.org/pdf/1808.03728)

