---
layout: post
title: "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model"
date: 2016-01-06 06:03:17
categories: arXiv_SD
tags: arXiv_SD Attention
author: Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, Gholamreza Haffari
mathjax: true
---

* content
{:toc}

##### Abstract
Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.

##### Abstract (translated by Google)
机器翻译的神经编解码器模型取得了令人印象深刻的结果，与传统的翻译模型相媲美。然而，他们的建模方法过于简单，并省略了传统模型中的几个关键的归纳偏差。在本文中，我们扩展注意神经翻译模型，包括基于字对齐模型的结构偏见，包括位置偏差，马尔可夫条件反射，生育率和翻译方向上的一致性。我们展示了基于多个语言对的基准注意模型和基于标准短语的模型的改进，在低资源设置下对困难语言进行评估。

##### URL
[https://arxiv.org/abs/1601.01085](https://arxiv.org/abs/1601.01085)

##### PDF
[https://arxiv.org/pdf/1601.01085](https://arxiv.org/pdf/1601.01085)

