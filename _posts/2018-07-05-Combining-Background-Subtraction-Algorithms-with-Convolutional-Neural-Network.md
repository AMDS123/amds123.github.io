---
layout: post
title: "Combining Background Subtraction Algorithms with Convolutional Neural Network"
date: 2018-07-05 16:39:50
categories: arXiv_CV
tags: arXiv_CV Object_Detection Tracking CNN Object_Tracking Detection Recognition
author: Dongdong Zeng, Ming Zhu, Arjan Kuijper
mathjax: true
---

* content
{:toc}

##### Abstract
Accurate and fast extraction of foreground object is a key prerequisite for a wide range of computer vision applications such as object tracking and recognition. Thus, enormous background subtraction methods for foreground object detection have been proposed in recent decades. However, it is still regarded as a tough problem due to a variety of challenges such as illumination variations, camera jitter, dynamic backgrounds, shadows, and so on. Currently, there is no single method that can handle all the challenges in a robust way. In this letter, we try to solve this problem from a new perspective by combining different state-of-the-art background subtraction algorithms to create a more robust and more advanced foreground detection algorithm. More concretely, a encoder-decoder fully convolutional neural network architecture is trained to automatically learn how to leverage the characteristics of different algorithms to fuse the results produced by different background subtraction algorithms and output a more precise result. Comprehensive experiments evaluated on the CDnet 2014 dataset demonstrate that the proposed method outperforms all the considered single background subtraction algorithm. And we show that our solution is more efficient than other combination strategies.

##### Abstract (translated by Google)
准确快速地提取前景对象是各种计算机视觉应用（如对象跟踪和识别）的关键先决条件。因此，近几十年来已经提出了用于前景物体检测的巨大背景减除方法。然而，由于诸如照明变化，相机抖动，动态背景，阴影等各种挑战，它仍然被认为是一个棘手的问题。目前，没有一种方法能够以稳健的方式处理所有挑战。在这封信中，我们试图通过结合不同的最先进的背景减法算法从一个新的角度来解决这个问题，以创建一个更健壮和更先进的前景检测算法。更具体地，编码器 - 解码器完全卷积神经网络架构被训练以自动学习如何利用不同算法的特性来融合由不同背景减法算法产生的结果并输出更精确的结果。在CDnet 2014数据集上评估的综合实验表明，所提出的方法优于所有考虑的单一背景减法算法。我们表明我们的解决方案比其他组合策略更有效。

##### URL
[http://arxiv.org/abs/1807.02080](http://arxiv.org/abs/1807.02080)

##### PDF
[http://arxiv.org/pdf/1807.02080](http://arxiv.org/pdf/1807.02080)

