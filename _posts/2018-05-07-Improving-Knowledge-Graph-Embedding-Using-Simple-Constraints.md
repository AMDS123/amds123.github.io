---
layout: post
title: "Improving Knowledge Graph Embedding Using Simple Constraints"
date: 2018-05-07 09:03:14
categories: arXiv_AI
tags: arXiv_AI Knowledge_Graph Knowledge Embedding Relation
author: Boyang Ding, Quan Wang, Bin Wang, Li Guo
mathjax: true
---

* content
{:toc}

##### Abstract
Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine {\it non-negativity constraints} on entity representations and {\it approximate entailment constraints} on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at \url{this https URL}.

##### Abstract (translated by Google)
将知识图（KGs）嵌入连续向量空间是当前研究的重点。早期的作品通过KG三元组开发的简单模型完成了这项任务。最近的尝试着重于设计更复杂的三重评分模型，或者将三倍以外的额外信息合并在一起。相比之下，本文研究了使用非常简单的约束来改善KG嵌入的潜力。我们考察关于实体表示的{\ it非负性约束}和关系表示的{\ it近似蕴含约束}。前者帮助学习实体的紧凑和可解释的表示。后者进一步将关系之间逻辑蕴涵的规律性编码为它们的分布式表示。这些限制强加了对嵌入空间结构的先验信念，而没有对效率或可伸缩性产生负面影响。对WordNet，Freebase和DBpedia的评估显示，我们的方法简单但出人意料地有效，显着且始终超越竞争基线。施加的约束确实提高了模型的可解释性，导致嵌入空间的结构大大增加。代码和数据可在\ url {this https URL}获得。

##### URL
[https://arxiv.org/abs/1805.02408](https://arxiv.org/abs/1805.02408)

##### PDF
[https://arxiv.org/pdf/1805.02408](https://arxiv.org/pdf/1805.02408)

