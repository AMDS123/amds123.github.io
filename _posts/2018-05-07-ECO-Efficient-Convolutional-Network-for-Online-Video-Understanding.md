---
layout: post
title: "ECO: Efficient Convolutional Network for Online Video Understanding"
date: 2018-05-07 09:46:08
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption CNN Classification Relation
author: Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox
mathjax: true
---

* content
{:toc}

##### Abstract
The state of the art in video understanding suffers from two problems: (1) The major part of reasoning is performed locally in the video, therefore, it misses important relationships within actions that span several seconds. (2) While there are local methods with fast per-frame processing, the processing of the whole video is not efficient and hampers fast video retrieval or online classification of long-term activities. In this paper, we introduce a network architecture that takes long-term content into account and enables fast per-video processing at the same time. The architecture is based on merging long-term content already in the network rather than in a post-hoc fusion. Together with a sampling strategy, which exploits that neighboring frames are largely redundant, this yields high-quality action classification and video captioning at up to 230 videos per second, where each video can consist of a few hundred frames. The approach achieves competitive performance across all datasets while being 10x to 80x faster than state-of-the-art methods.

##### Abstract (translated by Google)
视频理解的最新技术存在两个问题：（1）推理的主要部分是在视频中本地执行的，因此，它错过了跨越几秒钟的动作中的重要关系。 （2）虽然存在快速逐帧处理的本地方法，但是整个视频的处理效率不高并且妨碍了长期活动的快速视频检索或在线分类。在本文中，我们介绍了一种网络架构，该架构将长期内容考虑在内，并同时实现快速的每视频处理。该体系结构基于合并网络中已有的长期内容而不是后期融合。结合采样策略，利用相邻帧在很大程度上是多余的，这可以产生高质量的动作分类和视频字幕，每秒最多230个视频，每个视频可以包含几百帧。该方法在所有数据集中实现了竞争性能，同时比最先进的方法快10到80倍。

##### URL
[https://arxiv.org/abs/1804.09066](https://arxiv.org/abs/1804.09066)

##### PDF
[https://arxiv.org/pdf/1804.09066](https://arxiv.org/pdf/1804.09066)

