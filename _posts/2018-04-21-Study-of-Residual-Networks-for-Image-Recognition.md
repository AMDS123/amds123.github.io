---
layout: post
title: "Study of Residual Networks for Image Recognition"
date: 2018-04-21 23:04:53
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Classification Recognition
author: Mohammad Sadegh Ebrahimi, Hossein Karkeh Abadi
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks demonstrate to have a high performance on image classification tasks while being more difficult to train. Due to the complexity and vanishing gradient problem, it normally takes a lot of time and more computational power to train deeper neural networks. Deep residual networks (ResNets) can make the training process faster and attain more accuracy compared to their equivalent neural networks. ResNets achieve this improvement by adding a simple skip connection parallel to the layers of convolutional neural networks. In this project we first design a ResNet model that can perform the image classification task on the Tiny ImageNet dataset with a high accuracy, then we compare the performance of this ResNet model with its equivalent Convolutional Network (ConvNet). Our findings illustrate that ResNets are more prone to overfitting despite their higher accuracy. Several methods to prevent overfitting such as adding dropout layers and stochastic augmentation of the training dataset has been studied in this work.

##### Abstract (translated by Google)
深度神经网络证明在图像分类任务上具有高性能，同时更难以训练。由于复杂性和渐变梯度问题，训练更深层的神经网络通常需要大量的时间和更多的计算能力。与其等效的神经网络相比，深度残差网络（ResNets）可以使训练过程更快并且获得更高的精度。 ResNets通过添加一个平行于卷积神经网络层的简单跳过连接来实现这种改进。在这个项目中，我们首先设计了一个ResNet模型，它可以高精度地在Tiny ImageNet数据集上执行图像分类任务，然后我们比较此ResNet模型与其等效卷积网络（ConvNet）的性能。我们的研究结果表明ResNets尽管准确度更高，但更容易出现过度拟合。本文研究了防止过度拟合的几种方法，如添加丢失层和训练数据集的随机增强。

##### URL
[https://arxiv.org/abs/1805.00325](https://arxiv.org/abs/1805.00325)

##### PDF
[https://arxiv.org/pdf/1805.00325](https://arxiv.org/pdf/1805.00325)

