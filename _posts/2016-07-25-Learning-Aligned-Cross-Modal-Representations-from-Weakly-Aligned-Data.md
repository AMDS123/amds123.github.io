---
layout: post
title: "Learning Aligned Cross-Modal Representations from Weakly Aligned Data"
date: 2016-07-25 14:38:36
categories: arXiv_CV
tags: arXiv_CV CNN
author: Lluis Castrejon, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba
mathjax: true
---

* content
{:toc}

##### Abstract
People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize cross-modal scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.

##### Abstract (translated by Google)
人们可以通过许多不同形式的场景来识别超出自然的图像。在本文中，我们研究如何学习跨模态转移的跨模态场景表示。为了研究这个问题，我们引入了一个新的跨模态场景数据集。虽然卷积神经网络可以很好地对十字模态场景进行分类，但是它们也学习了一种不是跨模态对齐的中间表示，这对于跨模式传输应用是不希望的。我们提出的方法来调整跨模态卷积神经网络，使他们有一个共同的表示形式，不知道的形式。我们的实验表明，我们的场景表示可以帮助跨模式转换表示来检索。此外，我们的可视化表明，单位出现在共同的代表性，往往激活一致的概念独立的形式。

##### URL
[https://arxiv.org/abs/1607.07295](https://arxiv.org/abs/1607.07295)

##### PDF
[https://arxiv.org/pdf/1607.07295](https://arxiv.org/pdf/1607.07295)

