---
layout: post
title: "Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition"
date: 2017-05-25 15:33:19
categories: arXiv_CV
tags: arXiv_CV Knowledge Segmentation CNN Language_Model Prediction Recognition
author: Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, Terry Lyons
mathjax: true
---

* content
{:toc}

##### Abstract
Online handwritten Chinese text recognition (OHCTR) is a challenging problem as it involves a large-scale character set, ambiguous segmentation, and variable-length input sequences. In this paper, we exploit the outstanding capability of path signature to translate online pen-tip trajectories into informative signature feature maps using a sliding window-based method, successfully capturing the analytic and geometric properties of pen strokes with strong local invariance and robustness. A multi-spatial-context fully convolutional recurrent network (MCFCRN) is proposed to exploit the multiple spatial contexts from the signature feature maps and generate a prediction sequence while completely avoiding the difficult segmentation problem. Furthermore, an implicit language model is developed to make predictions based on semantic context within a predicting feature sequence, providing a new perspective for incorporating lexicon constraints and prior knowledge about a certain language in the recognition procedure. Experiments on two standard benchmarks, Dataset-CASIA and Dataset-ICDAR, yielded outstanding results, with correct rates of 97.10% and 97.15%, respectively, which are significantly better than the best result reported thus far in the literature.

##### Abstract (translated by Google)
在线手写中文文本识别（OHCTR）是一个具有挑战性的问题，因为它涉及大规模字符集，模糊分割和可变长度输入序列。在本文中，我们利用路径签名的出色能力，利用基于滑动窗口的方法将在线笔尖轨迹翻译成信息签名特征图，成功捕捉具有强局部不变性和鲁棒性的笔划的解析几何特性。提出了一种多空间上下文完全卷积循环网络（MCFCRN），用于从签名特征映射中挖掘多个空间上下文，生成预测序列，完全避免了难以分割的问题。此外，还开发了隐式语言模型，在预测特征序列的基础上进行基于语义上下文的预测，为识别过程中结合词汇约束和特定语言的先验知识提供了新的视角。对两个标准基准数据库CASIA和数据集ICDAR进行的实验取得了较好的结果，正确率分别为97.10％和97.15％，明显优于文献报道的最好结果。

##### URL
[https://arxiv.org/abs/1610.02616](https://arxiv.org/abs/1610.02616)

##### PDF
[https://arxiv.org/pdf/1610.02616](https://arxiv.org/pdf/1610.02616)

