---
layout: post
title: "Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince and the Past Tense Debate"
date: 2018-07-12 18:44:34
categories: arXiv_CL
tags: arXiv_CL RNN
author: Christo Kirov, Ryan Cotterell
mathjax: true
---

* content
{:toc}

##### Abstract
Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter, Pinker &amp; Prince (1988) presented a comprehensive rebuttal of many of Rumelhart and McClelland's claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland (1986) model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince's criticisms without requiring any simplication of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a re-examination of their utility in linguistic and cognitive modeling.

##### Abstract (translated by Google)
NLP的进步有助于推进认知建模吗？我们通过回到经典的案例研究来研究人工神经网络的作用，这是当前许多常见NLP任务中的现有技术。 1986年，Rumelhart和McClelland着名地引入了一种神经结构，该结构学会将英语动词词干转换为过去时态。此后不久，Pinker＆amp; Prince（1988）对Rumelhart和McClelland的许多说法进行了全面的反驳。他们攻击的大部分力量都集中在Rumelhart和McClelland（1986）模型的经验不足上。然而，今天，这种模式已经过时了。我们表明，现代NLP系统中使用的编码器 - 解码器网络架构消除了大多数Pinker和Prince的批评，而不需要对过去时态映射问题进行任何简化。我们认为现代网络的实证表现需要重新审视它们在语言和认知建模中的效用。

##### URL
[http://arxiv.org/abs/1807.04783](http://arxiv.org/abs/1807.04783)

##### PDF
[http://arxiv.org/pdf/1807.04783](http://arxiv.org/pdf/1807.04783)

