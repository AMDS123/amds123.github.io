---
layout: post
title: "Factorization tricks for LSTM networks"
date: 2017-05-04 17:17:55
categories: arXiv_SD
tags: arXiv_SD RNN
author: Oleksii Kuchaiev, Boris Ginsburg
mathjax: true
---

* content
{:toc}

##### Abstract
We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.

##### Abstract (translated by Google)
我们提出了两个简单的方法来减少参数数量，加速大型长期短期记忆（LSTM）网络的训练：第一个是将LSTM矩阵的“设计矩阵分解”成两个较小矩阵的乘积，第二个是把LSTM矩阵，它的输入和状态划分成独立的组。这两种方法都使我们能够更快地训练大型LSTM网络，以应对最先进的困惑。在十亿字基准测试中，我们将单一模型的困惑度降低到23.36。

##### URL
[https://arxiv.org/abs/1703.10722](https://arxiv.org/abs/1703.10722)

##### PDF
[https://arxiv.org/pdf/1703.10722](https://arxiv.org/pdf/1703.10722)

