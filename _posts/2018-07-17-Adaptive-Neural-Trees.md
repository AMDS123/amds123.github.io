---
layout: post
title: "Adaptive Neural Trees"
date: 2018-07-17 23:01:35
categories: arXiv_CV
tags: arXiv_CV CNN Represenation_Learning Inference
author: Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi, Aditya Nori
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs), a model that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving over 99% and 90% accuracy on MNIST and CIFAR-10 datasets, ANTs benefit from (i) faster inference via conditional computation, (ii) increased interpretability via hierarchical clustering e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.

##### Abstract (translated by Google)
深度神经网络和决策树在很大程度上独立的范例上运行;通常，前者使用预先指定的体系结构执行表示学习，而后者的特征在于通过数据驱动的体系结构学习预先指定的特征的层次结构。我们通过自适应神经树（ANT）将两者结合起来，ANT是一种将表示学习结合到决策树的边缘，路由函数和叶节点中的模型，以及基于反向传播的训练算法，该算法从原始模块自适应地增长体系结构（例如，卷积层）。我们证明，虽然在MNIST和CIFAR-10数据集上实现了超过99％和90％的准确度，但ANT受益于（i）通过条件计算的更快推断，（ii）通过分层聚类增加可解释性，例如，学习有意义的类关联，例如分离自然对象和人造对象，以及（iii）使体系结构适应训练数据集的大小和复杂性的机制。

##### URL
[https://arxiv.org/abs/1807.06699](https://arxiv.org/abs/1807.06699)

##### PDF
[https://arxiv.org/pdf/1807.06699](https://arxiv.org/pdf/1807.06699)

