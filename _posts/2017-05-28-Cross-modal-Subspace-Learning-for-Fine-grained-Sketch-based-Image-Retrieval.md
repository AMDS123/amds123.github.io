---
layout: post
title: "Cross-modal Subspace Learning for Fine-grained Sketch-based Image Retrieval"
date: 2017-05-28 03:45:26
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval
author: Peng Xu, Qiyue Yin, Yongye Huang, Yi-Zhe Song, Zhanyu Ma, Liang Wang, Tao Xiang, W. Bastiaan Kleijn, Jun Guo
mathjax: true
---

* content
{:toc}

##### Abstract
Sketch-based image retrieval (SBIR) is challenging due to the inherent domain-gap between sketch and photo. Compared with pixel-perfect depictions of photos, sketches are iconic renderings of the real world with highly abstract. Therefore, matching sketch and photo directly using low-level visual clues are unsufficient, since a common low-level subspace that traverses semantically across the two modalities is non-trivial to establish. Most existing SBIR studies do not directly tackle this cross-modal problem. This naturally motivates us to explore the effectiveness of cross-modal retrieval methods in SBIR, which have been applied in the image-text matching successfully. In this paper, we introduce and compare a series of state-of-the-art cross-modal subspace learning methods and benchmark them on two recently released fine-grained SBIR datasets. Through thorough examination of the experimental results, we have demonstrated that the subspace learning can effectively model the sketch-photo domain-gap. In addition we draw a few key insights to drive future research.

##### Abstract (translated by Google)
基于素描的图像检索（SBIR）由于素描和照片之间固有的领域差异而具有挑战性。与像素完美的照片描绘相比，草图是高度抽象的真实世界的标志性渲染。因此，直接使用低级视觉线索来匹配草图和照片是不够的，因为在两种模式之间在语义上遍历的共同的低级子空间是不平凡的。大多数现有的SBIR研究并不直接解决这个跨模态问题。这自然会促使我们去探索SBIR中跨模态检索方法的有效性，并成功应用于图像文本匹配。在本文中，我们介绍并比较了一系列最先进的跨模态子空间学习方法，并在两个最近发布的细粒度SBIR数据集上进行了基准测试。通过对实验结果的深入研究，我们证明了子空间学习能够有效地模拟素描 - 照片领域的差距。此外，我们提出了一些关键的见解，以推动未来的研究。

##### URL
[https://arxiv.org/abs/1705.09888](https://arxiv.org/abs/1705.09888)

##### PDF
[https://arxiv.org/pdf/1705.09888](https://arxiv.org/pdf/1705.09888)

