---
layout: post
title: "Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images"
date: 2016-11-24 23:15:56
categories: arXiv_CL
tags: arXiv_CL Embedding RNN Relation
author: Junhua Mao, Jiajing Xu, Yushi Jing, Alan Yuille
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest. This dataset is more than 200 times larger than MS COCO, the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: this http URL

##### Abstract (translated by Google)
在本文中，我们着重于培训和评估有效的文字和视觉信息的文字嵌入。更具体地说，我们引入了一个大规模的数据集，其中包含3亿个句子，用于描述在Pinterest上从公共可用引脚（即用户上传的文字描述的图像）爬取和下载的4000多万个图像。这个数据集比标准的大规模图像数据集MS COCO大200倍以上。此外，我们构建一个评估数据集，直接评估词语嵌入在语义上相似或相关的词语和短语中的有效性。该评估数据集中的单词/短语对从图像搜索系统中的数百万用户的点击数据中收集，因此包含丰富的语义关系。基于这些数据集，我们提出并比较了几种基于递归神经网络（RNN）的多模式（文本和图像）模型。实验表明，我们的模型受益于将视觉信息纳入词嵌入，而权重分享策略对于学习这种多模式嵌入是至关重要的。项目页面是：这个http URL

##### URL
[https://arxiv.org/abs/1611.08321](https://arxiv.org/abs/1611.08321)

##### PDF
[https://arxiv.org/pdf/1611.08321](https://arxiv.org/pdf/1611.08321)

