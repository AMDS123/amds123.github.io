---
layout: post
title: "Improved Regularization Techniques for End-to-End Speech Recognition"
date: 2017-12-19 18:45:25
categories: arXiv_CL
tags: arXiv_CL Regularization Speech_Recognition Recognition
author: Yingbo Zhou, Caiming Xiong, Richard Socher
mathjax: true
---

* content
{:toc}

##### Abstract
Regularization is important for end-to-end speech models, since the models are highly flexible and easy to overfit. Data augmentation and dropout has been important for improving end-to-end models in other domains. However, they are relatively under explored for end-to-end speech models. Therefore, we investigate the effectiveness of both methods for end-to-end trainable, deep speech recognition models. We augment audio data through random perturbations of tempo, pitch, volume, temporal alignment, and adding random noise.We further investigate the effect of dropout when applied to the inputs of all layers of the network. We show that the combination of data augmentation and dropout give a relative performance improvement on both Wall Street Journal (WSJ) and LibriSpeech dataset of over 20%. Our model performance is also competitive with other end-to-end speech models on both datasets.

##### Abstract (translated by Google)
对于端到端的语音模型来说，正则化是非常重要的，因为这些模型是高度灵活和容易过度的。数据增加和丢失对于改进其他域中的端到端模型非常重要。然而，他们对于端到端语音模型的研究相对较少。因此，我们研究两种方法在端对端可训练，深度语音识别模型中的有效性。我们通过节奏，音高，音量，时间对齐和随机噪声的随机扰动来增加音频数据。我们进一步研究了当应用到网络的所有层的输入时丢失的影响。我们表明，数据增加和丢失的组合使华尔街日报（WSJ）和LibriSpeech数据集的相对性能提高了20％以上。我们的模型性能也与两个数据集上的其他端到端语音模型相竞争。

##### URL
[http://arxiv.org/abs/1712.07108](http://arxiv.org/abs/1712.07108)

##### PDF
[http://arxiv.org/pdf/1712.07108](http://arxiv.org/pdf/1712.07108)

