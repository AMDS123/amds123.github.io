---
layout: post
title: "Model-Based Active Exploration"
date: 2018-10-29 14:43:48
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Pranav Shyam, Wojciech Ja&#x15b;kowski, Faustino Gomez
mathjax: true
---

* content
{:toc}

##### Abstract
Efficient exploration is an unsolved problem in Reinforcement Learning. We introduce Model-Based Active eXploration (MAX), an algorithm that actively explores the environment. It minimizes data required to comprehensively model the environment by planning to observe novel events, instead of merely reacting to novelty encountered by chance. Non-stationarity induced by traditional exploration bonus techniques is avoided by constructing fresh exploration policies only at time of action. In semi-random toy environments where directed exploration is critical to make progress, our algorithm is at least an order of magnitude more efficient than strong baselines.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.12162](http://arxiv.org/abs/1810.12162)

##### PDF
[http://arxiv.org/pdf/1810.12162](http://arxiv.org/pdf/1810.12162)

