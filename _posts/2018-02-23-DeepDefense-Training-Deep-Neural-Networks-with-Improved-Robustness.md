---
layout: post
title: "DeepDefense: Training Deep Neural Networks with Improved Robustness"
date: 2018-02-23 05:02:59
categories: arXiv_CV
tags: arXiv_CV Adversarial Optimization Classification Prediction
author: Ziang Yan, Yiwen Guo, Changshui Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN models into making arbitrary predictions. To address this problem, we propose a training recipe named DeepDefense. Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms the state-of-the-arts by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results will be made publicly available.

##### Abstract (translated by Google)
尽管对各种计算机视觉任务有效，但深度神经网络（DNN）容易受到对抗性攻击，限制了它们在安全关键型系统中的应用。最近的作品已经显示了产生不可感知的扰动图像输入（也称为对抗性例子）的可能性，以欺骗训练有素的DNN模型来进行任意预测。为了解决这个问题，我们提出了一个名为DeepDefense的培训食谱。我们的核心思想是将基于对冲扰动的正规化器整合到分类目标中，以使所获得的模型能够直接而准确地抵御潜在的攻击。整个优化问题就像训练递归网络一样解决。实验结果表明，我们的方法在各种数据集（包括MNIST，CIFAR-10和ImageNet）以及不同的DNN体系结构上的性能优于现有技术。复制我们结果的代码和模型将公开发布。

##### URL
[http://arxiv.org/abs/1803.00404](http://arxiv.org/abs/1803.00404)

##### PDF
[http://arxiv.org/pdf/1803.00404](http://arxiv.org/pdf/1803.00404)

