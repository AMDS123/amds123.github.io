---
layout: post
title: "Dynamic Objects Segmentation for Visual Localization in Urban Environments"
date: 2018-07-09 09:17:59
categories: arXiv_RO
tags: arXiv_RO Segmentation Pose_Estimation CNN
author: Guoxiang Zhou, Berta Bescos, Marcin Dymczyk, Mark Pfeiffer, Jos&#xe9; Neira, Roland Siegwart
mathjax: true
---

* content
{:toc}

##### Abstract
Visual localization and mapping is a crucial capability to address many challenges in mobile robotics. It constitutes a robust, accurate and cost-effective approach for local and global pose estimation within prior maps. Yet, in highly dynamic environments, like crowded city streets, problems arise as major parts of the image can be covered by dynamic objects. Consequently, visual odometry pipelines often diverge and the localization systems malfunction as detected features are not consistent with the precomputed 3D model. In this work, we present an approach to automatically detect dynamic object instances to improve the robustness of vision-based localization and mapping in crowded environments. By training a convolutional neural network model with a combination of synthetic and real-world data, dynamic object instance masks are learned in a semi-supervised way. The real-world data can be collected with a standard camera and requires minimal further post-processing. Our experiments show that a wide range of dynamic objects can be reliably detected using the presented method. Promising performance is demonstrated on our own and also publicly available datasets, which also shows the generalization capabilities of this approach.

##### Abstract (translated by Google)
视觉定位和映射是解决移动机器人中许多挑战的关键能力。它构成了先前地图中局部和全局姿态估计的稳健，准确且具有成本效益的方法。然而，在高度动态的环境中，如拥挤的城市街道，由于动态物体可以覆盖图像的主要部分，因此会出现问题。因此，视觉测距管道经常发散并且定位系统发生故障，因为检测到的特征与预先计算的3D模型不一致。在这项工作中，我们提出了一种自动检测动态对象实例的方法，以提高在拥挤环境中基于视觉的本地化和映射的稳健性。通过训练具有合成和真实世界数据组合的卷积神经网络模型，以半监督方式学习动态对象实例掩模。可以使用标准相机收集真实世界的数据，并且需要最少的进一步后处理。我们的实验表明，使用所提出的方法可以可靠地检测各种动态物体。在我们自己的以及公开可用的数据集上展示了有希望的性能，这也显示了这种方法的泛化能力。

##### URL
[http://arxiv.org/abs/1807.02996](http://arxiv.org/abs/1807.02996)

##### PDF
[http://arxiv.org/pdf/1807.02996](http://arxiv.org/pdf/1807.02996)

