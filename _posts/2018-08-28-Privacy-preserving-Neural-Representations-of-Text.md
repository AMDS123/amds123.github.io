---
layout: post
title: "Privacy-preserving Neural Representations of Text"
date: 2018-08-28 16:57:37
categories: arXiv_CL
tags: arXiv_CL Adversarial Deep_Learning
author: Maximin Coavoux, Shashi Narayan, Shay B. Cohen
mathjax: true
---

* content
{:toc}

##### Abstract
This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user's device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.

##### Abstract (translated by Google)
本文讨论在隐私保护的背景下针对自然语言处理（NLP）的深度学习系统的对抗性攻击。我们研究了一种特定类型的攻击：攻击者窃听神经文本分类器的隐藏表示，并尝试恢复有关输入文本的信息。这种情况可能出现在神经网络的计算跨多个设备共享的情况下，例如，一些隐藏的表示由用户的设备计算并发送到基于云的模型。我们通过攻击者准确预测特定私人信息的能力来衡量隐藏表示的隐私，并描述隐私和神经表示的效用之间的权衡。最后，我们提出了几种基于修改过的训练目标的防御方法，并表明它们可以提高神经表征的隐私性。

##### URL
[http://arxiv.org/abs/1808.09408](http://arxiv.org/abs/1808.09408)

##### PDF
[http://arxiv.org/pdf/1808.09408](http://arxiv.org/pdf/1808.09408)

