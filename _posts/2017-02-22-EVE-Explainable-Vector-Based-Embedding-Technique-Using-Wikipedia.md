---
layout: post
title: "EVE: Explainable Vector Based Embedding Technique Using Wikipedia"
date: 2017-02-22 16:50:25
categories: arXiv_SD
tags: arXiv_SD Embedding Language_Model Detection
author: M. Atif Qureshi, Derek Greene
mathjax: true
---

* content
{:toc}

##### Abstract
We present an unsupervised explainable word embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a word using human-readable labels, thereby it readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed word embedding model, we consider its usefulness in three fundamental tasks: 1) intruder detection - to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster - to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first - to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art.

##### Abstract (translated by Google)
我们提出了一个无监督的可解释的词嵌入技术，称为EVE，它建立在维基百科的结构上。所提出的模型使用人类可读标签来定义表示单词的语义向量的维度，因此其易于解释。具体来说，每个向量是使用维基百科分类图结构连同维基百科文章链接结构一起构建的。为了检验所提出的词嵌入模型的有效性，我们考虑它在三个基本任务中的有用性：1）入侵者检测 - 评估其从相干向量列表中识别非相干向量的能力，2）聚类能力评估将相关向量组合在一起的趋势，同时将不相关的向量保持在单独的簇中;以及3）首先对相关项目进行排序 - 评估其按照结果的最高顺序排列与查询相关的向量（项目）的能力。对于每个任务，我们也提出了一个策略，从模型中产生一个任务特定的人类可解释的解释。这些证明了由EVE生成的可解释的嵌入的总体有效性。最后，我们比较了EVE和Word2Vec，FastText和GloVe嵌入技术在三个任务中的作用，并报告了最先进的技术。

##### URL
[https://arxiv.org/abs/1702.06891](https://arxiv.org/abs/1702.06891)

##### PDF
[https://arxiv.org/pdf/1702.06891](https://arxiv.org/pdf/1702.06891)

