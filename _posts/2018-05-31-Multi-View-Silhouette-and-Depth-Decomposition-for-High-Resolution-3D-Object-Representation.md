---
layout: post
title: "Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation"
date: 2018-05-31 21:01:40
categories: arXiv_CV
tags: arXiv_CV Super_Resolution Knowledge Prediction
author: Edward Smith, Scott Fujimoto, David Meger
mathjax: true
---

* content
{:toc}

##### Abstract
We consider the problem of scaling deep generative shape models to high-resolution. Drawing motivation from the canonical view representation of objects, we introduce a novel method for the fast up-sampling of 3D objects in voxel space through networks that perform super-resolution on the six orthographic depth projections. This allows us to generate high-resolution objects with more efficient scaling than methods which work directly in 3D. We decompose the problem of 2D depth super-resolution into silhouette and depth prediction to capture both structure and fine detail. This allows our method to generate sharp edges more easily than an individual network. We evaluate our work on multiple experiments concerning high-resolution 3D objects, and show our system is capable of accurately producing objects at resolutions as large as 512$\mathbf{\times}$512$\mathbf{\times}$512 -- the highest resolution reported for this task, to our knowledge. We achieve state-of-the-art performance on 3D object reconstruction from RGB images on the ShapeNet dataset, and further demonstrate the first effective 3D super-resolution method.

##### Abstract (translated by Google)
我们考虑将深度生成形状模型缩放到高分辨率的问题。从对象的典型视图表示中吸取动力，我们引入了一种新的方法，通过对六个正交深度投影执行超分辨率的网络，在体素空间中快速上采样三维对象。这使我们能够生成高分辨率的对象，比直接在3D中工作的方法具有更高的缩放比例。我们将二维深度超分辨率问题分解为轮廓和深度预测，以捕捉结构和细节。这使得我们的方法比单个网络更容易产生锐利的边缘。我们评估了关于高分辨率三维物体的多项实验的工作，并显示我们的系统能够以高达512×512的分辨率准确地生成物体。$ 512 $ \ mathbf {\ times} 512美元 - 最高据我们所知，这项任务的分辨率报告。我们通过ShapeNet数据集上的RGB图像实现了三维物体重建的最新性能，并进一步演示了第一种有效的3D超分辨率方法。

##### URL
[http://arxiv.org/abs/1802.09987](http://arxiv.org/abs/1802.09987)

##### PDF
[http://arxiv.org/pdf/1802.09987](http://arxiv.org/pdf/1802.09987)

