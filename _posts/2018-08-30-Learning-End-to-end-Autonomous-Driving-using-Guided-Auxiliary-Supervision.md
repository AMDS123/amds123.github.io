---
layout: post
title: "Learning End-to-end Autonomous Driving using Guided Auxiliary Supervision"
date: 2018-08-30 16:46:22
categories: arXiv_AI
tags: arXiv_AI Prediction
author: Ashish Mehta, Adithya Subramanian, Anbumani Subramanian
mathjax: true
---

* content
{:toc}

##### Abstract
Learning to drive faithfully in highly stochastic urban settings remains an open problem. To that end, we propose a Multi-task Learning from Demonstration (MT-LfD) framework which uses supervised auxiliary task prediction to guide the main task of predicting the driving commands. Our framework involves an end-to-end trainable network for imitating the expert demonstrator's driving commands. The network intermediately predicts visual affordances and action primitives through direct supervision which provide the aforementioned auxiliary supervised guidance. We demonstrate that such joint learning and supervised guidance facilitates hierarchical task decomposition, assisting the agent to learn faster, achieve better driving performance and increases transparency of the otherwise black-box end-to-end network. We run our experiments to validate the MT-LfD framework in CARLA, an open-source urban driving simulator. We introduce multiple non-player agents in CARLA and induce temporal noise in them for realistic stochasticity.

##### Abstract (translated by Google)
学会在高度随机的城市环境中忠实地驾驶仍然是一个悬而未决的问题。为此，我们提出了一种多任务学习演示（MT-LfD）框架，该框架使用监督辅助任务预测来指导预测驾驶命令的主要任务。我们的框架涉及一个端到端的可训练网络，用于模仿专家演示者的驾驶命令。网络通过直接监督中间地预测视觉可供性和动作原语，其提供上述辅助监督指导。我们证明了这种联合学习和监督指导有助于分层任务分解，帮助代理更快地学习，实现更好的驾驶性能并增加黑盒端到端网络的透明度。我们运行实验来验证CARLA中的MT-LfD框架，CARLA是一种开源城市驾驶模拟器。我们在CARLA中引入多个非玩家代理并在其中引发时间噪声以获得真实的随机性。

##### URL
[http://arxiv.org/abs/1808.10393](http://arxiv.org/abs/1808.10393)

##### PDF
[http://arxiv.org/pdf/1808.10393](http://arxiv.org/pdf/1808.10393)

