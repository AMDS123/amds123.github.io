---
layout: post
title: "Distraction-Based Neural Networks for Document Summarization"
date: 2016-10-26 18:57:00
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang
mathjax: true
---

* content
{:toc}

##### Abstract
Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long.

##### Abstract (translated by Google)
最近，神经网络学习的分布式表示在精细粒度（例如单词，短语，甚至是句子）中对自然语言进行建模时是有效的。是否以及如何扩展这种方法来帮助模拟更大范围的文本（例如文档）是有趣的，并且进一步的调查仍然是可取的。本文旨在为此目的而加强神经网络模型。文档级建模的典型问题是自动汇总，其目的是对文档进行建模以生成摘要。在本文中，我们提出神经模型来训练计算机，不仅要注意输入文档的具体区域和内容，而且还要关注它们在文档的不同内容之间进行转换，以便更好地把握总结的总体含义。如果不设计任何特征，我们将在两个大型数据集上训练模型。这些模型达到了最先进的性能，并且显着受益于分心建模，特别是当输入文档很长时。

##### URL
[https://arxiv.org/abs/1610.08462](https://arxiv.org/abs/1610.08462)

##### PDF
[https://arxiv.org/pdf/1610.08462](https://arxiv.org/pdf/1610.08462)

