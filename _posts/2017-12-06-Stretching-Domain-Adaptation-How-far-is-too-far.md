---
layout: post
title: "Stretching Domain Adaptation: How far is too far?"
date: 2017-12-06 17:03:07
categories: arXiv_CV
tags: arXiv_CV CNN Deep_Learning Recognition
author: Yunhan Zhao, Haider Ali, Rene Vidal
mathjax: true
---

* content
{:toc}

##### Abstract
While deep learning has led to significant advances in visual recognition over the past few years, such advances often require a lot of annotated data. While unsupervised domain adaptation has emerged as an alternative approach that doesn't require as much annotated data, prior evaluations of domain adaptation have been limited to relatively simple datasets. This work pushes the state of the art in unsupervised domain adaptation through an in depth evaluation of AlexNet, DenseNet and Residual Transfer Networks (RTN) on multimodal benchmark datasets that shows and identifies which layers more effectively transfer features across different domains. We also modify the existing RTN architecture and propose a novel domain adaptation architecture called "Deep MagNet" that combines Deep Convolutional Blocks with multiple Maximum Mean Discrepancy losses. Our experiments show quantitative and qualitative improvements in performance of our method on benchmarking datasets for complex data domains.

##### Abstract (translated by Google)
虽然深度学习在过去几年导致了视觉识别方面的显着进步，但是这种进步往往需要大量的注释数据。虽然无监督领域的适应已经出现作为一种替代方法，不需要太多的注释数据，领域适应的先前评估已被限制在相对简单的数据集。这项工作通过对多模式基准数据集上的AlexNet，DenseNet和残余传输网络（RTN）进行深入评估，推动了无监督域适应领域的发展，显示并识别哪些层能够更有效地跨越不同领域传输特征。我们还修改了现有的RTN架构，并提出了一种称为“Deep MagNet”的新型域适配架构，将深度卷积块与多个最大平均偏差损失相结合。我们的实验显示了我们的方法在复杂数据域的基准数据集性能方面的定量和定性改进。

##### URL
[http://arxiv.org/abs/1712.02286](http://arxiv.org/abs/1712.02286)

##### PDF
[http://arxiv.org/pdf/1712.02286](http://arxiv.org/pdf/1712.02286)

