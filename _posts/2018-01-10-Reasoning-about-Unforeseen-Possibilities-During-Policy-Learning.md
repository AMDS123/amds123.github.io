---
layout: post
title: "Reasoning about Unforeseen Possibilities During Policy Learning"
date: 2018-01-10 12:16:43
categories: arXiv_AI
tags: arXiv_AI
author: Craig Innes, Alex Lascarides, Stefano V Albrecht, Subramanian Ramamoorthy, Benjamin Rosman
mathjax: true
---

* content
{:toc}

##### Abstract
Methods for learning optimal policies in autonomous agents often assume that the way the domain is conceptualised---its possible states and actions and their causal structure---is known in advance and does not change during learning. This is an unrealistic assumption in many scenarios, because new evidence can reveal important information about what is possible, possibilities that the agent was not aware existed prior to learning. We present a model of an agent which both discovers and learns to exploit unforeseen possibilities using two sources of evidence: direct interaction with the world and communication with a domain expert. We use a combination of probabilistic and symbolic reasoning to estimate all components of the decision problem, including its set of random variables and their causal dependencies. Agent simulations show that the agent converges on optimal polices even when it starts out unaware of factors that are critical to behaving optimally.

##### Abstract (translated by Google)
在自主机构中学习最优策略的方法通常假设领域被概念化的方式 - 其可能的状态和行为及其因果结构 - 事先是已知的，在学习过程中不会改变。在许多情况下，这是一个不切实际的假设，因为新的证据可以揭示关于什么是可能的重要信息，以及代理人在学习之前不知道的可能性。我们提出了一个代理模型，它发现和学习利用两个来源的证据不可预知的可能性：与世界的直接互动和与领域专家的沟通。我们使用概率和符号推理的组合来估计决策问题的所有组成部分，包括随机变量集合及其因果依赖关系。智能体仿真表明智能体即使在开始意识不到对最佳行为至关重要的因素时，也会收敛于最优策略。

##### URL
[https://arxiv.org/abs/1801.03331](https://arxiv.org/abs/1801.03331)

##### PDF
[https://arxiv.org/pdf/1801.03331](https://arxiv.org/pdf/1801.03331)

