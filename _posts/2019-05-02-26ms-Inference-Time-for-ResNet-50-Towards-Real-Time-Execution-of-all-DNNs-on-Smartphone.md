---
layout: post
title: "26ms Inference Time for ResNet-50: Towards Real-Time Execution of all DNNs on Smartphone"
date: 2019-05-02 04:37:27
categories: arXiv_CV
tags: arXiv_CV Optimization Inference
author: Wei Niu, Xiaolong Ma, Yanzhi Wang, Bin Ren
mathjax: true
---

* content
{:toc}

##### Abstract
With the rapid emergence of a spectrum of high-end mobile devices, many applications that required desktop-level computation capability formerly can now run on these devices without any problem. However, without a careful optimization, executing Deep Neural Networks (a key building block of the real-time video stream processing that is the foundation of many popular applications) is still challenging, specifically, if an extremely low latency or high accuracy inference is needed. This work presents CADNN, a programming framework to efficiently execute DNN on mobile devices with the help of advanced model compression (sparsity) and a set of thorough architecture-aware optimization. The evaluation result demonstrates that CADNN outperforms all the state-of-the-art dense DNN execution frameworks like TensorFlow Lite and TVM.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.00571](http://arxiv.org/abs/1905.00571)

##### PDF
[http://arxiv.org/pdf/1905.00571](http://arxiv.org/pdf/1905.00571)

