---
layout: post
title: "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"
date: 2018-03-04 00:20:29
categories: arXiv_AI
tags: arXiv_AI CNN RNN Deep_Learning
author: Shaojie Bai, J. Zico Kolter, Vladlen Koltun
mathjax: true
---

* content
{:toc}

##### Abstract
For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.

##### Abstract (translated by Google)
对于大多数深度学习实践者而言，序列建模与循环网络是同义的。然而最近的结果表明，卷积架构在音频合成和机器翻译等任务上可以胜过复发网络。给定一个新的序列建模任务或数据集，应该使用哪种架构？我们对序列建模的通用卷积和循环体系结构进行了系统评估。这些模型在广泛的标准任务中进行评估，这些任务通常用于对循环网络进行基准测试。我们的研究结果表明，简单的卷积体系结构在不同范围的任务和数据集上胜过规范的经常性网络，如LSTM，同时展示更长的有效内存。我们得出结论：应该重新考虑序列建模和循环网络之间的共同关联，卷积网络应该被认为是序列建模任务的一个自然起点。

##### URL
[http://arxiv.org/abs/1803.01271](http://arxiv.org/abs/1803.01271)

##### PDF
[http://arxiv.org/pdf/1803.01271](http://arxiv.org/pdf/1803.01271)

