---
layout: post
title: "SGAD: Soft-Guided Adaptively-Dropped Neural Network"
date: 2018-07-04 02:23:10
categories: arXiv_CV
tags: arXiv_CV
author: Zhisheng Wang, Fangxuan Sun, Jun Lin, Zhongfeng Wang, Bo Yuan
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks (DNNs) have been proven to have many redundancies. Hence, many efforts have been made to compress DNNs. However, the existing model compression methods treat all the input samples equally while ignoring the fact that the difficulties of various input samples being correctly classified are different. To address this problem, DNNs with adaptive dropping mechanism are well explored in this work. To inform the DNNs how difficult the input samples can be classified, a guideline that contains the information of input samples is introduced to improve the performance. Based on the developed guideline and adaptive dropping mechanism, an innovative soft-guided adaptively-dropped (SGAD) neural network is proposed in this paper. Compared with the 32 layers residual neural networks, the presented SGAD can reduce the FLOPs by 77% with less than 1% drop in accuracy on CIFAR-10.

##### Abstract (translated by Google)
已经证明深度神经网络（DNN）具有许多冗余。因此，已经进行了许多努力来压缩DNN。然而，现有的模型压缩方法同等地处理所有输入样本，而忽略了各种输入样本被正确分类的困难不同的事实。为了解决这个问题，在这项工作中充分探索了具有自适应丢弃机制的DNN。为了通知DNN输入样本的分类难度，引入了包含输入样本信息的指南以提高性能。基于开发的指南和自适应丢弃机制，本文提出了一种创新的软导向自适应丢弃（SGAD）神经网络。与32层残余神经网络相比，所提出的SGAD可以将FLOP降低77％，而CIFAR-10的精度下降不到1％。

##### URL
[http://arxiv.org/abs/1807.01430](http://arxiv.org/abs/1807.01430)

##### PDF
[http://arxiv.org/pdf/1807.01430](http://arxiv.org/pdf/1807.01430)

