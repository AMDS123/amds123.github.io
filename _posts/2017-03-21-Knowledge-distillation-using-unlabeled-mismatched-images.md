---
layout: post
title: "Knowledge distillation using unlabeled mismatched images"
date: 2017-03-21 10:34:59
categories: arXiv_CV
tags: arXiv_CV Knowledge Image_Classification Classification
author: Mandar Kulkarni, Kalpesh Patil, Shirish Karande
mathjax: true
---

* content
{:toc}

##### Abstract
Current approaches for Knowledge Distillation (KD) either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance. Our examples include use of various datasets for stimulating MNIST and CIFAR teachers.

##### Abstract (translated by Google)
目前的知识蒸馏方法（KD）要么直接使用训练数据，要么来自训练数据分布。在本文中，我们证明了'错配'的未标记刺激对于图像分类网络执行KD的有效性。为了说明，我们考虑这种情况，即完全没有训练数据，或者不匹配的激励必须用于增加少量的训练数据。我们证明，刺激的复杂性是精馏的良好表现的关键因素。我们的例子包括使用各种数据集来刺激MNIST和CIFAR教师。

##### URL
[https://arxiv.org/abs/1703.07131](https://arxiv.org/abs/1703.07131)

##### PDF
[https://arxiv.org/pdf/1703.07131](https://arxiv.org/pdf/1703.07131)

