---
layout: post
title: "Extracting Universal Representations of Cognition across Brain-Imaging Studies"
date: 2018-09-17 06:19:11
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Arthur Mensch (PARIETAL), Julien Mairal (CNRS, LJK, Grenoble INP), Bertrand Thirion (PARIETAL), Ga&#xeb;l Varoquaux (PARIETAL)
mathjax: true
---

* content
{:toc}

##### Abstract
The size of publicly available data in cognitive neuro-imaging has increased a lot in recent years, thanks to strong research and community efforts. Exploiting this wealth of data demands new methods to turn the heterogeneous cognitive information held in different task-fMRI studies into common-universal-cognitive models. In this paper, we pool data from large fMRI repositories to predict psychological conditions from statistical brain maps across different studies and subjects. We leverage advances in deep learning, intermediate representations and multi-task learning to learn universal interpretable low-dimensional representations of brain images, usable for predicting psychological stimuli in all input studies. The method improves decoding performance for 80% of studies, by permitting cognitive information to flow from every study to the others: it notably gives a strong performance boost when decoding studies of small size. The trained low-dimensional representation-task-optimized networks-is interpretable as a set of basis cognitive dimensions relevant to meaningful categories of cognitive stimuli. Our approach opens new ways of extracting information from brain maps, overcoming the low power of typical fMRI studies.

##### Abstract (translated by Google)
由于强大的研究和社区努力，近年来认知神经成像中公开数据的大小增加了很多。利用这些丰富的数据需要新的方法将不同任务-fMRI研究中持有的异构认知信息转化为共同的通用认知模型。在本文中，我们汇集来自大型fMRI资料库的数据，以预测不同研究和受试者的统计脑图的心理状况。我们利用深度学习，中间表征和多任务学习的进步来学习脑图像的通用可解释的低维表示，可用于预测所有输入研究中的心理刺激。该方法通过允许认知信息从每项研究流向其他研究，提高了80％研究的解码性能：在解码小规模研究时，它显着提高了性能。训练的低维表示 - 任务优化网络 - 可被解释为与有意义的认知刺激类别相关的一组基础认知维度。我们的方法开辟了从脑图中提取信息的新方法，克服了典型fMRI研究的低功效。

##### URL
[http://arxiv.org/abs/1809.06035](http://arxiv.org/abs/1809.06035)

##### PDF
[http://arxiv.org/pdf/1809.06035](http://arxiv.org/pdf/1809.06035)

