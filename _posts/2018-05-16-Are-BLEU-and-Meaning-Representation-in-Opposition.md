---
layout: post
title: "Are BLEU and Meaning Representation in Opposition?"
date: 2018-05-16 21:42:21
categories: arXiv_CL
tags: arXiv_CL Attention NMT Classification
author: Ond&#x159;ej C&#xed;fka, Ond&#x159;ej Bojar
mathjax: true
---

* content
{:toc}

##### Abstract
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.

##### Abstract (translated by Google)
获得连续空间句子表示的可能方式之一是通过训练神经机器翻译（NMT）系统。然而，最近的注意机制去除了可以从中提取源句子表示的神经网络中的单个点。我们提出了细心的NMT架构的几个变种，使这个会议点回来。实证评估表明，翻译质量越好，所学习的句子表示就会在广泛的分类和相似性任务中发挥作用。

##### URL
[http://arxiv.org/abs/1805.06536](http://arxiv.org/abs/1805.06536)

##### PDF
[http://arxiv.org/pdf/1805.06536](http://arxiv.org/pdf/1805.06536)

