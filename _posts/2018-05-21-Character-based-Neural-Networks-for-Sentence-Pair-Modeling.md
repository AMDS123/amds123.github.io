---
layout: post
title: "Character-based Neural Networks for Sentence Pair Modeling"
date: 2018-05-21 21:36:09
categories: arXiv_CL
tags: arXiv_CL Embedding Inference Language_Model
author: Wuwei Lan, Wei Xu
mathjax: true
---

* content
{:toc}

##### Abstract
Sentence pair modeling is critical for many NLP tasks, such as paraphrase identification, semantic textual similarity, and natural language inference. Most state-of-the-art neural models for these tasks rely on pretrained word embedding and compose sentence-level semantics in varied ways; however, few works have attempted to verify whether we really need pretrained embeddings in these tasks. In this paper, we study how effective subword-level (character and character n-gram) representations are in sentence pair modeling. Though it is well-known that subword models are effective in tasks with single sentence input, including language modeling and machine translation, they have not been systematically studied in sentence pair modeling tasks where the semantic and string similarities between texts matter. Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification.

##### Abstract (translated by Google)
句子对建模对许多NLP任务而言至关重要，例如释义识别，语义文本相似性和自然语言推理。大多数用于这些任务的最先进的神经模型依赖于预训练词嵌入并以各种方式组成语句级语义;然而，很少有作品试图验证我们是否真的需要预嵌入这些任务。在本文中，我们研究了句子对建模中有效的子字级（字符和字符n-gram）表示。尽管众所周知，子句模型在单语输入任务（包括语言建模和机器翻译）方面是有效的，但在文本之间的语义和字符串相似性很重要的句子对建模任务中尚未系统地进行研究。我们的实验表明，没有任何预训练词嵌入的子词模型可以在两个社交媒体数据集上获得新的最新结果，并且在新闻数据上进行复述识别的竞争结果。

##### URL
[https://arxiv.org/abs/1805.08297](https://arxiv.org/abs/1805.08297)

##### PDF
[https://arxiv.org/pdf/1805.08297](https://arxiv.org/pdf/1805.08297)

