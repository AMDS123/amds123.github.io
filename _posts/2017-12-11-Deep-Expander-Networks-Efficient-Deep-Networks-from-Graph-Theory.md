---
layout: post
title: "Deep Expander Networks: Efficient Deep Networks from Graph Theory"
date: 2017-12-11 08:41:56
categories: arXiv_CV
tags: arXiv_CV Sparse Image_Classification Inference Classification
author: Ameya Prabhu, Girish Varma, Anoop Namboodiri
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks, while being unreasonably effective for several vision tasks, have their usage limited by the computational and memory requirements, both during training and inference stages. Analyzing and improving the connectivity patterns between layers of a network has resulted in several compact architectures like GoogleNet, ResNet and DenseNet-BC. In this work, we utilize results from graph theory to develop an efficient connection pattern between consecutive layers. Specifically, we use {\it expander graphs} that have excellent connectivity properties to develop a sparse network architecture, the deep expander network (X-Net). The X-Nets are shown to have high connectivity for a given level of sparsity. We also develop highly efficient training and inference algorithms for such networks. Experimental results show that we can achieve the similar or better accuracy as DenseNet-BC with two-thirds the number of parameters and FLOPs on several image classification benchmarks. We hope that this work motivates other approaches to utilize results from graph theory to develop efficient network architectures.

##### Abstract (translated by Google)
深度神经网络虽然对于多个视觉任务不合理有效，但其使用受到计算和存储要求的限制，无论是在训练阶段还是在推理阶段。分析和改善网络层之间的连接模式已经导致几个紧凑的体系结构，如GoogleNet，ResNet和DenseNet-BC。在这项工作中，我们利用图论的结果来开发连续层之间的高效连接模式。具体而言，我们使用具有优良连接属性的{\ it expander graph}来开发一个稀疏的网络体系结构 - 深度扩展网络（X-Net）。对于给定的稀疏水平，X网被证明具有高连接性。我们还为这些网络开发高效的训练和推理算法。实验结果表明，与DenseNet-BC相比，我们可以达到相似或更好的精度，在几个图像分类基准上，参数数量和FLOP数量可以达到三分之二。我们希望这个工作可以激励其他方法利用图论的结果来开发高效的网络架构。

##### URL
[http://arxiv.org/abs/1711.08757](http://arxiv.org/abs/1711.08757)

##### PDF
[http://arxiv.org/pdf/1711.08757](http://arxiv.org/pdf/1711.08757)

