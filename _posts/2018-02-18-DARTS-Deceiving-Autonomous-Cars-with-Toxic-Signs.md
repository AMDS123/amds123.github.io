---
layout: post
title: "DARTS: Deceiving Autonomous Cars with Toxic Signs"
date: 2018-02-18 19:39:28
categories: arXiv_CV
tags: arXiv_CV Adversarial Classification Recognition
author: Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang, Prateek Mittal
mathjax: true
---

* content
{:toc}

##### Abstract
Sign recognition is an integral part of autonomous cars. Any misclassification of traffic signs can potentially lead to a multitude of disastrous consequences, ranging from a life-threatening accident to a large-scale interruption of transportation services relying on autonomous cars. In this paper, we propose and examine realistic security attacks against sign recognition systems for Deceiving Autonomous caRs with Toxic Signs (we call the proposed attacks DARTS). 
 Leveraging the concept of adversarial examples, we modify innocuous signs/advertisements in the environment in such a way that they seem normal to human observers but are interpreted as the adversary's desired traffic sign by autonomous cars. Further, we pursue a fundamentally different perspective to attacking autonomous cars, motivated by the observation that the driver and vehicle-mounted camera see the environment from different angles (the camera commonly sees the road with a higher angle, e.g., from top of the car). We propose a novel attack against vehicular sign recognition systems: we create signs that change as they are viewed from different angles, and thus, can be interpreted differently by the driver and sign recognition. 
 We extensively evaluate the proposed attacks under various conditions: different distances, lighting conditions, and camera angles. We first examine our attacks virtually, i.e., we check if the digital images of toxic signs can deceive the sign recognition system. Further, we investigate the effectiveness of attacks in real-world settings: we print toxic signs, install them in the environment, capture videos using a vehicle-mounted camera, and process them using our sign recognition pipeline.

##### Abstract (translated by Google)
标志识别是自动驾驶汽车的一个组成部分。对交通标志的任何错误分类都可能导致多种灾难性后果，从威胁生命的事故到依赖自动驾驶汽车的大规模交通服务中断。在本文中，我们提出并研究了针对标识识别系统的实际安全攻击，用于欺骗具有毒性标志的自主计算机（我们称之为DARTS攻击）。
 利用对抗性例子的概念，我们修改环境中的无害标志/广告，使其看起来像人类观察者一样正常，但被自动驾驶汽车解释为对手想要的交通标志。此外，我们追求从根本上不同的角度来攻击自动驾驶汽车，因为观察到驾驶员和车载摄像头从不同角度看环境（摄像头通常以更高的角度看到道路，例如从汽车的顶部）。我们提出了一种针对车辆标志识别系统的新型攻击：我们创建的标志随着从不同角度进行查看而发生变化，因此可以通过驾驶员和标志识别进行不同的解释。
 我们在各种条件下广泛评估提出的攻击：不同的距离，照明条件和摄像机角度。我们首先检查我们的攻击事件，即检查有毒标志的数字图像是否会欺骗标志识别系统。此外，我们还调查了现实环境中攻击的有效性：我们打印有毒标志，将其安装在环境中，使用车载摄像头捕获视频，并使用我们的标志识别管线进行处理。

##### URL
[http://arxiv.org/abs/1802.06430](http://arxiv.org/abs/1802.06430)

##### PDF
[http://arxiv.org/pdf/1802.06430](http://arxiv.org/pdf/1802.06430)

