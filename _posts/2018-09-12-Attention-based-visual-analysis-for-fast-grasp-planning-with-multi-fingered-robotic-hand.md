---
layout: post
title: "Attention based visual analysis for fast grasp planning with multi-fingered robotic hand"
date: 2018-09-12 02:14:50
categories: arXiv_RO
tags: arXiv_RO Attention CNN
author: Zhen Deng, Ge Gao, Simone Frintrop, Jianwei Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
We present an attention based visual analysis framework to compute grasp-relevant information in order to guide grasp planning using a multi-fingered robotic hand. Our approach uses a computational visual attention model to locate regions of interest in a scene, and uses a deep convolutional neural network to detect grasp type and point for a sub-region of the object presented in a region of interest. We demonstrate the proposed framework in object grasping tasks, in which the information generated from the proposed framework is used as prior information to guide the grasp planning. Results show that the proposed framework can not only speed up grasp planning with more stable configurations, but also is able to handle unknown objects. Furthermore, our framework can handle cluttered scenarios. A new Grasp Type Dataset (GTD) that considers 6 commonly used grasp types and covers 12 household objects is also presented.

##### Abstract (translated by Google)
我们提出了一个基于注意力的视觉分析框架来计算掌握相关信息，以指导使用多指机器人手掌握计划。我们的方法使用计算视觉注意模型来定位场景中的感兴趣区域，并使用深度卷积神经网络来检测在感兴趣区域中呈现的对象的子区域的抓握类型和点。我们在对象抓取任务中演示了所提出的框架，其中从提出的框架生成的信息被用作指导抓握计划的先验信息。结果表明，所提出的框架不仅可以加快掌握规划，配置更稳定，而且能够处理未知对象。此外，我们的框架可以处理混乱的场景。还介绍了一种新的Grasp Type Dataset（GTD），它考虑了6种常用的抓取类型并涵盖了12种家用物品。

##### URL
[https://arxiv.org/abs/1809.04226](https://arxiv.org/abs/1809.04226)

##### PDF
[https://arxiv.org/pdf/1809.04226](https://arxiv.org/pdf/1809.04226)

