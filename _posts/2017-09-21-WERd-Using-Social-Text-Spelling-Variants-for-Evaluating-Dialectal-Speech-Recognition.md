---
layout: post
title: "WERd: Using Social Text Spelling Variants for Evaluating Dialectal Speech Recognition"
date: 2017-09-21 18:37:36
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition Recognition
author: Ahmed Ali, Preslav Nakov, Peter Bell, Steve Renals
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Such a situation is typical for machine translation (MT), and thus we borrow ideas from an MT evaluation metric, namely TERp, an extension of translation error rate which is closely-related to WER. In particular, in the process of comparing a hypothesis to a reference, we make use of spelling variants for words and phrases, which we mine from Twitter in an unsupervised fashion. Our experiments with evaluating ASR output for Egyptian Arabic, and further manual analysis, show that the resulting WERd (i.e., WER for dialects) metric, a variant of TERp, is more adequate than WER for evaluating dialectal ASR.

##### Abstract (translated by Google)
我们研究评估面向方言语音输入的自动语音识别（ASR）系统的问题。在这种情况下，一个主要的挑战是方言的拼写方式通常不是标准化的。从ASR评估的角度来看，这意味着预期产出没有明确的黄金标准，根据不同的人类注释者，可能会有几个可能的输出被认为是正确的，这使得标准误词率（WER）不足以作为评估指标。这种情况对于机器翻译（MT）是典型的，因此我们借鉴了MT评估指标的思想，即TERp，这是与WER密切相关的翻译错误率的扩展。特别是，在比较假设和参考的过程中，我们利用了拼写变体的单词和短语，我们从Twitter以无监督的方式挖掘。我们对埃及阿拉伯语的ASR输出进行评估的实验和进一步的手工分析表明，由此产生的WERd（即方言的WER）度量（TERp的变体）比WER评估方言ASR更充分。

##### URL
[https://arxiv.org/abs/1709.07484](https://arxiv.org/abs/1709.07484)

##### PDF
[https://arxiv.org/pdf/1709.07484](https://arxiv.org/pdf/1709.07484)

