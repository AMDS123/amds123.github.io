---
layout: post
title: "Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification"
date: 2016-11-12 09:25:54
categories: arXiv_CV
tags: arXiv_CV Classification
author: Fuqiang Liu, Fukun Bi, Yiding Yang, Liang Chen
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a universal method, Boost Picking, to train supervised classification models mainly by un-labeled data. Boost Picking only adopts two weak classifiers to estimate and correct the error. It is theoretically proved that Boost Picking could train a supervised model mainly by un-labeled data as effectively as the same model trained by 100% labeled data, only if recalls of the two weak classifiers are all greater than zero and the sum of precisions is greater than one. Based on Boost Picking, we present "Test along with Training (TawT)" to improve the generalization of supervised models. Both Boost Picking and TawT are successfully tested in varied little data sets.

##### Abstract (translated by Google)
本文提出了一种通用方法Boost Picking，主要通过未标注的数据来训练监督分类模型。 Boost Picking只采用两个弱分类器来估计和纠正错误。从理论上证明，只有两个弱分类器的召回率都大于零，且精度总和为零时，Boost Picking才可以训练一个主要由未标记数据组成的监督模型，与100％标记数据训练的相同模型一样有效大于一个。基于Boost Picking，我们提出“随着训练测试（TawT）”来改进监督模型的推广。 Boost Picking和TawT都在不同的小数据集中成功测试。

##### URL
[https://arxiv.org/abs/1602.05659](https://arxiv.org/abs/1602.05659)

##### PDF
[https://arxiv.org/e-print/1602.05659](https://arxiv.org/e-print/1602.05659)

