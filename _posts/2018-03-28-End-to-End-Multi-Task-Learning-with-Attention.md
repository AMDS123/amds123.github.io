---
layout: post
title: "End-to-End Multi-Task Learning with Attention"
date: 2018-03-28 16:15:45
categories: arXiv_CV
tags: arXiv_CV Attention
author: Shikun Liu, Edward Johns, Andrew J. Davison
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a novel multi-task learning architecture, which incorporates recent advances in attention mechanisms. Our approach, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with task-specific soft-attention modules, which are trainable in an end-to-end manner. These attention modules allow for learning of task-specific features from the global pool, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. Experiments on the CityScapes dataset show that our method outperforms several baselines in both single-task and multi-task learning, and is also more robust to the various weighting schemes in the multi-task loss function. We further explore the effectiveness of our method through experiments over a range of task complexities, and show how our method scales well with task complexity compared to baselines.

##### Abstract (translated by Google)
在本文中，我们提出了一种新颖的多任务学习架构，它包含了关注机制方面的最新进展。我们的方法 - 多任务注意网络（MTAN）由单个共享网络组成，其中包含一个全局特征池，以及可以端对端方式进行训练的特定于任务的软注意模块。这些注意模块允许从全局池中学习任务特定功能，同时允许在不同任务之间共享功能。该架构可以建立在任何前馈神经网络之上，易于实现并且参数有效。 CityScapes数据集上的实验表明，我们的方法在单任务和多任务学习中均优于多个基线，并且对于多任务丢失函数中的各种加权方案也更加健壮。我们通过对一系列任务复杂性进行实验，进一步探索了我们方法的有效性，并展示了我们的方法与基线相比如何随着任务复杂性而很好地扩展。

##### URL
[https://arxiv.org/abs/1803.10704](https://arxiv.org/abs/1803.10704)

##### PDF
[https://arxiv.org/pdf/1803.10704](https://arxiv.org/pdf/1803.10704)

