---
layout: post
title: "Generative Deep Neural Networks for Dialogue: A Short Review"
date: 2016-11-18 20:11:51
categories: arXiv_CL
tags: arXiv_CL Review Knowledge
author: Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau
mathjax: true
---

* content
{:toc}

##### Abstract
Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.

##### Abstract (translated by Google)
研究人员最近开始研究深度神经网络的对话应用。具体而言，生成序列到序列（Seq2Seq）模型已经显示出非结构化任务（如字级对话响应生成）的有希望的结果。希望这样的模型将能够利用大量的数据来学习有意义的自然语言表示和响应生成策略，同时需要最少量的领域知识和手工制作。一个重要的挑战是开发能够有效结合对话背景并产生有意义和多样化反应的模式。为了支持这个目标，我们回顾了最近提出的基于生成编码器 - 解码器神经网络架构的模型，并且表明这些模型具有更好的能力来结合长期的对话历史，模拟对话中的不确定性和模糊性，高层次的构图结构。

##### URL
[https://arxiv.org/abs/1611.06216](https://arxiv.org/abs/1611.06216)

##### PDF
[https://arxiv.org/pdf/1611.06216](https://arxiv.org/pdf/1611.06216)

