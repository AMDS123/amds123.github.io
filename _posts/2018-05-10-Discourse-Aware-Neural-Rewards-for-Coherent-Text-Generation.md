---
layout: post
title: "Discourse-Aware Neural Rewards for Coherent Text Generation"
date: 2018-05-10 00:51:06
categories: arXiv_CL
tags: arXiv_CL Text_Generation Reinforcement_Learning
author: Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, Yejin Choi
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we investigate the use of discourse-aware rewards with reinforcement learning to guide a model to generate long, coherent text. In particular, we propose to learn neural rewards to model cross-sentence ordering as a means to approximate desired discourse structure. Empirical results demonstrate that a generator trained with the learned reward produces more coherent and less repetitive text than models trained with cross-entropy or with reinforcement learning with commonly used scores as rewards.

##### Abstract (translated by Google)
在本文中，我们调查了使用强化学习的话语意识奖励来指导模型生成长而连贯的文本。特别是，我们建议学习神经奖励来模拟交叉句子排序，作为一种近似理想话语结构的手段。实证结果表明，使用学习奖励训练的生成器比使用交叉熵训练的模型或以常用分数作为奖励的强化学习产生更连贯且重复性更低的文本。

##### URL
[http://arxiv.org/abs/1805.03766](http://arxiv.org/abs/1805.03766)

##### PDF
[http://arxiv.org/pdf/1805.03766](http://arxiv.org/pdf/1805.03766)

