---
layout: post
title: "Learning to Embed Words in Context for Syntactic Tasks"
date: 2017-06-12 01:42:12
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Lifu Tu, Kevin Gimpel, Karen Livescu
mathjax: true
---

* content
{:toc}

##### Abstract
We present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.

##### Abstract (translated by Google)
我们提出了在周围的单词中嵌入单词的模型。这种模型，我们称之为令牌嵌入，表示一个单词的特征，这个单词是特定于特定的上下文的，例如单词意义，句法范畴和语义角色。我们探索基于标准神经网络体系结构的简单，高效的令牌嵌入模型。我们学习了大量未注释的文本中的令牌嵌入，并将它们评估为在少量注释数据上训练的词性标注器和依赖解析器的特性。我们发现赋予令牌嵌入的预测因子在一系列上下文窗口和训练集大小范围内始终优于基线预测值。

##### URL
[https://arxiv.org/abs/1706.02807](https://arxiv.org/abs/1706.02807)

##### PDF
[https://arxiv.org/pdf/1706.02807](https://arxiv.org/pdf/1706.02807)

