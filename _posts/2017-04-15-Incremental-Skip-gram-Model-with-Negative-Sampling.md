---
layout: post
title: "Incremental Skip-gram Model with Negative Sampling"
date: 2017-04-15 07:15:00
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Nobuhiro Kaji, Hayato Kobayashi
mathjax: true
---

* content
{:toc}

##### Abstract
This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.

##### Abstract (translated by Google)
本文从经验和理论两个角度探讨了负样本跳跃模型（SGNS）的增量训练策略。神经词嵌入的现有方法，包括SGNS，是多通道算法，因此不能执行增量模型更新。为了解决这个问题，我们提出一个SGNS的简单递增的扩展，并提供一个彻底的理论分析来证明其有效性。实证实验证明了理论分析的正确性以及增量算法的实用性。

##### URL
[https://arxiv.org/abs/1704.03956](https://arxiv.org/abs/1704.03956)

##### PDF
[https://arxiv.org/pdf/1704.03956](https://arxiv.org/pdf/1704.03956)

