---
layout: post
title: "Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation"
date: 2018-02-12 19:38:44
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Dane Corneil, Wulfram Gerstner, Johanni Brea
mathjax: true
---

* content
{:toc}

##### Abstract
Modern reinforcement learning algorithms reach super-human performance in many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce VaST (Variational State Tabulation), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular environment. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.

##### Abstract (translated by Google)
现代强化学习算法在许多电路板和视频游戏中都能达到超人类的表现，但它们效率不高，即通常需要比人类更多的玩乐体验才能达到相同的性能水平。为了提高样本效率，代理人可以建立环境模型并使用规划方法来更新其政策。在本文中，我们介绍VaST（变分状态表），它将具有高维状态空间（例如视觉输入空间）的环境映射到抽象表格环境。可以使用小型备份优先扫描高效的计划方法来更新状态操作值。我们展示了VaST如何快速学会如何在3D导航等任务中获得最大的回报，并有效地适应奖励或转换概率的突然变化。

##### URL
[http://arxiv.org/abs/1802.04325](http://arxiv.org/abs/1802.04325)

##### PDF
[http://arxiv.org/pdf/1802.04325](http://arxiv.org/pdf/1802.04325)

