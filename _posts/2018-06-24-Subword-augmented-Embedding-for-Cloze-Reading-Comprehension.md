---
layout: post
title: "Subword-augmented Embedding for Cloze Reading Comprehension"
date: 2018-06-24 08:18:12
categories: arXiv_CL
tags: arXiv_CL Embedding Represenation_Learning Deep_Learning
author: Zhuosheng Zhang, Yafang Huang, Hai Zhao
mathjax: true
---

* content
{:toc}

##### Abstract
Representation learning is the foundation of machine reading comprehension. In state-of-the-art models, deep learning methods broadly use word and character level representations. However, character is not naturally the minimal linguistic unit. In addition, with a simple concatenation of character and word embedding, previous models actually give suboptimal solution. In this paper, we propose to use subword rather than character for word embedding enhancement. We also empirically explore different augmentation strategies on subword-augmented embedding to enhance the cloze-style reading comprehension model reader. In detail, we present a reader that uses subword-level representation to augment word embedding with a short list to handle rare words effectively. A thorough examination is conducted to evaluate the comprehensive performance and generalization ability of the proposed reader. Experimental results show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets.

##### Abstract (translated by Google)
表征学习是机器阅读理解的基础。在最先进的模型中，深度学习方法广泛使用单词和字符级别表示。然而，性格本身并不是最小的语言单位。另外，通过字符和词嵌入的简单连接，以前的模型实际上给出次优解。在本文中，我们建议使用子字而不是字符来增强词嵌入。我们还通过实证探索了不同的增强子词的嵌入策略，以增强填充式阅读理解模型阅读器。具体来说，我们提供了一个使用子词级表示来扩充单词嵌入的短名单以有效处理罕见单词的阅读器。进行彻底检查以评估拟议读者的综合性能和泛化能力。实验结果表明，所提出的方法可以帮助读者显着优于各种公共数据集的最新基线。

##### URL
[http://arxiv.org/abs/1806.09103](http://arxiv.org/abs/1806.09103)

##### PDF
[http://arxiv.org/pdf/1806.09103](http://arxiv.org/pdf/1806.09103)

