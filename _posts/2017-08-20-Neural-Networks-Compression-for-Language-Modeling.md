---
layout: post
title: "Neural Networks Compression for Language Modeling"
date: 2017-08-20 13:37:06
categories: arXiv_CL
tags: arXiv_CL Inference RNN Language_Model
author: Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (RNNs). It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treebank (PTB) dataset we compare pruning, quantization, low-rank factorization, tensor train decomposition for LSTM networks in terms of model size and suitability for fast inference.

##### Abstract (translated by Google)
在本文中，我们考虑了几种基于递归神经网络（RNN）的语言建模问题的压缩技术。众所周知，传统的RNN，例如语言建模中基于LSTM的网络，具有高空间复杂度或大量推理时间的特点。这个问题对于移动应用程序来说尤其重要，在这个应用程序中，与远程服务器的持续交互是不合适通过使用Penn Treebank（PTB）数据集，我们比较了LSTM网络的修剪，量化，低秩因子分解，张量列车分解的模型大小和适合于快速推理。

##### URL
[https://arxiv.org/abs/1708.05963](https://arxiv.org/abs/1708.05963)

##### PDF
[https://arxiv.org/pdf/1708.05963](https://arxiv.org/pdf/1708.05963)

