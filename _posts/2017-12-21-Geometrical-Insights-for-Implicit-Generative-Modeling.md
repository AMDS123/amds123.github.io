---
layout: post
title: "Geometrical Insights for Implicit Generative Modeling"
date: 2017-12-21 08:11:44
categories: arXiv_AI
tags: arXiv_AI
author: Leon Bottou, Martin Arjovsky, David Lopez-Paz, Maxime Oquab
mathjax: true
---

* content
{:toc}

##### Abstract
Learning algorithms for implicit generative models can optimize a variety of criteria that measure how the data distribution differs from the implicit model distribution, including the Wasserstein distance, the Energy distance, and the Maximum Mean Discrepancy criterion. A careful look at the geometries induced by these distances on the space of probability measures reveals interesting differences. In particular, we can establish surprising approximate global convergence guarantees for the $1$-Wasserstein distance,even when the parametric generator has a nonconvex parametrization.

##### Abstract (translated by Google)
用于隐式生成模型的学习算法可以优化各种标准，以测量数据分布与隐式模型分布的不同，包括Wasserstein距离，能量距离和最大平均偏差标准。仔细观察由这些距离在概率测量空间上引起的几何形状揭示了有趣的差异。特别是，即使参数发生器具有非凸参数化，我们也可以为$ 1 $ -Wasserstein距离建立令人惊讶的近似全局收敛保证。

##### URL
[http://arxiv.org/abs/1712.07822](http://arxiv.org/abs/1712.07822)

##### PDF
[http://arxiv.org/pdf/1712.07822](http://arxiv.org/pdf/1712.07822)

