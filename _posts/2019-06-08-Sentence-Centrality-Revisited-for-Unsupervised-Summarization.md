---
layout: post
title: "Sentence Centrality Revisited for Unsupervised Summarization"
date: 2019-06-08 19:27:31
categories: arXiv_CL
tags: arXiv_CL Summarization Represenation_Learning
author: Hao Zheng, Mirella Lapata
mathjax: true
---

* content
{:toc}

##### Abstract
Single document summarization has enjoyed renewed interests in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a)~we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b)~we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.03508](http://arxiv.org/abs/1906.03508)

##### PDF
[http://arxiv.org/pdf/1906.03508](http://arxiv.org/pdf/1906.03508)

