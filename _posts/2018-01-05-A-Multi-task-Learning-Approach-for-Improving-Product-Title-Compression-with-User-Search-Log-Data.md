---
layout: post
title: "A Multi-task Learning Approach for Improving Product Title Compression with User Search Log Data"
date: 2018-01-05 11:52:44
categories: arXiv_CL
tags: arXiv_CL Attention Summarization Embedding Optimization
author: Jingang Wang, Junfeng Tian, Long Qiu, Sheng Li, Jun Lang, Luo Si, Man Lan
mathjax: true
---

* content
{:toc}

##### Abstract
It is a challenging and practical research problem to obtain effective compression of lengthy product titles for E-commerce. This is particularly important as more and more users browse mobile E-commerce apps and more merchants make the original product titles redundant and lengthy for Search Engine Optimization. Traditional text summarization approaches often require a large amount of preprocessing costs and do not capture the important issue of conversion rate in E-commerce. This paper proposes a novel multi-task learning approach for improving product title compression with user search log data. In particular, a pointer network-based sequence-to-sequence approach is utilized for title compression with an attentive mechanism as an extractive method and an attentive encoder-decoder approach is utilized for generating user search queries. The encoding parameters (i.e., semantic embedding of original titles) are shared among the two tasks and the attention distributions are jointly optimized. An extensive set of experiments with both human annotated data and online deployment demonstrate the advantage of the proposed research for both compression qualities and online business values.

##### Abstract (translated by Google)
对电子商务的冗长产品标题进行有效压缩是一个具有挑战性和实践性的研究问题。随着越来越多的用户浏览移动电子商务应用程序，越来越多的商家将搜索引擎优化的原始产品称号冗余冗长，这一点尤其重要。传统的文本摘要方法往往需要大量的预处理成本，而不能捕捉到电子商务中转换率的重要问题。本文提出了一种利用用户搜索日志数据改进产品标题压缩的多任务学习方法。特别地，基于指针网络的序列到序列方法被用于标题压缩，其中作为提取方法的是细致的机制，并且利用周密的编码器 - 解码器方法来生成用户搜索查询。在两个任务之间共享编码参数（即原始标题的语义嵌入），并且关注分布被共同优化。对人工注释数据和在线部署进行的大量实验证明了所提出的压缩质量和在线业务价值的研究的优势。

##### URL
[http://arxiv.org/abs/1801.01725](http://arxiv.org/abs/1801.01725)

##### PDF
[http://arxiv.org/pdf/1801.01725](http://arxiv.org/pdf/1801.01725)

