---
layout: post
title: "Dirichlet Variational Autoencoder for Text Modeling"
date: 2018-10-31 22:04:22
categories: arXiv_AI
tags: arXiv_AI Classification
author: Yijun Xiao, Tiancheng Zhao, William Yang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce an improved variational autoencoder (VAE) for text modeling with topic information explicitly modeled as a Dirichlet latent variable. By providing the proposed model topic awareness, it is more superior at reconstructing input texts. Furthermore, due to the inherent interactions between the newly introduced Dirichlet variable and the conventional multivariate Gaussian variable, the model is less prone to KL divergence vanishing. We derive the variational lower bound for the new model and conduct experiments on four different data sets. The results show that the proposed model is superior at text reconstruction across the latent space and classifications on learned representations have higher test accuracies.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.00135](http://arxiv.org/abs/1811.00135)

##### PDF
[http://arxiv.org/pdf/1811.00135](http://arxiv.org/pdf/1811.00135)

