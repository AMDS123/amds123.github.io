---
layout: post
title: "Improving Language Modeling using Densely Connected Recurrent Neural Networks"
date: 2017-07-19 14:49:35
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Fréderic Godin, Joni Dambre, Wesley De Neve
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al. 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.

##### Abstract (translated by Google)
在本文中，我们将密集连接层的新概念引入到递归神经网络中。我们在Penn Treebank语言建模任务上评估我们提出的架构。我们表明，我们可以获得类似的困惑分数与参加比较标准堆叠2层LSTM模型（Zaremba等2014）的参数少六倍。与目前跳过连接的使用情况相比，我们显示只有少数几个堆叠的层与跳过连接密集连接已经显着减少了困惑。

##### URL
[https://arxiv.org/abs/1707.06130](https://arxiv.org/abs/1707.06130)

##### PDF
[https://arxiv.org/pdf/1707.06130](https://arxiv.org/pdf/1707.06130)

