---
layout: post
title: "Troy: Give Attention to Saliency and for Saliency"
date: 2018-08-04 11:57:27
categories: arXiv_CV
tags: arXiv_CV Salient Segmentation Attention CNN Inference Classification Prediction Detection
author: Pingping Zhang, Huchuan Lu, Chunhua Shen
mathjax: true
---

* content
{:toc}

##### Abstract
Saliency detection or foreground segmentation is a fundamental and important task in computer vision, which can be treated as a pixel-wise classification problem. Recently, although fully convolutional network (FCN) based approaches have made remarkable progress in this task, segmenting salient objects in complex image scenes is still a challenging problem. In this paper, we argue that, when predicting the saliency of a given pixel, human-like attention mechanisms play an important role in structural saliency inference. Therefore, we propose a simple yet surprisingly effective self-gated soft-attention mechanism for fast saliency detection. The soft-attention mechanism generates a gating signal that is end-to-end trainable, which allows deep networks to contextualize local information useful for saliency prediction. In addition, the proposed attention mechanism is channel-wise, generic and can be easily incorporated into any existing FCN architectures like Trojan Horse, while only requiring negligible parameters. Extensive experiments verify the superior effectiveness of the proposed method. More specifically, our method achieves a new state-of-the-art performance on seven public saliency benchmarks, and outperforms the very recent methods with a large margin.

##### Abstract (translated by Google)
显着性检测或前景分割是计算机视觉中的基本且重要的任务，可以将其视为像素分类问题。最近，尽管基于完全卷积网络（FCN）的方法在该任务中取得了显着进步，但是在复杂图像场景中对显着对象进行分割仍然是一个具有挑战性的问题。在本文中，我们认为，在预测给定像素的显着性时，类人注意机制在结构显着性推断中起着重要作用。因此，我们提出了一种简单但令人惊讶的有效自我门控软注意机制，用于快速显着性检测。软关注机制产生端到端可训练的门控信号，这允许深度网络将对于显着性预测有用的本地信息进行上下文化。此外，建议的注意机制是通道的，通用的，可以很容易地结合到任何现有的FCN架构，如特洛伊木马，而只需要可忽略的参数。大量实验验证了该方法的优越性。更具体地说，我们的方法在七个公共显着性基准上实现了最新的最新性能，并且在很大程度上优于最近的方法。

##### URL
[http://arxiv.org/abs/1808.02373](http://arxiv.org/abs/1808.02373)

##### PDF
[http://arxiv.org/pdf/1808.02373](http://arxiv.org/pdf/1808.02373)

