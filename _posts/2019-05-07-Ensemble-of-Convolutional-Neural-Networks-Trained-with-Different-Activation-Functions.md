---
layout: post
title: "Ensemble of Convolutional Neural Networks Trained with Different Activation Functions"
date: 2019-05-07 11:09:32
categories: arXiv_CV
tags: arXiv_CV CNN Deep_Learning
author: Gianluca Maguolo, Loris Nanni, Stefano Ghidoni
mathjax: true
---

* content
{:toc}

##### Abstract
Activation functions play a vital role in the training of Convolutional Neural Networks. For this reason, to develop efficient and performing functions is a crucial problem in the deep learning community. Key to these approaches is to permit a reliable parameter learning, avoiding vanishing gradient problems. The goal of this work is to propose an ensemble of Convolutional Neural Networks trained using several different activation functions. Moreover, a novel activation function is here proposed for the first time. Our aim is to improve the performance of Convolutional Neural Networks in small/medium size biomedical datasets. Our results clearly show that the proposed ensemble outperforms Convolutional Neural Networks trained with standard ReLU as activation function. The proposed ensemble outperforms with a p-value of 0.01 each tested stand-alone activation function; for reliable performance comparison we have tested our approach in more than 10 datasets, using two well-known Convolutional Neural Network: Vgg16 and ResNet50. MATLAB code used here will be available at this https URL.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1905.02473](https://arxiv.org/abs/1905.02473)

##### PDF
[https://arxiv.org/pdf/1905.02473](https://arxiv.org/pdf/1905.02473)

