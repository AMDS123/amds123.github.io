---
layout: post
title: "Game-Based Video-Context Dialogue"
date: 2018-09-12 16:53:13
categories: arXiv_AI
tags: arXiv_AI Knowledge Attention
author: Ramakanth Pasunuru, Mohit Bansal
mathjax: true
---

* content
{:toc}

##### Abstract
Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrase-matching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.

##### Abstract (translated by Google)
当前的对话系统更多地关注文本和语音背景知识，并且通常基于两个发言者。最近的一些工作研究了基于静态图像的对话。然而，一些真实的人类交互也涉及动态视觉上下文（类似于视频）以及多个发言者之间的对话交换。为了更接近这种多模态会话技巧和视觉应用，我们推出了一个新的视频上下文，基于直播足球比赛视频和Twitch.tv聊天的多发言人对话数据集。这个具有挑战性的测试平台允许我们开发基于视觉的对话模型，该模型应该从实时视频生成相关的时间和空间事件语言，同时还与聊天历史相关。对于强基线，我们还提出了几种判别和生成模型，例如，基于三向注意力流（TriDAF）。我们通过检索排名 - 召回，自动词组匹配度量以及人类评估研究来评估这些模型。我们还提供数据集分析，模型消融和可视化，以了解不同模态和模型组件的贡献。

##### URL
[http://arxiv.org/abs/1809.04560](http://arxiv.org/abs/1809.04560)

##### PDF
[http://arxiv.org/pdf/1809.04560](http://arxiv.org/pdf/1809.04560)

