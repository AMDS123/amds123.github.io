---
layout: post
title: "Supervised Policy Update"
date: 2018-05-29 20:57:19
categories: arXiv_RO
tags: arXiv_RO Reinforcement_Learning Optimization
author: Quan Ho Vuong, Yiming Zhang, Keith W. Ross
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU optimizes over the proximal policy space to find a non-parameterized policy. It then solves a supervised regression problem to convert the non-parameterized policy to a parameterized policy, from which it draws new samples. There is significant flexibility in setting the labels in the supervised regression problem, with different settings corresponding to different underlying optimization problems. We develop a methodology for finding an optimal policy in the non-parameterized policy space, and show how Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) can be addressed by this methodology. In terms of sample efficiency, our experiments show SPU can outperform PPO for simulated robotic locomotion tasks.

##### Abstract (translated by Google)
我们提出了一种称为监督政策更新（SPU）的新样本有效方法，用于深入强化学习。从当前策略生成的数据开始，SPU优化近端策略空间以查找非参数化策略。然后，它解决监督回归问题，将非参数化策略转换为参数化策略，从中抽取新样本。在监督回归问题中设置标签具有很大的灵活性，不同的设置对应于不同的基础优化问题。我们开发了一种在非参数化策略空间中寻找最优策略的方法，并展示了如何通过此方法解决信任域策略优化（TRPO）和近端策略优化（PPO）。就样本效率而言，我们的实验表明，SPU可以比模拟机器人运动任务的PPO更胜一筹。

##### URL
[http://arxiv.org/abs/1805.11706](http://arxiv.org/abs/1805.11706)

##### PDF
[http://arxiv.org/pdf/1805.11706](http://arxiv.org/pdf/1805.11706)

