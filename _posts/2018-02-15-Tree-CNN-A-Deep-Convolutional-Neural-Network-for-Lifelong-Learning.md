---
layout: post
title: "Tree-CNN: A Deep Convolutional Neural Network for Lifelong Learning"
date: 2018-02-15 23:36:56
categories: arXiv_AI
tags: arXiv_AI GAN CNN Classification Detection Recognition
author: Deboleena Roy, Priyadarshini Panda, Kaushik Roy
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, Convolutional Neural Networks (CNNs) have shown remarkable performance in many computer vision tasks such as object recognition and detection. However, complex training issues, such as "catastrophic forgetting" and hyper-parameter tuning, make incremental learning in CNNs a difficult challenge. In this paper, we propose a hierarchical deep neural network, with CNNs at multiple levels, and a corresponding training method for lifelong learning. The network grows in a tree-like manner to accommodate the new classes of data without losing the ability to identify the previously trained classes. The proposed network was tested on CIFAR-10 and CIFAR-100 datasets, and compared against the method of fine tuning specific layers of a conventional CNN. We obtained comparable accuracies and achieved 40% and 20% reduction in training effort in CIFAR-10 and CIFAR 100 respectively. The network was able to organize the incoming classes of data into feature-driven super-classes. Our model improves upon existing hierarchical CNN models by adding the capability of self-growth and also yields important observations on feature selective classification.

##### Abstract (translated by Google)
近年来，卷积神经网络（CNN）在诸如物体识别和检测等许多计算机视觉任务中表现出卓越的性能。然而，诸如“灾难性遗忘”和超参数调整等复杂的培训问题使CNN的增量学习成为一项艰巨的挑战。在本文中，我们提出了一个分层次的深度神经网络，CNN在多个层次上，以及相应的终身学习训练方法。网络以类似树的方式增长，以适应新的数据类别，而不会失去识别以前训练过的类的能力。所提出的网络在CIFAR-10和CIFAR-100数据集上进行了测试，并与微调传统CNN的特定层的方法进行了比较。我们获得了可比的准确性，并分别在CIFAR-10和CIFAR 100中实现了40％和20％的培训工作量减少。网络能够将传入的数据类组织到功能驱动的超类中。我们的模型通过增加自我增长的能力来改进现有的分层CNN模型，并且对特征选择性分类产生重要的观察结果。

##### URL
[https://arxiv.org/abs/1802.05800](https://arxiv.org/abs/1802.05800)

##### PDF
[https://arxiv.org/pdf/1802.05800](https://arxiv.org/pdf/1802.05800)

