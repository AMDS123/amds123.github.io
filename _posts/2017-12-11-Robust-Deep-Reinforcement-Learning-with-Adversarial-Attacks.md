---
layout: post
title: "Robust Deep Reinforcement Learning with Adversarial Attacks"
date: 2017-12-11 02:58:13
categories: arXiv_RO
tags: arXiv_RO Adversarial Reinforcement_Learning
author: Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, Girish Chowdhary
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes adversarial attacks for Reinforcement Learning (RL) and then improves the robustness of Deep Reinforcement Learning algorithms (DRL) to parameter uncertainties with the help of these attacks. We show that even a naively engineered attack successfully degrades the performance of DRL algorithm. We further improve the attack using gradient information of an engineered loss function which leads to further degradation in performance. These attacks are then leveraged during training to improve the robustness of RL within robust control framework. We show that this adversarial training of DRL algorithms like Deep Double Q learning and Deep Deterministic Policy Gradients leads to significant increase in robustness to parameter variations for RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah environment.

##### Abstract (translated by Google)
本文提出了强化学习（RL）的对抗攻击，并借助这些攻击提高了深度增强学习算法（DRL）对参数不确定性的鲁棒性。我们表明，即使是一个天真的工程攻击成功地降低DRL算法的性能。我们使用工程损失函数的梯度信息来进一步改善攻击，这导致性能进一步下降。这些攻击随后在培训期间被利用，以在鲁棒控制框架内提高RL的鲁棒性。我们证明，这种深度双重Q学习和深层确定性策略梯度等DRL算法的对抗性训练，可显着提高RL基准（例如Cart-pole，Mountain Car，Hopper和Half Cheetah环境）的参数变化的鲁棒性。

##### URL
[https://arxiv.org/abs/1712.03632](https://arxiv.org/abs/1712.03632)

##### PDF
[https://arxiv.org/pdf/1712.03632](https://arxiv.org/pdf/1712.03632)

