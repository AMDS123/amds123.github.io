---
layout: post
title: "A Hierarchical Attention Based Seq2seq Model for Chinese Lyrics Generation"
date: 2019-06-15 06:58:42
categories: arXiv_CL
tags: arXiv_CL Attention Relation
author: Haoshen Fan, Jie Wang, Bojin Zhuang, Shaojun Wang, Jing Xiao
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we comprehensively study on context-aware generation of Chinese song lyrics. Conventional text generative models generate a sequence or sentence word by word, failing to consider the contextual relationship between sentences. Taking account into the characteristics of lyrics, a hierarchical attention based Seq2Seq (Sequence-to-Sequence) model is proposed for Chinese lyrics generation. With encoding of word-level and sentence-level contextual information, this model promotes the topic relevance and consistency of generation. A large Chinese lyrics corpus is also leveraged for model training. Eventually, results of automatic and human evaluations demonstrate that our model is able to compose complete Chinese lyrics with one united topic constraint.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.06481](http://arxiv.org/abs/1906.06481)

##### PDF
[http://arxiv.org/pdf/1906.06481](http://arxiv.org/pdf/1906.06481)

