---
layout: post
title: "Symmetry-invariant optimization in deep networks"
date: 2015-11-07 19:01:03
categories: arXiv_CV
tags: arXiv_CV Segmentation Optimization Gradient_Descent
author: Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla
mathjax: true
---

* content
{:toc}

##### Abstract
Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.

##### Abstract (translated by Google)
最近的研究强调了典型深度网络的权重空间中存在的尺度不变性或对称性，以及它对基于欧几里德梯度的随机梯度下降优化的不利影响。在这项工作中，我们表明，这些和其他常用的深层网络，如那些使用最大池和子采样层，具有更复杂的对称形式，由网络权重的基于尺度的重新参数化产生。然后，我们提出了两个基于随机梯度下降的学习的基于对称不变梯度的权重更新。我们基于MNIST数据集的经验证据表明，这些更新提高了测试性能，而不牺牲重量更新的计算效率。我们还用图像分割问题中的一个建议的权重更新来显示训练的结果。

##### URL
[https://arxiv.org/abs/1511.01754](https://arxiv.org/abs/1511.01754)

##### PDF
[https://arxiv.org/pdf/1511.01754](https://arxiv.org/pdf/1511.01754)

