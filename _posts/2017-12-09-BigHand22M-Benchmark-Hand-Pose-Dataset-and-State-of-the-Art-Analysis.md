---
layout: post
title: "BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis"
date: 2017-12-09 17:05:42
categories: arXiv_CV
tags: arXiv_CV Pose_Estimation Tracking Embedding
author: Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, Tae-Kyun Kim
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.

##### Abstract (translated by Google)
在本文中，我们介绍一个大规模的手势数据集，采用一种新颖的捕获方法收集。现有的数据集既可以通过合成生成，也可以通过深度传感器采集：合成数据集与真实深度图像呈现一定程度的外观差异，真实数据集的数量和覆盖范围有限，主要是由于难以注释。我们提出了一个跟踪系统，具有六个6D磁传感器和反向运动学，可以自动获得对运动范围的限制最小的捕获的深度图的21个关节的手部姿态注释。捕获协议旨在完全覆盖自然的手势空间。如嵌入图中所示，与现有基准相比，新数据集展现出更宽和更密集的手姿势范围。目前最先进的方法在数据集上进行评估，我们证明了跨基准性能的显着提高。我们还展示了在新数据集上训练CNN的自我中心手姿态估计方面的重大改进。

##### URL
[http://arxiv.org/abs/1704.02612](http://arxiv.org/abs/1704.02612)

##### PDF
[http://arxiv.org/pdf/1704.02612](http://arxiv.org/pdf/1704.02612)

