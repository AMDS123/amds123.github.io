---
layout: post
title: "Efficient Adaptation of Pretrained Transformers for Abstractive Summarization"
date: 2019-06-01 03:05:31
categories: arXiv_CL
tags: arXiv_CL Summarization Embedding Language_Model
author: Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, Yejin Choi
mathjax: true
---

* content
{:toc}

##### Abstract
Large-scale learning of transformer language models has yielded improvements on a variety of natural language understanding tasks. Whether they can be effectively adapted for summarization, however, has been less explored, as the learned representations are less seamlessly integrated into existing neural text production architectures. In this work, we propose two solutions for efficiently adapting pretrained transformer language models as text summarizers: source embeddings and domain-adaptive training. We test these solutions on three abstractive summarization datasets, achieving new state of the art performance on two of them. Finally, we show that these improvements are achieved by producing more focused summaries with fewer superfluous and that performance improvements are more pronounced on more abstractive datasets.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.00138](http://arxiv.org/abs/1906.00138)

##### PDF
[http://arxiv.org/pdf/1906.00138](http://arxiv.org/pdf/1906.00138)

