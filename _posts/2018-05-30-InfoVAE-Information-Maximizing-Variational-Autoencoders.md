---
layout: post
title: "InfoVAE: Information Maximizing Variational Autoencoders"
date: 2018-05-30 17:28:31
categories: arXiv_AI
tags: arXiv_AI Inference Quantitative
author: Shengjia Zhao, Jiaming Song, Stefano Ermon
mathjax: true
---

* content
{:toc}

##### Abstract
A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.

##### Abstract (translated by Google)
学习生成模型的一个关键进展是使用与模型共同训练的分期推理分布。我们发现现有的变分自动编码器的训练目标可能会导致不准确的摊销推理分布，并且在某些情况下，改进目标可能会降低推理质量。另外，已经观察到，变化的自动编码器当与解码分配结合时过于灵活时倾向于忽略潜在变量。我们再次确定现有培训标准中的原因，并提出一套新的目标（InfoVAE）来缓解这些问题。我们表明，我们的模型可以显着提高变异后验的质量，并且可以有效地利用潜在特征，而不管解码分布的灵活性如何。通过广泛的定性和定量分析，我们证明我们的模型在多种性能指标上优于竞争方法。

##### URL
[http://arxiv.org/abs/1706.02262](http://arxiv.org/abs/1706.02262)

##### PDF
[http://arxiv.org/pdf/1706.02262](http://arxiv.org/pdf/1706.02262)

