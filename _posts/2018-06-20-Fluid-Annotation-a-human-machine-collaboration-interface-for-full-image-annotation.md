---
layout: post
title: "Fluid Annotation: a human-machine collaboration interface for full image annotation"
date: 2018-06-20 02:32:00
categories: arXiv_CV
tags: arXiv_CV Face
author: Mykhaylo Andriluka, Jasper R. R. Uijlings, Vittorio Ferrari
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce Fluid Annotation, an intuitive human-machine collaboration interface for annotating the class label and outline of every object and background region in an image. Fluid Annotation starts from the output of a strong neural network model, which the annotator can edit by correcting the labels of existing regions, adding new regions to cover missing objects, and removing incorrect regions. Fluid annotation has several attractive properties: (a) it is very efficient in terms of human annotation time; (b) it supports full images annotation in a single pass, as opposed to performing a series of small tasks in isolation, such as indicating the presence of objects, clicking on instances, or segmenting a single object known to be present. Fluid Annotation subsumes all these tasks in one unified interface. (c) it empowers the annotator to choose what to annotate and in which order. This enables to put human effort only on the errors the machine made, which helps using the annotation budget effectively. Through extensive experiments on the COCO+Stuff dataset, we demonstrate that Fluid Annotation leads to accurate annotations very efficiently, taking three times less annotation time than the popular LabelMe interface.

##### Abstract (translated by Google)
我们引入Fluid Annotation，一个直观的人机协作界面，用于注释图像中每个对象和背景区域的类标签和轮廓。流体注释从强神经网络模型的输出开始，注释器可以通过更正现有区域的标签，添加新的区域来覆盖缺失的对象以及删除不正确的区域来编辑。流体注释具有多个有吸引力的特性：（a）它在人类注释时间方面非常有效; （b）它支持一次完整图像注释，而不是单独执行一系列小任务，例如指示对象的存在，点击实例或分割已知存在的单个对象。 Fluid Annotation将所有这些任务包含在一个统一的界面中。 （c）授权注释者选择要注释的内容和顺序。这使得人力只能在机器制造的错误上下功夫，这有助于有效地使用注释预算。通过对COCO + Stuff数据集的广泛实验，我们证明流体注释可以非常有效地导出准确的注释，比流行的LabelMe界面少三倍的注释时间。

##### URL
[http://arxiv.org/abs/1806.07527](http://arxiv.org/abs/1806.07527)

##### PDF
[http://arxiv.org/pdf/1806.07527](http://arxiv.org/pdf/1806.07527)

