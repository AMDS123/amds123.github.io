---
layout: post
title: "Nonparametric Inference for Auto-Encoding Variational Bayes"
date: 2017-12-18 17:22:41
categories: arXiv_AI
tags: arXiv_AI Inference
author: Erik Bodin, Iman Malik, Carl Henrik Ek, Neill D. F. Campbell
mathjax: true
---

* content
{:toc}

##### Abstract
We would like to learn latent representations that are low-dimensional and highly interpretable. A model that has these characteristics is the Gaussian Process Latent Variable Model. The benefits and negative of the GP-LVM are complementary to the Variational Autoencoder, the former provides interpretable low-dimensional latent representations while the latter is able to handle large amounts of data and can use non-Gaussian likelihoods. Our inspiration for this paper is to marry these two approaches and reap the benefits of both. In order to do so we will introduce a novel approximate inference scheme inspired by the GP-LVM and the VAE. We show experimentally that the approximation allows the capacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large without losing a highly interpretable representation, allowing reconstruction quality to be unlimited by Z at the same time as a low-dimensional space can be used to perform ancestral sampling from as well as a means to reason about the embedded data.

##### Abstract (translated by Google)
我们希望学习低维和高度可解释的潜在表达。具有这些特征的模型是高斯过程潜变量模型。 GP-LVM的优点和缺点与Variational Autoencoder是互补的，前者提供可解释的低维潜在表示，而后者则能够处理大量数据，并且可以使用非高斯可能性。我们对这篇论文的启发是结合这两种方法，并从中获益。为了做到这一点，我们将引入一个由GP-LVM和VAE启发的新颖的近似推理方案。我们通过实验表明，近似允许VAE的生成瓶颈（Z）的容量是任意大的，而不会丢失高度可解释的表示，允许重建质量被Z同时作为低维空间可以用来进行祖先抽样，也可以用来推理嵌入数据。

##### URL
[http://arxiv.org/abs/1712.06536](http://arxiv.org/abs/1712.06536)

##### PDF
[http://arxiv.org/pdf/1712.06536](http://arxiv.org/pdf/1712.06536)

