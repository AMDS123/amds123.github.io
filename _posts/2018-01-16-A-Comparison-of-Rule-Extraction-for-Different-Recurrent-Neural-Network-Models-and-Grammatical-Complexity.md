---
layout: post
title: "A Comparison of Rule Extraction for Different Recurrent Neural Network Models and Grammatical Complexity"
date: 2018-01-16 03:19:37
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Qinglong Wang, Kaixuan Zhang, Alexander G. Ororbia II, Xinyu Xing, Xue Liu, C. Lee Giles
mathjax: true
---

* content
{:toc}

##### Abstract
It has been shown that rules can be extracted from highly non-linear, recursive models such as recurrent neural networks (RNNs). The RNN models mostly investigated include both Elman networks and second-order recurrent networks. Recently, new types of RNNs have demonstrated superior power in handling many machine learning tasks, especially when structural data is involved such as language modeling. Here, we empirically evaluate different recurrent models on the task of learning deterministic finite automata (DFA), the seven Tomita grammars. We are interested in the capability of recurrent models with different architectures in learning and expressing regular grammars, which can be the building blocks for many applications dealing with structural data. Our experiments show that a second-order RNN provides the best and stablest performance of extracting DFA over all Tomita grammars and that other RNN models are greatly influenced by different Tomita grammars. To better understand these results, we provide a theoretical analysis of the "complexity" of different grammars, by introducing the entropy and the averaged edit distance of regular grammars defined in this paper. Through our analysis, we categorize all Tomita grammars into different classes, which explains the inconsistency in the performance of extraction observed across all RNN models.

##### Abstract (translated by Google)
已经表明规则可以从高度非线性递归模型（例如递归神经网络（RNN））中提取。主要研究的RNN模型包括Elman网络和二阶循环网络。最近，新型RNN在处理许多机器学习任务方面表现出优越的能力，特别是当涉及结构化数据如语言建模时。在这里，我们经验性地评估了学习确定性有限自动机（DFA），七个Tomita语法的任务的不同经常性模型。我们感兴趣的是具有不同架构的循环模型在学习和表达常规语法方面的能力，这可以成为处理结构数据的许多应用的基石。我们的实验表明，二阶RNN提供了在所有Tomita语法上提取DFA的最好和最稳定的性能，并且其他RNN模型受到不同的Tomita语法的很大影响。为了更好地理解这些结果，本文通过引入本文定义的规则语法的熵和平均编辑距离，对不同语法的“复杂性”进行理论分析。通过我们的分析，我们将所有Tomita语法分为不同的类别，这解释了在所有RNN模型中观察到的提取性能的不一致性。

##### URL
[http://arxiv.org/abs/1801.05420](http://arxiv.org/abs/1801.05420)

##### PDF
[http://arxiv.org/pdf/1801.05420](http://arxiv.org/pdf/1801.05420)

