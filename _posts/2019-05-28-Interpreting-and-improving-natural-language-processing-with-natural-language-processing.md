---
layout: post
title: "Interpreting and improving natural-language processing with natural language-processing"
date: 2019-05-28 14:13:09
categories: arXiv_CL
tags: arXiv_CL Attention Embedding
author: Mariya Toneva, Leila Wehbe
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. Despite much work, it is still unclear what the representations learned by these networks correspond to. We propose here a novel approach for interpreting neural networks that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally use the insights from the attention experiments to alter BERT: we remove the learned attention at shallow layers, and show that this manipulation improves performance on a wide range of syntactic tasks. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1905.11833](https://arxiv.org/abs/1905.11833)

##### PDF
[https://arxiv.org/pdf/1905.11833](https://arxiv.org/pdf/1905.11833)

