---
layout: post
title: "Learning the Number of Neurons in Deep Networks"
date: 2017-01-13 05:21:29
categories: arXiv_CV
tags: arXiv_CV
author: Jose M Alvarez, Mathieu Salzmann
mathjax: true
---

* content
{:toc}

##### Abstract
Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\% while retaining or even improving the network accuracy.

##### Abstract (translated by Google)
如今，深层网络的每层中的层数和神经元的数量通常是手动设置的。虽然非常深的和广泛的网络通常被证明是有效的，但是它们具有很高的记忆和计算成本，从而使它们在受限制的平台上不切实际。但是，这些网络已知具有许多冗余参数，因此原则上可以用更紧凑的体系结构来代替。在本文中，我们介绍一种自动确定学习期间深层网络中每层神经元数量的方法。为此，我们建议在网络的参数上使用群稀疏正规化器，其中每个群被定义为作用于单个神经元。从一个完整的网络开始，我们表明，我们的方法可以减少高达80％的参数，同时保留甚至提高网络的准确性。

##### URL
[https://arxiv.org/abs/1611.06321](https://arxiv.org/abs/1611.06321)

##### PDF
[https://arxiv.org/pdf/1611.06321](https://arxiv.org/pdf/1611.06321)

