---
layout: post
title: "Binary Constrained Deep Hashing Network for Image Retrieval without Human Annotation"
date: 2018-07-20 04:15:25
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval Knowledge Attention
author: Thanh-Toan Do, Dang-Khoa Le Tan, Trung Pham, Tuan Hoang, Huu Le, Ngai-Man Cheung, Ian Reid
mathjax: true
---

* content
{:toc}

##### Abstract
Learning compact binary codes for image retrieval problem using deep neural networks has attracted increasing attention recently. However, training deep hashing networks is challenging due to the binary constraints on the hash codes, the similarity preserving properties, and the requirement for a vast amount of labelled images. To the best of our knowledge, none of the existing methods has tackled all of these challenges completely in a unified framework. In this work, we propose a novel end-toend deep hashing approach, which is trained to produce binary codes directly from image pixels without human intervention. In particular, our main contribution is to propose a novel pairwise loss function, which simultaneously encodes the distances between pairs of binary codes, and the binary quantization error. We propose an efficient parameter learning algorithm for this loss function. In addition, to provide similar/dissimilar images for our pairwise loss function, we exploit 3D models reconstructed from unlabeled images for automatic generation of enormous similar/dissimilar pairs. Extensive experiments on three image retrieval benchmark datasets demonstrate the superior performance of the proposed method.

##### Abstract (translated by Google)
使用深度神经网络学习用于图像检索问题的紧凑二进制码最近引起了越来越多的关然而，由于哈希码的二元约束，相似性保持特性以及对大量标记图像的需求，训练深度哈希网络是具有挑战性的。据我们所知，现有方法都没有完全在统一框架内解决所有这些挑战。在这项工作中，我们提出了一种新颖的端到端深度哈希方法，该方法经过训练，可直接从图像像素生成二进制代码而无需人工干预。特别是，我们的主要贡献是提出一种新的成对损失函数，它同时编码二进制码对之间的距离和二进制量化误差。我们为这种损失函数提出了一种有效的参数学习算法。此外，为了为我们的成对损失函数提供相似/不相似的图像，我们利用从未标记图像重建的3D模型来自动生成巨大的相似/不相似的对。对三个图像检索基准数据集的广泛实验证明了所提出方法的优越性能。

##### URL
[http://arxiv.org/abs/1802.07437](http://arxiv.org/abs/1802.07437)

##### PDF
[http://arxiv.org/e-print/1802.07437](http://arxiv.org/e-print/1802.07437)

