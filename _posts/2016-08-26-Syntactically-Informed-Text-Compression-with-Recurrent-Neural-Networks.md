---
layout: post
title: "Syntactically Informed Text Compression with Recurrent Neural Networks"
date: 2016-08-26 20:55:41
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: David Cox
mathjax: true
---

* content
{:toc}

##### Abstract
We present a self-contained system for constructing natural language models for use in text compression. Our system improves upon previous neural network based models by utilizing recent advances in syntactic parsing -- Google's SyntaxNet -- to augment character-level recurrent neural networks. RNNs have proven exceptional in modeling sequence data such as text, as their architecture allows for modeling of long-term contextual information.

##### Abstract (translated by Google)
我们提出了一个自包含的系统来构建用于文本压缩的自然语言模型。我们的系统利用最近在语法分析方面取得的进展（Google的SyntaxNet）来增强字符级递归神经网络，从而改进了以前基于神经网络的模型。 RNN在建模序列数据（例如文本）方面已被证明是不寻常的，因为其架构允许对长期上下文信息进行建模。

##### URL
[https://arxiv.org/abs/1608.02893](https://arxiv.org/abs/1608.02893)

##### PDF
[https://arxiv.org/pdf/1608.02893](https://arxiv.org/pdf/1608.02893)

