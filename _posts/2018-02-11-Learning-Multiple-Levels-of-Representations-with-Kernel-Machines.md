---
layout: post
title: "Learning Multiple Levels of Representations with Kernel Machines"
date: 2018-02-11 17:18:28
categories: arXiv_AI
tags: arXiv_AI Classification
author: Shiyu Duan, Yunmei Chen, Jose Principe
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a connectionist-inspired kernel machine model with three key advantages over traditional kernel machines. First, it is capable of learning distributed and hierarchical representations. Second, its performance is highly robust to the choice of kernel function. Third, the solution space is not limited to the span of images of training data in reproducing kernel Hilbert space (RKHS). Together with the architecture, we propose a greedy learning algorithm that allows the proposed multilayer network to be trained layer-wise without backpropagation by optimizing the geometric properties of images in RKHS. With a single fixed generic kernel for each layer and two layers in total, our model compares favorably with state-of-the-art multiple kernel learning algorithms using significantly more kernels and popular deep architectures on widely used classification benchmarks.

##### Abstract (translated by Google)
我们提出了一个由连接主义启发的内核机器模型，其相对传统内核机器具有三个关键优势首先，它能够学习分布式和分层表示。其次，它的性能对内核函数的选择非常稳健。第三，解空间不限于再生核希尔伯特空间（RKHS）中训练数据图像的范围。与架构一起，我们提出了一种贪心学习算法，通过优化RKHS中图像的几何特性，允许所提出的多层网络在没有反向传播的情况下进行分层训练。对于每个层和总共两层的单个固定通用内核，我们的模型与最先进的多核内核学习算法相比，在广泛使用的分类基准测试中使用了更多的内核和深度架构。

##### URL
[http://arxiv.org/abs/1802.03774](http://arxiv.org/abs/1802.03774)

##### PDF
[http://arxiv.org/pdf/1802.03774](http://arxiv.org/pdf/1802.03774)

