---
layout: post
title: "While Tuning is Good, No Tuner is Best"
date: 2018-07-29 21:26:12
categories: arXiv_AI
tags: arXiv_AI Optimization Prediction
author: Huy Tu, Vivek Nair
mathjax: true
---

* content
{:toc}

##### Abstract
Hyperparameter tuning is the black art of automatically finding a good combination of control parameters for a data miner. While widely applied in Software Engineering, there has not been much discussion on which hyperparameter tuner is best for software analytics. 
 To address this gap in the literature, this paper applied a range of hyperparameter optimizers (grid search, differential evolution, random search, SMAC) to defect prediction. No hyperparameter optimizer was observed to be "best" and, for one of the two evaluation measures studied here (F-measure), hyperparameter optimization, in 50\% cases, was no better than using default configurations. 
 We conclude that hyperparameter optimization is more nuanced than previously believed. While such optimization can certainly lead to large improvements in the performance of classifiers used in software analytics, it remains to be seen which specific optimizers should be endorsed.

##### Abstract (translated by Google)
超参数调整是自动为数据挖掘器找到控制参数的良好组合的黑色技术。虽然在软件工程中得到广泛应用，但对于哪种超参数调谐器最适合软件分析的讨论并不多。
 为了解决文献中的这一差距，本文将一系列超参数优化器（网格搜索，差分进化，随机搜索，SMAC）应用于缺陷预测。没有观察到超参数优化器是“最佳”的，并且对于这里研究的两个评估测量之一（F-测量），在50％的情况下，超参数优化并不比使用默认配置更好。
 我们得出结论，超参数优化比以前认为的更细微。虽然这种优化肯定会导致软件分析中使用的分类器性能的大幅提升，但仍需要了解哪些特定的优化器应该得到认可。

##### URL
[http://arxiv.org/abs/1807.11112](http://arxiv.org/abs/1807.11112)

##### PDF
[http://arxiv.org/pdf/1807.11112](http://arxiv.org/pdf/1807.11112)

