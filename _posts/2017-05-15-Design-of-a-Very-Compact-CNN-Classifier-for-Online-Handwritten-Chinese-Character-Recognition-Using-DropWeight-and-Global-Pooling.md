---
layout: post
title: "Design of a Very Compact CNN Classifier for Online Handwritten Chinese Character Recognition Using DropWeight and Global Pooling"
date: 2017-05-15 13:18:38
categories: arXiv_CV
tags: arXiv_CV CNN Recognition
author: Xuefeng Xiao, Yafeng Yang, Tasweer Ahmad, Lianwen Jin, Tianhai Chang
mathjax: true
---

* content
{:toc}

##### Abstract
Currently, owing to the ubiquity of mobile devices, online handwritten Chinese character recognition (HCCR) has become one of the suitable choice for feeding input to cell phones and tablet devices. Over the past few years, larger and deeper convolutional neural networks (CNNs) have extensively been employed for improving character recognition performance. However, its substantial storage requirement is a significant obstacle in deploying such networks into portable electronic devices. To circumvent this problem, we propose a novel technique called DropWeight for pruning redundant connections in the CNN architecture. It is revealed that the proposed method not only treats streamlined architectures such as AlexNet and VGGNet well but also exhibits remarkable performance for deep residual network and inception network. We also demonstrate that global pooling is a better choice for building very compact online HCCR systems. Experiments were performed on the ICDAR-2013 online HCCR competition dataset using our proposed network, and it is found that the proposed approach requires only 0.57 MB for storage, whereas state-of-the-art CNN-based methods require up to 135 MB; meanwhile the performance is decreased only by 0.91%.

##### Abstract (translated by Google)
目前，由于移动设备无处不在，在线手写汉字识别（HCCR）已经成为手机和平板设备输入的合适选择之一。在过去的几年中，越来越大和更深的卷积神经网络（CNN）被广泛用于提高字符识别性能。然而，其实质的存储要求是将这种网络部署到便携式电子设备中的重大障碍。为了避免这个问题，我们提出了一种名为DropWeight的新技术，用于修剪CNN架构中的冗余连接。结果表明，所提出的方法不仅能很好地处理AlexNet和VGGNet等流线型架构，而且在深度残留网络和初始网络中表现出了显着的性能。我们还证明，建立非常紧凑的在线HCCR系统，全球联营是一个更好的选择。使用我们提出的网络在ICDAR-2013在线HCCR竞争数据集上进行了实验，发现所提出的方法仅需要0.57MB用于存储，而最先进的基于CNN的方法需要高达135MB;同时表现仅下降0.91％。

##### URL
[https://arxiv.org/abs/1705.05207](https://arxiv.org/abs/1705.05207)

##### PDF
[https://arxiv.org/pdf/1705.05207](https://arxiv.org/pdf/1705.05207)

