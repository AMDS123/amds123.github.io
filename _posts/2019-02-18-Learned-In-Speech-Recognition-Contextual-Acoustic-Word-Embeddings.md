---
layout: post
title: "Learned In Speech Recognition: Contextual Acoustic Word Embeddings"
date: 2019-02-18 23:06:56
categories: arXiv_CL
tags: arXiv_CL Attention Speech_Recognition Embedding Inference Language_Model Recognition
author: Shruti Palaskar, Vikas Raunak, Florian Metze
mathjax: true
---

* content
{:toc}

##### Abstract
End-to-end acoustic-to-word speech recognition models have recently gained popularity because they are easy to train, scale well to large amounts of training data, and do not require a lexicon. In addition, word models may also be easier to integrate with downstream tasks such as spoken language understanding, because inference (search) is much simplified compared to phoneme, character or any other sort of sub-word units. In this paper, we describe methods to construct contextual acoustic word embeddings directly from a supervised sequence-to-sequence acoustic-to-word speech recognition model using the learned attention distribution. On a suite of 16 standard sentence evaluation tasks, our embeddings show competitive performance against a word2vec model trained on the speech transcriptions. In addition, we evaluate these embeddings on a spoken language understanding task, and observe that our embeddings match the performance of text-based embeddings in a pipeline of first performing speech recognition and then constructing word embeddings from transcriptions.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.06833](http://arxiv.org/abs/1902.06833)

##### PDF
[http://arxiv.org/pdf/1902.06833](http://arxiv.org/pdf/1902.06833)

