---
layout: post
title: "Audio-Visual Event Localization in Unconstrained Videos"
date: 2018-03-23 15:34:03
categories: arXiv_CV
tags: arXiv_CV Attention Relation
author: Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we introduce a novel problem of audio-visual event localization in unconstrained videos. We define an audio-visual event as an event that is both visible and audible in a video segment. We collect an Audio-Visual Event(AVE) dataset to systemically investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization. We develop an audio-guided visual attention mechanism to explore audio-visual correlations, propose a dual multimodal residual network (DMRN) to fuse information over the two modalities, and introduce an audio-visual distance learning network to handle the cross-modality localization. Our experiments support the following findings: joint modeling of auditory and visual modalities outperforms independent modeling, the learned attention can capture semantics of sounding objects, temporal alignment is important for audio-visual fusion, the proposed DMRN is effective in fusing audio-visual features, and strong correlations between the two modalities enable cross-modality localization.

##### Abstract (translated by Google)
在本文中，我们介绍了一个无约束视频中视听事件定位的新问题。我们将视听事件定义为在视频片段中既可视又可听的事件。我们收集一个视听事件（AVE）数据集来系统地调查三个时间本地化任务：监督和弱监督的视听事件定位和跨模态定位。我们开发了一个音频引导视觉注意机制来探索视听关联，提出一个双重多模式残留网络（DMRN）来融合这两种模式的信息，并引入一个视听远程学习网络来处理跨模态定位。我们的实验支持以下研究结果：听觉和视觉模态的联合建模优于独立建模，所学知识可以捕捉探测对象的语义，时间对齐对于音频 - 视频融合非常重要，所提出的DMRN在融合视听特性，两种模式之间的强相关性使跨模态本地化成为可能。

##### URL
[https://arxiv.org/abs/1803.08842](https://arxiv.org/abs/1803.08842)

##### PDF
[https://arxiv.org/pdf/1803.08842](https://arxiv.org/pdf/1803.08842)

