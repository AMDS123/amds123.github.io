---
layout: post
title: "An Attention-Gated Convolutional Neural Network for Sentence Classification"
date: 2018-08-28 13:35:25
categories: arXiv_CL
tags: arXiv_CL Attention CNN Classification
author: Yang Liu, Lixin Ji, Ruiyang Huang, Tuosiyu Ming, Chao Gao
mathjax: true
---

* content
{:toc}

##### Abstract
The classification task of sentences is very challenging because of the limited contextual information that sentences contain. In this paper, we propose an Attention Gated Convolutional Neural Network (AGCNN) for sentence classification, which generates attention weights from the feature's context windows of different sizes by using specialized convolution encoders, to enhance the influence of critical features in predicting the sentence's category. Experimental results demonstrate that our model could achieve a general accuracy improvement highest up to 3.1% (compared with standard CNN models), and gain competitive results over the strong baseline methods on four out of the six tasks. Besides, we propose an activation function named Natural Logarithm rescaled Rectified Linear Unit (NLReLU). Experimental results show that NLReLU could outperform ReLU and performs comparably to other well-known activation functions on AGCNN.

##### Abstract (translated by Google)
由于句子包含的上下文信息有限，句子的分类任务非常具有挑战性。在本文中，我们提出了一种用于句子分类的注意门控卷积神经网络（AGCNN），它通过使用专门的卷积编码器从特征的不同大小的上下文窗口生成注意权重，以增强关键特征在预测句子类别中的影响。实验结果表明，我们的模型可以实现最高达3.1％的一般精度提升（与标准CNN模型相比），并且在六项任务中的四项中获得了比强基线方法更具竞争力的结果。此外，我们提出了一个名为Natural Logarithm rescaled Rectified Linear Unit（NLReLU）的激活函数。实验结果表明，NLReLU可以胜过ReLU，并且与AGCNN上其他众所周知的激活函数相当。

##### URL
[https://arxiv.org/abs/1808.07325](https://arxiv.org/abs/1808.07325)

##### PDF
[https://arxiv.org/pdf/1808.07325](https://arxiv.org/pdf/1808.07325)

