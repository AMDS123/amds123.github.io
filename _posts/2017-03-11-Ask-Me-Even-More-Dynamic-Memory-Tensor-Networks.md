---
layout: post
title: "Ask Me Even More: Dynamic Memory Tensor Networks"
date: 2017-03-11 10:05:19
categories: arXiv_CV
tags: arXiv_CV QA Dynamic_Memory_Network Attention Weakly_Supervised Face Memory_Networks
author: Govardana Sachithanandam Ramachandran, Ajay Sohmshetty
mathjax: true
---

* content
{:toc}

##### Abstract
We examine Memory Networks for the task of question answering (QA), under common real world scenario where training examples are scarce and under weakly supervised scenario, that is only extrinsic labels are available for training. We propose extensions for the Dynamic Memory Network (DMN), specifically within the attention mechanism, we call the resulting Neural Architecture as Dynamic Memory Tensor Network (DMTN). Ultimately, we see that our proposed extensions results in over 80% improvement in the number of task passed against the baselined standard DMN and 20% more task passed compared to state-of-the-art End-to-End Memory Network for Facebook's single task weakly trained 1K bAbi dataset.

##### Abstract (translated by Google)
我们研究记忆网络的问题回答（QA）的任务，在常见的现实世界的情况下，训练的例子是稀缺的，在弱监督的情况下，只有外在的标签可供培训。我们提出了对动态内存网络（DMN）的扩展，特别是在注意机制内，我们称之为动态内存张量网络（DMTN）。最终，我们看到，我们提出的扩展可以使基准标准DMN的传输任务数量提高80％以上，并且相比于最先进的Facebook端到端内存网络，传输的任务可以多传输20％任务弱训练1K bAbi数据集。

##### URL
[https://arxiv.org/abs/1703.03939](https://arxiv.org/abs/1703.03939)

##### PDF
[https://arxiv.org/pdf/1703.03939](https://arxiv.org/pdf/1703.03939)

