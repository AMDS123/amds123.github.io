---
layout: post
title: "Compressing Word Embeddings"
date: 2016-05-16 17:19:51
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Martin Andrews
mathjax: true
---

* content
{:toc}

##### Abstract
Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. However, these vector space representations (created through large-scale text analysis) are typically stored verbatim, since their internal structure is opaque. Using word-analogy tests to monitor the level of detail stored in compressed re-representations of the same vector space, the trade-offs between the reduction in memory usage and expressiveness are investigated. A simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10, with only minimal impact on performance. Then, using the same `bit budget', a binary (approximate) factorisation of the same space is also explored, with the aim of creating an equivalent representation with better interpretability.

##### Abstract (translated by Google)
最近用于学习词的向量空间表示的方法已经成功地使用向量算法捕捉细粒度的语义和句法规则。但是，这些向量空间表示（通过大规模文本分析创建）通常是逐字存储的，因为它们的内部结构是不透明的。使用词类比测试来监视存储在相同向量空间的压缩重新表示中的细节水平，调查内存使用和表现力减少之间的折衷。概述一个简单的方案，可以将最先进的嵌入的内存占用减少10倍，而对性能影响甚微。然后，使用相同的“比特预算”，还探索了相同空间的二元（近似）因式分解，目的是创建具有更好解释性的等同表示。

##### URL
[https://arxiv.org/abs/1511.06397](https://arxiv.org/abs/1511.06397)

##### PDF
[https://arxiv.org/pdf/1511.06397](https://arxiv.org/pdf/1511.06397)

