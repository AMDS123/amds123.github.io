---
layout: post
title: "Self-Regulating Artificial General Intelligence"
date: 2018-02-15 21:00:42
categories: arXiv_AI
tags: arXiv_AI
author: Joshua S. Gans
mathjax: true
---

* content
{:toc}

##### Abstract
Here we examine the paperclip apocalypse concern for artificial general intelligence (or AGI) whereby a superintelligent AI with a simple goal (ie., producing paperclips) accumulates power so that all resources are devoted towards that simple goal and are unavailable for any other use. We provide conditions under which a paper apocalypse can arise but also show that, under certain architectures for recursive self-improvement of AIs, that a paperclip AI may refrain from allowing power capabilities to be developed. The reason is that such developments pose the same control problem for the AI as they do for humans (over AIs) and hence, threaten to deprive it of resources for its primary goal.

##### Abstract (translated by Google)
在这里，我们考察回形针启示对人工智能（或AGI）的关注，即具有简单目标（即生成回形针）的超智能AI会积累权力，以便所有资源都致力于实现这一简单目标，并且不可用于任何其他用途。我们提供了一个纸质启示可能出现的条件，但也表明，在某些AI的递归自我改进架构下，回形针AI可能会避免允许开发电源功能。原因是这样的发展对人工智能和人类（对认知智能）造成相同的控制问题，因此可能会剥夺其资源以实现其主要目标。

##### URL
[http://arxiv.org/abs/1711.04309](http://arxiv.org/abs/1711.04309)

##### PDF
[http://arxiv.org/pdf/1711.04309](http://arxiv.org/pdf/1711.04309)

