---
layout: post
title: 'Modeling Coverage for Neural Machine Translation'
date: 2016-08-06 17:13:04
categories: arXiv_CL
tags: arXiv_CL NMT
author: Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li
---

* content
{:toc}

##### Abstract
Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.

##### Abstract (translated by Google)
注意机制通过联合学习对齐和翻译，增强了最先进的神经机器翻译（NMT）。它倾向于忽略过去的对齐信息，然而，这往往导致翻译和翻译不足。为了解决这个问题，我们在本文中提出了基于覆盖的NMT。我们维护一个覆盖矢量来跟踪注意历史。将覆盖矢量提供给关注模型以帮助调整将来的关注，这使得NMT系统可以更多地考虑未翻译的源词汇。实验表明，与标准的基于注意力的NMT相比，所提出的方法显着提高了翻译质量和对齐质量。

##### URL
[https://arxiv.org/abs/1601.04811](https://arxiv.org/abs/1601.04811)

