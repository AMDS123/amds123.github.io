---
layout: post
title: "Neural Discourse Modeling of Conversations"
date: 2016-07-15 16:43:40
categories: arXiv_CL
tags: arXiv_CL RNN Quantitative Relation
author: John M. Pierre, Mark Butler, Jacob Portnoff, Luis Aguilar
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks have shown recent promise in many language-related tasks such as the modeling of conversations. We extend RNN-based sequence to sequence models to capture the long range discourse across many turns of conversation. We perform a sensitivity analysis on how much additional context affects performance, and provide quantitative and qualitative evidence that these models are able to capture discourse relationships across multiple utterances. Our results quantifies how adding an additional RNN layer for modeling discourse improves the quality of output utterances and providing more of the previous conversation as input also improves performance. By searching the generated outputs for specific discourse markers we show how neural discourse models can exhibit increased coherence and cohesion in conversations.

##### Abstract (translated by Google)
深度神经网络已经在许多与语言相关的任务（例如对话建模）中显示出最近的希望。我们将基于RNN的序列扩展到序列模型，以跨多次谈话捕获长距离话语。我们对额外的上下文对性能的影响进行敏感性分析，并提供定量和定性的证据，证明这些模型能够捕捉多个话语中的话语关系。我们的结果量化了如何为建模话语增加额外的RNN层来提高输出话语的质量并且提供更多的前面的对话作为输入也提高了性能。通过搜索特定话语标记的生成输出，我们展示了神经话语模型如何在对话中表现出增强的一致性和内聚性。

##### URL
[https://arxiv.org/abs/1607.04576](https://arxiv.org/abs/1607.04576)

##### PDF
[https://arxiv.org/pdf/1607.04576](https://arxiv.org/pdf/1607.04576)

