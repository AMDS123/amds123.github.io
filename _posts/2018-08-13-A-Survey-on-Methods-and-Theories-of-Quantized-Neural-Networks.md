---
layout: post
title: "A Survey on Methods and Theories of Quantized Neural Networks"
date: 2018-08-13 14:11:43
categories: arXiv_AI
tags: arXiv_AI Review Speech_Recognition Survey Inference Recognition
author: Yunhui Guo
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition. For all its popularity, deep neural networks are also criticized for consuming a lot of memory and draining battery life of devices during training and inference. This makes it hard to deploy these models on mobile or embedded devices which have tight resource constraints. Quantization is recognized as one of the most effective approaches to satisfy the extreme memory requirements that deep neural network models demand. Instead of adopting 32-bit floating point format to represent weights, quantized representations store weights using more compact formats such as integers or even binary numbers. Despite a possible degradation in predictive performance, quantization provides a potential solution to greatly reduce the model size and the energy consumption. In this survey, we give a thorough review of different aspects of quantized neural networks. Current challenges and trends of quantized neural networks are also discussed.

##### Abstract (translated by Google)
深度神经网络是许多现实世界任务的最先进方法，例如计算机视觉，自然语言处理和语音识别。尽管其深受欢迎，但深度神经网络也被批评为在训练和推理期间消耗大量内存并耗尽设备的电池寿命。这使得很难在具有严格资源限制的移动或嵌入式设备上部署这些模型。量化被认为是满足深度神经网络模型所需的极端存储器需求的最有效方法之一。而不是采用32位浮点格式来表示权重，量化表示使用更紧凑的格式（例如整数或甚至二进制数）来存储权重。尽管预测性能可能下降，但量化提供了一种潜在的解决方案，可大大减小模型尺寸和能耗。在本次调查中，我们对量化神经网络的不同方面进行了全面的回顾。还讨论了量化神经网络的当前挑战和趋势。

##### URL
[http://arxiv.org/abs/1808.04752](http://arxiv.org/abs/1808.04752)

##### PDF
[http://arxiv.org/pdf/1808.04752](http://arxiv.org/pdf/1808.04752)

