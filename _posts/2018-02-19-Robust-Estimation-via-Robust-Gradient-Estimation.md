---
layout: post
title: "Robust Estimation via Robust Gradient Estimation"
date: 2018-02-19 01:49:31
categories: arXiv_AI
tags: arXiv_AI Gradient_Descent
author: Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, Pradeep Ravikumar
mathjax: true
---

* content
{:toc}

##### Abstract
We provide a new computationally-efficient class of estimators for risk minimization. We show that these estimators are robust for general statistical models: in the classical Huber epsilon-contamination model and in heavy-tailed settings. Our workhorse is a novel robust variant of gradient descent, and we provide conditions under which our gradient descent variant provides accurate estimators in a general convex risk minimization problem. We provide specific consequences of our theory for linear regression, logistic regression and for estimation of the canonical parameters in an exponential family. These results provide some of the first computationally tractable and provably robust estimators for these canonical statistical models. Finally, we study the empirical performance of our proposed methods on synthetic and real datasets, and find that our methods convincingly outperform a variety of baselines.

##### Abstract (translated by Google)
我们为风险最小化提供了一种新的计算有效的估计量类。我们证明这些估计量对于一般的统计模型是稳健的：在经典的胡贝尔epsilon污染模型和重尾设置。我们的主力是一种新颖的梯度下降鲁棒变体，我们提供了一些条件，我们的梯度下降变体提供了一般凸风险最小化问题的准确估计量。我们为线性回归，逻辑回归以及估计指数族中的典范参数提供了我们理论的具体后果。这些结果为这些典型统计模型提供了一些首先在计算上易于处理且可证明的稳健估计。最后，我们研究了我们提出的合成和实际数据集方法的经验性能，并发现我们的方法令人信服地胜过了各种基线。

##### URL
[http://arxiv.org/abs/1802.06485](http://arxiv.org/abs/1802.06485)

##### PDF
[http://arxiv.org/pdf/1802.06485](http://arxiv.org/pdf/1802.06485)

