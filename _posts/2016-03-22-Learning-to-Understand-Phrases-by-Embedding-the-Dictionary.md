---
layout: post
title: "Learning to Understand Phrases by Embedding the Dictionary"
date: 2016-03-22 16:30:17
categories: arXiv_SD
tags: arXiv_SD Knowledge Embedding
author: Felix Hill, Kyunghyun Cho, Anna Korhonen, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: "reverse dictionaries" that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.

##### Abstract (translated by Google)
学习丰富的语义词表示的分布式模型是最近的NLP研究的成功故事。但是，开发学习有用的短语和句子表达的模型已经证明了更加困难。我们建议使用日常词典中的定义作为缩小词法和短语语义之间差距的一种手段。可以有效地训练神经语言嵌入模型，以将字典定义（短语）映射到由这些定义定义的单词的（词汇）表示。我们提出了这些架构的两个应用：“反向字典”，它返回一个给定义或描述的概念名称和一般知识纵横字问题解答者。在这两个任务上，根据少数几种免费提供的词汇资源对定义进行训练的神经语言嵌入模型的性能与依赖于重要任务特定工程的现有商业系统一样好或者更好。结果突出显示了神经嵌入体系结构和基于定义的训练对于开发模式的理解短语和句子的有效性。

##### URL
[https://arxiv.org/abs/1504.00548](https://arxiv.org/abs/1504.00548)

##### PDF
[https://arxiv.org/pdf/1504.00548](https://arxiv.org/pdf/1504.00548)

