---
layout: post
title: "Combining Axiom Injection and Knowledge Base Completion for Efficient Natural Language Inference"
date: 2018-11-15 06:40:39
categories: arXiv_AI
tags: arXiv_AI Knowledge Inference
author: Masashi Yoshikawa, Koji Mineshima, Hiroshi Noji, Daisuke Bekki
mathjax: true
---

* content
{:toc}

##### Abstract
In logic-based approaches to reasoning tasks such as Recognizing Textual Entailment (RTE), it is important for a system to have a large amount of knowledge data. However, there is a tradeoff between adding more knowledge data for improved RTE performance and maintaining an efficient RTE system, as such a big database is problematic in terms of the memory usage and computational complexity. In this work, we show the processing time of a state-of-the-art logic-based RTE system can be significantly reduced by replacing its search-based axiom injection (abduction) mechanism by that based on Knowledge Base Completion (KBC). We integrate this mechanism in a Coq plugin that provides a proof automation tactic for natural language inference. Additionally, we show empirically that adding new knowledge data contributes to better RTE performance while not harming the processing speed in this framework.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.06203](http://arxiv.org/abs/1811.06203)

##### PDF
[http://arxiv.org/pdf/1811.06203](http://arxiv.org/pdf/1811.06203)

