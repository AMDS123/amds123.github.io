---
layout: post
title: "Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning"
date: 2018-05-22 01:56:51
categories: arXiv_CV
tags: arXiv_CV Image_Caption Adversarial Caption CNN RNN
author: Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Cho-Jui Hsieh
mathjax: true
---

* content
{:toc}

##### Abstract
Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check whether neural image captioning systems can be mislead to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.

##### Abstract (translated by Google)
视觉语言接地在现代神经图像字幕系统中被广泛研究，其通常采用由两个主要部件组成的编码器 - 解码器框架：用于图像特征提取的卷积神经网络（CNN）和用于语言字幕生成的递归神经网络（RNN） 。为了研究语言基础对机器视觉和感知的敌对扰动的鲁棒性，我们提出了Show-and-Fool，这是一种用于在神经图像字幕中制作对抗性例子的新颖算法。所提出的算法提供两种评估方法，其检查神经图像字幕系统是否可能被误导以输出一些随机选择的字幕或关键字。我们广泛的实验表明，我们的算法可以成功地制作具有随机目标标题或关键词的视觉相似对抗示例，并且可以使对抗示例高度转移到其他图像字幕系统。因此，我们的方法导致了神经图像字幕的新鲁棒性含义和视觉语言基础的新颖见解。

##### URL
[https://arxiv.org/abs/1712.02051](https://arxiv.org/abs/1712.02051)

##### PDF
[https://arxiv.org/pdf/1712.02051](https://arxiv.org/pdf/1712.02051)

