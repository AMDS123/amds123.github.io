---
layout: post
title: "Transformed Residual Quantization for Approximate Nearest Neighbor Search"
date: 2015-12-22 01:12:54
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Jiangbo Yuan, Xiuwen Liu
mathjax: true
---

* content
{:toc}

##### Abstract
The success of product quantization (PQ) for fast nearest neighbor search depends on the exponentially reduced complexities of both storage and computation with respect to the codebook size. Recent efforts have been focused on employing sophisticated optimization strategies, or seeking more effective models. Residual quantization (RQ) is such an alternative that holds the same property as PQ in terms of the aforementioned complexities. In addition to being a direct replacement of PQ, hybrids of PQ and RQ can yield more gains for approximate nearest neighbor search. This motivated us to propose a novel approach to optimizing RQ and the related hybrid models. With an observation of the general randomness increase in a residual space, we propose a new strategy that jointly learns a local transformation per residual cluster with an ultimate goal to reduce overall quantization errors. We have shown that our approach can achieve significantly better accuracy on nearest neighbor search than both the original and the optimized PQ on several very large scale benchmarks.

##### Abstract (translated by Google)
用于快速最近邻搜索的产品量化（PQ）的成功取决于关于码本大小的存储和计算两者的指数降低的复杂度。最近的努力一直集中在采用复杂的优化策略，或寻求更有效的模型。残差量化（RQ）就是上述复杂性与PQ相同的性质。除了直接替代PQ之外，PQ和RQ的混合可以为近似最近邻搜索产生更多的收益。这激励我们提出一种新的方法来优化RQ和相关的混合模型。随着对剩余空间随机性增加的观察，我们提出了一个新的策略，联合学习每个残差聚类的局部变换，最终目标是减少整体量化误差。我们已经表明，我们的方法可以在几个超大规模的基准测试中获得比最初的和最优化的PQ更好的最近邻搜索精度。

##### URL
[https://arxiv.org/abs/1512.06925](https://arxiv.org/abs/1512.06925)

##### PDF
[https://arxiv.org/pdf/1512.06925](https://arxiv.org/pdf/1512.06925)

