---
layout: post
title: "Single-View and Multi-View Depth Fusion"
date: 2017-06-27 09:37:04
categories: arXiv_CV
tags: arXiv_CV
author: José M. Fácil, Alejo Concha, Luis Montesano, Javier Civera
mathjax: true
---

* content
{:toc}

##### Abstract
Dense and accurate 3D mapping from a monocular sequence is a key technology for several applications and still an open research area. This paper leverages recent results on single-view CNN-based depth estimation and fuses them with multi-view depth estimation. Both approaches present complementary strengths. Multi-view depth is highly accurate but only in high-texture areas and high-parallax cases. Single-view depth captures the local structure of mid-level regions, including texture-less areas, but the estimated depth lacks global coherence. The single and multi-view fusion we propose is challenging in several aspects. First, both depths are related by a deformation that depends on the image content. Second, the selection of multi-view points of high accuracy might be difficult for low-parallax configurations. We present contributions for both problems. Our results in the public datasets of NYUv2 and TUM shows that our algorithm outperforms the individual single and multi-view approaches. A video showing the key aspects of mapping in our Single and Multi-view depth proposal is available at this https URL

##### Abstract (translated by Google)
单眼序列密集而准确的三维测绘是多个应用的​​关键技术，仍然是一个开放的研究领域。本文利用最新的基于单视图CNN的深度估计结果，并通过多视点深度估计进行融合。两种方法都具有互补性。多视点深度是高精度的，但只有在高纹理区域和高视差的情况下。单视点深度捕捉中等地区的局部结构，包括无纹理区域，但估计的深度缺乏全局一致性。我们提出的单视图和多视图融合在几个方面是具有挑战性的。首先，两个深度都与依赖于图像内容的变形有关。其次，对于低视差配置，选择高精度的多视点可能是困难的。我们提出两个问题的贡献。我们在NYUv2和TUM的公共数据集中的结果表明，我们的算法优于单个和多个视图方法。这个https网址提供了一个视频，其中显示了我们的单视图和多视图深度提案中的关键映射

##### URL
[https://arxiv.org/abs/1611.07245](https://arxiv.org/abs/1611.07245)

##### PDF
[https://arxiv.org/pdf/1611.07245](https://arxiv.org/pdf/1611.07245)

