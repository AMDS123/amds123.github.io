---
layout: post
title: "Learning A Task-Specific Deep Architecture For Clustering"
date: 2015-10-16 06:38:37
categories: arXiv_CV
tags: arXiv_CV Sparse Deep_Learning
author: Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang
mathjax: true
---

* content
{:toc}

##### Abstract
While sparse coding-based clustering methods have shown to be successful, their bottlenecks in both efficiency and scalability limit the practical usage. In recent years, deep learning has been proved to be a highly effective, efficient and scalable feature learning tool. In this paper, we propose to emulate the sparse coding-based clustering pipeline in the context of deep learning, leading to a carefully crafted deep model benefiting from both. A feed-forward network structure, named TAGnet, is constructed based on a graph-regularized sparse coding algorithm. It is then trained with task-specific loss functions from end to end. We discover that connecting deep learning to sparse coding benefits not only the model performance, but also its initialization and interpretation. Moreover, by introducing auxiliary clustering tasks to the intermediate feature hierarchy, we formulate DTAGnet and obtain a further performance boost. Extensive experiments demonstrate that the proposed model gains remarkable margins over several state-of-the-art methods.

##### Abstract (translated by Google)
虽然基于稀疏编码的聚类方法已经证明是成功的，但是它们在效率和可伸缩性方面的瓶颈限制了实际使用。近年来，深度学习被证明是一种高效，高效，可扩展的特征学习工具。在本文中，我们建议在深度学习的背景下模拟基于稀疏编码的聚类管道，从而导致精心制作的深层模型受益于两者。前馈网络结构TAGnet是基于图正则化稀疏编码算法构建的。然后从头到尾训练出特定于任务的损失函数。我们发现，将深度学习与稀疏编码相结合，不仅有利于模型的性能，而且有利于其初始化和解释。而且，通过将辅助聚类任务引入到中间特征层次结构中，我们制定了DTAGnet并获得了进一步的性能提升。大量实验表明，所提出的模型在几种最先进的方法上获得了显着的利润。

##### URL
[https://arxiv.org/abs/1509.00151](https://arxiv.org/abs/1509.00151)

##### PDF
[https://arxiv.org/pdf/1509.00151](https://arxiv.org/pdf/1509.00151)

