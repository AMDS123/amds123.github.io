---
layout: post
title: "On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow"
date: 2017-04-26 20:47:36
categories: arXiv_CL
tags: arXiv_CL Classification
author: Markus Borg, Iben Lennerstad, Rasmus Ros, Elizabeth Bjarnason
mathjax: true
---

* content
{:toc}

##### Abstract
Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training.

##### Abstract (translated by Google)
丰富的数据是成功机器学习的关键。但是，监督式学习需要经常难以获得的注释数据。在资源有限的分类任务中，主动学习（Active Learning，AL）有望引导注释者使用为分类器带来最大价值的例子。 AL可以成功地与自我训练相结合，即用一个分类器是最确定的未标记的例子来扩展一个训练集。我们报告了我们在系统地使用AL方面的经验，为堆栈溢出帖子训练一个SVM分类器，讨论软件组件的性能。我们表明，被认为是分类器最有价值的训练样例也是人类最难以注释的。尽管仔细地进行了注释标准的演变，但我们报告的评价者之间的协议较低，但我们也提出了缓解策略。最后，基于一个注释者的工作，我们表明，自我训练可以提高分类的准确性。我们通过讨论有意使用AL和自我训练的未来文本矿工的含义来结束本文。

##### URL
[https://arxiv.org/abs/1705.02395](https://arxiv.org/abs/1705.02395)

##### PDF
[https://arxiv.org/pdf/1705.02395](https://arxiv.org/pdf/1705.02395)

