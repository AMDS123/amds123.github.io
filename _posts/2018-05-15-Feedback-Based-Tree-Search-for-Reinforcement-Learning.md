---
layout: post
title: "Feedback-Based Tree Search for Reinforcement Learning"
date: 2018-05-15 17:53:58
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Classification Recommendation
author: Daniel R. Jiang, Emmanuel Ekwedike, Han Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of artificial intelligence (AI) application domains, we propose a model-based reinforcement learning (RL) technique that iteratively applies MCTS on batches of small, finite-horizon versions of the original infinite-horizon Markov decision process. The terminal condition of the finite-horizon problems, or the leaf-node evaluator of the decision tree generated by MCTS, is specified using a combination of an estimated value function and an estimated policy function. The recommendations generated by the MCTS procedure are then provided as feedback in order to refine, through classification and regression, the leaf-node evaluator for the next iteration. We provide the first sample complexity bounds for a tree search-based RL algorithm. In addition, we show that a deep neural network implementation of the technique can create a competitive AI agent for the popular multi-player online battle arena (MOBA) game King of Glory.

##### Abstract (translated by Google)
受最近在许多人工智能（AI）应用领域中的蒙特卡洛树搜索（MCTS）的成功启发，我们提出了一种基于模型的强化学习（RL）技术，该技术将MCTS反复应用于成批的小型有限视野版本的原始无限时域马尔可夫决策过程。使用估计值函数和估计的策略函数的组合指定有限时域问题的终端条件或由MCTS生成的决策树的叶节点评估器。然后将MCTS过程生成的建议作为反馈提供，以便通过分类和回归改进下一次迭代的叶节点评估器。我们提供了基于树搜索的RL算法的第一个样本复杂性界限。另外，我们展示了该技术的深度神经网络实现可以为流行的多玩家在线战斗竞技场（MOBA）游戏King of Glory创建一个具有竞争力的AI代理。

##### URL
[http://arxiv.org/abs/1805.05935](http://arxiv.org/abs/1805.05935)

##### PDF
[http://arxiv.org/pdf/1805.05935](http://arxiv.org/pdf/1805.05935)

