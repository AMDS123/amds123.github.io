---
layout: post
title: "Non-distributional Word Vector Representations"
date: 2015-06-17 07:40:14
categories: arXiv_CL
tags: arXiv_CL Sparse Represenation_Learning Relation
author: Manaal Faruqui, Chris Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.

##### Abstract (translated by Google)
数据驱动的表达式学习是一种在NLP中至关重要的技术。尽管作为下游任务的特征来源具有无可争议的有用性，但这样的向量倾向于由不可解释的组件组成，其与传统的词汇语义理论的类别的关系最好是微不足道的。我们提出了一个从WordNet，FrameNet等手工语言资源构造可解释的单词向量的方法。这些向量是二元的（即只包含0和1），并且是99.9％稀疏的。我们分析他们的表现在最先进的评估方法的词向量的分布模型，并发现他们是标准分配方法的竞争力。

##### URL
[https://arxiv.org/abs/1506.05230](https://arxiv.org/abs/1506.05230)

##### PDF
[https://arxiv.org/pdf/1506.05230](https://arxiv.org/pdf/1506.05230)

