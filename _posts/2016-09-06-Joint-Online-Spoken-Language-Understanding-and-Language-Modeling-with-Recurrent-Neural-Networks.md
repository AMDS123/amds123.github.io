---
layout: post
title: "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks"
date: 2016-09-06 09:45:51
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model Detection
author: Bing Liu, Ian Lane
mathjax: true
---

* content
{:toc}

##### Abstract
Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent estimation as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.

##### Abstract (translated by Google)
说话人意图检测和语义槽填充是对话系统的口语理解（SLU）中的两个关键任务。在本文中，我们描述一个循环神经网络（RNN）模型，联合执行意图检测，槽填充和语言建模。神经网络模型不断更新意图估计作为转录的话语中的单词到达，并将其用作联合模型中的上下文特征。在ATIS基准数据集上对语言模型和在线SLU模型进行评估。在语言建模任务上，与独立训练语言模型相比，我们的联合模型相对于困惑度降低了11.8％。在SLU任务上，我们的联合模型在意图检测错误率方面比独立任务训练模型性能提高了22.3％，插槽填充F1得分略有下降。联合模型在具有噪声的语音输入的实际ASR设置中也显示出有利的性能。

##### URL
[https://arxiv.org/abs/1609.01462](https://arxiv.org/abs/1609.01462)

##### PDF
[https://arxiv.org/pdf/1609.01462](https://arxiv.org/pdf/1609.01462)

