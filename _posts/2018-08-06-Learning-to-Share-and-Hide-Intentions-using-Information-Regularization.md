---
layout: post
title: "Learning to Share and Hide Intentions using Information Regularization"
date: 2018-08-06 20:10:27
categories: arXiv_AI
tags: arXiv_AI Regularization Reinforcement_Learning
author: DJ Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, David Schwab
mathjax: true
---

* content
{:toc}

##### Abstract
Learning to cooperate with friends and compete with foes is a key component of multi-agent reinforcement learning. Typically to do so, one requires access to either a model of or interaction with the other agent(s). Here we show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction. Our approach is to encourage an agent to reveal or hide their intentions using an information-theoretic regularizer. We consider both the mutual information between goal and action given state, as well as the mutual information between goal and state. We show how to stochastically optimize these regularizers in a way that is easy to integrate with policy gradient reinforcement learning. Finally, we demonstrate that cooperative (competitive) policies learned with our approach lead to more (less) reward for a second agent in two simple asymmetric information games.

##### Abstract (translated by Google)
学习与朋友合作并与敌人竞争是多智能体强化学习的关键组成部分。通常，为此，需要访问其他代理的模型或与其他代理交互。在这里，我们将展示如何在没有这种模型或互动的非对称信息游戏中学习有效的合作和竞争策略。我们的方法是鼓励代理人使用信息理论规范者揭示或隐藏他们的意图。我们既考虑目标与行动之间的相互信息，也考虑目标与国家之间的相互信息。我们展示了如何以易于与政策梯度强化学习相结合的方式随机优化这些正规化器。最后，我们证明了使用我们的方法学习的合作（竞争）政策可以在两个简单的非对称信息游戏中为第二个代理商带来更多（更少）的回报。

##### URL
[http://arxiv.org/abs/1808.02093](http://arxiv.org/abs/1808.02093)

##### PDF
[http://arxiv.org/pdf/1808.02093](http://arxiv.org/pdf/1808.02093)

