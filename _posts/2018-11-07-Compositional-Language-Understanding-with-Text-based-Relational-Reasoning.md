---
layout: post
title: "Compositional Language Understanding with Text-based Relational Reasoning"
date: 2018-11-07 16:17:48
categories: arXiv_AI
tags: arXiv_AI QA Inference Language_Model Relation
author: Koustuv Sinha, Shagun Sodhani, William L. Hamilton, Joelle Pineau
mathjax: true
---

* content
{:toc}

##### Abstract
Neural networks for natural language reasoning have largely focused on extractive, fact-based question-answering (QA) and common-sense inference. However, it is also crucial to understand the extent to which neural networks can perform relational reasoning and combinatorial generalization from natural language---abilities that are often obscured by annotation artifacts and the dominance of language modeling in standard QA benchmarks. In this work, we present a novel benchmark dataset for language understanding that isolates performance on relational reasoning. We also present a neural message-passing baseline and show that this model, which incorporates a relational inductive bias, is superior at combinatorial generalization compared to a traditional recurrent neural network approach.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.02959](http://arxiv.org/abs/1811.02959)

##### PDF
[http://arxiv.org/pdf/1811.02959](http://arxiv.org/pdf/1811.02959)

