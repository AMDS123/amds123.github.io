---
layout: post
title: "A Distributional Semantics Approach to Implicit Language Learning"
date: 2016-06-29 12:08:51
categories: arXiv_CL
tags: arXiv_CL
author: Dimitrios Alikaniotis, John N. Williams
mathjax: true
---

* content
{:toc}

##### Abstract
In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a Vector-Space model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans.

##### Abstract (translated by Google)
在本文中，我们表明，在隐式语言学习条件下考虑概念可用性时，分布信息尤为重要。根据不同行为实验的结果，我们认为语义规律的隐含可学习性取决于相关概念在语言使用中的反映程度。在我们的模拟中，我们在英文或中文语料库上训练一个矢量空间模型，然后把结果表示输入到一个前馈神经网络。神经网络的任务是找到词表示和小说词之间的映射。使用来自四个使用不同语义操作的行为实验的数据集，我们能够获得与人类获得的学习模式非常相似的学习模式。

##### URL
[https://arxiv.org/abs/1606.09058](https://arxiv.org/abs/1606.09058)

##### PDF
[https://arxiv.org/pdf/1606.09058](https://arxiv.org/pdf/1606.09058)

