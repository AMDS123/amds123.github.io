---
layout: post
title: "xUnit: Learning a Spatial Activation Function for Efficient Image Restoration"
date: 2017-11-21 14:05:33
categories: arXiv_CV
tags: arXiv_CV Super_Resolution
author: Idan Kligvasser, Tamar Rott Shaham, Tomer Michaeli
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance.

##### Abstract (translated by Google)
近年来，深度神经网络（DNN）在许多低级视觉任务中取得了前所未有的性能。然而，最先进的结果通常是通过非常深的网络来实现的，这个网络可以达到几十个层次，数千万个参数。为了使DNN在资源有限的平台上实现，有必要削弱性能和效率之间的权衡。在本文中，我们提出了一种新的激活单元，它特别适用于图像恢复问题。与广泛的像素激活单元（如ReLU和S形）相反，我们的单元实现了具有空间连接的可学习非线性函数。这使得网络能够捕捉更复杂的特征，因此需要显着更少的层数才能达到相同的性能。我们通过对已经被认为非常小的去噪，去噪和超分辨率的最先进的网络进行实验来说明我们单元的有效性。通过我们的方法，我们能够将这些模型进一步减少近50％，而不会导致性能下降。

##### URL
[https://arxiv.org/abs/1711.06445](https://arxiv.org/abs/1711.06445)

##### PDF
[https://arxiv.org/pdf/1711.06445](https://arxiv.org/pdf/1711.06445)

