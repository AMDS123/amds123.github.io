---
layout: post
title: "Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification"
date: 2017-10-18 01:48:02
categories: arXiv_CV
tags: arXiv_CV Re-identification Knowledge Person_Re-identification Represenation_Learning
author: Dangwei Li, Xiaotang Chen, Zhang Zhang, Kaiqi Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, eg pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.

##### Abstract (translated by Google)
人员重新识别（ReID）是在不同的摄像头上识别同一个人。由于人物姿态，遮挡，背景杂乱等因素的巨大变化，这是一个具有挑战性的任务。如何提取强大的特征是ReID中的一个基本问题，目前仍是一个悬而未决的问题。在本文中，我们设计了一个多尺度上下文感知网络（MSCAN）来学习全身和身体各部分的强大特征，通过在每一层中叠加多尺度卷积，可以很好地捕捉本地的情境知识。此外，我们建议使用具有新颖空间约束的空间变换网络（STN）来学习和定位可变形的行人部分，而不是使用预定义的刚性部分。学习的身体部位可以释放一些困难，例如姿势变化和背景混乱，以部分为基础的表示。最后，通过多人识别任务，将全身和身体各部分的表征学习过程整合为一个统一的人ReID框架。对目前具有挑战性的大规模人员ReID数据集（包括基于图像的Market1501，CUHK03和基于序列的MARS数据集）的广泛评估表明，所提出的方法实现了最新的结果。

##### URL
[https://arxiv.org/abs/1710.06555](https://arxiv.org/abs/1710.06555)

##### PDF
[https://arxiv.org/pdf/1710.06555](https://arxiv.org/pdf/1710.06555)

