---
layout: post
title: "Analysing Results from AI Benchmarks: Key Indicators and How to Obtain Them"
date: 2018-11-20 11:26:36
categories: arXiv_AI
tags: arXiv_AI
author: Fernando Mart&#xed;nez-Plumed, Jos&#xe9; Hern&#xe1;ndez-Orallo
mathjax: true
---

* content
{:toc}

##### Abstract
Item response theory (IRT) can be applied to the analysis of the evaluation of results from AI benchmarks. The two-parameter IRT model provides two indicators (difficulty and discrimination) on the side of the item (or AI problem) while only one indicator (ability) on the side of the respondent (or AI agent). In this paper we analyse how to make this set of indicators dual, by adding a fourth indicator, generality, on the side of the respondent. Generality is meant to be dual to discrimination, and it is based on difficulty. Namely, generality is defined as a new metric that evaluates whether an agent is consistently good at easy problems and bad at difficult ones. With the addition of generality, we see that this set of four key indicators can give us more insight on the results of AI benchmarks. In particular, we explore two popular benchmarks in AI, the Arcade Learning Environment (Atari 2600 games) and the General Video Game AI competition. We provide some guidelines to estimate and interpret these indicators for other AI benchmarks and competitions.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.08186](http://arxiv.org/abs/1811.08186)

##### PDF
[http://arxiv.org/pdf/1811.08186](http://arxiv.org/pdf/1811.08186)

