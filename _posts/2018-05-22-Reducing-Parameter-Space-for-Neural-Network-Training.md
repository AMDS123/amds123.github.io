---
layout: post
title: "Reducing Parameter Space for Neural Network Training"
date: 2018-05-22 01:08:40
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Tong Qin, Ling Zhou, Dongbin Xiu
mathjax: true
---

* content
{:toc}

##### Abstract
For neural networks (NNs) with rectified linear unit (ReLU) or binary activation functions, we show that their training can be accomplished in a reduced parameter space. Specifically, the weights in each neuron can be trained on the unit sphere, as opposed to the entire space, and the threshold can be trained in a bounded interval, as opposed to the real line. We show that the NNs in the reduced parameter space are mathematically equivalent to the standard NNs with parameters in the whole space. The reduced parameter space shall facilitate the optimization procedure for the network training, as the search space becomes (much) smaller. We demonstrate the improved training performance using numerical examples.

##### Abstract (translated by Google)
对于具有整流线性单元（ReLU）或二元激活函数的神经网络（NNs），我们表明他们的训练可以在减小的参数空间中完成。具体而言，可以在单位球上训练每个神经元的权重，而不是整个空间，并且阈值可以在有界区间训练，而不是真实线。我们表明，减少参数空间中的神经网络在数学上等同于整个空间中具有参数的标准神经网络。随着搜索空间变得（更小），减小的参数空间将有利于网络训练的优化过程。我们用数值例子证明了改进的训练性能。

##### URL
[https://arxiv.org/abs/1805.08340](https://arxiv.org/abs/1805.08340)

##### PDF
[https://arxiv.org/pdf/1805.08340](https://arxiv.org/pdf/1805.08340)

