---
layout: post
title: "Generating Adversarial Fragments with Adversarial Networks for Physical-world Implementation"
date: 2019-07-09 22:46:10
categories: arXiv_CV
tags: arXiv_CV Adversarial Knowledge GAN Image_Classification Classification
author: Zelun Kong, Cong Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Although deep neural networks have been widely applied in many application domains, they are found to be vulnerable to adversarial attacks. A recent promising set of attacking techniques have been proposed, which mainly focus on generating adversarial examples under digital-world settings. Such strategies are unfortunately not implementable for any physical-world scenarios such as autonomous driving. In this paper, we present FragGAN, a new GAN-based framework which is capable of generating an adversarial image which differs from the original input image only through replacing a targeted fragment within the image using a corresponding visually indistinguishable adversarial fragment. FragGAN ensures that the resulting entire image is effective in attacking. For any physical-world implementation, an attacker could physically print out the adversarial fragment and then paste it onto the original fragment (e.g., a roadside sign for autonomous driving scenarios). FragGAN also enables clean-label attacks against image classification, as the resulting attacks may succeed even without modifying any essential content of an image. Extensive experiments including physical-world case studies on state-of-the-art autonomous steering and image classification models demonstrate that FragGAN is highly effective and superior to simple extensions of existing approaches. To the best of our knowledge, FragGAN is the first approach that can implement effective and clean-label physical-world attacks.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.04449](http://arxiv.org/abs/1907.04449)

##### PDF
[http://arxiv.org/pdf/1907.04449](http://arxiv.org/pdf/1907.04449)

