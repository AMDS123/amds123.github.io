---
layout: post
title: "Learning Sentence Representation with Guidance of Human Attention"
date: 2017-05-09 10:48:01
categories: arXiv_CL
tags: arXiv_CL Attention
author: Shaonan Wang, Jiajun Zhang, Chengqing Zong
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, much progress has been made in learning general-purpose sentence representations that can be used across domains. However, most of the existing models typically treat each word in a sentence equally. In contrast, extensive studies have proven that human read sentences efficiently by making a sequence of fixation and saccades. This motivates us to improve sentence representations by assigning different weights to the vectors of the component words, which can be treated as an attention mechanism on single sentences. To that end, we propose two novel attention models, in which the attention weights are derived using significant predictors of human reading time, i.e., Surprisal, POS tags and CCG supertags. The extensive experiments demonstrate that the proposed methods significantly improve upon the state-of-the-art sentence representation models.

##### Abstract (translated by Google)
最近，在学习跨领域使用的通用句子表示方面已经取得了很大进展。但是，大多数现有的模型通常都会平等地对待每个单词。相比之下，广泛的研究已经证明，人类通过一系列的注视和扫视来有效地阅读句子。这就促使我们通过对成分词的向量赋予不同的权重来改善句子表征，这可以作为对单个句子的注意机制。为此，我们提出了两种新颖的注意模型，其中注意力是使用人类阅读时间的显着预测因子，即Surprisal，POS标签和CCG超标记导出的。广泛的实验证明，所提出的方法显着地改善了现有技术的句子表示模型。

##### URL
[https://arxiv.org/abs/1609.09189](https://arxiv.org/abs/1609.09189)

##### PDF
[https://arxiv.org/pdf/1609.09189](https://arxiv.org/pdf/1609.09189)

