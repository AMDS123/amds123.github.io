---
layout: post
title: "Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures"
date: 2017-10-21 21:02:46
categories: arXiv_CV
tags: arXiv_CV Knowledge Attention Caption
author: Gaurav Mittal, Tanya Marwah, Vineeth N. Balasubramanian
mathjax: true
---

* content
{:toc}

##### Abstract
This paper introduces a novel approach for generating videos called Synchronized Deep Recurrent Attentive Writer (Sync-DRAW). Sync-DRAW can also perform text-to-video generation which, to the best of our knowledge, makes it the first approach of its kind. It combines a Variational Autoencoder~(VAE) with a Recurrent Attention Mechanism in a novel manner to create a temporally dependent sequence of frames that are gradually formed over time. The recurrent attention mechanism in Sync-DRAW attends to each individual frame of the video in sychronization, while the VAE learns a latent distribution for the entire video at the global level. Our experiments with Bouncing MNIST, KTH and UCF-101 suggest that Sync-DRAW is efficient in learning the spatial and temporal information of the videos and generates frames with high structural integrity, and can generate videos from simple captions on these datasets. (Accepted as oral paper in ACM-Multimedia 2017)

##### Abstract (translated by Google)
本文介绍了一种用于生成视频的新方法，称为同步深度递归静止写入器（Sync-DRAW）。 Sync-DRAW还可以执行文本到视频的生成，据我们所知，这是第一种类型的方法。它以新颖的方式将变分自动编码器〜（VAE）与循环注意机制相结合，以创建随时间逐渐形成的时间依赖的帧序列。 Sync-DRAW中的循环注意机制在同步中参与视频的每个单独帧，而VAE在全局级别学习整个视频的潜在分布。我们使用Bouncing MNIST，KTH和UCF-101进行的实验表明，Sync-DRAW可以有效地学习视频的空间和时间信息，并生成具有高度结构完整性的帧，并且可以从这些数据集上的简单字幕生成视频。 （在2017年ACM-Multimedia中被接受为口头报告）

##### URL
[https://arxiv.org/abs/1611.10314](https://arxiv.org/abs/1611.10314)

##### PDF
[https://arxiv.org/pdf/1611.10314](https://arxiv.org/pdf/1611.10314)

