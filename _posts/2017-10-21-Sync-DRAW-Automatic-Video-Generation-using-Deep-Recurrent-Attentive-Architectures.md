---
layout: post
title: 'Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures'
date: 2017-10-21 21:02:46
categories: arXiv_CV
tags: arXiv_CV Caption
author: Gaurav Mittal, Tanya Marwah, Vineeth N. Balasubramanian
---

* content
{:toc}

##### Abstract
This paper introduces a novel approach for generating videos called Synchronized Deep Recurrent Attentive Writer (Sync-DRAW). Sync-DRAW can also perform text-to-video generation which, to the best of our knowledge, makes it the first approach of its kind. It combines a Variational Autoencoder~(VAE) with a Recurrent Attention Mechanism in a novel manner to create a temporally dependent sequence of frames that are gradually formed over time. The recurrent attention mechanism in Sync-DRAW attends to each individual frame of the video in sychronization, while the VAE learns a latent distribution for the entire video at the global level. Our experiments with Bouncing MNIST, KTH and UCF-101 suggest that Sync-DRAW is efficient in learning the spatial and temporal information of the videos and generates frames with high structural integrity, and can generate videos from simple captions on these datasets. (Accepted as oral paper in ACM-Multimedia 2017)

##### Abstract (translated by Google)
本文介绍了一种用于生成视频的新方法，称为同步深度复发细致写入器（Sync-DRAW）。 Sync-DRAW还可以执行文本到视频的生成，据我们所知，这使得它成为同类中的第一种方法。它以一种新颖的方式将变分自动编码器（VAE）与经常性注意机制相结合，以创建随时间逐渐形成的时间相关的帧序列。 Sync-DRAW中的经常性关注机制在同步化过程中出现在视频的每个帧中，而VAE则在全局层次上学习整个视频的潜在分布。我们使用Bouncing MNIST，KTH和UCF-101进行的实验表明，Sync-DRAW在学习视频的时空信息方面非常高效，可以生成高结构完整性的帧，并且可以从这些数据集的简单字幕生成视频。 （作为ACM-Multimedia 2017的口头报告接受）

##### URL
[https://arxiv.org/abs/1611.10314](https://arxiv.org/abs/1611.10314)

