---
layout: post
title: "Prioritized Sequence Experience Replay"
date: 2019-05-25 15:38:00
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Marc Brittain, Josh Bertram, Xuxi Yang, Peng Wei
mathjax: true
---

* content
{:toc}

##### Abstract
Experience replay is widely used in deep reinforcement learning algorithms and allows agents to remember and learn from experiences from the past. In an effort to learn more efficiently, researchers proposed prioritized experience replay (PER) which samples important transitions more frequently. In this paper, we propose Prioritized Sequence Experience Replay (PSER) a framework for prioritizing sequences of experience in an attempt to both learn more efficiently and to obtain better performance. We compare performance of uniform, PER and PSER sampling techniques in DQN on the Atari 2600 benchmark and show DQN with PSER substantially outperforms PER and uniform sampling.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.12726](http://arxiv.org/abs/1905.12726)

##### PDF
[http://arxiv.org/pdf/1905.12726](http://arxiv.org/pdf/1905.12726)

