---
layout: post
title: "Advances in Pre-Training Distributed Word Representations"
date: 2017-12-26 21:00:04
categories: arXiv_CL
tags: arXiv_CL
author: Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin
mathjax: true
---

* content
{:toc}

##### Abstract
Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.

##### Abstract (translated by Google)
现在，许多自然语言处理应用程序都依赖于从大型文本语料库（例如新闻收集，维基百科和网络爬取）估算的预先训练的词表示。在本文中，我们展示了如何使用已知技巧的组合来训练高质量的单词向量表示，然而这些技巧很少一起使用。我们工作的主要成果是一系列新的公开可用的预训练模型，这些模型在许多任务上大大优于现有技术水平。

##### URL
[http://arxiv.org/abs/1712.09405](http://arxiv.org/abs/1712.09405)

##### PDF
[http://arxiv.org/pdf/1712.09405](http://arxiv.org/pdf/1712.09405)

