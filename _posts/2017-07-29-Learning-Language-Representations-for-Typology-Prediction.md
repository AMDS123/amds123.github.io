---
layout: post
title: "Learning Language Representations for Typology Prediction"
date: 2017-07-29 23:38:25
categories: arXiv_CL
tags: arXiv_CL NMT
author: Chaitanya Malaviya, Graham Neubig, Patrick Littell
mathjax: true
---

* content
{:toc}

##### Abstract
One central mystery of neural NLP is what neural models "know" about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one neural machine translation (NMT) system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages' geographic and phylogenetic neighbors.

##### Abstract (translated by Google)
神经NLP的一个中心奥秘是神经模型“知道”他们的主题。当一个神经机器翻译系统学习从一种语言翻译到另一种语言时，它学习语言的语法还是语义？能否从系统中提取这些知识来填补人类科学知识的漏洞？现有的类型数据库仅包含几百种语言的相对完整的特征规范。利用一千多种语言的平行文本的存在，我们构建了一个从1017种语言到英文的大规模多对一神经机器翻译（NMT）系统，并用它来预测类型数据库中缺少的信息。实验表明，所提出的方法不仅能够推断句法，而且还能够推断语音和语音库存特征，并且能够在可以获取关于语言的地理和系统发育邻居的信息的基线上进行改进。

##### URL
[https://arxiv.org/abs/1707.09569](https://arxiv.org/abs/1707.09569)

##### PDF
[https://arxiv.org/pdf/1707.09569](https://arxiv.org/pdf/1707.09569)

