---
layout: post
title: "Reward Constrained Policy Optimization"
date: 2018-05-28 17:31:11
categories: arXiv_AI
tags: arXiv_AI Regularization Reinforcement_Learning Optimization
author: Chen Tessler, Daniel J. Mankowitz, Shie Mannor
mathjax: true
---

* content
{:toc}

##### Abstract
Teaching agents to perform tasks using Reinforcement Learning is no easy feat. As the goal of reinforcement learning agents is to maximize the accumulated reward, they often find loopholes and misspecifications in the reward signal which lead to unwanted behavior. To overcome this, often, regularization is employed through the technique of reward shaping - the agent is provided an additional weighted reward signal, meant to lead it towards a desired behavior. The weight is considered as a hyper-parameter and is selected through trial and error, a time consuming and computationally intensive task. In this work, we present a novel multi-timescale approach for constrained policy optimization, called, 'Reward Constrained Policy Optimization' (RCPO), which enables policy regularization without the use of reward shaping. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies.

##### Abstract (translated by Google)
教学代理人使用强化学习来完成任务并非易事。由于强化学习代理人的目标是最大化积累的奖励，他们经常在奖励信号中发现漏洞和错误规定，导致不需要的行为。为了克服这个问题，通常通过奖励形成技术来使用正则化 - 代理人被提供额外的加权回报信号，意在引导其朝向期望的行为。权重被认为是超参数，并且通过反复试验来选择，这是一项耗时且计算密集的任务。在这项工作中，我们提出了一种用于约束策略优化的新型多时间尺度方法，称为“奖励约束策略优化”（RCPO），它可以在不使用奖励整形的情况下实现政策规范化。我们证明了我们方法的收敛性，并提供了它的训练约束满足策略能力的经验证据。

##### URL
[http://arxiv.org/abs/1805.11074](http://arxiv.org/abs/1805.11074)

##### PDF
[http://arxiv.org/pdf/1805.11074](http://arxiv.org/pdf/1805.11074)

