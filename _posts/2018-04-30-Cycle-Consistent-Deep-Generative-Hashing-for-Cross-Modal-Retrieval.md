---
layout: post
title: "Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval"
date: 2018-04-30 01:28:20
categories: arXiv_CV
tags: arXiv_CV Adversarial Embedding Relation
author: Lin Wu, Yang Wang, Ling Shao
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a novel deep generative approach to cross-modal retrieval to learn hash functions in the absence of paired training samples through the cycle consistency loss. Our proposed approach employs adversarial training scheme to lean a couple of hash functions enabling translation between modalities while assuming the underlying semantic relationship. To induce the hash codes with semantics to the input-output pair, cycle consistency loss is further proposed upon the adversarial training to strengthen the correlations between inputs and corresponding outputs. Our approach is generative to learn hash functions such that the learned hash codes can maximally correlate each input-output correspondence, meanwhile can also regenerate the inputs so as to minimize the information loss. The learning to hash embedding is thus performed to jointly optimize the parameters of the hash functions across modalities as well as the associated generative models. Extensive experiments on a variety of large-scale cross-modal data sets demonstrate that our proposed method achieves better retrieval results than the state-of-the-arts.

##### Abstract (translated by Google)
在本文中，我们提出了一种新的深层生成方法来跨模态检索，以通过周期一致性丢失来避免配对训练样本中的散列函数。我们提出的方法采用对抗训练方案来倾斜一些哈希函数，使模态之间进行转换，同时假设潜在的语义关系。为了将哈希码与输入 - 输出对的语义联系起来，在对抗训练的基础上进一步提出了循环一致性损失，以加强输入和相应输出之间的相关性。我们的方法是生成学习哈希函数，使学习哈希代码可以最大限度地关联每个输入输出对应关系，同时还可以重新生成输入，以最大限度地减少信息丢失。散列嵌入的学习因此被执行以联合地优化模式中的散列函数的参数以及相关联的生成模型。对各种大规模跨模态数据集的大量实验表明，我们提出的方法比现有技术获得更好的检索结果。

##### URL
[https://arxiv.org/abs/1804.11013](https://arxiv.org/abs/1804.11013)

##### PDF
[https://arxiv.org/pdf/1804.11013](https://arxiv.org/pdf/1804.11013)

