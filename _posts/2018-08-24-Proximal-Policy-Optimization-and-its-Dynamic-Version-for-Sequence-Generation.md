---
layout: post
title: "Proximal Policy Optimization and its Dynamic Version for Sequence Generation"
date: 2018-08-24 02:14:43
categories: arXiv_CL
tags: arXiv_CL Adversarial QA Reinforcement_Learning Optimization
author: Yi-Lin Tuan, Jinzhi Zhang, Yujia Li, Hung-yi Lee
mathjax: true
---

* content
{:toc}

##### Abstract
In sequence generation task, many works use policy gradient for model optimization to tackle the intractable backpropagation issue when maximizing the non-differentiable evaluation metrics or fooling the discriminator in adversarial learning. In this paper, we replace policy gradient with proximal policy optimization (PPO), which is a proved more efficient reinforcement learning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We demonstrate the efficacy of PPO and PPO-dynamic on conditional sequence generation tasks including synthetic experiment and chit-chat chatbot. The results show that PPO and PPO-dynamic can beat policy gradient by stability and performance.

##### Abstract (translated by Google)
在序列生成任务中，许多工作使用策略梯度进行模型优化，以在最大化不可微分的评估指标或欺骗对抗性学习中的鉴别器时解决难以处理的反向传播问题。在本文中，我们用近端策略优化（PPO）代替策略梯度，这是一种被证明更有效的强化学习算法，并提出了PPO（PPO动态）的动态方法。我们证明了PPO和PPO-dynamic对条件序列生成任务的有效性，包括合成实验和聊天聊天机器人。结果表明，PPO和PPO动态可以通过稳定性和性能击败政策梯度。

##### URL
[http://arxiv.org/abs/1808.07982](http://arxiv.org/abs/1808.07982)

##### PDF
[http://arxiv.org/pdf/1808.07982](http://arxiv.org/pdf/1808.07982)

