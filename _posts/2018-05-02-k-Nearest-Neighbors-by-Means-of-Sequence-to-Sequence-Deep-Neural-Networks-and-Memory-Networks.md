---
layout: post
title: "k-Nearest Neighbors by Means of Sequence to Sequence Deep Neural Networks and Memory Networks"
date: 2018-05-02 19:19:19
categories: arXiv_CV
tags: arXiv_CV Classification Memory_Networks
author: Yiming Xu, Diego Klabjan
mathjax: true
---

* content
{:toc}

##### Abstract
k-Nearest Neighbors is one of the most fundamental but effective classification models. In this paper, we propose two families of models built on a sequence to sequence model and a memory network model to mimic the k-Nearest Neighbors model, which generate a sequence of labels, a sequence of out-of-sample feature vectors and a final label for classification, and thus they could also function as oversamplers. We also propose 'out-of-core' versions of our models which assume that only a small portion of data can be loaded into memory. Computational experiments show that our models outperform k-Nearest Neighbors, a feed-forward neural network and a memory network, due to the fact that our models must produce additional output and not just the label. As an oversample on imbalanced datasets, the sequence to sequence kNN model often outperforms Synthetic Minority Over-sampling Technique and Adaptive Synthetic Sampling.

##### Abstract (translated by Google)
k-Nearest Neighbors是最基本但最有效的分类模型之一。在本文中，我们提出了两个基于序列模型的模型族和一个模拟k-Nearest Neighbors模型的存储网络模型，它们生成一系列标签，一系列样本外特征向量和一个用于分类的最终标签，因此它们也可以用作过滤器。我们还提出了我们模型的“非核心”版本，它们假设只有一小部分数据可以加载到内存中。计算实验表明，我们的模型优于k-Nearest Neighbors，前馈神经网络和内存网络，因为我们的模型必须产生额外的输出，而不仅仅是标签。作为不平衡数据集的过采样，序列kNN模型的序列通常优于合成少数过采样技术和自适应合成采样。

##### URL
[https://arxiv.org/abs/1804.11214](https://arxiv.org/abs/1804.11214)

##### PDF
[https://arxiv.org/pdf/1804.11214](https://arxiv.org/pdf/1804.11214)

