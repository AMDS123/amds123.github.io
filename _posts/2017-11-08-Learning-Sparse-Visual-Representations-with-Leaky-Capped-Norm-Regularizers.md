---
layout: post
title: "Learning Sparse Visual Representations with Leaky Capped Norm Regularizers"
date: 2017-11-08 07:54:41
categories: arXiv_CV
tags: arXiv_CV Regularization Sparse Knowledge
author: Jianqiao Wangni, Dahua Lin
mathjax: true
---

* content
{:toc}

##### Abstract
Sparsity inducing regularization is an important part for learning over-complete visual representations. Despite the popularity of $\ell_1$ regularization, in this paper, we investigate the usage of non-convex regularizations in this problem. Our contribution consists of three parts. First, we propose the leaky capped norm regularization (LCNR), which allows model weights below a certain threshold to be regularized more strongly as opposed to those above, therefore imposes strong sparsity and only introduces controllable estimation bias. We propose a majorization-minimization algorithm to optimize the joint objective function. Second, our study over monocular 3D shape recovery and neural networks with LCNR outperforms $\ell_1$ and other non-convex regularizations, achieving state-of-the-art performance and faster convergence. Third, we prove a theoretical global convergence speed on the 3D recovery problem. To the best of our knowledge, this is the first convergence analysis of the 3D recovery problem.

##### Abstract (translated by Google)
稀疏诱导正则化是学习过度完整视觉表示的重要组成部分。尽管$ \ ell_1 $正则化的普及，在本文中，我们调查了在这个问题中非凸正则化的用法。我们的贡献包括三个部分。首先，我们提出了泄漏加盖规范正则化（LCNR），它允许低于一定阈值的模型权重比上面更强调整，因此强加稀疏性，只引入可控的估计偏差。我们提出了一种主要化最小化算法来优化联合目标函数。其次，我们对具有LCNR的单眼三维形状恢复和神经网络的研究优于$ \ ell_1 $和其他非凸正则化，实现了最先进的性能和更快的收敛性。第三，我们证明了3D恢复问题的理论全局收敛速度。就我们所知，这是3D恢复问题的第一个收敛分析。

##### URL
[https://arxiv.org/abs/1711.02857](https://arxiv.org/abs/1711.02857)

##### PDF
[https://arxiv.org/pdf/1711.02857](https://arxiv.org/pdf/1711.02857)

