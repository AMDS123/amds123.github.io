---
layout: post
title: "Open Sesame: Getting Inside BERT's Linguistic Knowledge"
date: 2019-06-04 19:41:10
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention Quantitative
author: Yongjie Lin, Yi Chern Tan, Robert Frank
mathjax: true
---

* content
{:toc}

##### Abstract
How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT's representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT's representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT's representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.01698](http://arxiv.org/abs/1906.01698)

##### PDF
[http://arxiv.org/pdf/1906.01698](http://arxiv.org/pdf/1906.01698)

