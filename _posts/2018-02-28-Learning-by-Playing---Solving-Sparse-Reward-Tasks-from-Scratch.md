---
layout: post
title: "Learning by Playing - Solving Sparse Reward Tasks from Scratch"
date: 2018-02-28 18:15:49
categories: arXiv_RO
tags: arXiv_RO Sparse Reinforcement_Learning
author: Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, Jost Tobias Springenberg
mathjax: true
---

* content
{:toc}

##### Abstract
We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.

##### Abstract (translated by Google)
我们提出定期辅助控制（SAC-X），这是一种在强化学习（RL）环境下的新型学习范式。 SAC-X能够在存在多个稀疏奖励信号的情况下从头开始学习复杂行为。为此，代理人配备了一套通用的辅助任务，它试图通过非政策RL同时学习。我们方法背后的关键思想是，主动（学习）调度和辅助策略的执行，可以使代理人有效地探索其环境 - 使其能够在稀少的回报RL中表现突出。我们在几个具有挑战性的机器人操作设置中的实验证明了我们的方法的力量

##### URL
[http://arxiv.org/abs/1802.10567](http://arxiv.org/abs/1802.10567)

##### PDF
[http://arxiv.org/pdf/1802.10567](http://arxiv.org/pdf/1802.10567)

