---
layout: post
title: "Interpretable Explanations of Black Boxes by Meaningful Perturbation"
date: 2018-01-09 13:53:21
categories: arXiv_AI
tags: arXiv_AI Salient Prediction
author: Ruth Fong, Andrea Vedaldi
mathjax: true
---

* content
{:toc}

##### Abstract
As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.

##### Abstract (translated by Google)
随着机器学习算法越来越多地应用于诸如医疗诊断或自主驾驶等高影响但高风险的任务，研究人员可以解释这些算法如何达到预测是至关重要的。近年来，已经开发了许多图像显着性方法来总结高度复杂的神经网络在图像中“看”的位置以作为其预测的证据。然而，这些技术受到启发性和建筑限制的限制。在本文中，我们做出了两个主要的贡献：首先，我们提出了一个通用框架来学习任何黑箱算法的各种解释。其次，我们专门研究框架，以找到对分类器决策负有最大责任的图像部分。与以前的作品不同，我们的方法是模型不可知和可测试的，因为它是基于明确和可解释的图像干扰。

##### URL
[http://arxiv.org/abs/1704.03296](http://arxiv.org/abs/1704.03296)

##### PDF
[http://arxiv.org/pdf/1704.03296](http://arxiv.org/pdf/1704.03296)

