---
layout: post
title: "Optimizing Deep Neural Network Architecture: A Tabu Search Based Approach"
date: 2018-08-17 20:12:26
categories: arXiv_AI
tags: arXiv_AI Optimization Classification Gradient_Descent
author: Tarun Kumar Gupta, Khalid Raza
mathjax: true
---

* content
{:toc}

##### Abstract
The performance of Feedforward neural network (FNN) fully de-pends upon the selection of architecture and training algorithm. FNN architecture can be tweaked using several parameters, such as the number of hidden layers, number of hidden neurons at each hidden layer and number of connections between layers. There may be exponential combinations for these architectural attributes which may be unmanageable manually, so it requires an algorithm which can automatically design an optimal architecture with high generalization ability. Numerous optimization algorithms have been utilized for FNN architecture determination. This paper proposes a new methodology which can work on the estimation of hidden layers and their respective neurons for FNN. This work combines the advantages of Tabu search (TS) and Gradient descent with momentum backpropagation (GDM) training algorithm to demonstrate how Tabu search can automatically select the best architecture from the populated architectures based on minimum testing error criteria. The proposed approach has been tested on four classification benchmark dataset of different size.

##### Abstract (translated by Google)
前馈神经网络（FNN）的性能完全取决于体系结构和训练算法的选择。可以使用多个参数来调整FNN架构，例如隐藏层的数量，每个隐藏层处的隐藏神经元的数量以及层之间的连接数量。对于这些可能无法手动管理的架构属性，可能存在指数组合，因此需要一种能够自动设计具有高泛化能力的最佳架构的算法。已经将许多优化算法用于FNN架构确定。本文提出了一种新方法，可用于估计FNN的隐层及其各自的神经元。这项工作结合了Tabu搜索（TS）和Gradient下降与动量反向传播（GDM）训练算法的优点，以演示Tabu搜索如何根据最小测试错误标准从填充的体系结构中自动选择最佳体系结构。已经针对不同大小的四个分类基准数据集测试了所提出的方法。

##### URL
[http://arxiv.org/abs/1808.05979](http://arxiv.org/abs/1808.05979)

##### PDF
[http://arxiv.org/pdf/1808.05979](http://arxiv.org/pdf/1808.05979)

