---
layout: post
title: "Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition"
date: 2015-09-08 15:29:23
categories: arXiv_CL
tags: arXiv_CL Knowledge Speech_Recognition Classification Language_Model Recognition
author: Mortaza Doulaty, Oscar Saz, Thomas Hain
mathjax: true
---

* content
{:toc}

##### Abstract
Speech recognition systems are often highly domain dependent, a fact widely reported in the literature. However the concept of domain is complex and not bound to clear criteria. Hence it is often not evident if data should be considered to be out-of-domain. While both acoustic and language models can be domain specific, work in this paper concentrates on acoustic modelling. We present a novel method to perform unsupervised discovery of domains using Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is assumed to exist in the data, whereby each audio segment can be considered to be a weighted mixture of domain properties. The classification of audio segments into domains allows the creation of domain specific acoustic models for automatic speech recognition. Experiments are conducted on a dataset of diverse speech data covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech, with a joint training set of 60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to LDA based domains was shown to yield relative Word Error Rate (WER) improvements of up to 16% relative, compared to pooled training, and up to 10%, compared with models adapted with human-labelled prior domain knowledge.

##### Abstract (translated by Google)
语音识别系统通常是高度依赖于领域的，这是文献中广泛报道的一个事实。然而，域的概念是复杂的，不一定要明确的标准。因此，如果数据被认为是超出范围的话，通常并不明显。虽然声学和语言模型都可以是领域特定的，但本文中的工作集中在声学建模上。我们提出了一个新的方法来执行无监督发现领域使用潜在Dirichlet分配（LDA）建模。这里假定数据中存在一组隐藏域，由此每个音频片段可以被认为是域属性的加权混合。将音频段分类到域允许创建用于自动语音识别的领域特定声学模型。对包括广播和电视广播，电话对话，会议，讲座和朗读演讲的不同语音数据的数据集进行实验，共60小时的训练集和6小时的测试集。相较于合并训练，证明相对于基于LDA的域的最大后验（MAP）适应性的相对字错误率（WER）改进高达16％，并且相对于用人标记的事先适应的模型改善达10％领域知识。

##### URL
[https://arxiv.org/abs/1509.02412](https://arxiv.org/abs/1509.02412)

##### PDF
[https://arxiv.org/pdf/1509.02412](https://arxiv.org/pdf/1509.02412)

