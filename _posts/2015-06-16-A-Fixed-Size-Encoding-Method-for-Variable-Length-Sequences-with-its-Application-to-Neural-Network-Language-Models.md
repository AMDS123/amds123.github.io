---
layout: post
title: "A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models"
date: 2015-06-16 18:41:05
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.

##### Abstract (translated by Google)
在本文中，我们提出了新的固定大小的常规遗忘编码（FOFE）方法，它可以将任何可变长度的单词序列几乎唯一地编码成固定大小的表示。 FOFE可以根据单词的位置使用简单的常规遗忘机制来按照序列对单词的顺序进行建模。在这项工作中，我们已经应用FOFE前馈神经网络语言模型（FNN-LM）。实验结果表明，在不使用任何反馈反馈的情况下，基于FOFE的FNN-LM不仅可以显着优于标准固定输入FNN-LM，而且还可以显着优于流行的RNN-LM。

##### URL
[https://arxiv.org/abs/1505.01504](https://arxiv.org/abs/1505.01504)

##### PDF
[https://arxiv.org/pdf/1505.01504](https://arxiv.org/pdf/1505.01504)

