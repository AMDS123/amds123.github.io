---
layout: post
title: "Learning how to learn: an adaptive dialogue agent for incrementally learning visually grounded word meanings"
date: 2017-09-29 14:21:31
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Yanchao Yu, Arash Eshghi, Oliver Lemon
mathjax: true
---

* content
{:toc}

##### Abstract
We present an optimised multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human tutor, trained on real human-human tutoring data. Within a life-long interactive learning period, the agent, trained using Reinforcement Learning (RL), must be able to handle natural conversations with human users and achieve good learning performance (accuracy) while minimising human effort in the learning process. We train and evaluate this system in interaction with a simulated human tutor, which is built on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual learning task. The results show that: 1) The learned policy can coherently interact with the simulated user to achieve the goal of the task (i.e. learning visual attributes of objects, e.g. colour and shape); and 2) it finds a better trade-off between classifier accuracy and tutoring costs than hand-crafted rule-based policies, including ones with dynamic policies.

##### Abstract (translated by Google)
我们提出了一个优化的多模态对话代理，用于交互式学习来自人类导师的视觉基础词汇含义，并在真实的人类辅导数据上进行训练。在终身互动学习阶段，使用强化学习（RL）进行训练的代理必须能够处理与用户的自然对话，并在学习过程中最大限度地减少人力成本，同时获得良好的学习表现（准确性）。我们训练和评估这个系统与一个模拟人类导师交互，这是建立在BURCHAK语料库 - 一个人类 - 人类对话数据集的视觉学习任务。结果表明：1）学习策略可以与模拟用户相互作用，实现任务的目标（即学习对象的视觉属性，如颜色和形状）;和2）在分类器准确性和辅导成本之间找到比手工制定的基于规则的策略（包括具有动态策略的策略）更好的折衷。

##### URL
[https://arxiv.org/abs/1709.10423](https://arxiv.org/abs/1709.10423)

##### PDF
[https://arxiv.org/pdf/1709.10423](https://arxiv.org/pdf/1709.10423)

