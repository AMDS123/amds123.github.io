---
layout: post
title: "Skeleton Based Human Action Recognition with Global Context-Aware Attention LSTM Networks"
date: 2017-12-13 09:49:38
categories: arXiv_CV
tags: arXiv_CV Attention Action_Recognition RNN Recognition
author: Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, Alex C. Kot
mathjax: true
---

* content
{:toc}

##### Abstract
Human action recognition in 3D skeleton sequences has attracted a lot of research attention. Recently, Long Short-Term Memory (LSTM) networks have shown promising performance in this task due to their strengths in modeling the dependencies and dynamics in sequential data. As not all skeletal joints are informative for action recognition, and the irrelevant joints often bring noise which can degrade the performance, we need to pay more attention to the informative ones. However, the original LSTM network does not have explicit attention ability. In this paper, we propose a new class of LSTM network, Global Context-Aware Attention LSTM (GCA-LSTM), for skeleton based action recognition. This network is capable of selectively focusing on the informative joints in each frame of each skeleton sequence by using a global context memory cell. To further improve the attention capability of our network, we also introduce a recurrent attention mechanism, with which the attention performance of the network can be enhanced progressively. Moreover, we propose a stepwise training scheme in order to train our network effectively. Our approach achieves state-of-the-art performance on five challenging benchmark datasets for skeleton based action recognition.

##### Abstract (translated by Google)
三维骨架序列中的人体动作识别已经引起了很多研究的关注。最近，由于长期短期记忆（LSTM）网络在顺序数据中的依赖性和动态性建模方面的优势，因此在这项任务中表现出有希望的性能。由于不是所有的骨骼关节都为动作识别提供了信息，而且不相关的关节通常会带来噪声，从而降低了性能，所以我们需要更多的关注信息。但是，原来的LSTM网络没有明确的注意力。在本文中，我们提出了一种新的基于骨架的动作识别LSTM网络，全局上下文感知注意LSTM（GCA-LSTM）。通过使用全局上下文存储器单元，该网络能够选择性地关注每个骨架序列的每个帧中的信息性关节。为了进一步提高我们网络的注意力，我们还引入了经常性的注意机制，可以逐步提高网络的注意力。此外，我们提出了一个逐步的培训计划，以有效地训练我们的网络。我们的方法在基于骨骼动作识别的五个具有挑战性的基准数据集上实现了最先进的性能。

##### URL
[http://arxiv.org/abs/1707.05740](http://arxiv.org/abs/1707.05740)

##### PDF
[http://arxiv.org/pdf/1707.05740](http://arxiv.org/pdf/1707.05740)

