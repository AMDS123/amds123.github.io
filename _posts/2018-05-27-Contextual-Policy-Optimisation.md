---
layout: post
title: "Contextual Policy Optimisation"
date: 2018-05-27 17:50:22
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Supratik Paul, Michael A. Osborne, Shimon Whiteson
mathjax: true
---

* content
{:toc}

##### Abstract
Policy gradient methods have been successfully applied to a variety of reinforcement learning tasks. However, while learning in a simulator, these methods do not utilise the opportunity to improve learning by adjusting certain environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but that are controllable in a simulator. This can lead to slow learning, or convergence to highly suboptimal policies. In this paper, we present contextual policy optimisation (CPO). The central idea is to use Bayesian optimisation to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this Bayesian optimisation practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. We apply CPO to a number of continuous control tasks of varying difficulty and show that CPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling but are key to learning good policies.

##### Abstract (translated by Google)
政策梯度方法已成功应用于各种强化学习任务。然而，在模拟器中学习时，这些方法并没有利用机会通过调整某些环境变量来改善学习：由物理设置中的环境随机确定的但是可在模拟器中控制的不可观察状态特征。这可能会导致学习缓慢，或者会导致很不理想的政策。在本文中，我们提出了上下文策略优化（CPO）。中心思想是使用贝叶斯优化来主动选择环境变量的分布，以最大化策略梯度方法的每次迭代产生的改进。为了实现这种贝叶斯优化，我们提供了两个易于计算的当前策略的低维指纹。我们将CPO应用于许多不同难度的连续控制任务，并表明CPO可以有效地学习对重大罕见事件有效的策略，这些事件在随机抽样中不可能观察到，但是学习良好策略的关键。

##### URL
[http://arxiv.org/abs/1805.10662](http://arxiv.org/abs/1805.10662)

##### PDF
[http://arxiv.org/pdf/1805.10662](http://arxiv.org/pdf/1805.10662)

