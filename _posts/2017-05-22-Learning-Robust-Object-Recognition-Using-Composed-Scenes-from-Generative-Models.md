---
layout: post
title: "Learning Robust Object Recognition Using Composed Scenes from Generative Models"
date: 2017-05-22 07:50:24
categories: arXiv_CV
tags: arXiv_CV CNN Inference Classification Recognition
author: Hao Wang, Xingyu Lin, Yimeng Zhang, Tai Sing Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent feedback connections in the mammalian visual system have been hypothesized to play a role in synthesizing input in the theoretical framework of analysis by synthesis. The comparison of internally synthesized representation with that of the input provides a validation mechanism during perceptual inference and learning. Inspired by these ideas, we proposed that the synthesis machinery can compose new, unobserved images by imagination to train the network itself so as to increase the robustness of the system in novel scenarios. As a proof of concept, we investigated whether images composed by imagination could help an object recognition system to deal with occlusion, which is challenging for the current state-of-the-art deep convolutional neural networks. We fine-tuned a network on images containing objects in various occlusion scenarios, that are imagined or self-generated through a deep generator network. Trained on imagined occluded scenarios under the object persistence constraint, our network discovered more subtle and localized image features that were neglected by the original network for object classification, obtaining better separability of different object classes in the feature space. This leads to significant improvement of object recognition under occlusion for our network relative to the original network trained only on un-occluded images. In addition to providing practical benefits in object recognition under occlusion, this work demonstrates the use of self-generated composition of visual scenes through the synthesis loop, combined with the object persistence constraint, can provide opportunities for neural networks to discover new relevant patterns in the data, and become more flexible in dealing with novel situations.

##### Abstract (translated by Google)
假设哺乳动物视觉系统中的反馈反馈连接在合成分析的理论框架中对合成输入起作用。内部综合表示与输入表达的比较为知觉推理和学习提供了验证机制。受到这些思想的启发，我们提出合成机器可以通过想象组合新的，不可观测的图像来训练网络本身，从而增加系统在新情景下的鲁棒性。作为一个概念证明，我们研究了由想象组成的图像是否可以帮助一个对象识别系统来处理遮挡，这对于当前最先进的深度卷积神经网络是具有挑战性的。我们对包含各种遮挡场景中的对象的图像进行了微调，这些遮挡场景是通过深度生成器网络设想或自行生成的。在对象持久约束条件下，我们的网络发现了一些被微观和局部化的图像特征，这些特征被原始网络忽略了，因此在特征空间中获得了更好的不同对象类别的可分离性。这导致相对于仅在未被遮挡的图像上训练的原始网络，在我们的网络的遮挡下物体识别的显着改进。除了在遮挡下的物体识别中提供实际益处之外，本工作还展示了通过合成循环使用自生成的视觉场景组合，结合对象持久性约束，可以为神经网络提供机会以发现新的相关模式数据，并在处理新情况时变得更加灵活。

##### URL
[https://arxiv.org/abs/1705.07594](https://arxiv.org/abs/1705.07594)

##### PDF
[https://arxiv.org/pdf/1705.07594](https://arxiv.org/pdf/1705.07594)

