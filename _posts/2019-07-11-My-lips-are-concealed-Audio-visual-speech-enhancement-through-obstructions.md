---
layout: post
title: "My lips are concealed: Audio-visual speech enhancement through obstructions"
date: 2019-07-11 02:05:48
categories: arXiv_CV
tags: arXiv_CV
author: Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman
mathjax: true
---

* content
{:toc}

##### Abstract
Our objective is an audio-visual model for separating a single speaker from a mixture of sounds such as other speakers and background noise. Moreover, we wish to hear the speaker even when the visual cues are temporarily absent due to occlusion. To this end we introduce a deep audio-visual speech enhancement network that is able to separate a speaker's voice by conditioning on both the speaker's lip movements and/or a representation of their voice. The voice representation can be obtained by either (i) enrollment, or (ii) by self-enrollment -- learning the representation on-the-fly given sufficient unobstructed visual input. The model is trained by blending audios, and by introducing artificial occlusions around the mouth region that prevent the visual modality from dominating. The method is speaker-independent, and we demonstrate it on real examples of speakers unheard (and unseen) during training. The method also improves over previous models in particular for cases of occlusion in the visual modality.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.04975](http://arxiv.org/abs/1907.04975)

##### PDF
[http://arxiv.org/pdf/1907.04975](http://arxiv.org/pdf/1907.04975)

