---
layout: post
title: "To Frontalize or Not To Frontalize: A Study of Face Pre-Processing Techniques and Their Impact on Recognition"
date: 2017-04-21 19:38:13
categories: arXiv_CV
tags: arXiv_CV Face CNN Deep_Learning Recognition Face_Recognition
author: Sandipan Banerjee, Joel Brogan, Janez Krizaj, Aparna Bharati, Brandon RichardWebster, Vitomir Struc, Patrick Flynn, Walter Scheirer
mathjax: true
---

* content
{:toc}

##### Abstract
Face recognition performance has improved remarkably in the last decade. Much of this success can be attributed to the development of deep learning techniques such as convolutional neural networks (CNNs). While CNNs have pushed the state-of-the-art forward, their training process requires a large amount of clean and correctly labelled training data. If a CNN is intended to tolerate facial pose, then we face an important question: should this training data be diverse in its pose distribution, or should face images be normalized to a single pose in a pre-processing step? To address this question, we evaluate a number of popular facial landmarking and pose correction algorithms to understand their effect on facial recognition performance. Additionally, we introduce a new, automatic, single-image frontalization scheme that exceeds the performance of current algorithms. CNNs trained using sets of different pre-processing methods are used to extract features from the Point and Shoot Challenge (PaSC) and CMU Multi-PIE datasets. We assert that the subsequent verification and recognition performance serves to quantify the effectiveness of each pose correction scheme.

##### Abstract (translated by Google)
人脸识别性能在过去的十年中有了显着的提高。这种成功的很大一部分可以归因于卷积神经网络（CNN）等深度学习技术的发展。虽然有线电视新闻网推进了最新的技术发展，但是他们的培训过程需要大量清晰和正确标记的培训数据。如果一个CNN的目的是容忍面部姿态，那么我们就面临一个重要的问题：这个训练数据的姿态分布应该是多样的，还是应该在预处理步骤中将面部图像归一化为单个姿态？为了解决这个问题，我们评估了一些流行的面部标志和姿态校正算法，以了解它们对面部识别性能的影响。另外，我们引入了一种新的，自动的，单一图像的正面化方案，超出了当前算法的性能。使用多组不同的预处理方法训练的CNN被用来从点和射击挑战（PaSC）和CMU多PIE数据集中提取特征。我们断言，随后的验证和识别表现有助于量化每个姿势矫正方案的有效性。

##### URL
[https://arxiv.org/abs/1610.04823](https://arxiv.org/abs/1610.04823)

##### PDF
[https://arxiv.org/pdf/1610.04823](https://arxiv.org/pdf/1610.04823)

