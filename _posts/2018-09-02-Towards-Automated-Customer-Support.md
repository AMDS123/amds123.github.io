---
layout: post
title: "Towards Automated Customer Support"
date: 2018-09-02 06:22:39
categories: arXiv_CL
tags: arXiv_CL QA Attention
author: Momchil Hardalov, Ivan Koychev, Preslav Nakov
mathjax: true
---

* content
{:toc}

##### Abstract
Recent years have seen growing interest in conversational agents, such as chatbots, which are a very good fit for automated customer support because the domain in which they need to operate is narrow. This interest was in part inspired by recent advances in neural machine translation, esp. the rise of sequence-to-sequence (seq2seq) and attention-based models such as the Transformer, which have been applied to various other tasks and have opened new research directions in question answering, chatbots, and conversational systems. Still, in many cases, it might be feasible and even preferable to use simple information retrieval techniques. Thus, here we compare three different models:(i) a retrieval model, (ii) a sequence-to-sequence model with attention, and (iii) Transformer. Our experiments with the Twitter Customer Support Dataset, which contains over two million posts from customer support services of twenty major brands, show that the seq2seq model outperforms the other two in terms of semantics and word overlap.

##### Abstract (translated by Google)
近年来，人们越来越关注会话代理，例如聊天机器人，它非常适合自动化客户支持，因为它们需要运营的领域很窄。这种兴趣部分地受到神经机器翻译的最新进展的启发，特别是。序列到序列（seq2seq）和基于注意力的模型（如变形金刚）的兴起，已经应用于各种其他任务，并在问答室，聊天机器人和会话系统中开辟了新的研究方向。尽管如此，在许多情况下，使用简单的信息检索技术可能是可行的，甚至更可取。因此，在这里我们比较三种不同的模型：（i）检索模型，（ii）具有注意力的序列到序列模型，以及（iii）变压器。我们对Twitter客户支持数据集的实验表明，seq2seq模型在语义和字重叠方面优于其他两个，其中包含来自20个主要品牌的客户支持服务的200多万篇帖子。

##### URL
[http://arxiv.org/abs/1809.00303](http://arxiv.org/abs/1809.00303)

##### PDF
[http://arxiv.org/pdf/1809.00303](http://arxiv.org/pdf/1809.00303)

