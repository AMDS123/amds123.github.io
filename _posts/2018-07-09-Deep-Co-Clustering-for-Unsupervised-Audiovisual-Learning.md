---
layout: post
title: "Deep Co-Clustering for Unsupervised Audiovisual Learning"
date: 2018-07-09 13:13:10
categories: arXiv_CV
tags: arXiv_CV Face CNN Detection
author: Di Hu, Feiping Nie, Xuelong Li
mathjax: true
---

* content
{:toc}

##### Abstract
The seen birds twitter, the running cars accompany with noise, people talks by face-to-face, etc. These naturally audiovisual correspondences provide the possibilities to explore and understand the outside world. However, the mixed multiple objects and sounds make it intractable to perform efficient matching in the unconstrained environment. To settle this problem, we propose to adequately excavate audio and visual components and perform elaborate correspondence learning among them. Concretely, a novel unsupervised audiovisual learning model is proposed, named as Deep Co-Clustering (DCC), that synchronously performs sets of clustering with multimodal vectors of convolutional maps in different shared spaces for capturing multiple audiovisual correspondences. And such integrated multimodal clustering network can be effectively trained with max-margin loss in the end-to-end fashion. Amounts of experiments in feature evaluation and audiovisual tasks are performed. The results demonstrate that DCC can learn effective unimodal representation, with which the classifier can even outperform human. Further, DCC shows noticeable performance in the task of sound localization, multisource detection, and audiovisual understanding.

##### Abstract (translated by Google)
看到的鸟儿叽叽喳喳，跑步车伴随着噪音，人们面对面交谈等等。这些自然的视听通信提供了探索和理解外部世界的可能性。然而，混合的多个对象和声音使得在无约束环境中执行有效匹配变得棘手。为了解决这个问题，我们建议充分挖掘音频和视觉组件，并在它们之间进行精细的通信学习。具体地，提出了一种新的无监督视听学习模型，称为深度协同聚类（DCC），其与不同共享空间中的卷积图的多模态向量同步地执行聚类集合，以捕获多个视听对应。并且这种集成的多模式集群网络可以以端到端的方式进行有效的最大边际损失训练。执行特征评估和视听任务中的实验量。结果表明，DCC可以学习有效的单峰表示，分类器甚至可以胜过人类。此外，DCC在声音定位，多源检测和视听理解的任务中表现出显着的性能。

##### URL
[http://arxiv.org/abs/1807.03094](http://arxiv.org/abs/1807.03094)

##### PDF
[http://arxiv.org/pdf/1807.03094](http://arxiv.org/pdf/1807.03094)

