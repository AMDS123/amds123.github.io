---
layout: post
title: "Revisiting the Hierarchical Multiscale LSTM"
date: 2018-07-10 12:46:30
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: &#xc1;kos K&#xe1;d&#xe1;r, Marc-Alexandre C&#xf4;t&#xe9;, Grzegorz Chrupa&#x142;a, Afra Alishahi
mathjax: true
---

* content
{:toc}

##### Abstract
Hierarchical Multiscale LSTM (Chung et al., 2016a) is a state-of-the-art language model that learns interpretable structure from character-level input. Such models can provide fertile ground for (cognitive) computational linguistics studies. However, the high complexity of the architecture, training procedure and implementations might hinder its applicability. We provide a detailed reproduction and ablation study of the architecture, shedding light on some of the potential caveats of re-purposing complex deep-learning architectures. We further show that simplifying certain aspects of the architecture can in fact improve its performance. We also investigate the linguistic units (segments) learned by various levels of the model, and argue that their quality does not correlate with the overall performance of the model on language modeling.

##### Abstract (translated by Google)
Hierarchical Multiscale LSTM（Chung et al。，2016a）是一种先进的语言模型，它从字符级输入中学习可解释的结构。这些模型可以为（认知）计算语言学研究提供肥沃的基础。但是，架构，培训过程和实现的高度复杂性可能会妨碍其适用性。我们提供了详细的体系复制和消融研究，揭示了复杂深度学习体系结构的一些潜在警告。我们进一步表明，简化架构的某些方面实际上可以改善其性能。我们还研究了不同层次的模型所学习的语言单位（片段），并认为它们的质量与语言建模模型的整体表现无关。

##### URL
[http://arxiv.org/abs/1807.03595](http://arxiv.org/abs/1807.03595)

##### PDF
[http://arxiv.org/pdf/1807.03595](http://arxiv.org/pdf/1807.03595)

