---
layout: post
title: "Contrastive Multiview Coding"
date: 2019-06-13 17:49:20
categories: arXiv_CV
tags: arXiv_CV Classification
author: Yonglong Tian, Dilip Krishnan, Phillip Isola
mathjax: true
---

* content
{:toc}

##### Abstract
Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, viewed by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ``dog" can be seen, heard, and felt). We hypothesize that a powerful representation is one that models view-invariant factors. Based on this hypothesis, we investigate a contrastive coding scheme, in which a representation is learned that aims to maximize mutual information between different views but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. The resulting learned representations perform above the state of the art for downstream tasks such as object classification, compared to formulations based on predictive learning or single view reconstruction, and improve as more views are added. Code and reference implementations are released on our project page: this http URL.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1906.05849](https://arxiv.org/abs/1906.05849)

##### PDF
[https://arxiv.org/pdf/1906.05849](https://arxiv.org/pdf/1906.05849)

