---
layout: post
title: 'Exploiting Cross-Sentence Context for Neural Machine Translation'
date: 2017-07-23 05:19:42
categories: arXiv_CL
tags: arXiv_CL NMT
author: Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu
---

* content
{:toc}

##### Abstract
In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.

##### Abstract (translated by Google)
在翻译过程中，将整个文档视为一个整体可以帮助解决歧义和不一致之处。在本文中，我们提出了一个交叉句子的情境感知方法，并调查历史背景信息对神经机器翻译（NMT）的性能的影响。首先，这个历史是以等级的方式总结的。然后，我们用两种策略将历史表示集成到NMT中：1）编码器和解码器状态的热启动，以及2）用于更新解码器状态的辅助上下文源。一个大型的中英文翻译任务的实验结果表明，我们的方法显着提高了强烈的基于注意力的NMT系统高达+2.1 BLEU点。

##### URL
[https://arxiv.org/abs/1704.04347](https://arxiv.org/abs/1704.04347)

