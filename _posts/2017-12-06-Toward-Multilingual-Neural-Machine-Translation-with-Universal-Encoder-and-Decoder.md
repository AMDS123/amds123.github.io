---
layout: post
title: 'Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder'
date: 2017-12-06 09:30:05
categories: arXiv_CL
tags: arXiv_CL NMT
author: Thanh-Le Ha, Jan Niehues, Alexander Waibel
---

* content
{:toc}

##### Abstract
In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.

##### Abstract (translated by Google)
在本文中，我们首先尝试在统一的方法下构建一个多语言的神经机器翻译框架。那么，我们就可以把注意力为基础的NMT用于多对多的多语翻译任务。我们的方法不需要对网络体系结构进行任何特殊的处理，它使我们能够以标准的培训方式学习最少量的免费参数。我们的方法已经在资源不足的翻译方案中显示出其有效性，相当大的提高了2.6 BLEU点。此外，该方法在翻译任务中得到了有意义和有希望的结果，即在源语言和目标语言之间没有直接的平行语料库。

##### URL
[https://arxiv.org/abs/1611.04798](https://arxiv.org/abs/1611.04798)

