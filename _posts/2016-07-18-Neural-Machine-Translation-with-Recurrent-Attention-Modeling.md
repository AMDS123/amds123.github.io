---
layout: post
title: "Neural Machine Translation with Recurrent Attention Modeling"
date: 2016-07-18 14:44:26
categories: arXiv_CL
tags: arXiv_CL Attention Relation
author: Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, Alex Smola
mathjax: true
---

* content
{:toc}

##### Abstract
Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.

##### Abstract (translated by Google)
在产生翻译的同时，了解在以前的时间步骤中关注了哪些词汇，是预测未来将要处理的词汇的丰富信息来源。我们改进了Bahdanau等人的注意模式。 （2014）通过使用每个输入词的一个循环网络明确地建模每个词的先前和随后的关注水平之间的关系。这种架构很容易捕捉到丰富的特征，例如生育率和相对扭曲的规律性。在实验中，我们展示了我们关注的参数化提高了翻译质量。

##### URL
[https://arxiv.org/abs/1607.05108](https://arxiv.org/abs/1607.05108)

##### PDF
[https://arxiv.org/pdf/1607.05108](https://arxiv.org/pdf/1607.05108)

