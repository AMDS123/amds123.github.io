---
layout: post
title: "The physical structure of grammatical correlations: equivalences, formalizations and consequences"
date: 2017-08-25 16:04:57
categories: arXiv_CL
tags: arXiv_CL Language_Model Relation
author: Angel J. Gallego, Roman Orus
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we consider some well-known facts in syntax from a physics perspective, which allows us to establish some remarkable equivalences. Specifically, we observe that the operation MERGE put forward by N. Chomsky in 1995 can be interpreted as a physical information coarse-graining. Thus, MERGE in linguistics entails information renormalization in physics, according to different time scales. We make this point mathematically formal in terms of language models, i.e., probability distributions over word sequences, widely used in natural language processing as well as other ambits. In this setting, MERGE corresponds to a 3-index probability tensor implementing a coarse-graining, akin to a probabilistic context-free grammar. The probability vectors of meaningful sentences are naturally given by tensor networks (TN) that are mostly loop-free, such as Tree Tensor Networks and Matrix Product States. These structures have short-ranged correlations in the syntactic distance by construction and, because of the peculiarities of human language, they are extremely efficient to manipulate computationally. We also propose how to obtain such language models from probability distributions of certain TN quantum states, which we show to be efficiently preparable by a quantum computer. Moreover, using tools from quantum information and entanglement theory, we use these quantum states to prove classical lower bounds on the perplexity of the probability distribution for a set of words in a sentence. Implications of these results are discussed in the ambits of theoretical and computational linguistics, artificial intelligence, programming languages, RNA and protein sequencing, quantum many-body systems, and beyond.

##### Abstract (translated by Google)
在本文中，我们从物理角度考虑一些众所周知的语法事实，这使得我们可以建立一些显着的等价性。具体来说，我们观察到，乔姆斯基在1995年提出的MERGE操作可以被解释为一种物理信息粗粒化。因此，语言学中的MERGE根据不同的时间尺度，需要物理学中的信息重整化。在语言模型方面，我们用数学形式来表达这个观点，即在单词序列上的概率分布，广泛用于自然语言处理以及其他领域。在这种情况下，MERGE对应于实现粗粒化的3索引概率张量，类似于概率上下文无关文法。有意义句子的概率向量自然是由大多数无环的张量网络（TN）给出的，如树张量网络和矩阵乘积状态。这些结构在构造的句法距离上具有短距离的相关性，并且由于人类语言的特殊性，它们在计算上操作是非常有效的。我们还提出如何从某些TN量子态的概率分布中获得这样的语言模型，我们证明这种模型可以通过量子计算机有效地制备。此外，利用量子信息和纠缠理论中的工具，我们用这些量子态来证明一个句子中一组词的概率分布的困惑性的经典下界。这些结果的意义在理论和计算语言学，人工智能，程序设计语言，RNA和蛋白质测序，量子多体系统等领域进行了讨论。

##### URL
[https://arxiv.org/abs/1708.01525](https://arxiv.org/abs/1708.01525)

##### PDF
[https://arxiv.org/pdf/1708.01525](https://arxiv.org/pdf/1708.01525)

