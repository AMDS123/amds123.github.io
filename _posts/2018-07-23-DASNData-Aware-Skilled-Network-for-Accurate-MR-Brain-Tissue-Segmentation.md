---
layout: post
title: "DASN:Data-Aware Skilled Network for Accurate MR Brain Tissue Segmentation"
date: 2018-07-23 08:19:19
categories: arXiv_CV
tags: arXiv_CV Segmentation CNN Deep_Learning Recognition
author: Yang Deng, Yao Sun, Yongpei Zhu, Shuo Zhang, Mingwang Zhu, Kehong Yuan
mathjax: true
---

* content
{:toc}

##### Abstract
Accurate segmentation of MR brain tissue is a crucial step for diagnosis, surgical planning, and treatment of brain abnormalities. Automatic and reliable segmenta-tion methods are required to assist doctor. Over the last few years, deep learning especially deep convolutional neural networks (CNNs) have emerged as one of the most prominent approaches for image recognition problems in various do-mains. But the improvement of deep networks always needs inspiration, which is rare for the ordinary. Until now,there have been reasonable MR brain tissue segmentation methods,all of which can achieve promising performance. These different methods have their own characteristic and are distinctive for data sets. In other words, different models performance vary widely on the same data sets and each model has what it is skilled in. It is on the basis of this, we propose a judgement to distinguish data sets that different models are good at. With our method, the segmentation accuracy can be improved easily based on the existing models, neither without increasing training data nor improving the network. We validate our method on the widely used IBSR 18 dataset and obtain average dice ratio of 88.06%,while it is 85.82% and 86.92% when only using separate one model respectively.

##### Abstract (translated by Google)
MR脑组织的准确分割是诊断，手术计划和脑异常治疗的关键步骤。需要自动可靠的分段方法来协助医生。在过去几年中，深度学习尤其是深度卷积神经网络（CNN）已成为各种信号中图像识别问题最突出的方法之一。但深度网络的改进总是需要灵感，这对普通人来说是罕见的。到目前为止，已经有了合理的MR脑组织分割方法，所有这些方法都可以取得很好的效果。这些不同的方法具有各自的特征，并且与数据集不同。换句话说，不同模型的性能在相同的数据集上变化很大，每个模型都具有熟练的功能。正是在此基础上，我们提出了一种区分不同模型擅长的数据集的判断。利用我们的方法，可以在现有模型的基础上轻松提高分割精度，既不增加训练数据也不改善网络。我们在广泛使用的IBSR 18数据集上验证了我们的方法，并且获得了平均骰子比率为88.06％，而当仅使用单独的一个模型时，它们分别为85.82％和86.92％。

##### URL
[http://arxiv.org/abs/1807.08473](http://arxiv.org/abs/1807.08473)

##### PDF
[http://arxiv.org/pdf/1807.08473](http://arxiv.org/pdf/1807.08473)

