---
layout: post
title: "$EVA^2$ : Exploiting Temporal Redundancy in Live Computer Vision"
date: 2018-03-16 16:59:47
categories: arXiv_CV
tags: arXiv_CV CNN
author: Mark Buckler, Philip Bedoukian, Suren Jayasuriya, Adrian Sampson
mathjax: true
---

* content
{:toc}

##### Abstract
Hardware support for deep convolutional neural networks (CNNs) is critical to advanced computer vision in mobile and embedded devices. Current designs, however, accelerate generic CNNs; they do not exploit the unique characteristics of real-time vision. We propose to use the temporal redundancy in natural video to avoid unnecessary computation on most frames. A new algorithm, activation motion compensation, detects changes in the visual input and incrementally updates a previously-computed output. The technique takes inspiration from video compression and applies well-known motion estimation techniques to adapt to visual changes. We use an adaptive key frame rate to control the trade-off between efficiency and vision quality as the input changes. We implement the technique in hardware as an extension to existing state-of-the-art CNN accelerator designs. The new unit reduces the average energy per frame by 54.2%, 61.7%, and 87.6% for three CNNs with less than 1% loss in vision accuracy.

##### Abstract (translated by Google)
深度卷积神经网络（CNN）的硬件支持对于移动和嵌入式设备中高级计算机视觉至关重要。然而，目前的设计加速了通用CNNs;他们没有利用实时视觉的独特特征。我们建议在自然视频中使用时间冗余，以避免在大多数帧上进行不必要的计算。一种新的算法 - 激活运动补偿 - 检测视觉输入中的变化并递增地更新先前计算的输出。该技术从视频压缩中获得灵感，并应用众所周知的运动估计技术来适应视觉变化。随着输入变化，我们使用自适应关键帧速率来控制效率和视觉质量之间的平衡。我们将硬件技术作为现有最先进的CNN加速器设计的延伸来实施。三台CNN的视频精度下降不到1％，新设备的平均能耗分别降低了54.2％，61.7％和87.6％。

##### URL
[https://arxiv.org/abs/1803.06312](https://arxiv.org/abs/1803.06312)

##### PDF
[https://arxiv.org/pdf/1803.06312](https://arxiv.org/pdf/1803.06312)

