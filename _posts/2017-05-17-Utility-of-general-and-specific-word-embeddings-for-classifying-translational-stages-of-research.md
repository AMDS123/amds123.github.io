---
layout: post
title: "Utility of general and specific word embeddings for classifying translational stages of research"
date: 2017-05-17 17:08:11
categories: arXiv_CL
tags: arXiv_CL Text_Classification Embedding Classification Language_Model
author: Vincent Major, Alisa Surkis, Yindalon Aphinyanaphongs
mathjax: true
---

* content
{:toc}

##### Abstract
Conventional text classification models make a bag-of-words assumption reducing text, fundamentally a sequence of words, into word occurrence counts per document. Recent algorithms such as word2vec and fastText are capable of learning semantic meaning and similarity between words in an entirely unsupervised manner using a contextual window and doing so much faster than previous methods. Each word is represented as a vector such that similar meaning words such as 'strong' and 'powerful' are in the same general Euclidian space. Open questions about these embeddings include their usefulness across classification tasks and the optimal set of documents to build the embeddings. In this work, we demonstrate the usefulness of embeddings for improving the state of the art in classification for our tasks and demonstrate that specific word embeddings built in the domain and for the tasks can improve performance over general word embeddings (learnt on news articles, Wikipedia or PubMed).

##### Abstract (translated by Google)
传统的文本分类模型使得文字假设将文本（基本上是一系列的文字）减少为每个文档的文字出现次数。最近的算法如word2vec和fastText能够使用上下文窗口以完全无监督的方式学习单词之间的语义和相似性，并且比以前的方法快得多。每个单词被表示为一个向量，使得类似的意义词，如“强”和“强”在同一个一般的欧几里德空间。有关这些嵌入的开放问题包括它们在分类任务中的有用性以及构建嵌入的最佳文档集合。在这项工作中，我们展示了嵌入的有用性，以改善我们的任务分类的艺术水平，并证明特定的字嵌入建立在领域和任务可以提高性能超过一般的词嵌入（在新闻文章上学习，维基百科或PubMed）。

##### URL
[https://arxiv.org/abs/1705.06262](https://arxiv.org/abs/1705.06262)

##### PDF
[https://arxiv.org/pdf/1705.06262](https://arxiv.org/pdf/1705.06262)

