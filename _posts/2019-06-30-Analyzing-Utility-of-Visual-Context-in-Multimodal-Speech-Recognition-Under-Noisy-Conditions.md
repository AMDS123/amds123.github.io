---
layout: post
title: "Analyzing Utility of Visual Context in Multimodal Speech Recognition Under Noisy Conditions"
date: 2019-06-30 21:49:07
categories: arXiv_CL
tags: arXiv_CL Adversarial Speech_Recognition Inference Recognition
author: Tejas Srinivasan, Ramon Sanabria, Florian Metze
mathjax: true
---

* content
{:toc}

##### Abstract
Multimodal learning allows us to leverage information from multiple sources (visual, acoustic and text), similar to our experience of the real world. However, it is currently unclear to what extent auxiliary modalities improve performance over unimodal models, and under what circumstances the auxiliary modalities are useful. We examine the utility of the auxiliary visual context in Multimodal Automatic Speech Recognition in adversarial settings, where we deprive the models from partial audio signal during inference time. Our experiments show that while MMASR models show significant gains over traditional speech-to-text architectures (upto 4.2% WER improvements), they do not incorporate visual information when the audio signal has been corrupted. This shows that current methods of integrating the visual modality do not improve model robustness to noise, and we need better visually grounded adaptation techniques.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.00477](http://arxiv.org/abs/1907.00477)

##### PDF
[http://arxiv.org/pdf/1907.00477](http://arxiv.org/pdf/1907.00477)

