---
layout: post
title: "Video Description: A Survey of Methods, Datasets and Evaluation Metrics"
date: 2018-06-01 04:31:58
categories: arXiv_CV
tags: arXiv_CV Video_Indexing Survey Deep_Learning Language_Model
author: Nayyer Aafaq, Syed Zulqarnain Gilani, Wei Liu, Ajmal Mian
mathjax: true
---

* content
{:toc}

##### Abstract
Automatic video description is useful for assisting the visually impaired, human computer interaction, robotics and video indexing. The past few years have seen a surge of research interest in this area due to the unprecedented success of deep learning in computer vision and natural language processing. Numerous methods, datasets and evaluation measures have been proposed in the literature calling the need for a comprehensive survey to better focus research efforts in this flourishing direction. This paper answers exactly to this need by surveying state of the art approaches including deep learning models; comparing benchmark datasets in terms of their domain, number of classes, and repository size; and identifying the pros and cons of various evaluation metrics such as BLEU, ROUGE, METEOR, CIDEr, SPICE and WMD. Our survey shows that video description research has a long way to go before it can match human performance and that the main reasons for this shortfall are twofold. Firstly, existing datasets do not adequately represent the diversity in open domain videos and complex linguistic structures. Secondly, current measures of evaluation are not aligned with human judgement. For example, the same video can have very different, yet correct descriptions. We conclude that there is a need for improvement in evaluation measures as well as datasets in terms of size, diversity and annotation accuracy because they directly influence the development of better video description models. From an algorithmic point of view, diagnosis of the description quality is challenging because of the difficultly to assess the level of contribution from visual features compared to the bias that comes naturally from the language model adopted.

##### Abstract (translated by Google)
自动视频描述对于帮助视障人士，人机交互，机器人和视频索引非常有用。由于计算机视觉和自然语言处理方面的深度学习取得了前所未有的成功，过去几年来，在这方面的研究兴趣激增。在文献中已经提出了许多方法，数据集和评估方法，这些方法称需要进行全面的调查以更好地将研究工作集中在这个繁荣的方向上。本文通过调查包括深度学习模型在内的最先进的方法来准确回答这种需求;比较基准数据集的域名，类别数量和存储库大小;并确定诸如BLEU，ROUGE，METEOR，CIDEr，SPICE和WMD等各种评估指标的优缺点。我们的调查显示，视频描述研究在匹配人类表现之前还有很长的路要走，而造成这种不足的主要原因是双重的。首先，现有的数据集不足以代表开放域视频和复杂语言结构的多样性。其次，目前的评估措施不符合人的判断。例如，同一个视频可能会有非常不同但正确的描述。我们得出结论认为，需要改进评估措施以及数据集的大小，多样性和注释准确性，因为它们直接影响着更好的视频描述模型的发展。从算法的角度来看，描述质量的诊断是具有挑战性的，因为难以评估视觉特征的贡献水平与采用的语言模型自然产生的偏差之间的差异。

##### URL
[http://arxiv.org/abs/1806.00186](http://arxiv.org/abs/1806.00186)

##### PDF
[http://arxiv.org/pdf/1806.00186](http://arxiv.org/pdf/1806.00186)

