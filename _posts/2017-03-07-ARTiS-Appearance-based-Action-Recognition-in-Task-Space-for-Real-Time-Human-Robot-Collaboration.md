---
layout: post
title: "ARTiS: Appearance-based Action Recognition in Task Space for Real-Time Human-Robot Collaboration"
date: 2017-03-07 03:58:38
categories: arXiv_CV
tags: arXiv_CV Knowledge Action_Recognition Recognition
author: Markus Eich, Sareh Shirazi, Gordon Wyeth
mathjax: true
---

* content
{:toc}

##### Abstract
To have a robot actively supporting a human during a collaborative task, it is crucial that robots are able to identify the current action in order to predict the next one. Common approaches make use of high-level knowledge, such as object affordances, semantics or understanding of actions in terms of pre- and post-conditions. These approaches often require hand-coded a priori knowledge, time- and resource-intensive or supervised learning techniques. We propose to reframe this problem as an appearance-based place recognition problem. In our framework, we regard sequences of visual images of human actions as a map in analogy to the visual place recognition problem. Observing the task for the second time, our approach is able to recognize pre-observed actions in a one-shot learning approach and is thereby able to recognize the current observation in the task space. We propose two new methods for creating and aligning action observations within a task map. We compare and verify our approaches with real data of humans assembling several types of IKEA flat packs.

##### Abstract (translated by Google)
为了让机器人在协作任务中积极支持人类，机器人能够识别当前动作以预测下一个动作是至关重要的。常见的方法是利用高层次的知识，如对象的可供性，语义或者对前后条件的理解。这些方法往往需要手工编码的先验知识，时间和资源密集型或监督学习技术。我们建议把这个问题重新定义为一个基于外观的地方识别问题。在我们的框架中，我们把人类行为的视觉图像序列看作是类似于视觉地点识别问题的地图。我们的方法第二次观察任务，能够识别单步学习方法中预先观察到的动作，从而能够识别任务空间中的当前观察。我们提出了两种新的方法来创建和对齐任务图中的动作观察。我们比较和验证我们的方法与真人数据组装宜家扁平包装的几种类型。

##### URL
[https://arxiv.org/abs/1610.05432](https://arxiv.org/abs/1610.05432)

##### PDF
[https://arxiv.org/pdf/1610.05432](https://arxiv.org/pdf/1610.05432)

