---
layout: post
title: "Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy"
date: 2019-07-24 05:46:27
categories: arXiv_AI
tags: arXiv_AI Sparse Reinforcement_Learning
author: Yijie Guo, Jongwook Choi, Marcin Moczulski, Samy Bengio, Mohammad Norouzi, Honglak Lee
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a method for learning a trajectory-conditioned policy to imitate diverse demonstrations from the agent's own past experiences. We demonstrate that such self-imitation drives exploration in diverse directions and increases the chance of finding a globally optimal solution in reinforcement learning problems, especially when the reward is sparse and deceptive. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various sparse-reward reinforcement learning tasks with local optima. In particular, we report a state-of-the-art score of more than 25,000 points on Montezuma's Revenge without using expert demonstrations or resetting to arbitrary states.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.10247](http://arxiv.org/abs/1907.10247)

##### PDF
[http://arxiv.org/pdf/1907.10247](http://arxiv.org/pdf/1907.10247)

