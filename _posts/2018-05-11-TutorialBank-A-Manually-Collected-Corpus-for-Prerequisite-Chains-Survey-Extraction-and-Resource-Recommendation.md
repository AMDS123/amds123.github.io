---
layout: post
title: "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation"
date: 2018-05-11 23:13:34
categories: arXiv_CL
tags: arXiv_CL Survey Relation Recommendation
author: Alexander R. Fabbri, Irene Li, Prawat Trairatvorakul, Yijiao He, Wei Tai Ting, Robert Tung, Caitlin Westerfield, Dragomir R. Radev
mathjax: true
---

* content
{:toc}

##### Abstract
The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce TutorialBank, a new, publicly available dataset which aims to facilitate NLP education and research. We have manually collected and categorized over 6,300 resources on NLP as well as the related fields of Artificial Intelligence (AI), Machine Learning (ML) and Information Retrieval (IR). Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research.

##### Abstract (translated by Google)
自然语言处理（NLP）领域正在迅速发展，每天发布新的研究成果，并提供大量的教程，代码库和其他在线资源。为了学习这个充满活力的领域或了解最新研究的最新情况，学生以及教育工作者和研究人员必须不断筛选多个来源以找到有价值的相关信息。为了解决这种情况，我们引入了TutorialBank，这是一个新的公开可用数据集，旨在促进NLP教育和研究。我们已经在NLP以及人工智能（AI），机器学习（ML）和信息检索（IR）等相关领域手工收集并分类了6,300多种资源。我们的数据集尤其是最大的人工挑选NLP教育资源语料库，不包括学术论文。此外，我们为这些资源创建了一个搜索引擎和一个命令行工具，并且注释了语料库以包括研究主题列表，每个主题的相关资源，主题之间的先决关系，个别资源的相关子部分，以及其他注释。我们正在发布数据集并提供进一步研究的几种途径。

##### URL
[https://arxiv.org/abs/1805.04617](https://arxiv.org/abs/1805.04617)

##### PDF
[https://arxiv.org/pdf/1805.04617](https://arxiv.org/pdf/1805.04617)

