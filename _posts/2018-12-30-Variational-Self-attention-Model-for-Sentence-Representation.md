---
layout: post
title: "Variational Self-attention Model for Sentence Representation"
date: 2018-12-30 15:36:11
categories: arXiv_CL
tags: arXiv_CL Attention Inference Detection
author: Qiang Zhang, Shangsong Liang, Emine Yilmaz
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a variational self-attention model (VSAM) that employs variational inference to derive self-attention. We model the self-attention vector as random variables by imposing a probabilistic distribution. The self-attention mechanism summarizes source information as an attention vector by weighted sum, where the weights are a learned probabilistic distribution. Compared with conventional deterministic counterpart, the stochastic units incorporated by VSAM allow multi-modal attention distributions. Furthermore, by marginalizing over the latent variables, VSAM is more robust against overfitting. Experiments on the stance detection task demonstrate the superiority of our method.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.11559](http://arxiv.org/abs/1812.11559)

##### PDF
[http://arxiv.org/pdf/1812.11559](http://arxiv.org/pdf/1812.11559)

