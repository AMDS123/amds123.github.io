---
layout: post
title: "Distilling Knowledge Using Parallel Data for Far-field Speech Recognition"
date: 2018-02-20 02:51:34
categories: arXiv_CL
tags: arXiv_CL Knowledge Speech_Recognition Recognition
author: Jiangyan Yi, Jianhua Tao, Zhengqi Wen, Bin Liu
mathjax: true
---

* content
{:toc}

##### Abstract
In order to improve the performance for far-field speech recognition, this paper proposes to distill knowledge from the close-talking model to the far-field model using parallel data. The close-talking model is called the teacher model. The far-field model is called the student model. The student model is trained to imitate the output distributions of the teacher model. This constraint can be realized by minimizing the Kullback-Leibler (KL) divergence between the output distribution of the student model and the teacher model. Experimental results on AMI corpus show that the best student model achieves up to 4.7% absolute word error rate (WER) reduction when compared with the conventionally-trained baseline models.

##### Abstract (translated by Google)
为了提高远场语音识别的性能，本文提出使用并行数据将知识从近距离通话模型提取到远场模型。专心模式被称为教师模式。远场模型被称为学生模型。训练学生模型以模仿教师模型的输出分布。这种约束可以通过最小化学生模型的输出分布和教师模型之间的Kullback-Leibler（KL）分歧来实现。 AMI语料库的实验结果表明，与传统训练的基线模型相比，最佳学生模型的绝对误词率（WER）降低了4.7％。

##### URL
[http://arxiv.org/abs/1802.06941](http://arxiv.org/abs/1802.06941)

##### PDF
[http://arxiv.org/pdf/1802.06941](http://arxiv.org/pdf/1802.06941)

