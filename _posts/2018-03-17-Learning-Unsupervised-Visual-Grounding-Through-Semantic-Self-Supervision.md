---
layout: post
title: "Learning Unsupervised Visual Grounding Through Semantic Self-Supervision"
date: 2018-03-17 13:46:59
categories: arXiv_CV
tags: arXiv_CV Quantitative
author: Syed Ashar Javed, Shreyas Saxena, Vineet Gandhi
mathjax: true
---

* content
{:toc}

##### Abstract
Localizing natural language phrases in images is a challenging problem that requires joint understanding of both the textual and visual modalities. In the unsupervised setting, lack of supervisory signals exacerbate this difficulty. In this paper, we propose a novel framework for unsupervised visual grounding which uses concept learning as a proxy task to obtain self-supervision. The simple intuition behind this idea is to encourage the model to localize to regions which can explain some semantic property in the data, in our case, the property being the presence of a concept in a set of images. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our approach and show a 5.6% improvement over the current state of the art on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and comparable to state-of-art performance on the Flickr30k dataset.

##### Abstract (translated by Google)
在图像中本地化自然语言短语是一个具有挑战性的问题，需要对文本和视觉形式进行共同理解。在无人监督的情况下，缺乏监督信号会加剧这一困难。在这篇论文中，我们提出了一个无监督视觉接地的新框架，它使用概念学习作为获取自我监督的代理任务。这个想法背后的简单直觉就是鼓励模型定位到可以解释数据中某些语义属性的区域，在我们的例子中，属性是一组图像中概念的存在。我们提供了彻底的定量和定性实验来证明我们的方法的有效性，并且显示比Visual Genome数据集上当前的艺术水平提高了5.6％，ReferItGame数据集提高了5.8％，并且与目前的艺术表现相比Flickr30k数据集。

##### URL
[https://arxiv.org/abs/1803.06506](https://arxiv.org/abs/1803.06506)

##### PDF
[https://arxiv.org/pdf/1803.06506](https://arxiv.org/pdf/1803.06506)

