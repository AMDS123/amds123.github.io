---
layout: post
title: "A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition"
date: 2018-05-30 03:25:14
categories: arXiv_CV
tags: arXiv_CV Action_Recognition CNN Relation Recognition
author: Thao Le Minh, Nakamasa Inoue, Koichi Shinoda
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a new framework for human action recognition from 3D skeleton sequences. Previous studies do not fully utilize the temporal relationships between video segments in a human action. Some studies successfully used very deep Convolutional Neural Network (CNN) models but often suffer from the data insufficiency problem. In this study, we first segment a skeleton sequence into distinct temporal segments in order to exploit the correlations between them. The temporal and spatial features of skeleton sequences are then extracted simultaneously by utilizing a fine-to-coarse (F2C) CNN architecture optimized for human skeleton sequences. We evaluate our proposed method on NTU RGB+D and SBU Kinect Interaction dataset. It achieves 79.6% and 84.6% of accuracies on NTU RGB+D with cross-object and cross-view protocol, respectively, which are almost identical with the state-of-the-art performance. In addition, our method significantly improves the accuracy of the actions in two-person interactions.

##### Abstract (translated by Google)
本文提出了一种新的3D骨架序列人类行为识别框架。以前的研究没有充分利用人类行为中视频片段之间的时间关系。一些研究成功地使用了非常深的卷积神经网络（CNN）模型，但经常遭受数据不足问题的困扰。在这项研究中，我们首先将骨架序列分割成不同的时间段，以利用它们之间的相关性。然后通过利用针对人骨架序列优化的精细 - 粗糙（F2C）CNN架构同时提取骨架​​序列的时间和空间特征。我们评估了我们在NTU RGB + D和SBU Kinect Interaction数据集上提出的方法。它在NTU RGB + D上的准确率分别达到了79.6％和84.6％，其交叉对象和交叉视图协议分别与最新的性能几乎相同。此外，我们的方法显着提高了双人互动中的动作的准确性。

##### URL
[https://arxiv.org/abs/1805.11790](https://arxiv.org/abs/1805.11790)

##### PDF
[https://arxiv.org/pdf/1805.11790](https://arxiv.org/pdf/1805.11790)

