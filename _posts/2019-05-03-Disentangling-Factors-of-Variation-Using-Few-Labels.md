---
layout: post
title: "Disentangling Factors of Variation Using Few Labels"
date: 2019-05-03 16:23:49
categories: arXiv_AI
tags: arXiv_AI Represenation_Learning
author: Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar R&#xe4;tsch, Bernhard Sch&#xf6;lkopf, Olivier Bachem
mathjax: true
---

* content
{:toc}

##### Abstract
Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a very limited amount of supervision, for example through manual labeling of training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 29000 models under well-defined and reproducible experimental conditions. We first observe that a very limited number of labeled examples (0.01--0.5% of the data set) is sufficient to perform model selection on state-of-the-art unsupervised models. Yet, if one has access to labels for supervised model selection, this raises the natural question of whether they should also be incorporated into the training process. As a case-study, we test the benefit of introducing (very limited) supervision into existing state-of-the-art unsupervised disentanglement methods exploiting both the values of the labels and the ordinal information that can be deduced from them. Overall, we empirically validate that with very little and potentially imprecise supervision it is possible to reliably learn disentangled representations.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.01258](http://arxiv.org/abs/1905.01258)

##### PDF
[http://arxiv.org/pdf/1905.01258](http://arxiv.org/pdf/1905.01258)

