---
layout: post
title: "Recurrent Neural Network Training with Dark Knowledge Transfer"
date: 2016-05-08 12:40:35
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention Speech_Recognition Transfer_Learning RNN Prediction Recognition
author: Zhiyuan Tang, Dong Wang, Zhiyong Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data.

##### Abstract (translated by Google)
递归神经网络（RNN），尤其是长时间短时记忆（LSTM），在自动语音识别（ASR）中得到了广泛的关注。尽管已经有一些成功的案例被报道过，但是训练RNN依然非常具有挑战性，尤其是在训练数据有限的情况下。最近的研究发现，一个训练有素的模型可以作为一个教师来训练其他的孩子模型，通过使用教师模型产生的预测作为监督。这种知识转移学习已经被用来训练一个复杂的简单的神经网络，以便最终的表现可以达到一个不可能通过定期训练获得的水平。在本文中，我们采用知识转移学习的方法来训练使用深度神经网络（DNN）模型作为教师的RNN（精确的LSTM）。这与现有的大部分关于知识转移学习的研究不同，因为教师（DNN）被认为比孩子（RNN）弱。然而，我们在ASR任务上的实验表明，它的工作相当好：即使在有限的训练数据的情况下，这种方法也不能在学习方案上应用任何技巧。

##### URL
[https://arxiv.org/abs/1505.04630](https://arxiv.org/abs/1505.04630)

##### PDF
[https://arxiv.org/pdf/1505.04630](https://arxiv.org/pdf/1505.04630)

