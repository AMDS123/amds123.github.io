---
layout: post
title: "Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks"
date: 2017-08-16 14:06:34
categories: arXiv_CL
tags: arXiv_CL OCR Embedding Optimization RNN Detection Recommendation Recognition
author: Nils Reimers, Iryna Gurevych
mathjax: true
---

* content
{:toc}

##### Abstract
Selecting optimal parameters for a neural network architecture can often make the difference between mediocre and state-of-the-art performance. However, little is published which parameters and design choices should be evaluated or selected making the correct hyperparameter optimization often a "black art that requires expert experiences" (Snoek et al., 2012). In this paper, we evaluate the importance of different network design choices and hyperparameters for five common linguistic sequence tagging tasks (POS, Chunking, NER, Entity Recognition, and Event Detection). We evaluated over 50.000 different setups and found, that some parameters, like the pre-trained word embeddings or the last layer of the network, have a large impact on the performance, while other parameters, for example the number of LSTM layers or the number of recurrent units, are of minor importance. We give a recommendation on a configuration that performs well among different tasks.

##### Abstract (translated by Google)
为神经网络体系结构选择最佳参数往往可以使普通和最先进的性能有所不同。然而，很少公布哪些参数和设计选择应该被评估或选择，使得正确的超参数优化通常是“需要专家经验的黑色艺术”（Snoek等人，2012）。在本文中，我们评估了五种常见的语言序列标记任务（POS，分块，NER，实体识别和事件检测）的不同网络设计选择和超参数的重要性。我们对超过50,000个不同的设置进行了评估，发现一些参数（如预先训练的词嵌入或网络的最后一层）对性能影响很大，而其他参数（例如LSTM层的数量或数量的经常性单位，是不重要的。我们给出了一个在不同任务中表现良好的配置建议。

##### URL
[https://arxiv.org/abs/1707.06799](https://arxiv.org/abs/1707.06799)

##### PDF
[https://arxiv.org/pdf/1707.06799](https://arxiv.org/pdf/1707.06799)

