---
layout: post
title: "Nature Inspired Dimensional Reduction Technique for Fast and Invariant Visual Feature Extraction"
date: 2019-07-01 16:58:45
categories: arXiv_CV
tags: arXiv_CV Salient Weakly_Supervised Classification
author: Ravimal Bandara, Lochandaka Ranathunga, Nor Aniza Abdullah
mathjax: true
---

* content
{:toc}

##### Abstract
Fast and invariant feature extraction is crucial in certain computer vision applications where the computation time is constrained in both training and testing phases of the classifier. In this paper, we propose a nature-inspired dimensionality reduction technique for fast and invariant visual feature extraction. The human brain can exchange the spatial and spectral resolution to reconstruct missing colors in visual perception. The phenomenon is widely used in the printing industry to reduce the number of colors used to print, through a technique, called color dithering. In this work, we adopt a fast error-diffusion color dithering algorithm to reduce the spectral resolution and extract salient features by employing novel Hessian matrix analysis technique, which is then described by a spatial-chromatic histogram. The computation time, descriptor dimensionality and classification performance of the proposed feature are assessed under drastic variances in orientation, viewing angle and illumination of objects comparing with several different state-of-the-art handcrafted and deep-learned features. Extensive experiments on two publicly available object datasets, coil-100 and ALOI carried on both a desktop PC and a Raspberry Pi device show multiple advantages of using the proposed approach, such as the lower computation time, high robustness, and comparable classification accuracy under weakly supervised environment. Further, it showed the capability of operating solely inside a conventional SoC device utilizing a small fraction of the available hardware resources.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.01102](http://arxiv.org/abs/1907.01102)

##### PDF
[http://arxiv.org/pdf/1907.01102](http://arxiv.org/pdf/1907.01102)

