---
layout: post
title: "Improving Transferability of Deep Neural Networks"
date: 2018-07-30 17:34:24
categories: arXiv_CV
tags: arXiv_CV Image_Classification Transfer_Learning Classification Deep_Learning Recognition
author: Parijat Dube, Bishwaranjan Bhattacharjee, Elisabeth Petit-Bois, Matthew Hill
mathjax: true
---

* content
{:toc}

##### Abstract
Learning from small amounts of labeled data is a challenge in the area of deep learning. This is currently addressed by Transfer Learning where one learns the small data set as a transfer task from a larger source dataset. Transfer Learning can deliver higher accuracy if the hyperparameters and source dataset are chosen well. One of the important parameters is the learning rate for the layers of the neural network. We show through experiments on the ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates. We also show that the images/label parameter for a dataset can potentially be used to determine optimal learning rates for the layers to get the best overall accuracy. We additionally validate this method on a sample of real-world image classification tasks from a public visual recognition API.

##### Abstract (translated by Google)
从少量标记数据中学习是深度学习领域的一项挑战。目前，转学习解决了这一问题，其中人们将较小的数据集从较大的源数据集中学习为转移任务。如果选择了超参数和源数据集，则转移学习可以提供更高的准确性。其中一个重要参数是神经网络层的学习率。我们通过ImageNet22​​k和Oxford Flowers数据集的实验证明，通过正确选择学习率，可以获得127％范围内准确度的提高。我们还展示了数据集的图像/标签参数可以用于确定图层的最佳学习速率，以获得最佳的整体准确性。我们还在公共视觉识别API的真实图像分类任务样本上验证了这种方法。

##### URL
[http://arxiv.org/abs/1807.11459](http://arxiv.org/abs/1807.11459)

##### PDF
[http://arxiv.org/pdf/1807.11459](http://arxiv.org/pdf/1807.11459)

