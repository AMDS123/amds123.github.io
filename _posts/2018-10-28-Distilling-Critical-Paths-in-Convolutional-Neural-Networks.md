---
layout: post
title: "Distilling Critical Paths in Convolutional Neural Networks"
date: 2018-10-28 00:56:45
categories: arXiv_CV
tags: arXiv_CV CNN
author: Fuxun Yu, Zhuwei Qin, Xiang Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network compression and acceleration are widely demanded currently due to the resource constraints on most deployment targets. In this paper, through analyzing the filter activation, gradients, and visualizing the filters' functionality in convolutional neural networks, we show that the filters in higher layers learn extremely task-specific features, which are exclusive for only a small subset of the overall tasks, or even a single class. Based on such findings, we reveal the critical paths of information flow for different classes. And by their intrinsic property of exclusiveness, we propose a critical path distillation method, which can effectively customize the convolutional neural networks to small ones with much smaller model size and less computation.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.02643](http://arxiv.org/abs/1811.02643)

##### PDF
[http://arxiv.org/pdf/1811.02643](http://arxiv.org/pdf/1811.02643)

