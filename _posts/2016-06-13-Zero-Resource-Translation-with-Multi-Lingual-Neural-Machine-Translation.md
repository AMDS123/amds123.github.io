---
layout: post
title: "Zero-Resource Translation with Multi-Lingual Neural Machine Translation"
date: 2016-06-13 22:40:33
categories: arXiv_CL
tags: arXiv_CL Attention
author: Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos T. Yarman Vural, Kyunghyun Cho
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.

##### Abstract (translated by Google)
在本文中，我们提出了一种新的微调算法，用于最近引入的多途径，多语种神经机器翻译，使零资源机器翻译。当与新的多对一翻译策略一起使用时，我们凭经验证明，这种微调算法允许多途径，多语言模型翻译零资源语言对（1）以及训练的单对神经翻译模型最多可以有1M个相同语言对的直接平行语句，（2）比基于数据透视的翻译策略更好，同时只保留一个与注意有关的参数副本。

##### URL
[https://arxiv.org/abs/1606.04164](https://arxiv.org/abs/1606.04164)

##### PDF
[https://arxiv.org/pdf/1606.04164](https://arxiv.org/pdf/1606.04164)

