---
layout: post
title: "Structured Knowledge Distillation for Semantic Segmentation"
date: 2019-03-11 10:05:09
categories: arXiv_CV
tags: arXiv_CV Knowledge Segmentation GAN Image_Classification Semantic_Segmentation Classification Prediction
author: Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, Jingdong Wang
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we investigate the knowledge distillation strategy for training small semantic segmentation networks by making use of large networks. We start from the straightforward scheme, pixel-wise distillation, which applies the distillation scheme adopted for image classification and performs knowledge distillation for each pixel~\emph{separately}. We further propose to distill the \emph{structured} knowledge from large networks to small networks, which is motivated by that semantic segmentation is a structured prediction problem. We study two structured distillation schemes: (i) \emph{pair-wise} distillation that distills the pairwise similarities, and (ii) \emph{holistic} distillation that uses GAN to distill holistic knowledge. The effectiveness of our knowledge distillation approaches is demonstrated by extensive experiments on three scene parsing datasets: Cityscapes, Camvid and ADE20K.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1903.04197](https://arxiv.org/abs/1903.04197)

##### PDF
[https://arxiv.org/pdf/1903.04197](https://arxiv.org/pdf/1903.04197)

