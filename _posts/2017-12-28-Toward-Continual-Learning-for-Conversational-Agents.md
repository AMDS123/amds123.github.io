---
layout: post
title: "Toward Continual Learning for Conversational Agents"
date: 2017-12-28 17:21:42
categories: arXiv_AI
tags: arXiv_AI Knowledge
author: Sungjin Lee
mathjax: true
---

* content
{:toc}

##### Abstract
While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data. Previous studies adopted a hybrid approach with knowledge-based components to abstract out domain-specific things or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using the recent development in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows the conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning for conversation systems. We verified the efficacy of our method through a conversational skill transfer from synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.

##### Abstract (translated by Google)
尽管端到端的神经对话模型已经在减少由传统的复杂系统架构引起的手工特征和错误方面取得了有希望的进展，但是它们通常需要大量的数据。以前的研究采用了基于知识的组件的混合方法来抽象特定领域的事物或增加数据以涵盖更多不同的模式。相反，我们建议直接解决这个问题，利用最近在神经模型连续学习领域的发展。具体而言，我们采用了一个领域独立的神经对话模型，并引入了一种新型的神经连续学习算法，使会话代理以一种数据有效的方式跨越不同的任务积累技能。就我们所知，这是第一个将持续学习应用于会话系统的工作。我们通过将会话技巧从合成对话或人 - 人对话转移到客户支持领域的人机对话来验证我们方法的有效性。

##### URL
[http://arxiv.org/abs/1712.09943](http://arxiv.org/abs/1712.09943)

##### PDF
[http://arxiv.org/pdf/1712.09943](http://arxiv.org/pdf/1712.09943)

