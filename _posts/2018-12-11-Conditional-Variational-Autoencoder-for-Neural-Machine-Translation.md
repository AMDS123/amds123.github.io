---
layout: post
title: "Conditional Variational Autoencoder for Neural Machine Translation"
date: 2018-12-11 14:05:24
categories: arXiv_CL
tags: arXiv_CL Attention Face Text_Generation NMT Inference
author: Artidoro Pagnoni, Kevin Liu, Shangyan Li
mathjax: true
---

* content
{:toc}

##### Abstract
We explore the performance of latent variable models for conditional text generation in the context of neural machine translation (NMT). Similar to Zhang et al., we augment the encoder-decoder NMT paradigm by introducing a continuous latent variable to model features of the translation process. We extend this model with a co-attention mechanism motivated by Parikh et al. in the inference network. Compared to the vision domain, latent variable models for text face additional challenges due to the discrete nature of language, namely posterior collapse. We experiment with different approaches to mitigate this issue. We show that our conditional variational model improves upon both discriminative attention-based translation and the variational baseline presented in Zhang et al. Finally, we present some exploration of the learned latent space to illustrate what the latent variable is capable of capturing. This is the first reported conditional variational model for text that meaningfully utilizes the latent variable without weakening the translation model.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.04405](http://arxiv.org/abs/1812.04405)

##### PDF
[http://arxiv.org/pdf/1812.04405](http://arxiv.org/pdf/1812.04405)

