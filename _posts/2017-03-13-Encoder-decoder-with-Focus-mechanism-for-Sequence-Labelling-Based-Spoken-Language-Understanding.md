---
layout: post
title: "Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken Language Understanding"
date: 2017-03-13 14:50:11
categories: arXiv_CL
tags: arXiv_CL Attention Speech_Recognition RNN Deep_Learning Memory_Networks Recognition
author: Su Zhu, Kai Yu
mathjax: true
---

* content
{:toc}

##### Abstract
This paper investigates the framework of encoder-decoder with attention for sequence labelling based spoken language understanding. We introduce Bidirectional Long Short Term Memory - Long Short Term Memory networks (BLSTM-LSTM) as the encoder-decoder model to fully utilize the power of deep learning. In the sequence labelling task, the input and output sequences are aligned word by word, while the attention mechanism cannot provide the exact alignment. To address this limitation, we propose a novel focus mechanism for encoder-decoder framework. Experiments on the standard ATIS dataset showed that BLSTM-LSTM with focus mechanism defined the new state-of-the-art by outperforming standard BLSTM and attention based encoder-decoder. Further experiments also show that the proposed model is more robust to speech recognition errors.

##### Abstract (translated by Google)
本文研究了基于口头语言理解的序列标注注意编解码器的框架。我们引入双向长期短期记忆 - 长期短期记忆网络（BLSTM-LSTM）作为编解码器模型，充分利用深度学习的力量。在序列标注任务中，输入和输出序列逐字对齐，而注意机制不能提供精确的对齐。为了解决这个限制，我们提出了一个新颖的编码器 - 解码器框架焦点机制。在标准ATIS数据集上的实验表明，具有聚焦机制的BLSTM-LSTM通过优于标准的BLSTM和基于注意力的编码器 - 解码器来定义新的最新技术。进一步的实验也表明，所提出的模型对于语音识别错误更加鲁棒。

##### URL
[https://arxiv.org/abs/1608.02097](https://arxiv.org/abs/1608.02097)

##### PDF
[https://arxiv.org/pdf/1608.02097](https://arxiv.org/pdf/1608.02097)

