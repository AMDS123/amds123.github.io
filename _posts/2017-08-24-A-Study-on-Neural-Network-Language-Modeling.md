---
layout: post
title: "A Study on Neural Network Language Modeling"
date: 2017-08-24 02:14:50
categories: arXiv_CL
tags: arXiv_CL Knowledge RNN Language_Model
author: Dengliang Shi
mathjax: true
---

* content
{:toc}

##### Abstract
An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed.

##### Abstract (translated by Google)
本文对神经网络语言建模（NNLM）进行了详尽的研究。描述和检查了不同的基本神经网络语言模型的体系结构。对基本神经网络语言模型包括重要性抽样，词类，缓存和双向递归神经网络（BiRNN）等进行了一系列不同的改进，并对各种技术的优缺点进行了评估。然后，从模型体系结构和知识表示方面探讨了神经网络语言建模的局限性。部分来自单词序列的统计信息在按照一定的顺序逐字处理时会丢失，通过更新权重矩阵和向量来训练神经网络的机制对NNLM的任何显着增强都施加了严格的限制。对于知识表示，神经网络语言模型表示的知识是来自某个训练数据集的单词序列的近似概率分布，而不是一个语言本身的知识或者以自然语言的单词序列表达的信息。最后讨论了进一步改进神经网络语言建模的一些方向。

##### URL
[https://arxiv.org/abs/1708.07252](https://arxiv.org/abs/1708.07252)

##### PDF
[https://arxiv.org/pdf/1708.07252](https://arxiv.org/pdf/1708.07252)

