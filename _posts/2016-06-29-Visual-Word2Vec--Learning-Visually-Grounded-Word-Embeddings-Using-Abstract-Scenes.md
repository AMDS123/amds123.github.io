---
layout: post
title: "Visual Word2Vec : Learning Visually Grounded Word Embeddings Using Abstract Scenes"
date: 2016-06-29 18:15:25
categories: arXiv_CL
tags: arXiv_CL Image_Retrieval Embedding Classification Language_Model Relation
author: Satwik Kottur, Ramakrishna Vedantam, José M. F. Moura, Devi Parikh
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, although "eats" and "stares at" seem unrelated in text, they share semantics visually. When people are eating something, they also tend to stare at the food. Grounding diverse relations like "eats" and "stares at" into vision remains challenging, despite recent progress in vision. We note that the visual grounding of words depends on semantics, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. We show improvements over text-only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets are available online.

##### Abstract (translated by Google)
我们提出了一个模型来学习视觉基础词嵌入（vis-w2v）来捕捉语义相关性的视觉概念。虽然使用文本训练的单词嵌入非常成功，但他们不能发现隐含在我们的视觉世界中的语义相关性的概念。例如，虽然“吃”和“凝视”在文本中似乎不相关，但它们在视觉上共享语义。当人们吃东西时，他们也倾向于盯着食物。尽管近期取得了进展，但将诸如“吃”和“凝视”视为多种多样的关系仍然充满挑战。我们注意到，单词的视觉基础取决于语义，而不是文字像素。因此，我们使用剪贴画创建的抽象场景来提供视觉接地。我们发现，我们学习的嵌入捕获细粒度，视觉基础的语义相关性概念。我们在三个任务上显示了纯文字单词嵌入（word2vec）的改进：常识断言分类，视觉释义和基于文本的图像检索。我们的代码和数据集可以在线获得。

##### URL
[https://arxiv.org/abs/1511.07067](https://arxiv.org/abs/1511.07067)

##### PDF
[https://arxiv.org/pdf/1511.07067](https://arxiv.org/pdf/1511.07067)

