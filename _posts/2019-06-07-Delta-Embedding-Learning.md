---
layout: post
title: "Delta Embedding Learning"
date: 2019-06-07 03:37:17
categories: arXiv_CL
tags: arXiv_CL Regularization Embedding
author: Xiao Zhang, Ji Wu, Dejing Dou
mathjax: true
---

* content
{:toc}

##### Abstract
Unsupervised word embeddings have become a popular approach of word representation in NLP tasks. However there are limitations to the semantics represented by unsupervised embeddings, and inadequate fine-tuning of embeddings can lead to suboptimal performance. We propose a novel learning technique called Delta Embedding Learning, which can be applied to general NLP tasks to improve performance by optimized tuning of the word embeddings. A structured regularization is applied to the embeddings to ensure they are tuned in an incremental way. As a result, the tuned word embeddings become better word representations by absorbing semantic information from supervision without "forgetting." We apply the method to various NLP tasks and see a consistent improvement in performance. Evaluation also confirms the tuned word embeddings have better semantic properties.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.04160](http://arxiv.org/abs/1812.04160)

##### PDF
[http://arxiv.org/pdf/1812.04160](http://arxiv.org/pdf/1812.04160)

