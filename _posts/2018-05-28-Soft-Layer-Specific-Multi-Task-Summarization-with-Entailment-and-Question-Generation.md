---
layout: post
title: "Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation"
date: 2018-05-28 16:05:39
categories: arXiv_AI
tags: arXiv_AI Salient Summarization Quantitative
author: Han Guo, Ramakanth Pasunuru, Mohit Bansal
mathjax: true
---

* content
{:toc}

##### Abstract
An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model's learned saliency and entailment skills.

##### Abstract (translated by Google)
准确抽象的文档摘要应包含其所有重要信息，并且应由输入文档在逻辑上引入。我们通过使用问题生成和生成的辅助任务通过多任务学习来改进抽象概括的这些重要方面，其中前者教授摘要模型如何寻找突出的疑问值得细节，后者教导模型如何重写摘要是输入文档的定向逻辑子集。我们还提出了跨三个任务的多个编码器和解码器层以及软共享机制（并且显示性能消融和每个贡献的分析示例）的高级（语义）层特定共享的新型多任务体系结构。总体而言，我们在CNN / DailyMail和Gigaword数据集以及DUC-2002传输设置上取得了显着的改进。我们还提出了一些关于我们模型的学习显着性和潜在技能的定量和定性分析研究。

##### URL
[http://arxiv.org/abs/1805.11004](http://arxiv.org/abs/1805.11004)

##### PDF
[http://arxiv.org/pdf/1805.11004](http://arxiv.org/pdf/1805.11004)

