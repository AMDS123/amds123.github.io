---
layout: post
title: "Attention-Based Convolutional Neural Network for Machine Comprehension"
date: 2016-02-13 14:38:47
categories: arXiv_SD
tags: arXiv_SD QA Attention CNN Deep_Learning
author: Wenpeng Yin, Sebastian Ebert, Hinrich Schütze
mathjax: true
---

* content
{:toc}

##### Abstract
Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of passage, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.

##### Abstract (translated by Google)
理解开放域文本是自然语言处理（NLP）的主要挑战之一。机器理解基准评估系统仅基于文本内容理解文本的能力。在这项工作中，我们调查MCTest上的机器理解，一个问答（QA）基准。之前的工作主要是基于特征工程的方法。我们提出了一个神经网络框架，命名为层次关注卷积神经网络（HABCNN），以解决这个任务，没有任何手动设计的功能。具体来说，我们通过两条路线来探索HABCNN，一个是通过传统的联合建模，问答，一个是通过文本包含。 HABCNN采用注意机制来检测与回答问题相关的关键短语，关键句和关键片段。实验表明，HABCNN比之前的深度学习方法有了很大的提高。

##### URL
[https://arxiv.org/abs/1602.04341](https://arxiv.org/abs/1602.04341)

##### PDF
[https://arxiv.org/pdf/1602.04341](https://arxiv.org/pdf/1602.04341)

