---
layout: post
title: "Hierarchical Actor-Critic"
date: 2018-02-27 16:01:40
categories: arXiv_AI
tags: arXiv_AI Sparse Reinforcement_Learning
author: Andrew Levy, Robert Platt, Kate Saenko
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to learn at different resolutions in time may help overcome one of the main challenges in deep reinforcement learning -- sample efficiency. Hierarchical agents that operate at different levels of temporal abstraction can learn tasks more quickly because they can divide the work of learning behaviors among multiple policies and can also explore the environment at a higher level. In this paper, we present a novel approach to hierarchical reinforcement learning called Hierarchical Actor-Critic (HAC) that enables agents to learn to break down problems involving continuous action spaces into simpler subproblems belonging to different time scales. HAC has two key advantages over most existing hierarchical learning methods: (i) the potential for faster learning as agents learn short policies at each level of the hierarchy and (ii) an end-to-end approach. We demonstrate that HAC significantly accelerates learning in a series of tasks that require behavior over a relatively long time horizon and involve sparse rewards.

##### Abstract (translated by Google)
以不同分辨率及时学习的能力可能有助于克服深度强化学习中的主要挑战之一 - 样本效率。在不同时间抽象层次上运行的层级代理可以更快地学习任务，因为他们可以将学习行为的工作分为多个策略，并且还可以在更高层次上探索环境。在本文中，我们提出了一种分层强化学习的新方法，称为Hierarchical Actor-Critic（HAC），它使代理可以学习将涉及连续动作空间的问题分解为属于不同时间尺度的更简单的子问题。 HAC与大多数现有的分层学习方法相比具有两个关键优势：（i）由于座席在层次结构的每个级别学习短策略，以及（ii）端到端方法，因此可以更快地进行学习。我们证明HAC显着加速了一系列需要在相对较长的时间范围内行为并涉及稀疏奖励的任务的学习。

##### URL
[http://arxiv.org/abs/1712.00948](http://arxiv.org/abs/1712.00948)

##### PDF
[http://arxiv.org/pdf/1712.00948](http://arxiv.org/pdf/1712.00948)

