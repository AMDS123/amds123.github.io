---
layout: post
title: "Smooth and Efficient Policy Exploration for Robot Trajectory Learning"
date: 2018-06-20 05:37:30
categories: arXiv_RO
tags: arXiv_RO
author: Shidi Li, Chee-Meng Chew, Velusamy Subramaniam
mathjax: true
---

* content
{:toc}

##### Abstract
Many policy search algorithms have been proposed for robot learning and proved to be practical in real robot applications. However, there are still hyperparameters in the algorithms, such as the exploration rate, which requires manual tuning. The existing methods to design the exploration rate manually or automatically may not be general enough or hard to apply in the real robot. In this paper, we propose a learning model to update the exploration rate adaptively. The overall algorithm is a combination of methods proposed by other researchers. Smooth trajectories for the robot can be produced by the algorithm and the updated exploration rate maximizes the lower bound of the expected return. Our method is tested in the ball-in-cup problem. The results show that our method can receive the same learning outcome as the previous methods but with fewer iterations.

##### Abstract (translated by Google)
许多策略搜索算法已经被提出用于机器人学习，并被证明在实际机器人应用中是实用的。但是，算法中仍然存在超参数，例如需要手动调整的勘探速率。现有的手动或自动设计探索速率的方法可能不够普遍或难以应用于真实机器人。在本文中，我们提出了一个自适应地更新勘探速度的学习模型。整体算法是其他研究人员提出的方法的组合。算法可以生成机器人的平滑轨迹，并且更新后的探索速率可以最大化预期回报的下限。我们的方法是在球入杯问题中进行测试。结果表明，我们的方法可以获得与先前方法相同的学习结果，但迭代次数较少。

##### URL
[http://arxiv.org/abs/1804.04903](http://arxiv.org/abs/1804.04903)

##### PDF
[http://arxiv.org/pdf/1804.04903](http://arxiv.org/pdf/1804.04903)

