---
layout: post
title: "HyperPower: Power- and Memory-Constrained Hyper-Parameter Optimization for Neural Networks"
date: 2017-12-06 23:44:22
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Dimitrios Stamoulis, Ermao Cai, Da-Cheng Juan, Diana Marculescu
mathjax: true
---

* content
{:toc}

##### Abstract
While selecting the hyper-parameters of Neural Networks (NNs) has been so far treated as an art, the emergence of more complex, deeper architectures poses increasingly more challenges to designers and Machine Learning (ML) practitioners, especially when power and memory constraints need to be considered. In this work, we propose HyperPower, a framework that enables efficient Bayesian optimization and random search in the context of power- and memory-constrained hyper-parameter optimization for NNs running on a given hardware platform. HyperPower is the first work (i) to show that power consumption can be used as a low-cost, a priori known constraint, and (ii) to propose predictive models for the power and memory of NNs executing on GPUs. Thanks to HyperPower, the number of function evaluations and the best test error achieved by a constraint-unaware method are reached up to 112.99x and 30.12x faster, respectively, while never considering invalid configurations. HyperPower significantly speeds up the hyper-parameter optimization, achieving up to 57.20x more function evaluations compared to constraint-unaware methods for a given time interval, effectively yielding significant accuracy improvements by up to 67.6%.

##### Abstract (translated by Google)
虽然选择神经网络（NN）的超参数到目前为止已被视为一门艺术，但是更复杂，更深层的体系结构的出现给设计者和机器学习（ML）从业者提出了越来越多的挑战，特别是当功率和内存限制需要时要考虑。在这项工作中，我们提出HyperPower框架，该框架能够在给定硬件平台上运行的神经网络的功率和内存受限的超参数优化的情况下实现高效的贝叶斯优化和随机搜索。 HyperPower是第一个表明功耗可以被用作低成本，先验已知约束的工作，并且（ii）提出在GPU上执行的神经网络的功率和存储的预测模型。借助HyperPower，功能评估的数量和最大的测试误差都可以通过约束不知道的方法分别达到112.99倍和30.12倍，而不会考虑无效的配置。 HyperPower显着加快了超参数优化，与给定时间间隔内不考虑约束的方法相比，实现了高达57.20倍的功能评估，有效地显着提高了高达67.6％的准确度。

##### URL
[https://arxiv.org/abs/1712.02446](https://arxiv.org/abs/1712.02446)

##### PDF
[https://arxiv.org/pdf/1712.02446](https://arxiv.org/pdf/1712.02446)

