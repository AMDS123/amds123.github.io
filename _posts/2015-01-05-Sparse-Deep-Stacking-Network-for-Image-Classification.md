---
layout: post
title: "Sparse Deep Stacking Network for Image Classification"
date: 2015-01-05 08:07:31
categories: arXiv_CV
tags: arXiv_CV Regularization Sparse Image_Classification Inference Classification Recognition
author: Jun Li, Heyou Chang, Jian Yang
mathjax: true
---

* content
{:toc}

##### Abstract
Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene.

##### Abstract (translated by Google)
稀疏编码可以学习良好的鲁棒表示噪声和模型更高阶的表示图像分类。然而，即使监督信号被用于学习稀疏编码技术中的紧凑和有区别的字典，推理算法在计算上也是昂贵的。幸运的是，已经提出了一种简化的神经网络模块（SNNM）来直接学习用于避免昂贵的推理的判别性词典。但SNNM模块忽略稀疏表示。因此，我们通过添加混合范数正则化（l1 / l2范数）来提出一个稀疏的SNNM模块。稀疏SNNM模块被进一步堆叠以建立稀疏深层堆叠网络（S-DSN）。在实验中，我们用四个数据库来评估S-DSN，包括Extended YaleB，AR，15场景和Caltech101。实验结果表明，我们的模型优于相关的分类方法，只有一个线性分类器。值得注意的是，在15场景中我们的识别准确率达到了98.8％。

##### URL
[https://arxiv.org/abs/1501.00777](https://arxiv.org/abs/1501.00777)

##### PDF
[https://arxiv.org/pdf/1501.00777](https://arxiv.org/pdf/1501.00777)

