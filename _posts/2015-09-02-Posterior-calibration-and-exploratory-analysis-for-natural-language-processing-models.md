---
layout: post
title: "Posterior calibration and exploratory analysis for natural language processing models"
date: 2015-09-02 17:26:24
categories: arXiv_CL
tags: arXiv_CL
author: Khanh Nguyen, Brendan O'Connor
mathjax: true
---

* content
{:toc}

##### Abstract
Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that (1) the quality of a model' s posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies, and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task.

##### Abstract (translated by Google)
自然语言处理中的许多模型定义了语言结构上的概率分布。我们认为：（1）模型后验分布的质量可以而且应该直接评估，概率是否与经验频率相对应;（2）NLP不确定性不仅可以被预测到管道组成部分，而且可以被预测数据分析，告诉用户何时信任和不信任NLP分析。我们提出了一种分析校准的方法，并将其用于比较几种常用模型的误差校准。我们还提供了一个可以为政治事件提取任务创建置信区间的共参数抽样算法。

##### URL
[https://arxiv.org/abs/1508.05154](https://arxiv.org/abs/1508.05154)

##### PDF
[https://arxiv.org/pdf/1508.05154](https://arxiv.org/pdf/1508.05154)

