---
layout: post
title: "A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations"
date: 2018-05-18 03:45:06
categories: arXiv_AI
tags: arXiv_AI Salient CNN
author: Weili Nie, Yang Zhang, Ankit Patel
mathjax: true
---

* content
{:toc}

##### Abstract
Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation(GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less class-sensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery and thus are unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.

##### Abstract (translated by Google)
已经提出基于反向传播的可视化来解释卷积神经网络（CNN），然而没有理论证明它们的行为是正确的：引导反向传播（GBP）和反卷积网络（DeconvNet）产生更多的人类可解释的但类别敏感的可视化而不是显着性地图。受此启发，我们制定了一个理论解释，揭示GBP和DeconvNet本质上是在进行（部分）图像恢复，因此与网络决策无关。具体而言，我们的分析表明，由GBP和DeconvNet引入的落后ReLU以及CNN中的本地连接是引人注目的可视化的两个主要原因。提供了支持理论分析的大量实验。

##### URL
[http://arxiv.org/abs/1805.07039](http://arxiv.org/abs/1805.07039)

##### PDF
[http://arxiv.org/pdf/1805.07039](http://arxiv.org/pdf/1805.07039)

