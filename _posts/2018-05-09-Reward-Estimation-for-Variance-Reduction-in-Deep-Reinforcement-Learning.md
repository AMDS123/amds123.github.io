---
layout: post
title: "Reward Estimation for Variance Reduction in Deep Reinforcement Learning"
date: 2018-05-09 03:11:29
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Joshua Romoff, Alexandre Pich&#xe9;, Peter Henderson, Vincent Francois-Lavet, Joelle Pineau
mathjax: true
---

* content
{:toc}

##### Abstract
In reinforcement learning (RL), stochastic environments can make learning a policy difficult due to high degrees of variance. As such, variance reduction methods have been investigated in other works, such as advantage estimation and control-variates estimation. Here, we propose to learn a separate reward estimator to train the value function, to help reduce variance caused by a noisy reward signal. This results in theoretical reductions in variance in the tabular case, as well as empirical improvements in both the function approximation and tabular settings in environments where rewards are stochastic. To do so, we use a modified version of Advantage Actor Critic (A2C) on variations of Atari games.

##### Abstract (translated by Google)
在强化学习（RL）中，由于高度的变化，随机环境可能使得学习政策变得困难。因此，在其他工作中已经研究了方差减少方法，例如优势估计和控制变量估计。在这里，我们建议学习一个单独的奖励估计器来训练价值函数，以帮助减少由嘈杂的奖励信号引起的变化。这导致了表格案例的方差的理论减少，以及在奖励是随机的环境中的函数逼近和表格设置的经验改进。为此，我们在Atari游戏的变体上使用Advantage Actor Critic（A2C）的修改版本。

##### URL
[http://arxiv.org/abs/1805.03359](http://arxiv.org/abs/1805.03359)

##### PDF
[http://arxiv.org/pdf/1805.03359](http://arxiv.org/pdf/1805.03359)

