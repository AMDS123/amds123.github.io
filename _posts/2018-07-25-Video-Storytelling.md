---
layout: post
title: "Video Storytelling"
date: 2018-07-25 02:43:19
categories: arXiv_CV
tags: arXiv_CV Reinforcement_Learning Embedding Quantitative
author: Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli
mathjax: true
---

* content
{:toc}

##### Abstract
Bridging vision and natural language is a longstanding goal in computer vision and multimedia research. While earlier works focus on generating a single-sentence description for visual content, recent works have studied paragraph generation. In this work, we introduce the problem of video storytelling, which aims at generating coherent and succinct stories for long videos. Video storytelling introduces new challenges, mainly due to the diversity of the story and the length and complexity of the video. We propose novel methods to address the challenges. First, we propose a context-aware framework for multimodal embedding learning, where we design a Residual Bidirectional Recurrent Neural Network to leverage contextual information from past and future. Second, we propose a Narrator model to discover the underlying storyline. The Narrator is formulated as a reinforcement learning agent which is trained by directly optimizing the textual metric of the generated story. We evaluate our method on the Video Story dataset, a new dataset that we have collected to enable the study. We compare our method with multiple state-of-the-art baselines, and show that our method achieves better performance, in terms of quantitative measures and user study.

##### Abstract (translated by Google)
桥梁视觉和自然语言是计算机视觉和多媒体研究的长期目标。虽然早期的作品专注于为视觉内容生成单句描述，但最近的作品研究了段落生成。在这项工作中，我们介绍了视频讲故事的问题，旨在为长视频制作连贯而简洁的故事。视频讲故事引入了新的挑战，主要是由于故事的多样性以及视频的长度和复杂性。我们提出了解决这些挑战的新方法。首先，我们提出了一种用于多模态嵌入学习的上下文感知框架，其中我们设计了一个残余双向递归神经网络来利用过去和未来的上下文信息。其次，我们提出了一个叙述者模型来发现潜在的故事情节。叙述者被制定为强化学习代理，通过直接优化所生成故事的文本度量来训练。我们在视频故事数据集上评估我们的方法，这是我们为了启用该研究而收集的新数据集。我们将我们的方法与多个最先进的基线进行比较，并表明我们的方法在定量测量和用户研究方面实现了更好的性能。

##### URL
[http://arxiv.org/abs/1807.09418](http://arxiv.org/abs/1807.09418)

##### PDF
[http://arxiv.org/pdf/1807.09418](http://arxiv.org/pdf/1807.09418)

