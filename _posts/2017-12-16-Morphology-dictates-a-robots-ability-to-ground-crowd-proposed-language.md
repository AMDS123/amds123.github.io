---
layout: post
title: "Morphology dictates a robot's ability to ground crowd-proposed language"
date: 2017-12-16 00:31:36
categories: arXiv_AI
tags: arXiv_AI Prediction
author: Zahra Mahoor, Jack Felag, Josh Bongard
mathjax: true
---

* content
{:toc}

##### Abstract
As more robots act in physical proximity to people, it is essential to ensure they make decisions and execute actions that align with human values. To do so, robots need to understand the true intentions behind human-issued commands. In this paper, we define a safe robot as one that receives a natural-language command from humans, considers an action in response to that command, and accurately predicts how humans will judge that action is executed in reality. Our contribution is two-fold: First, we introduce a web platform for human users to propose commands to simulated robots. The robots receive commands and act based on those proposed commands, and then the users provide positive and/or negative reinforcement. Next, we train a critic for each robot to predict the crowd's responses to one of the crowd-proposed commands. Second, we show that the morphology of a robot plays a role in the way it grounds language: The critics show that two of the robots used in the experiment achieve a lower prediction error than the others. Thus, those two robots are safer, according to our definition, since they ground the proposed command more accurately.

##### Abstract (translated by Google)
随着越来越多的机器人在物理上与人类接近，确保他们做出决定并执行符合人类价值的行为是至关重要的。为此，机器人需要了解人类命令背后的真实意图。在本文中，我们将安全机器人定义为从人类接收自然语言命令的机器人，根据该命令考虑动作并准确预测人类将如何判断在现实中执行的动作。我们的贡献有两个方面：首先，我们引入一个网络平台，供用户向模拟机器人提出命令。机器人接收命令并基于提出的命令行动，然后用户提供正面和/或负面的强化。接下来，我们训练每个机器人的评论家，以预测人群对群众提出的命令之一的反应。其次，我们展示了一个机器人的形态在其语言理论中的作用：批评者表示，实验中使用的两个机器人的预测误差比其他机器人要低。因此，根据我们的定义，这两个机器人更安全，因为他们更准确地提出了拟议的命令。

##### URL
[http://arxiv.org/abs/1712.05881](http://arxiv.org/abs/1712.05881)

##### PDF
[http://arxiv.org/pdf/1712.05881](http://arxiv.org/pdf/1712.05881)

