---
layout: post
title: "One-shot and few-shot learning of word embeddings"
date: 2018-01-02 16:53:05
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Deep_Learning
author: Andrew K. Lampinen, James L. McClelland
mathjax: true
---

* content
{:toc}

##### Abstract
Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.

##### Abstract (translated by Google)
标准的深度学习系统需要数千或数百万个例子来学习一个概念，而且不能轻易地整合新的概念。相比之下，人类有一个令人难以置信的能力来做一次或少量的学习。例如，从听到一个句子中的单词，人类可以通过利用周围单词的语法和语义告诉我们，来推断这个单词。在这里，我们从中吸取灵感来突出一个简单的技术，通过这个技巧，深层循环网络可以类似地利用他们的先前知识，从小数据中学习一个新词的有用表示。这可以使自然语言处理系统更灵活，让他们不断从他们遇到的新词中学习。

##### URL
[http://arxiv.org/abs/1710.10280](http://arxiv.org/abs/1710.10280)

##### PDF
[http://arxiv.org/pdf/1710.10280](http://arxiv.org/pdf/1710.10280)

