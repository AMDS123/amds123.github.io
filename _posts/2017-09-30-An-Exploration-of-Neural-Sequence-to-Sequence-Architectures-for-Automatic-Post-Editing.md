---
layout: post
title: "An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing"
date: 2017-09-30 13:03:33
categories: arXiv_CL
tags: arXiv_CL Attention
author: Marcin Junczys-Dowmunt, Roman Grundkiewicz
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs $mt$ (raw MT output) and $src$ (source language input) in a single neural architecture, modeling $\{mt, src\} \rightarrow pe$ directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.

##### Abstract (translated by Google)
在这项工作中，我们探索了适用于机器翻译输出自动后期编辑任务的多种神经架构。我们专注于在单个神经架构中将输入$ mt $（原始MT输出）和$ src $（源语言输入）相结合的神经端到端模型，建模$ \ {mt，src \} \ rightarrow pe $直。除此之外，我们还研究了似乎非常适合于单语任务的硬注意模型的影响，以及这两种想法的组合。我们报告了WMT-2016自动后期编辑共享任务期间提供的数据集的结果，并且可以证明在单一模型中将APE情景中的所有可用数据纳入到双重关注模型可以改进最佳共享任务系统其他发布的结果在共享任务之后。与注意力相结合的双重注意模型仍然具有竞争力，尽管对输入进行了较少的更改。

##### URL
[https://arxiv.org/abs/1706.04138](https://arxiv.org/abs/1706.04138)

##### PDF
[https://arxiv.org/pdf/1706.04138](https://arxiv.org/pdf/1706.04138)

