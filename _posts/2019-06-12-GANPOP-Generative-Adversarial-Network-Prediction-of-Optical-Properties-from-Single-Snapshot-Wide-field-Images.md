---
layout: post
title: "GANPOP: Generative Adversarial Network Prediction of Optical Properties from Single Snapshot Wide-field Images"
date: 2019-06-12 19:55:49
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Deep_Learning Prediction
author: Mason T. Chen, Faisal Mahmood, Jordan A. Sweer, Nicholas J. Durr
mathjax: true
---

* content
{:toc}

##### Abstract
We present a deep learning framework for wide-field, content-aware estimation of absorption and scattering coefficients of tissues, called Generative Adversarial Network Prediction of Optical Properties (GANPOP). Spatial frequency domain imaging is used to obtain ground-truth optical properties from in vivo human hands, freshly resected human esophagectomy samples and homogeneous tissue phantoms. Images of objects with either flat-field or structured illumination are paired with registered optical property maps and are used to train conditional generative adversarial networks that estimate optical properties from a single input image. We benchmark this approach by comparing GANPOP to a single-snapshot optical property (SSOP) technique, using a normalized mean absolute error (NMAE) metric. In human gastrointestinal specimens, GANPOP estimates both reduced scattering and absorption coefficients at 660 nm from a single 0.2 /mm spatial frequency illumination image with 58% higher accuracy than SSOP. When applied to both in vivo and ex vivo swine tissues, a GANPOP model trained solely on human specimens and phantoms estimates optical properties with approximately 43% improvement over SSOP, indicating adaptability to sample variety. Moreover, we demonstrate that GANPOP estimates optical properties from flat-field illumination images with similar error to SSOP, which requires structured-illumination. Given a training set that appropriately spans the target domain, GANPOP has the potential to enable rapid and accurate wide-field measurements of optical properties, even from conventional imaging systems with flat-field illumination.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1906.05360](https://arxiv.org/abs/1906.05360)

##### PDF
[https://arxiv.org/pdf/1906.05360](https://arxiv.org/pdf/1906.05360)

