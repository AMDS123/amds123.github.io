---
layout: post
title: "Latent hypernet: Exploring all Layers from Convolutional Neural Networks"
date: 2017-11-07 18:32:40
categories: arXiv_CV
tags: arXiv_CV Knowledge CNN Recognition
author: Artur Jordao, Ricardo Kloss, William Robson Schwartz
mathjax: true
---

* content
{:toc}

##### Abstract
Since Convolutional Neural Networks (ConvNets) are able to simultaneously learn features and classifiers to discriminate different categories of activities, recent works have employed ConvNets approaches to perform human activity recognition (HAR) based on wearable sensors, allowing the removal of expensive human work and expert knowledge. However, these approaches have their power of discrimination limited mainly by the large number of parameters that compose the network and the reduced number of samples available for training. Inspired by this, we propose an accurate and robust approach, referred to as Latent HyperNet (LHN). The LHN uses feature maps from early layers (hyper) and projects them, individually, onto a low dimensionality space (latent). Then, these latent features are concatenated and presented to a classifier. To demonstrate the robustness and accuracy of the LHN, we evaluate it using four different networks architectures in five publicly available HAR datasets based on wearable sensors, which vary in the sampling rate and number of activities. Our experiments demonstrate that the proposed LHN is able to produce rich information, improving the results regarding the original ConvNets. Furthermore, the method outperforms existing state-of-the-art methods.

##### Abstract (translated by Google)
由于卷积神经网络（ConvNets）能够同时学习特征和分类器来区分不同类别的活动，最近的作品采用了ConvNets方法来执行基于可穿戴传感器的人类活动识别（HAR），从而允许去除昂贵的人力工作和专家知识。然而，这些方法主要是由构成网络的大量参数以及可用于训练的样本数量减少来区分的。受此启发，我们提出了一种准确和可靠的方法，称为Latent HyperNet（LHN）。 LHN使用来自早期图层（hyper）的特征地图，并将它们分别投影到低维空间（潜在的）上。然后，将这些潜在特征串联起来并呈现给分类器。为了证明LHN的鲁棒性和准确性，我们使用四种不同的网络架构在基于可穿戴传感器的五个公开可用的HAR数据集中评估它，所述可穿戴传感器在采样率和活动数量上有所不同。我们的实验表明，所提出的LHN能够产生丰富的信息，改善原始ConvNets的结果。此外，该方法胜过现有的最先进的方法。

##### URL
[https://arxiv.org/abs/1711.02652](https://arxiv.org/abs/1711.02652)

##### PDF
[https://arxiv.org/pdf/1711.02652](https://arxiv.org/pdf/1711.02652)

