---
layout: post
title: "Fingerprint Policy Optimisation for Robust Reinforcement Learning"
date: 2018-09-15 14:20:57
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Supratik Paul, Michael A. Osborne, Shimon Whiteson
mathjax: true
---

* content
{:toc}

##### Abstract
Policy gradient methods have been successfully applied to a variety of reinforcement learning tasks. However, while learning in a simulator, these methods do not utilise the opportunity to improve learning by adjusting certain environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but that are controllable in a simulator. This can lead to slow learning or convergence to highly suboptimal policies if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO) which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. We apply FPO to a number of continuous control tasks of varying difficulty and show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling but are key to learning good policies.

##### Abstract (translated by Google)
政策梯度方法已成功应用于各种强化学习任务。然而，在模拟器中学习时，这些方法没有利用机会通过调整某些环境变量来改善学习：不可观察的状态特征，其由物理设置中的环境随机确定，但是在模拟器中是可控的。如果环境变量对转换动态产生很大影响，这可能导致学习缓慢或收敛到非常不理想的策略。在本文中，我们提出了指纹策略优化（FPO），它发现了一种在环境变量分布中期望最优的策略。中心思想是使用贝叶斯优化（BO）来主动选择环境变量的分布，以最大化策略梯度方法的每次迭代所产生的改进。为了使这个BO变得实用，我们提供了两个易于计算的当前策略的低维指纹。我们将FPO应用于许多不同难度的连续控制任务，并表明FPO可以有效地学习对重大罕见事件具有鲁棒性的策略，这些事件不太可能在随机抽样下观察到，但却是学习良好政策的关键。

##### URL
[http://arxiv.org/abs/1805.10662](http://arxiv.org/abs/1805.10662)

##### PDF
[http://arxiv.org/pdf/1805.10662](http://arxiv.org/pdf/1805.10662)

