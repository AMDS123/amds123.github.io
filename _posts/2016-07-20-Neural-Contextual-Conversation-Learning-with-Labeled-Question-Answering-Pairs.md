---
layout: post
title: "Neural Contextual Conversation Learning with Labeled Question-Answering Pairs"
date: 2016-07-20 03:25:31
categories: arXiv_CL
tags: arXiv_CL Attention
author: Kun Xiong, Anqi Cui, Zefeng Zhang, Ming Li
mathjax: true
---

* content
{:toc}

##### Abstract
Neural conversational models tend to produce generic or safe responses in different contexts, e.g., reply \textit{"Of course"} to narrative statements or \textit{"I don't know"} to questions. In this paper, we propose an end-to-end approach to avoid such problem in neural generative models. Additional memory mechanisms have been introduced to standard sequence-to-sequence (seq2seq) models, so that context can be considered while generating sentences. Three seq2seq models, which memorize a fix-sized contextual vector from hidden input, hidden input/output and a gated contextual attention structure respectively, have been trained and tested on a dataset of labeled question-answering pairs in Chinese. The model with contextual attention outperforms others including the state-of-the-art seq2seq models on perplexity test. The novel contextual model generates diverse and robust responses, and is able to carry out conversations on a wide range of topics appropriately.

##### Abstract (translated by Google)
神经交谈模型倾向于在不同的背景下产生一般的或安全的回答，例如对于叙述性陈述的答复\“文本”“当然”或对问题的“文本”我不知道。在本文中，我们提出了一个端到端的方法来避免神经生成模型中的这种问题。额外的记忆机制已被引入标准的序列到序列（seq2seq）模型，所以在生成句子时可以考虑上下文。从隐藏输入，隐藏输入/输出和门控上下文关注结构分别记忆固定大小的上下文向量的三个seq2seq模型已经在中文标记的问答对的数据集上进行了训练和测试。具有上下文关注的模型胜过其他人，包括最新的seq2seq模型在困惑测试。新颖的上下文模型可以产生多样和强大的反应，并且能够适当地在广泛的话题上进行对话。

##### URL
[https://arxiv.org/abs/1607.05809](https://arxiv.org/abs/1607.05809)

##### PDF
[https://arxiv.org/pdf/1607.05809](https://arxiv.org/pdf/1607.05809)

