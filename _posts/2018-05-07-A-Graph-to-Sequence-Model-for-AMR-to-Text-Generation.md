---
layout: post
title: "A Graph-to-Sequence Model for AMR-to-Text Generation"
date: 2018-05-07 12:31:27
categories: arXiv_CL
tags: arXiv_CL Face Text_Generation RNN
author: Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea
mathjax: true
---

* content
{:toc}

##### Abstract
The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.

##### Abstract (translated by Google)
AMR到文本生成的问题是恢复表示与输入AMR图表相同含义的文本。当前最先进的方法使用序列 - 序列模型，利用LSTM编码线性AMR结构。虽然能够对非局部语义信息建模，但序列LSTM可能会丢失AMR图结构中的信息，因此面临着大图的挑战，从而导致长序列。我们引入了一个神经图 - 序列模型，使用一种新颖的LSTM结构来直接编码图层级语义。在标准基准测试中，我们的模型显示了文献中现有方法的优越结果。

##### URL
[http://arxiv.org/abs/1805.02473](http://arxiv.org/abs/1805.02473)

##### PDF
[http://arxiv.org/pdf/1805.02473](http://arxiv.org/pdf/1805.02473)

