---
layout: post
title: "Multilingual Neural Machine Translation with Task-Specific Attention"
date: 2018-06-08 17:18:26
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Graeme Blackwood, Miguel Ballesteros, Todd Ward
mathjax: true
---

* content
{:toc}

##### Abstract
Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1806.03280](https://arxiv.org/abs/1806.03280)

##### PDF
[https://arxiv.org/pdf/1806.03280](https://arxiv.org/pdf/1806.03280)

