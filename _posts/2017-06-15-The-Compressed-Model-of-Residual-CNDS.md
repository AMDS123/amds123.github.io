---
layout: post
title: "The Compressed Model of Residual CNDS"
date: 2017-06-15 02:17:53
categories: arXiv_CV
tags: arXiv_CV CNN Optimization
author: Hussam Qassim, David Feinzimer, Abhishek Verma
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural networks have achieved a great success in the recent years. Although, the way to maximize the performance of the convolutional neural networks still in the beginning. Furthermore, the optimization of the size and the time that need to train the convolutional neural networks is very far away from reaching the researcher's ambition. In this paper, we proposed a new convolutional neural network that combined several techniques to boost the optimization of the convolutional neural network in the aspects of speed and size. As we used our previous model Residual-CNDS (ResCNDS), which solved the problems of slower convergence, overfitting, and degradation, and compressed it. The outcome model called Residual-Squeeze-CNDS (ResSquCNDS), which we demonstrated on our sold technique to add residual learning and our model of compressing the convolutional neural networks. Our model of compressing adapted from the SQUEEZENET model, but our model is more generalizable, which can be applied almost to any neural network model, and fully integrated into the residual learning, which addresses the problem of the degradation very successfully. Our proposed model trained on very large-scale MIT Places365-Standard scene datasets, which backing our hypothesis that the new compressed model inherited the best of the previous ResCNDS8 model, and almost get the same accuracy in the validation Top-1 and Top-5 with 87.64% smaller in size and 13.33% faster in the training time.

##### Abstract (translated by Google)
卷积神经网络近年来取得了巨大的成功。尽管如此，最大化卷积神经网络性能的方式还处于起步阶段。此外，需要训练卷积神经网络的规模和时间的优化离研究者的雄心还有很远的距离。在本文中，我们提出了一种新的卷积神经网络，它结合了几种技术来提高卷积神经网络在速度和大小方面的优化。正如我们使用我们以前的Residual-CNDS（ResCNDS）模型，它解决了收敛速度慢，过拟合和退化问题，并对其进行了压缩。结果模型称为Residual-Squeeze-CNDS（ResSquCNDS），我们在我们的销售技术上添加了剩余学习和我们的压缩卷积神经网络的模型。我们的压缩模​​型采用SQUEEZENET模型进行改造，但是我们的模型更具概括性，几乎可以应用于任何神经网络模型，并且完全集成到残差学习中，从而很好地解决了退化问题。我们提出的模型是在非常大规模的MIT Places365标准场景数据集上进行训练的，这些数据支持我们的假设，即新的压缩模型继承了先前的ResCNDS8模型的最佳性能，并且在验证Top-1和Top-5中几乎得到相同的精度培训时间缩短87.64％，速度提高13.33％。

##### URL
[https://arxiv.org/abs/1706.06419](https://arxiv.org/abs/1706.06419)

##### PDF
[https://arxiv.org/pdf/1706.06419](https://arxiv.org/pdf/1706.06419)

