---
layout: post
title: "Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations"
date: 2019-04-11 04:04:06
categories: arXiv_CV
tags: arXiv_CV Image_Caption Adversarial Caption Embedding Relation
author: Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, Wei-Ying Ma
mathjax: true
---

* content
{:toc}

##### Abstract
We propose the Unified Visual-Semantic Embeddings (Unified VSE) for learning a joint space of visual representation and textual semantics. The model unifies the embeddings of concepts at different levels: objects, attributes, relations, and full scenes. We view the sentential semantics as a combination of different semantic components such as objects and relations; their embeddings are aligned with different image regions. A contrastive learning approach is proposed for the effective learning of this fine-grained alignment from only image-caption pairs. We also present a simple yet effective approach that enforces the coverage of caption embeddings on the semantic components that appear in the sentence. We demonstrate that the Unified VSE outperforms baselines on cross-modal retrieval tasks; the enforcement of the semantic coverage improves the model's robustness in defending text-domain adversarial attacks. Moreover, our model empowers the use of visual cues to accurately resolve word dependencies in novel sentences.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.05521](http://arxiv.org/abs/1904.05521)

##### PDF
[http://arxiv.org/pdf/1904.05521](http://arxiv.org/pdf/1904.05521)

