---
layout: post
title: "Temporal Attention Model for Neural Machine Translation"
date: 2016-08-09 19:42:14
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Baskaran Sankaran, Haitao Mi, Yaser Al-Onaizan, Abe Ittycheriah
mathjax: true
---

* content
{:toc}

##### Abstract
Attention-based Neural Machine Translation (NMT) models suffer from attention deficiency issues as has been observed in recent research. We propose a novel mechanism to address some of these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. We compare our approach against the baseline NMT model and two other related approaches that address this issue either explicitly or implicitly. Large-scale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related NMT approaches. Our model further outperforms strong SMT baselines in some settings even without using ensembles.

##### Abstract (translated by Google)
基于注意力的神经机器翻译（NMT）模型遭受注意力不足问题，如最近的研究所观察到的。我们提出了一种新的机制来解决这些局限性，并提高NMT的关注度。具体而言，我们的方法暂时（在每个句子中）记忆对齐，并且随着解码器生成候选翻译，用累积的时间记忆调整注意力。我们将我们的方法与基线NMT模型以及其他两种明确或隐含地解决此问题的相关方法进行比较。两个语言对的大规模实验表明，我们的方法比基线和相关的NMT方法获得更好和更强的收益。即使没有使用合奏，我们的模型在某些设置中的性能也会进一步优于强大的SMT基线。

##### URL
[https://arxiv.org/abs/1608.02927](https://arxiv.org/abs/1608.02927)

##### PDF
[https://arxiv.org/pdf/1608.02927](https://arxiv.org/pdf/1608.02927)

