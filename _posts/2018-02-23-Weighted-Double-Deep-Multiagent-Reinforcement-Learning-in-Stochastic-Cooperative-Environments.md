---
layout: post
title: "Weighted Double Deep Multiagent Reinforcement Learning in Stochastic Cooperative Environments"
date: 2018-02-23 14:03:22
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Yan Zheng, Jianye Hao, Zongzhang Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Despite single agent deep reinforcement learning has achieved significant success due to the experience replay mechanism, Concerns should be reconsidered in multiagent environments. This work focus on the stochastic cooperative environment. We apply a specific adaptation to one recently proposed weighted double estimator and propose a multiagent deep reinforcement learning framework, named Weighted Double Deep Q-Network (WDDQN). To achieve efficient cooperation, \textit{Lenient Reward Network} and \textit{Mixture Replay Strategy} are introduced. By utilizing the deep neural network and the weighted double estimator, WDDQN can not only reduce the bias effectively but also be extended to many deep RL scenarios with only raw pixel images as input. Empirically, the WDDQN outperforms the existing DRL algorithm (double DQN) and multiagent RL algorithm (lenient Q-learning) in terms of performance and convergence within stochastic cooperative environments.

##### Abstract (translated by Google)
尽管由于体验重播机制，单一代理深度强化学习取得了显着成功，但在多代理环境中应重新考虑Concerns。这项工作专注于随机合作环境。我们对一个最近提出的加权双重估计器应用了一个特定的自适应，并提出了一个名为Weighted Double Deep Q-Network（WDDQN）的多主体深度强化学习框架。为了实现有效的合作，引入了\ textit {Lenient Reward Network}和\ textit {Mixture Replay Strategy}。通过利用深度神经网络和加权双重估计器，WDDQN不仅能有效地降低偏差，而且还可以扩展到许多只有原始像素图像作为输入的深度RL情景。从经验上讲，WDDQN在随机协作环境中的性能和收敛性方面优于现有的DRL算法（双DQN）和多代理RL算法（宽松Q学习）。

##### URL
[http://arxiv.org/abs/1802.08534](http://arxiv.org/abs/1802.08534)

##### PDF
[http://arxiv.org/pdf/1802.08534](http://arxiv.org/pdf/1802.08534)

