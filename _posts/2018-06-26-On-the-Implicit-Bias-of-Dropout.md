---
layout: post
title: "On the Implicit Bias of Dropout"
date: 2018-06-26 03:08:21
categories: arXiv_AI
tags: arXiv_AI Optimization Deep_Learning
author: Poorya Mianjy, Raman Arora, Rene Vidal
mathjax: true
---

* content
{:toc}

##### Abstract
Algorithmic approaches endow deep learning systems with implicit bias that helps them generalize even in over-parametrized settings. In this paper, we focus on understanding such a bias induced in learning through dropout, a popular technique to avoid overfitting in deep learning. For single hidden-layer linear neural networks, we show that dropout tends to make the norm of incoming/outgoing weight vectors of all the hidden nodes equal. In addition, we provide a complete characterization of the optimization landscape induced by dropout.

##### Abstract (translated by Google)
算法方法赋予深度学习系统隐含的偏见，即使在过度参数化的设置中也可以帮助他们推广。在本文中，我们重点了解通过辍学引起的学习偏见，这是一种避免深度学习过度拟合的流行技术。对于单隐层线性神经网络，我们证明了丢包倾向于使所有隐藏节点的输入/输出权向量的规范相等。此外，我们还提供了由退出引起的优化景观的完整表征。

##### URL
[http://arxiv.org/abs/1806.09777](http://arxiv.org/abs/1806.09777)

##### PDF
[http://arxiv.org/pdf/1806.09777](http://arxiv.org/pdf/1806.09777)

