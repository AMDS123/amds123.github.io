---
layout: post
title: "Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition"
date: 2015-11-16 22:19:11
categories: arXiv_CV
tags: arXiv_CV Embedding Recognition
author: Mengyi Liu, Shiguang Shan, Ruiping Wang, Xilin Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Facial expression is temporally dynamic event which can be decomposed into a set of muscle motions occurring in different facial regions over various time intervals. For dynamic expression recognition, two key issues, temporal alignment and semantics-aware dynamic representation, must be taken into account. In this paper, we attempt to solve both problems via manifold modeling of videos based on a novel mid-level representation, i.e. \textbf{expressionlet}. Specifically, our method contains three key stages: 1) each expression video clip is characterized as a spatial-temporal manifold (STM) formed by dense low-level features; 2) a Universal Manifold Model (UMM) is learned over all low-level features and represented as a set of local modes to statistically unify all the STMs. 3) the local modes on each STM can be instantiated by fitting to UMM, and the corresponding expressionlet is constructed by modeling the variations in each local mode. With above strategy, expression videos are naturally aligned both spatially and temporally. To enhance the discriminative power, the expressionlet-based STM representation is further processed with discriminant embedding. Our method is evaluated on four public expression databases, CK+, MMI, Oulu-CASIA, and FERA. In all cases, our method outperforms the known state-of-the-art by a large margin.

##### Abstract (translated by Google)
面部表情是时间上的动态事件，可以分解为不同面部区域在不同时间间隔出现的一组肌肉运动。对于动态表达式识别，必须考虑两个关键问题，即时间对齐和语义感知动态表示。在本文中，我们试图通过基于新颖的中间表示的视频的流形建模来解决这两个问题，即\ textbf {expressionlet}。具体来说，我们的方法包括三个关键阶段：1）每个表达式视频片段的特点是由密集的低级特征形成的时空流形（STM） 2）通用流形模型（UMM）被学习到所有的低级特征，并被表示为统一统一所有STM的一组本地模式。 3）每个STM的本地模式可以通过拟合UMM来实例化，并且通过建模每个本地模式的变化来构造相应的表达式。通过上述策略，表情视频在空间和时间上自然对齐。为了增强判别能力，基于表达式的STM表示用判别嵌入进一步处理。我们的方法在四个公共表达数据库，CK +，MMI，Oulu-CASIA和口交评估。在所有情况下，我们的方法大大超过了已知的最新技术水平。

##### URL
[https://arxiv.org/abs/1511.05204](https://arxiv.org/abs/1511.05204)

##### PDF
[https://arxiv.org/pdf/1511.05204](https://arxiv.org/pdf/1511.05204)

