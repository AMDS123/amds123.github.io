---
layout: post
title: "Neural Architectures for Fine-grained Entity Type Classification"
date: 2017-02-21 06:49:42
categories: arXiv_CL
tags: arXiv_CL Attention Classification Quantitative
author: Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we investigate several neural network architectures for fine-grained entity type classification. Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions. Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task. We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the well- established FIGER (GOLD) dataset.

##### Abstract (translated by Google)
在这项工作中，我们调查了几个神经网络架构的细粒度实体类型分类。特别是，我们考虑扩展最近提出的细心的神经架构，并作出三个关键的贡献。以前在细致的神经架构上的工作不考虑手工制作的功能，我们结合了学习和手工制作的功能，并观察它们相互补充。此外，通过定量分析，我们确定注意机制能够学习到句法头部和包含提及的短语，其中两个都是已知的用于我们任务的强手工特征。我们通过分层标签编码方法实现参数共享，在低维投影中为每个类型层次结构显示清晰的集群。最后，尽管使用相同的评估数据集，文献中经常比较使用不同数据训练的模型。我们确定，训练数据的选择对性能有重大影响，对于先前提出的方法，微小的F1分数减少了多达9.85％。尽管如此，我们最好的模型在完善的FIGER（GOLD）数据集上获得了75.36％的微小F1分数的最新结果。

##### URL
[https://arxiv.org/abs/1606.01341](https://arxiv.org/abs/1606.01341)

##### PDF
[https://arxiv.org/pdf/1606.01341](https://arxiv.org/pdf/1606.01341)

