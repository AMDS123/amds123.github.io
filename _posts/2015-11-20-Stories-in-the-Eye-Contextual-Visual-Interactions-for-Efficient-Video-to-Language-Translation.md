---
layout: post
title: "Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation"
date: 2015-11-20 16:33:13
categories: arXiv_CL
tags: arXiv_CL Knowledge Language_Model Relation Recognition
author: Anirudh Goyal, Marius Leordeanu
mathjax: true
---

* content
{:toc}

##### Abstract
Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge.

##### Abstract (translated by Google)
整合更高层次的视觉和语言解释是人类智能的核心。随着图像中视觉类别自动识别的快速发展，人们对视频动态时空领域的高层次理解以及对自然语言的翻译还远未得到解决。尽管大多数关于视觉到文本翻译的工作使用预先学习或预先建立的计算语言模型，但本文中我们提出一种仅使用视觉来有效学习如何将视频内容翻译成语言的方法。我们以简单的形式发现主要演员扮演的故事，同时仅使用视觉线索表示对象及其相互作用。我们的方法以分层方式学习更高层次的表示，用于识别涉及的主体，动作和对象，其相关的背景背景以及它们随时间的相互作用。我们有一个三阶段的方法：首先我们考虑到在地方一级的个体实体的特征，然后我们考虑这些对象和行为之间的关系和他们的视频背景，第三，我们把他们的时空关系看作是输入最高层次的分类器。因此，我们的方法基于它们在直接从训练数据中直接学习到的整体视觉故事中扮演的角色，以主题，动词和宾语的形式找到对视频的连贯的语言描述，而不使用已知的语言模型。我们在包含大量YouTube视频片段的大型数据集上测试了我们方法的效率，并且展示了最先进的性能，通常优于目前使用更复杂，预先学习的语言知识的方法。

##### URL
[https://arxiv.org/abs/1511.06674](https://arxiv.org/abs/1511.06674)

##### PDF
[https://arxiv.org/pdf/1511.06674](https://arxiv.org/pdf/1511.06674)

