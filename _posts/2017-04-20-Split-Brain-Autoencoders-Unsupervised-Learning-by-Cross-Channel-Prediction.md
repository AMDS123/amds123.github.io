---
layout: post
title: "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction"
date: 2017-04-20 08:12:21
categories: arXiv_CV
tags: arXiv_CV Transfer_Learning Represenation_Learning Prediction
author: Richard Zhang, Phillip Isola, Alexei A. Efros
mathjax: true
---

* content
{:toc}

##### Abstract
We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.

##### Abstract (translated by Google)
我们提出了用于无监督表示学习的裂脑自动编码器，这是对传统自编码器体系结构的直接修改。该方法增加了一个分裂到网络，导致两个不相交的子网络。每个子网络都经过训练可以执行一项艰巨的任务 - 从另一个子网络预测数据通道的一个子集。子网络一起从整个输入信号中提取特征。通过迫使网络解决跨渠道预测任务，我们在网络中引入一个表示，可以很好地转移到其他看不见的任务。这种方法在几个大型的传输学习基准上达到了最先进的性能。

##### URL
[https://arxiv.org/abs/1611.09842](https://arxiv.org/abs/1611.09842)

##### PDF
[https://arxiv.org/pdf/1611.09842](https://arxiv.org/pdf/1611.09842)

