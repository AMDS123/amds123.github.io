---
layout: post
title: "An Analysis of Ability in Deep Neural Networks"
date: 2017-06-29 00:23:16
categories: arXiv_SD
tags: arXiv_SD
author: John P. Lalor, Hao Wu, Tsendsuren Munkhdalai, Hong Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks (DNNs) have made significant progress in a number of Machine Learning applications. However without a consistent set of evaluation tasks, interpreting performance across test datasets is impossible. In most previous work, characteristics of individual data points are not considered during evaluation, and each data point is treated equally. Using Item Response Theory (IRT) from psychometrics it is possible to model characteristics of specific data points that then inform an estimate of model ability as compared to a population of humans. We report the results of several experiments to determine how different Deep Neural Network (DNN) models perform under different training circumstances with respect to ability. As DNNs train on larger datasets, performance begins to look like human performance under the assumptions of IRT models. That is, easy questions start to have a higher probability of being answered correctly than harder questions. We also report the results of additional analyses regarding model robustness to noise and performance as a function of training set size that further inform our main conclusion

##### Abstract (translated by Google)
深度神经网络（DNN）在许多机器学习应用中取得了重大进展。但是，如果没有一套完整的评估任务，则跨测试数据集解释性能是不可能的。在大多数以前的工作中，评估过程中不考虑个别数据点的特征，每个数据点都是平等对待的。使用来自心理测量学的项目反应理论（IRT），可以对特定数据点的特征进行建模，然后通知与人类群体相比的模型能力的估计。我们报告了几个实验的结果，以确定不同的深度神经网络（DNN）模型在不同的训练情况下在能力方面的表现。由于DNN在较大的数据集上训练，在IRT模型的假设下，性能开始看起来像人的表现。也就是说，简单的问题开始有比较难的问题有更高的被正确回答的可能性。我们还报告了关于模型对噪声和性能的鲁棒性的附加分析的结果作为训练集大小的函数，这进一步说明了我们的主要结论

##### URL
[https://arxiv.org/abs/1702.04811](https://arxiv.org/abs/1702.04811)

##### PDF
[https://arxiv.org/pdf/1702.04811](https://arxiv.org/pdf/1702.04811)

