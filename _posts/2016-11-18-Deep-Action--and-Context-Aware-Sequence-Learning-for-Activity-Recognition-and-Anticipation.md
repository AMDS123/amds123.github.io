---
layout: post
title: "Deep Action- and Context-Aware Sequence Learning for Activity Recognition and Anticipation"
date: 2016-11-18 01:41:40
categories: arXiv_CV
tags: arXiv_CV Action_Recognition Recognition
author: Mohammad Sadegh Aliakbarian, Fatemehsadat Saleh, Basura Fernando, Mathieu Salzmann, Lars Petersson, Lars Andersson
mathjax: true
---

* content
{:toc}

##### Abstract
Action recognition and anticipation are key to the success of many computer vision applications. Existing methods can roughly be grouped into those that extract global, context-aware representations of the entire image or sequence, and those that aim at focusing on the regions where the action occurs. While the former may suffer from the fact that context is not always reliable, the latter completely ignore this source of information, which can nonetheless be helpful in many situations. In this paper, we aim at making the best of both worlds by developing an approach that leverages both context-aware and action-aware features. At the core of our method lies a novel multi-stage recurrent architecture that allows us to effectively combine these two sources of information throughout a video. This architecture first exploits the global, context-aware features, and merges the resulting representation with the localized, action-aware ones. Our experiments on standard datasets evidence the benefits of our approach over methods that use each information type separately. We outperform the state-of-the-art methods that, as us, rely only on RGB frames as input for both action recognition and anticipation.

##### Abstract (translated by Google)
行动的认可和期待是许多计算机视觉应用成功的关键。现有的方法大致可以分为那些提取整个图像或序列的全局，上下文感知的表示，以及旨在集中在发生动作的区域的方法。虽然前者可能受到上下文并不总是可靠的事实的困扰，但是后者完全忽视了这种信息来源，这在很多情况下仍然有帮助。在本文中，我们的目标是通过开发一种利用上下文感知和动作感知功能的方法来实现两全其美。我们的方法的核心是一个新颖的多阶段经常性架构，使我们能够在视频中有效地结合这两个信息源。这种架构首先利用全局的上下文感知功能，并将生成的表示与本地化的，动作感知的功能进行合并。我们对标准数据集的实验证明了我们的方法相对于单独使用每种信息类型的方法的好处。我们超越了最先进的方法，就像我们一样，只依靠RGB帧作为动作识别和预测的输入。

##### URL
[https://arxiv.org/abs/1611.05520](https://arxiv.org/abs/1611.05520)

##### PDF
[https://arxiv.org/pdf/1611.05520](https://arxiv.org/pdf/1611.05520)

