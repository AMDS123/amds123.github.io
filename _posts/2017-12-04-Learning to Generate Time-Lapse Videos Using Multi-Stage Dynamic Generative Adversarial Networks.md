---
layout: post
title: 'Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks'
date: 2017-12-05 21:10:17
categories: arXiv_CV
tags: arXiv_CV 'GAN'
author: Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo
---

* content
{:toc}

##### Abstract
Taking a photo outside, can we predict the immediate future, \textit{e.g.}, how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to $128\times 128$ resolution for 32 frames. Quantitative and qualitative experiment results have demonstrated the superiority of our model over the state-of-the-art models.

##### Abstract (translated by Google)
在外面拍一张照片，我们可以预测不久的将来吗，例如，云将如何在天空中移动？我们通过提供基于生成对抗网络（GAN）的两阶段方法来生成高分辨率的现实时间推移视频来解决这个问题。鉴于第一帧，我们的模型学习产生长期的未来帧。第一阶段为每一帧生成真实内容的视频。第二阶段通过强化它在第一阶段生成的视频，以更接近真实的视频关于运动动态。为了进一步鼓励在最终生成的视频中的生动的运动，使用克拉矩阵来更精确地对运动进行建模。我们建立一个大规模的时间推移数据集，并在这个新的数据集上测试我们的方法。使用我们的模型，我们能够生成高达128美元的真实视频，128帧的分辨率为32帧。定量和定性的实验结果证明了我们的模型优于最先进的模型。

##### URL
[http://arxiv.org/abs/1709.07592](http://arxiv.org/abs/1709.07592)

