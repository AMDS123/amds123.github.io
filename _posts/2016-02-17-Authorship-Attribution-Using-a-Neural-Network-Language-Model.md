---
layout: post
title: "Authorship Attribution Using a Neural Network Language Model"
date: 2016-02-17 04:06:28
categories: arXiv_SD
tags: arXiv_SD Classification Language_Model
author: Zhenhao Ge, Yufang Sun, Mark J. T. Smith
mathjax: true
---

* content
{:toc}

##### Abstract
In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2:5% reduction in perplexity and increases author classification accuracy by 3:43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at this https URL

##### Abstract (translated by Google)
在实践中，由于数据资源有限，为个别作者培训语言模型通常是昂贵的。在这种情况下，神经网络语言模型（NNLMs）通常优于传统的非参数N-gram模型。在这里，我们调查了作者归因问题的前馈NNLM的性能，作者集规模适中，数据相对有限。我们也考虑文本主题如何影响性能。与使用Kneser-Ney平滑处理的构造良好的N-gram基线方法相比，该方法可使混淆度降低近2％，使作者分类准确率平均提高3：43％，只需5个测试句。在测试数据的准确性和需求方面，性能与最先进的技术非常具有竞争力。源代码，预处理数据集，方法和结果的详细说明可在此https网址获得

##### URL
[https://arxiv.org/abs/1602.05292](https://arxiv.org/abs/1602.05292)

##### PDF
[https://arxiv.org/pdf/1602.05292](https://arxiv.org/pdf/1602.05292)

