---
layout: post
title: "Cold-Start Reinforcement Learning with Softmax Policy Gradient"
date: 2017-10-13 21:20:00
categories: arXiv_CV
tags: arXiv_CV Image_Caption Summarization Reinforcement_Learning Caption Prediction
author: Nan Ding, Radu Soricut
mathjax: true
---

* content
{:toc}

##### Abstract
Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.

##### Abstract (translated by Google)
强化学习的政策梯度方法有两个常见的和不受欢迎的开销程序，即热启动训练和样本方差减少。在本文中，我们描述了一种基于softmax值函数的强化学习方法，该方法不需要这些过程。我们的方法结合了策略梯度方法的优点和最大似然方法的效率和简单性。我们将这种新的冷启动强化学习方法应用于结构化输出预测问题的训练序列生成模型。经验证据在自动摘要和图像字幕任务上验证了这种方法。

##### URL
[https://arxiv.org/abs/1709.09346](https://arxiv.org/abs/1709.09346)

##### PDF
[https://arxiv.org/pdf/1709.09346](https://arxiv.org/pdf/1709.09346)

