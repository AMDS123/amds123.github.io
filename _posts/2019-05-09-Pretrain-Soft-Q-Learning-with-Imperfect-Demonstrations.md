---
layout: post
title: "Pretrain Soft Q-Learning with Imperfect Demonstrations"
date: 2019-05-09 09:23:53
categories: arXiv_AI
tags: arXiv_AI Regularization Reinforcement_Learning
author: Xiaoqin Zhang, Yunfei Li, Huimin Ma, Xiong Luo
mathjax: true
---

* content
{:toc}

##### Abstract
Pretraining reinforcement learning methods with demonstrations has been an important concept in the study of reinforcement learning since a large amount of computing power is spent on online simulations with existing reinforcement learning algorithms. Pretraining reinforcement learning remains a significant challenge in exploiting expert demonstrations whilst keeping exploration potentials, especially for value based methods. In this paper, we propose a pretraining method for soft Q-learning. Our work is inspired by pretraining methods for actor-critic algorithms since soft Q-learning is a value based algorithm that is equivalent to policy gradient. The proposed method is based on $\gamma$-discounted biased policy evaluation with entropy regularization, which is also the updating target of soft Q-learning. Our method is evaluated on various tasks from Atari 2600. Experiments show that our method effectively learns from imperfect demonstrations, and outperforms other state-of-the-art methods that learn from expert demonstrations.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.03501](http://arxiv.org/abs/1905.03501)

##### PDF
[http://arxiv.org/pdf/1905.03501](http://arxiv.org/pdf/1905.03501)

