---
layout: post
title: "EgoSampling: Wide View Hyperlapse from Egocentric Videos"
date: 2017-01-12 15:12:19
categories: arXiv_CV
tags: arXiv_CV
author: Tavi Halperin, Yair Poleg, Chetan Arora, Shmuel Peleg
mathjax: true
---

* content
{:toc}

##### Abstract
The possibility of sharing one's point of view makes use of wearable cameras compelling. These videos are often long, boring and coupled with extreme shake, as the camera is worn on a moving person. Fast forwarding (i.e. frame sampling) is a natural choice for quick video browsing. However, this accentuates the shake caused by natural head motion in an egocentric video, making the fast forwarded video useless. We propose EgoSampling, an adaptive frame sampling that gives stable, fast forwarded, hyperlapse videos. Adaptive frame sampling is formulated as an energy minimization problem, whose optimal solution can be found in polynomial time. We further turn the camera shake from a drawback into a feature, enabling the increase in field-of-view of the output video. This is obtained when each output frame is mosaiced from several input frames. The proposed technique also enables the generation of a single hyperlapse video from multiple egocentric videos, allowing even faster video consumption.

##### Abstract (translated by Google)
分享自己的观点的可能性使可穿戴相机的使用引人注目。这些影片经常是漫长的，无聊的，加上极端的震动，因为相机穿在移动的人身上。快速转发（即帧采样）是快速视频浏览的自然选择。然而，这加剧了以自我为中心的视频中由自然头部动作引起的震动，使得快速转发的视频无用。我们提出EgoSampling，这是一种自适应帧采样，可以提供稳定，快速转发的超立体视频。自适应帧采样被表述为能量最小化问题，其最优解可以在多项式时间中找到。我们进一步将相机晃动从一个缺点变成一个功能，使输出视频的视野增加。当每个输出帧由几个输入帧拼凑而成时，就可以得到这个结果。所提出的技术还使得能够从多个以自我为中心的视频生成单一的hyperlapse视频，允许甚至更快的视频消费。

##### URL
[https://arxiv.org/abs/1604.07741](https://arxiv.org/abs/1604.07741)

##### PDF
[https://arxiv.org/pdf/1604.07741](https://arxiv.org/pdf/1604.07741)

