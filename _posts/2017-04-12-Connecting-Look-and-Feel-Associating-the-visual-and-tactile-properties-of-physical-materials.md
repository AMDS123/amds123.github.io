---
layout: post
title: "Connecting Look and Feel: Associating the visual and tactile properties of physical materials"
date: 2017-04-12 16:28:14
categories: arXiv_CV
tags: arXiv_CV Embedding
author: Wenzhen Yuan, Shaoxiong Wang, Siyuan Dong, Edward Adelson
mathjax: true
---

* content
{:toc}

##### Abstract
For machines to interact with the physical world, they must understand the physical properties of objects and materials they encounter. We use fabrics as an example of a deformable material with a rich set of mechanical properties. A thin flexible fabric, when draped, tends to look different from a heavy stiff fabric. It also feels different when touched. Using a collection of 118 fabric sample, we captured color and depth images of draped fabrics along with tactile data from a high resolution touch sensor. We then sought to associate the information from vision and touch by jointly training CNNs across the three modalities. Through the CNN, each input, regardless of the modality, generates an embedding vector that records the fabric's physical property. By comparing the embeddings, our system is able to look at a fabric image and predict how it will feel, and vice versa. We also show that a system jointly trained on vision and touch data can outperform a similar system trained only on visual data when tested purely with visual inputs.

##### Abstract (translated by Google)
对于机器与物理世界交互，他们必须了解物体和物质的物理属性。我们使用织物作为具有丰富机械性能的可变形材料的例子。薄薄的柔软面料在披上时，看起来与厚重的硬质面料有所不同。触摸时也感觉不同。使用118个织物样品的集合，我们捕获悬垂织物的颜色和深度图像以及来自高分辨率触摸传感器的触觉数据。然后，我们试图通过联合培训CNN在三种模式中将视觉和触觉的信息联系起来。通过CNN，每个输入，不管模式如何，都会生成一个记录织物物理属性的嵌入矢量。通过比较嵌入，我们的系统能够查看织物图像并预测它的感觉，反之亦然。我们还表明，一个视觉和触摸数据共同训练的系统可以超越一个类似的系统训练只有在视觉数据纯粹用视觉输入进行测试。

##### URL
[https://arxiv.org/abs/1704.03822](https://arxiv.org/abs/1704.03822)

##### PDF
[https://arxiv.org/pdf/1704.03822](https://arxiv.org/pdf/1704.03822)

