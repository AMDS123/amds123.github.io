---
layout: post
title: "Bilingual Distributed Word Representations from Document-Aligned Comparable Data"
date: 2016-02-28 12:47:15
categories: arXiv_CL
tags: arXiv_CL Embedding Represenation_Learning
author: Ivan Vulić, Marie-Francine Moens
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.

##### Abstract (translated by Google)
我们提出了一种从非平行文档对齐数据中学习双语词表示的新模型。继最近在词表示学习方面取得的进展之后，我们的模型学习了密集的实值词向量，即双语词嵌入（BWE）。与先前关于诱导严重依赖平行句子对齐的语料库和/或词典等易于获取的翻译资源的BWE相比，文章揭示了BWE可以仅以文档对齐的可比较数据为基础学习，而不需要额外的词汇资源句法信息。我们提出了一个比较我们的方法与先前的模型学习双语词表示从可比较的数据依赖多语言概率主题建模（MuPTM）框架，以及与分布式本地上下文计数模型。我们在两个语义任务中展示了诱导BWEs的效用：（1）双语词典提取，（2）在多义词的上下文中提示单词翻译。我们简单而有效的基于BWE的模型显着优于基于MuPTM和上下文计数表示模型的可比较数据以及先前的基于BWE的模型，并获得针对所有三个测试语言对的两个任务的最佳报告结果。

##### URL
[https://arxiv.org/abs/1509.07308](https://arxiv.org/abs/1509.07308)

##### PDF
[https://arxiv.org/pdf/1509.07308](https://arxiv.org/pdf/1509.07308)

