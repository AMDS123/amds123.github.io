---
layout: post
title: "Multispectral Pedestrian Detection via Simultaneous Detection and Segmentation"
date: 2018-08-14 17:59:12
categories: arXiv_CV
tags: arXiv_CV Object_Detection Segmentation Attention Semantic_Segmentation Classification Detection
author: Chengyang Li, Dan Song, Ruofeng Tong, Min Tang
mathjax: true
---

* content
{:toc}

##### Abstract
Multispectral pedestrian detection has attracted increasing attention from the research community due to its crucial competence for many around-the-clock applications (e.g., video surveillance and autonomous driving), especially under insufficient illumination conditions. We create a human baseline over the KAIST dataset and reveal that there is still a large gap between current top detectors and human performance. To narrow this gap, we propose a network fusion architecture, which consists of a multispectral proposal network to generate pedestrian proposals, and a subsequent multispectral classification network to distinguish pedestrian instances from hard negatives. The unified network is learned by jointly optimizing pedestrian detection and semantic segmentation tasks. The final detections are obtained by integrating the outputs from different modalities as well as the two stages. The approach significantly outperforms state-of-the-art methods on the KAIST dataset while remain fast. Additionally, we contribute a sanitized version of training annotations for the KAIST dataset, and examine the effects caused by different kinds of annotation errors. Future research of this problem will benefit from the sanitized version which eliminates the interference of annotation errors.

##### Abstract (translated by Google)
多光谱行人探测由于其对许多全天候应用（例如视频监控和自动驾驶）的关键能力而引起了研究界越来越多的关注，特别是在光照条件不足的情况下。我们在KAIST数据集上创建了人类基线，并揭示了当前顶级探测器与人类表现之间仍存在较大差距。为了缩小这一差距，我们提出了一种网络融合架构，其中包括用于生成行人建议的多光​​谱提议网络，以及随后的多光谱分类网络，以区分行人实例和硬性负面。通过联合优化行人检测和语义分割任务来学习统一网络。通过整合不同模态的输出以及两个阶段来获得最终的检测结果。该方法明显优于KAIST数据集上最先进的方法，同时保持快速。此外，我们为KAIST数据集提供了一个已清理的训练注释版本，并检查由不同类型的注释错误引起的效果。对该问题的未来研究将受益于消毒版本，该版本消除了注释错误的干扰。

##### URL
[http://arxiv.org/abs/1808.04818](http://arxiv.org/abs/1808.04818)

##### PDF
[http://arxiv.org/pdf/1808.04818](http://arxiv.org/pdf/1808.04818)

