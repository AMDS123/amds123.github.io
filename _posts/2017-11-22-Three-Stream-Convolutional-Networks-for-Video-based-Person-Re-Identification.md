---
layout: post
title: "Three-Stream Convolutional Networks for Video-based Person Re-Identification"
date: 2017-11-22 15:05:58
categories: arXiv_CV
tags: arXiv_CV Re-identification Person_Re-identification CNN
author: Zeng Yu, Tianrui Li, Ning Yu, Xun Gong, Ke Chen, Yi Pan
mathjax: true
---

* content
{:toc}

##### Abstract
This paper aims to develop a new architecture that can make full use of the feature maps of convolutional networks. To this end, we study a number of methods for video-based person re-identification and make the following findings: 1) Max-pooling only focuses on the maximum value of a receptive field, wasting a lot of information. 2) Networks with different streams even including the one with the worst performance work better than networks with same streams, where each one has the best performance alone. 3) A full connection layer at the end of convolutional networks is not necessary. Based on these studies, we propose a new convolutional architecture termed Three-Stream Convolutional Networks (TSCN). It first uses different streams to learn different aspects of feature maps for attentive spatio-temporal fusion of video, and then merges them together to study some union features. To further utilize the feature maps, two architectures are designed by using the strategies of multi-scale and upsampling. Comparative experiments on iLIDS-VID, PRID-2011 and MARS datasets illustrate that the proposed architectures are significantly better for feature extraction than the state-of-the-art models.

##### Abstract (translated by Google)
本文旨在开发一种能充分利用卷积网络特征映射的新架构。为此，我们研究了一系列基于视频的人物再认识方法，并做出如下研究结论：1）最大汇集只关注接受场的最大值，浪费大量的信息。 2）具有不同流的网络甚至包括具有最差性能的网络比具有相同流的网络更好，其中每个网络具有最好的性能。 3）卷积网络末端的完整连接层不是必需的。基于这些研究，我们提出了一种称为三流卷积网络（TSCN）的新的卷积结构。它首先使用不同的流来学习视频的细节时空融合的特征地图的不同方面，然后将它们合并在一起来研究一些联合特征。为了进一步利用特征映射，使用多尺度和上采样策略设计了两种架构。在iLIDS-VID，PRID-2011和MARS数据集上进行的比较实验表明，所提出的体系结构对于特征提取比现有技术模型明显更好。

##### URL
[https://arxiv.org/abs/1712.01652](https://arxiv.org/abs/1712.01652)

##### PDF
[https://arxiv.org/pdf/1712.01652](https://arxiv.org/pdf/1712.01652)

