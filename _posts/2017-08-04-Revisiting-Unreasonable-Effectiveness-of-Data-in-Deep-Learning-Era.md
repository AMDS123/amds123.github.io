---
layout: post
title: "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"
date: 2017-08-04 01:33:22
categories: arXiv_CV
tags: arXiv_CV Object_Detection Segmentation Pose_Estimation Image_Classification Semantic_Segmentation Represenation_Learning Classification Deep_Learning Detection Relation
author: Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta
mathjax: true
---

* content
{:toc}

##### Abstract
The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.

##### Abstract (translated by Google)
视觉深度学习的成功可以归结为：（a）高容量的模型; （b）增加计算能力; （c）大规模标记数据的可用性。自2012年以来，GPU的模型表示能力和计算能力已经有了显着的提高。但是最大的数据集的大小令人惊讶地保持不变。如果我们将数据集大小增加10倍或100倍，会发生什么？本文为解决“大数据”与视觉深度学习之间关系的谜团迈出了一大步。通过利用具有超过375M噪声标签的JFT-300M数据集来处理300M图像，我们研究了如果将这些数据用于表示学习如何改变当前视觉任务的性能。我们的论文提供了一些令人惊讶的（和一些预期的）发现。首先，我们发现基于训练数据量的对数增长的视觉任务的性能。其次，我们表明，表示学习（或预培训）仍然有很多的承诺。通过培训更好的基础模型，可以提高许多视觉任务的性能。最后，如预期的那样，我们为不同的视觉任务提供了最新的最新结果，包括图像分类，目标检测，语义分割和人体姿态估计。我们诚挚的希望是，这激励视觉界不要低估数据，发展集体努力建立更大的数据集。

##### URL
[https://arxiv.org/abs/1707.02968](https://arxiv.org/abs/1707.02968)

##### PDF
[https://arxiv.org/pdf/1707.02968](https://arxiv.org/pdf/1707.02968)

