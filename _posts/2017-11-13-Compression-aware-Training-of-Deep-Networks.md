---
layout: post
title: "Compression-aware Training of Deep Networks"
date: 2017-11-13 18:10:42
categories: arXiv_CV
tags: arXiv_CV
author: Jose M. Alvarez, Mathieu Salzmann
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.

##### Abstract (translated by Google)
近年来，随着神经网络日益深入的发展，各种应用领域也取得了很大的进展。不幸的是，这些网络的大量单位使得它们在计算和记忆方面都很昂贵。为了克服这个问题，利用深度网络过度参数的事实，已经提出了几种压缩策略。然而，这些方法通常从已经以标准方式训练的网络开始，而不考虑未来的压缩。在本文中，我们建议在训练过程中明确说明压缩。为此，我们引入一个正规化器，鼓励每一层的参数矩阵在训练过程中具有较低的等级。我们证明，训练期间的压缩记帐使我们能够比现有的压缩技术学得更加紧凑，但至少与之一样有效。

##### URL
[https://arxiv.org/abs/1711.02638](https://arxiv.org/abs/1711.02638)

##### PDF
[https://arxiv.org/pdf/1711.02638](https://arxiv.org/pdf/1711.02638)

