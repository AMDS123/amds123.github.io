---
layout: post
title: "Learning compositionally through attentive guidance"
date: 2018-09-07 09:46:30
categories: arXiv_AI
tags: arXiv_AI Attention
author: Dieuwke Hupkes, Anand Singh, Kris Korrel, German Kruszewski, Elia Bruni
mathjax: true
---

* content
{:toc}

##### Abstract
While neural network models have been successfully applied to domains that require substantial generalisation skills, recent studies have implied that they struggle when solving the task they are trained on requires inferring its underlying compositional structure. In this paper, we introduce Attentive Guidance, a mechanism to direct a sequence to sequence model equipped with attention to find more compositional solutions. We test it on two tasks, devised precisely to assess the compositional capabilities of neural models, and we show that vanilla sequence to sequence models with attention overfit the training distribution, while the guided versions come up with compositional solutions that fit the training and testing distributions almost equally well. Moreover, the learned solutions generalise even in cases where the training and testing distributions strongly diverge. In this way, we demonstrate that sequence to sequence models are capable of finding compositional solutions without requiring extra components. These results helps to disentangle the causes for the lack of systematic compositionality in neural networks, which can in turn fuel future work.

##### Abstract (translated by Google)
虽然神经网络模型已成功应用于需要大量泛化技能的领域，但最近的研究表明，在解决他们接受培训的任务时，他们很难解决其潜在的组成结构。在本文中，我们引入了注意指导，这是一种将序列引导至序列模型的机制，该模型具有注意力以找到更多的组成解决方案。我们对两个任务进行了测试，精确地设计用于评估神经模型的组成能力，并且我们展示了香草序列对序列模型的关注过度拟合训练分布，而引导版本提出了适合训练和测试分布的组合解决方案几乎同样好。此外，即使在训练和测试分布强烈偏离的情况下，所学习的解决方案也可以概括。通过这种方式，我们证明序列模型的序列能够在不需要额外组件的情况下找到组合解决方案。这些结果有助于解开神经网络中缺乏系统组合性的原因，这反过来又可以推动未来的工作。

##### URL
[http://arxiv.org/abs/1805.09657](http://arxiv.org/abs/1805.09657)

##### PDF
[http://arxiv.org/pdf/1805.09657](http://arxiv.org/pdf/1805.09657)

