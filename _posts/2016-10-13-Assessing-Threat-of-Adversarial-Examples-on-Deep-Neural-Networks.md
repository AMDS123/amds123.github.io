---
layout: post
title: "Assessing Threat of Adversarial Examples on Deep Neural Networks"
date: 2016-10-13 20:34:48
categories: arXiv_CV
tags: arXiv_CV Adversarial Classification
author: Abigail Graese, Andras Rozsa, Terrance E. Boult
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks are facing a potential security threat from adversarial examples, inputs that look normal but cause an incorrect classification by the deep neural network. For example, the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them. This research assesses the extent to which adversarial examples pose a security threat, when one considers the normal image acquisition process. This process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application, such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car. These small transformations negate the effect of the carefully crafted perturbations of adversarial examples, resulting in a correct classification by the deep neural network. Thus just acquiring the image decreases the potential impact of the proposed security threat. We also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples. Normal preprocessing, such as text binarization, almost completely neutralizes adversarial examples. This is the first paper to show that for text driven classification, adversarial examples are an academic curiosity, not a security threat.

##### Abstract (translated by Google)
深度神经网络正面临来自对抗性例子的潜在安全威胁，看起来正常的输入，但是由深度神经网络导致不正确的分类。例如，提议的威胁可能导致扫描支票上的手写数字被错误地分类，但在人们看到它们时看起来正常。这项研究评估了对抗性的例子在多大程度上构成安全威胁，当我们考虑正常的图像采集过程。通过模拟真实世界应用程序中获取图像时通常会发生的转换，例如使用扫描仪来获取数字以查看数量或使用自动驾驶汽车中的摄像头，可以模拟此过程。这些小小的转变否定了精心设计的对抗例子的扰动的效果，导致深度神经网络的正确分类。因此，仅仅获取图像减少了所提出的安全威胁的潜在影响。我们还表明，已经广泛使用的平均多种作物的过程中和了大多数敌对的例子。正常的预处理，如文本二值化，几乎完全抵消敌对的例子。这是第一篇文章表明，对于文本驱动的分类，敌对的例子是学术的好奇心，而不是安全威胁。

##### URL
[https://arxiv.org/abs/1610.04256](https://arxiv.org/abs/1610.04256)

##### PDF
[https://arxiv.org/pdf/1610.04256](https://arxiv.org/pdf/1610.04256)

