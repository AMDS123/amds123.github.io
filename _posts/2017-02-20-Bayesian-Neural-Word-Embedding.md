---
layout: post
title: "Bayesian Neural Word Embedding"
date: 2017-02-20 20:45:33
categories: arXiv_SD
tags: arXiv_SD Embedding Language_Model
author: Oren Barkan
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-Gram with negative sampling, known also as word2vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm. The algorithm relies on a Variational Bayes solution for the Skip-Gram objective and a detailed step by step description is provided. We present experimental results that demonstrate the performance of the proposed algorithm for word analogy and similarity tasks on six different datasets and show it is competitive with the original Skip-Gram method.

##### Abstract (translated by Google)
最近，在自然语言处理领域的一些作品提出了成功的词嵌入方法。其中，负面抽样的Skip-Gram，也被称为word2vec，提高了各种语言学任务的最新水平。在本文中，我们提出了一个可伸缩的贝叶斯神经词嵌入算法。该算法依赖于Skip-Gram目标的变分贝叶斯解决方案，并提供了详细的逐步描述。我们提出的实验结果证明了所提出的算法在六个不同数据集上的词类比和相似度任务的性能，并显示出它与原始跳跃克方法的竞争力。

##### URL
[https://arxiv.org/abs/1603.06571](https://arxiv.org/abs/1603.06571)

##### PDF
[https://arxiv.org/pdf/1603.06571](https://arxiv.org/pdf/1603.06571)

