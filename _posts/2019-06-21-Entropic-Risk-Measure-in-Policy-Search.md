---
layout: post
title: "Entropic Risk Measure in Policy Search"
date: 2019-06-21 12:38:05
categories: arXiv_RO
tags: arXiv_RO Optimization
author: David Nass, Boris Belousov, Jan Peters
mathjax: true
---

* content
{:toc}

##### Abstract
With the increasing pace of automation, modern robotic systems need to act in stochastic, non-stationary, partially observable environments. A range of algorithms for finding parameterized policies that optimize for long-term average performance have been proposed in the past. However, the majority of the proposed approaches does not explicitly take into account the variability of the performance metric, which may lead to finding policies that although performing well on average, can perform spectacularly bad in a particular run or over a period of time. To address this shortcoming, we study an approach to policy optimization that explicitly takes into account higher order statistics of the reward function. In this paper, we extend policy gradient methods to include the entropic risk measure in the objective function and evaluate their performance in simulation experiments and on a real-robot task of learning a hitting motion in robot badminton.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.09090](http://arxiv.org/abs/1906.09090)

##### PDF
[http://arxiv.org/pdf/1906.09090](http://arxiv.org/pdf/1906.09090)

