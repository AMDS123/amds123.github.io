---
layout: post
title: "Towards Conceptual Compression"
date: 2016-04-29 11:02:52
categories: arXiv_CV
tags: arXiv_CV
author: Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, Daan Wierstra
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.

##### Abstract (translated by Google)
我们介绍一个简单的经常变化的自动编码器架构，显着改善图像建模。该系统代表了ImageNet和Omniglot数据集的潜在变量模型的最新技术。我们表明，它自然地将全局概念信息从较低层次的细节中分离出来，从而解决了无监督学习的一个基本的期望特性。此外，限制我们仅存储关于图像的全局信息的可能性允许我们实现高质量的“概念压缩”。

##### URL
[https://arxiv.org/abs/1604.08772](https://arxiv.org/abs/1604.08772)

##### PDF
[https://arxiv.org/pdf/1604.08772](https://arxiv.org/pdf/1604.08772)

