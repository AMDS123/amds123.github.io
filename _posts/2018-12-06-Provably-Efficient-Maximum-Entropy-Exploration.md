---
layout: post
title: "Provably Efficient Maximum Entropy Exploration"
date: 2018-12-06 18:15:44
categories: arXiv_AI
tags: arXiv_AI
author: Elad Hazan, Sham M. Kakade, Karan Singh, Abby Van Soest
mathjax: true
---

* content
{:toc}

##### Abstract
Suppose an agent is in a (possibly unknown) Markov decision process (MDP) in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? One natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense. Despite the corresponding mathematical program being non-convex, our main result provides a provably efficient method (both in terms of sample size and computational complexity) to construct such a maximum-entropy exploratory policy. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.02690](http://arxiv.org/abs/1812.02690)

##### PDF
[http://arxiv.org/pdf/1812.02690](http://arxiv.org/pdf/1812.02690)

