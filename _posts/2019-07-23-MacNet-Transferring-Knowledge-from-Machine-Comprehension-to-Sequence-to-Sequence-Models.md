---
layout: post
title: "MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models"
date: 2019-07-23 04:38:09
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention Summarization NMT
author: Boyuan Pan, Yazheng Yang, Hao Li, Zhou Zhao, Yueting Zhuang, Deng Cai, Xiaofei He
mathjax: true
---

* content
{:toc}

##### Abstract
Machine Comprehension (MC) is one of the core problems in natural language processing, requiring both understanding of the natural language and knowledge about the world. Rapid progress has been made since the release of several benchmark datasets, and recently the state-of-the-art models even surpass human performance on the well-known SQuAD evaluation. In this paper, we transfer knowledge learned from machine comprehension to the sequence-to-sequence tasks to deepen the understanding of the text. We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models. Experiments on neural machine translation (NMT) and abstractive text summarization show that our proposed framework can significantly improve the performance of the baseline models, and our method for the abstractive text summarization achieves the state-of-the-art results on the Gigaword dataset.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.01816](https://arxiv.org/abs/1908.01816)

##### PDF
[https://arxiv.org/pdf/1908.01816](https://arxiv.org/pdf/1908.01816)

