---
layout: post
title: "Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks"
date: 2016-10-26 13:05:39
categories: arXiv_CL
tags: arXiv_CL OCR Speech_Recognition Recognition
author: Carsten Schnober, Steffen Eger, Erik-Lân Do Dinh, Iryna Gurevych
mathjax: true
---

* content
{:toc}

##### Abstract
We analyze the performance of encoder-decoder neural models and compare them with well-known established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs.

##### Abstract (translated by Google)
我们分析编码器 - 解码器神经模型的性能，并将其与已知的已建立的方法进行比较。后者代表不同类别的传统方法，适用于单调的序列到序列任务OCR校正后，拼写校正，字形到音素转换和词形化。这些任务对于包括数字人文，自动文本纠正和语音识别在内的各种高级研究领域具有实际意义。我们调查通用深度学习方法如何适应这些任务，以及它们如何与已建立的和更专业化的方法（包括我们自己的适应修剪后的CRF）相比较。

##### URL
[https://arxiv.org/abs/1610.07796](https://arxiv.org/abs/1610.07796)

##### PDF
[https://arxiv.org/pdf/1610.07796](https://arxiv.org/pdf/1610.07796)

