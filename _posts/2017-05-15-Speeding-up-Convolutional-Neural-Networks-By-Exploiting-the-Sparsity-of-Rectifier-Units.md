---
layout: post
title: "Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units"
date: 2017-05-15 05:03:28
categories: arXiv_CV
tags: arXiv_CV Sparse CNN
author: Shaohuai Shi, Xiaowen Chu
mathjax: true
---

* content
{:toc}

##### Abstract
Rectifier neuron units (ReLUs) have been widely used in deep convolutional networks. An ReLU converts negative values to zeros, and does not change positive values, which leads to a high sparsity of neurons. In this work, we first examine the sparsity of the outputs of ReLUs in some popular deep convolutional architectures. And then we use the sparsity property of ReLUs to accelerate the calculation of convolution by skipping calculations of zero-valued neurons. The proposed sparse convolution algorithm achieves some speedup improvements on CPUs compared to the traditional matrix-matrix multiplication algorithm for convolution when the sparsity is not less than 0.9.

##### Abstract (translated by Google)
整流器神经单元（ReLU）已被广泛应用于深度卷积网络中。 ReLU将负值转换为零，并且不改变正值，这导致神经元的高度稀疏性。在这项工作中，我们首先考察一些流行的深度卷积体系结构中ReLU输出的稀疏性。然后利用ReLUs的稀疏性来跳过零值神经元的计算来加速卷积计算。当稀疏度不小于0.9时，与传统的卷积矩阵乘法算法相比，所提出的稀疏卷积算法在CPU上实现了一些加速改进。

##### URL
[https://arxiv.org/abs/1704.07724](https://arxiv.org/abs/1704.07724)

##### PDF
[https://arxiv.org/pdf/1704.07724](https://arxiv.org/pdf/1704.07724)

