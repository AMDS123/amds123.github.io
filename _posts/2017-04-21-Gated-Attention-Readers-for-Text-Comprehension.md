---
layout: post
title: "Gated-Attention Readers for Text Comprehension"
date: 2017-04-21 18:50:05
categories: arXiv_CL
tags: arXiv_CL Attention Embedding
author: Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at this https URL

##### Abstract (translated by Google)
在本文中，我们研究了对文件回答问题的问题。我们的模型，门控注意（GA）阅读器，集成了多跳体系结构和基于查询嵌入和递归神经网络文档阅读器的中间状态之间的乘性交互作用的新型注意机制。这使读者能够在文档中构建特定于查询的表征符号，以便进行准确的答案选择。 GA阅读器在这个任务的三个基准测试中获得了最新的结果--CNN \＆Daily Mail新闻报道和Who Did What数据集。消融研究证明了乘性相互作用的有效性，并通过比较实施门控注意的替代组成算子来证明。该代码可在此https网址获得

##### URL
[https://arxiv.org/abs/1606.01549](https://arxiv.org/abs/1606.01549)

##### PDF
[https://arxiv.org/pdf/1606.01549](https://arxiv.org/pdf/1606.01549)

