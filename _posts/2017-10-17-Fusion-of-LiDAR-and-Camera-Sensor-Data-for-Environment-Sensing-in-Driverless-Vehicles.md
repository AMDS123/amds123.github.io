---
layout: post
title: "Fusion of LiDAR and Camera Sensor Data for Environment Sensing in Driverless Vehicles"
date: 2017-10-17 12:01:19
categories: arXiv_CV
tags: arXiv_CV Detection
author: Varuna De Silva, Jamie Roche, Ahmet Kondoz
mathjax: true
---

* content
{:toc}

##### Abstract
Driverless vehicles operate by sensing and perceiving its surrounding environment to make the accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of driverless vehicles. The heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a typical free space detection algorithm.

##### Abstract (translated by Google)
无人驾驶车辆通过感知和感知其周围环境来进行操作，以做出准确的驾驶决定。利用LiDAR，雷达，超声波传感器和摄像头等几种不同传感器的组合来感知无人驾驶车辆的周围环境。异构传感器同时捕捉环境的各种物理属性。传感器数据融合需要积极利用传感器的这种多模态性和冗余性来实现对环境的可靠和一致的感知。然而，这些多模式传感器数据流在许多方面彼此不同，例如时间和空间分辨率，数据格式和几何对齐。对于随后的感知算法来利用由多模式感测提供的分集，数据流需要在空间上，在几何上和时间上彼此对齐。在本文中，我们解决了融合光检测和测距（LiDAR）扫描仪和广角单目图像传感器输出的问题。 LiDAR扫描仪和图像传感器的输出具有不同的空间分辨率，需要相互对齐。使用几何模型来对两个传感器输出进行空间对准，然后是基于高斯过程（GP）回归的分辨率匹配算法，以用可量化的不确定性对缺失数据进行插值。结果表明，所提出的传感器数据融合框架显着地帮助后续的感知步骤，如典型的自由空间检测算法的性能改进所示。

##### URL
[https://arxiv.org/abs/1710.06230](https://arxiv.org/abs/1710.06230)

##### PDF
[https://arxiv.org/pdf/1710.06230](https://arxiv.org/pdf/1710.06230)

