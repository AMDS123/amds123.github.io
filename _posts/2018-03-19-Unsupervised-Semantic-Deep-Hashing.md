---
layout: post
title: "Unsupervised Semantic Deep Hashing"
date: 2018-03-19 13:42:23
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval CNN Classification
author: Sheng Jin
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, deep hashing methods have been proved to be efficient since it employs convolutional neural network to learn features and hashing codes simultaneously. However, these methods are mostly supervised. In real-world application, it is a time-consuming and overloaded task for annotating a large number of images. In this paper, we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method, namely unsupervised semantic deep hashing (\textbf{USDH}), uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model: 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy, and 4) invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have demonstrated that \textbf{USDH} outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments on Oxford 17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.

##### Abstract (translated by Google)
近年来，深哈希方法被证明是有效的，因为它使用卷积神经网络来同时学习特征和散列码。但是，这些方法大都是受监督的。在现实世界的应用程序中，这是一个耗时且超载的任务，用于注释大量图像。本文提出了一种新的无监督深度哈希方法用于大规模图像检索。我们的方法，即无监督语义深度散列（\ textbf {USDH}），使用保存在CNN要素层中的语义信息来指导网络的训练。基于VGG-19模型，我们对哈希码学习实施四个标准：1）在哈希空间中保存特征空间的相关信息; 2）最小化二进制代码和散列码之间的量化损失; 3）通过使用最大信息熵来改善散列码中每个比特的使用，以及4）对图像旋转不变。在CIFAR-10和NUSWIDE上的大量实验已经证明\ textbf {USDH}优于几种最先进的无监督哈希方法用于图像检索。我们还对Oxford 17数据集进行了细粒度分类实验，以验证其对其他计算机视觉任务的效率。

##### URL
[https://arxiv.org/abs/1803.06911](https://arxiv.org/abs/1803.06911)

##### PDF
[https://arxiv.org/pdf/1803.06911](https://arxiv.org/pdf/1803.06911)

