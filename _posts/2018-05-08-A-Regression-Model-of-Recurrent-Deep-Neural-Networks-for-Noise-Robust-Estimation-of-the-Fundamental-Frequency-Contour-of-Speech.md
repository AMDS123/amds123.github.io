---
layout: post
title: "A Regression Model of Recurrent Deep Neural Networks for Noise Robust Estimation of the Fundamental Frequency Contour of Speech"
date: 2018-05-08 11:54:06
categories: arXiv_SD
tags: arXiv_SD Tracking RNN Classification
author: Akihiro Kato, Tomi Kinnunen
mathjax: true
---

* content
{:toc}

##### Abstract
The fundamental frequency (F0) contour of speech is a key aspect to represent speech prosody that finds use in speech and spoken language analysis such as voice conversion and speech synthesis as well as speaker and language identification. This work proposes new methods to estimate the F0 contour of speech using deep neural networks (DNNs) and recurrent neural networks (RNNs). They are trained using supervised learning with the ground truth of F0 contours. The latest prior research addresses this problem first as a frame-by-frame-classification problem followed by sequence tracking using deep neural network hidden Markov model (DNN-HMM) hybrid architecture. This study, however, tackles the problem as a regression problem instead, in order to obtain F0 contours with higher frequency resolution from clean and noisy speech. Experiments using PTDB-TUG corpus contaminated with additive noise (NOISEX-92) show the proposed method improves gross pitch error (GPE) by more than 25 % at signal-to-noise ratios (SNRs) between -10 dB and +10 dB as compared with one of the most noise-robust F0 trackers, PEFAC. Furthermore, the performance on fine pitch error (FPE) is improved by approximately 20 % against a state-of-the-art DNN-HMM-based approach.

##### Abstract (translated by Google)
语音的基频（F0）轮廓是表示语音韵律的关键方面，其可用于语音和口语分析，例如语音转换和语音合成以及说话者和语言识别。这项工作提出了使用深度神经网络（DNN）和递归神经网络（RNN）来估计语音的F0轮廓的新方法。他们使用监督式学习和F0轮廓的基本事实进行训练。先前的最新研究首先将这个问题作为一个逐帧分类问题，然后是使用深度神经网络隐马尔可夫模型（DNN-HMM）混合体系结构的序列跟踪。然而，这项研究将问题解决为一个回归问题，以便从干净而嘈杂的语音中获得具有较高频率分辨率的F0轮廓。使用被添加噪声（NOISEX-92）污染的PTDB-TUG语料库的实验表明，所提出的方法在-10dB和+ 10dB之间的信噪比（SNR）下将总体螺距误差（GPE）提高了25％以上与其中一款最具噪声强大的F0追踪器PEFAC相比。此外，针对基于最新DNN-HMM的方法，细节距误差（FPE）的性能提高了大约20％。

##### URL
[http://arxiv.org/abs/1805.02958](http://arxiv.org/abs/1805.02958)

##### PDF
[http://arxiv.org/pdf/1805.02958](http://arxiv.org/pdf/1805.02958)

