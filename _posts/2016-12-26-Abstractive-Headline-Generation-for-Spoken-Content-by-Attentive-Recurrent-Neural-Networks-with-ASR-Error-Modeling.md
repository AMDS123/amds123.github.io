---
layout: post
title: "Abstractive Headline Generation for Spoken Content by Attentive Recurrent Neural Networks with ASR Error Modeling"
date: 2016-12-26 13:13:38
categories: arXiv_CL
tags: arXiv_CL Summarization RNN Deep_Learning
author: Lang-Chi Yu, Hung-yi Lee, Lin-shan Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Headline generation for spoken content is important since spoken content is difficult to be shown on the screen and browsed by the user. It is a special type of abstractive summarization, for which the summaries are generated word by word from scratch without using any part of the original content. Many deep learning approaches for headline generation from text document have been proposed recently, all requiring huge quantities of training data, which is difficult for spoken document summarization. In this paper, we propose an ASR error modeling approach to learn the underlying structure of ASR error patterns and incorporate this model in an Attentive Recurrent Neural Network (ARNN) architecture. In this way, the model for abstractive headline generation for spoken content can be learned from abundant text data and the ASR data for some recognizers. Experiments showed very encouraging results and verified that the proposed ASR error model works well even when the input spoken content is recognized by a recognizer very different from the one the model learned from.

##### Abstract (translated by Google)
口头内容的标题生成是重要的，因为口头内容难以在屏幕上显示并被用户浏览。它是一种特殊类型的抽象概括，其摘要是从头开始逐字地生成的，而不使用原始内容的任何部分。文本文档标题生成的许多深度学习方法最近被提出，都需要大量的训练数据，这对于口头文档摘要来说是困难的。在本文中，我们提出了一种ASR误差建模方法来学习ASR误差模式的基本结构，并将此模型纳入注意力递归神经网络（ARNN）体系结构中。这样，可以从丰富的文本数据和一些识别器的ASR数据中学习用于口头内容的抽象标题生成模型。实验显示了非常令人鼓舞的结果，并证实了即使当输入的口头内容被识别器识别出与模型学到的非常不同的时候，所提出的ASR误差模型也能很好地工作。

##### URL
[https://arxiv.org/abs/1612.08375](https://arxiv.org/abs/1612.08375)

##### PDF
[https://arxiv.org/pdf/1612.08375](https://arxiv.org/pdf/1612.08375)

