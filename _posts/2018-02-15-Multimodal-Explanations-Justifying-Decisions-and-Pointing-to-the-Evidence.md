---
layout: post
title: "Multimodal Explanations: Justifying Decisions and Pointing to the Evidence"
date: 2018-02-15 19:12:03
categories: arXiv_AI
tags: arXiv_AI QA Attention Classification Quantitative VQA Recognition
author: Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, Marcus Rohrbach
mathjax: true
---

* content
{:toc}

##### Abstract
Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.

##### Abstract (translated by Google)
在许多环境中，既有效又可解释的深层模型是可取的;先前可解释的模型一直是单峰的，可以提供基于图像的关注权重可视化或基于文本的事后理由生成。我们提出一种多模式的解释方法，并认为这两种模式提供了互补的说明力量。我们收集两个新的数据集来定义和评估这个任务，并提出一个新的模型，可以提供联合文本理性生成和注意可视化。我们的数据集定义了活动识别任务（ACT-X）和视觉问答应答任务（VQA-X）的分类决策的视觉和文本理由。我们在数量上表明，使用文本解释进行培训不仅可以产生更好的文本理由模型，还可以更好地定位支持决策的证据。我们还定性地展示了视觉解释比文本解释更具洞察力的情况，反之亦然，支持我们的论点：多模式解释模型提供了超越单峰方法的显着优势。

##### URL
[http://arxiv.org/abs/1802.08129](http://arxiv.org/abs/1802.08129)

##### PDF
[http://arxiv.org/pdf/1802.08129](http://arxiv.org/pdf/1802.08129)

