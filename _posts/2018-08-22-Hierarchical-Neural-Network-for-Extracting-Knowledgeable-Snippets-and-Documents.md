---
layout: post
title: "Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents"
date: 2018-08-22 05:57:13
categories: arXiv_AI
tags: arXiv_AI Knowledge Prediction Relation
author: Ganbin Zhou, Rongyu Cao, Xiang Ao, Ping Luo, Fen Lin, Leyu Lin, Qing He
mathjax: true
---

* content
{:toc}

##### Abstract
In this study, we focus on extracting knowledgeable snippets and annotating knowledgeable documents from Web corpus, consisting of the documents from social media and We-media. Informally, knowledgeable snippets refer to the text describing concepts, properties of entities, or relations among entities, while knowledgeable documents are the ones with enough knowledgeable snippets. These knowledgeable snippets and documents could be helpful in multiple applications, such as knowledge base construction and knowledge-oriented service. Previous studies extracted the knowledgeable snippets using the pattern-based method. Here, we propose the semantic-based method for this task. Specifically, a CNN based model is developed to extract knowledgeable snippets and annotate knowledgeable documents simultaneously. Additionally, a "low-level sharing, high-level splitting" structure of CNN is designed to handle the documents from different content domains. Compared with building multiple domain-specific CNNs, this joint model not only critically saves the training time, but also improves the prediction accuracy visibly. The superiority of the proposed method is demonstrated in a real dataset from Wechat public platform.

##### Abstract (translated by Google)
在本研究中，我们专注于从Web语料库中提取知识渊博的片段和注释知识渊博的文档，其中包括来自社交媒体和We-media的文档。非正式地，知识渊博的片段是指描述概念，实体属性或实体之间关系的文本，而知识渊博的文档则是具有足够知识片段的文档。这些知识渊博的片段和文档可能有助于多种应用，例如知识库构建和面向知识的服务。以前的研究使用基于模式的方法提取了知识渊博的片段。在这里，我们提出了基于语义的方法来完成这项任务。具体而言，开发基于CNN的模型以提取知识渊博的片段并同时注释知识渊博的文档。此外，CNN的“低级共享，高级别拆分”结构旨在处理来自不同内容域的文档。与构建多个特定领域的CNN相比，该联合模型不仅可以大大节省训练时间，而且可以明显提高预测精度。在Wechat公共平台的真实数据集中演示了所提方法的优越性。

##### URL
[http://arxiv.org/abs/1808.07228](http://arxiv.org/abs/1808.07228)

##### PDF
[http://arxiv.org/pdf/1808.07228](http://arxiv.org/pdf/1808.07228)

