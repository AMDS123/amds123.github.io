---
layout: post
title: "Robust Incremental Neural Semantic Graph Parsing"
date: 2017-07-27 15:39:41
categories: arXiv_CL
tags: arXiv_CL Attention Embedding
author: Jan Buys, Phil Blunsom
mathjax: true
---

* content
{:toc}

##### Abstract
Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.

##### Abstract (translated by Google)
将句子解析为语言表达的语义表征是自然语言处理的关键目标。然而，统计分析几乎专注于双语依赖或特定领域的逻辑形式。我们提出了一种基于神经编码器 - 解码器转换的解析器，它是最小递归语义（MRS）的第一个全覆盖语义图解析器。模型体系结构使用基于堆栈的嵌入特征，与非特定谓词及其标记对齐一起预测图形。我们的解析器比MRS上基于注意力的基准线更精确，另外还有一个额外的抽象含义表示（AMR）基准测试，GPU批处理使其比基于语法的高精度解析器快一个数量级。此外，MRS解析器的86.69％Smatch评分高于AMR解析的上限，使得MRS成为语义表示的一个有吸引力的选择。

##### URL
[https://arxiv.org/abs/1704.07092](https://arxiv.org/abs/1704.07092)

##### PDF
[https://arxiv.org/pdf/1704.07092](https://arxiv.org/pdf/1704.07092)

