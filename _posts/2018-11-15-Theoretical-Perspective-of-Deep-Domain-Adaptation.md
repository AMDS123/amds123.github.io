---
layout: post
title: "Theoretical Perspective of Deep Domain Adaptation"
date: 2018-11-15 06:27:15
categories: arXiv_AI
tags: arXiv_AI Transfer_Learning
author: Trung Le, Khanh Nguyen, Dinh Phung
mathjax: true
---

* content
{:toc}

##### Abstract
Deep domain adaptation has recently undergone a big success. Compared with shallow domain adaptation, deep domain adaptation has shown higher predictive performance and stronger capacity to tackle structural data (e.g., image and sequential data). The underlying idea of deep domain adaptation is to bridge the gap between source and target domains in a joint feature space so that a supervised classifier trained on labeled source data can be nicely transferred to the target domain. This idea is certainly appealing and motivating, but under the theoretical perspective, none of the theory has been developed to support this. In this paper, we have developed a rigorous theory to explain why we can bridge the relevant gap in an intermediate joint space. Under the light of our proposed theory, it turns out that there is a strong connection between deep domain adaptation and Wasserstein (WS) distance. More specifically, our theory revolves the following points: i) first, we propose a context wherein we can perfectly perform a transfer learning and ii) second, we further prove that by means of bridging the relevant gap and minimizing some reconstruction errors we are minimizing a WS distance between the push forward source distribution and the target distribution via a transport that maps from the source to target domains.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.06199](http://arxiv.org/abs/1811.06199)

##### PDF
[http://arxiv.org/pdf/1811.06199](http://arxiv.org/pdf/1811.06199)

