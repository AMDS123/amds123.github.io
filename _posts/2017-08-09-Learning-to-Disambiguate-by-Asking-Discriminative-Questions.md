---
layout: post
title: "Learning to Disambiguate by Asking Discriminative Questions"
date: 2017-08-09 08:52:25
categories: arXiv_CV
tags: arXiv_CV Image_Caption Weakly_Supervised Caption Quantitative VQA
author: Yining Li, Chen Huang, Xiaoou Tang, Chen-Change Loy
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to ask questions is a powerful tool to gather information in order to learn about the world and resolve ambiguities. In this paper, we explore a novel problem of generating discriminative questions to help disambiguate visual instances. Our work can be seen as a complement and new extension to the rich research studies on image captioning and question answering. We introduce the first large-scale dataset with over 10,000 carefully annotated images-question tuples to facilitate benchmarking. In particular, each tuple consists of a pair of images and 4.6 discriminative questions (as positive samples) and 5.9 non-discriminative questions (as negative samples) on average. In addition, we present an effective method for visual discriminative question generation. The method can be trained in a weakly supervised manner without discriminative images-question tuples but just existing visual question answering datasets. Promising results are shown against representative baselines through quantitative evaluations and user studies.

##### Abstract (translated by Google)
提问的能力是收集信息以了解世界和解决含糊之处的有力工具。在本文中，我们探讨了一个产生判别问题的新问题，以帮助消除视觉实例的歧义。我们的工作可以看作是对图像字幕和问答的丰富研究的补充和新的扩展。我们引入了第一个大型数据集，其中包含10,000多个经过仔细注释的图像 - 问题元组，以便于进行基准测试。特别是，每个元组平均由一对图像和4.6个判别问题（作为正样本）和5.9个非判别问题（作为负样本）组成。此外，我们提出了一种有效的视觉辨别问题生成方法。该方法可以以弱监督的方式训练，而没有有区别的图像 - 问题元组，而只是现有的视觉问题回答数据集。通过定量评估和用户研究，在有代表性的基线上显示出有希望的结果。

##### URL
[https://arxiv.org/abs/1708.02760](https://arxiv.org/abs/1708.02760)

##### PDF
[https://arxiv.org/pdf/1708.02760](https://arxiv.org/pdf/1708.02760)

