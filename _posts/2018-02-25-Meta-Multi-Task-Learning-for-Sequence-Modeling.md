---
layout: post
title: "Meta Multi-Task Learning for Sequence Modeling"
date: 2018-02-25 09:01:25
categories: arXiv_AI
tags: arXiv_AI Knowledge Text_Classification Represenation_Learning Classification
author: Junkun Chen, Xipeng Qiu, Pengfei Liu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic composition functions have been playing a pivotal role in neural representation learning of text sequences. In spite of their success, most existing models suffer from the underfitting problem: they use the same shared compositional function on all the positions in the sequence, thereby lacking expressive power due to incapacity to capture the richness of compositionality. Besides, the composition functions of different tasks are independent and learned from scratch. In this paper, we propose a new sharing scheme of composition function across multiple tasks. Specifically, we use a shared meta-network to capture the meta-knowledge of semantic composition and generate the parameters of the task-specific semantic composition models. We conduct extensive experiments on two types of tasks, text classification and sequence tagging, which demonstrate the benefits of our approach. Besides, we show that the shared meta-knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.

##### Abstract (translated by Google)
语义构成函数在文本序列的神经表示学习中起着举足轻重的作用。尽管他们取得了成功，但大多数现有模型都遭遇了不足的问题：他们在序列中的所有位置上使用相同的共享组合函数，因此缺乏表达能力，因为无法捕获组合性的丰富性。此外，不同任务的组成功能是独立的，从头开始学习。在本文中，我们提出了跨多个任务的组合功能的新分享方案。具体而言，我们使用共享元网络来捕获语义构成的元知识并生成特定于任务的语义构成模型的参数。我们对两种类型的任务进行了广泛的实验，文本分类和序列标签，这些都证明了我们的方法的好处。此外，我们表明，我们提出的模型学到的共享元知识可以被看作是现成的知识并且很容易转移到新的任务上。

##### URL
[http://arxiv.org/abs/1802.08969](http://arxiv.org/abs/1802.08969)

##### PDF
[http://arxiv.org/pdf/1802.08969](http://arxiv.org/pdf/1802.08969)

