---
layout: post
title: "Bridging Knowledge Gaps in Neural Entailment via Symbolic Models"
date: 2018-08-28 14:45:47
categories: arXiv_AI
tags: arXiv_AI Knowledge Prediction
author: Dongyeop Kang, Tushar Khot, Ashish Sabharwal, Peter Clark
mathjax: true
---

* content
{:toc}

##### Abstract
Most textual entailment models focus on lexical gaps between the premise text and the hypothesis, but rarely on knowledge gaps. We focus on filling these knowledge gaps in the Science Entailment task, by leveraging an external structured knowledge base (KB) of science facts. Our new architecture combines standard neural entailment models with a knowledge lookup module. To facilitate this lookup, we propose a fact-level decomposition of the hypothesis, and verifying the resulting sub-facts against both the textual premise and the structured KB. Our model, NSnet, learns to aggregate predictions from these heterogeneous data formats. On the SciTail dataset, NSnet outperforms a simpler combination of the two predictions by 3% and the base entailment model by 5%.

##### Abstract (translated by Google)
大多数文本蕴涵模型都侧重于前提文本和假设之间的词汇差距，但很少涉及知识差距。我们通过利用科学事实的外部结构化知识库（KB），专注于填补科学蕴涵任务中的这些知识空白。我们的新架构将标准神经蕴涵模型与知识查找模块相结合。为了便于查找，我们建议对假设进行事实级分解，并针对文本前提和结构化KB验证结果子事实。我们的模型NSnet学习汇总来自这些异构数据格式的预测。在SciTail数据集上，NSnet的表现优于两个预测的简单组合3％，基本蕴涵模型5％。

##### URL
[http://arxiv.org/abs/1808.09333](http://arxiv.org/abs/1808.09333)

##### PDF
[http://arxiv.org/pdf/1808.09333](http://arxiv.org/pdf/1808.09333)

