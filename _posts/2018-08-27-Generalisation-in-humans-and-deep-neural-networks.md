---
layout: post
title: "Generalisation in humans and deep neural networks"
date: 2018-08-27 09:17:57
categories: arXiv_AI
tags: arXiv_AI CNN Classification Deep_Learning Recognition
author: Robert Geirhos, Carlos R. Medina Temme, Jonas Rauber, Heiko H. Schuett, Matthias Bethge, Felix A. Wichmann
mathjax: true
---

* content
{:toc}

##### Abstract
We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.

##### Abstract (translated by Google)
我们比较了人类和当前卷积深度神经网络（DNN）在12种不同类型图像降级下的物体识别的鲁棒性。首先，使用三个众所周知的DNN（ResNet-152，VGG-19，GoogLeNet），我们发现人类视觉系统对几乎所有测试的图像处理都更加稳健，并且我们观察到人类和DNN之间逐渐出现分类错误模式。当信号变弱时其次，我们证明直接训练失真图像的DNN在他们训练的精确失真类型上始终超越人类表现，但是当在其他失真类型上测试时，它们显示极差的泛化能力。例如，对盐和胡椒噪声的训练并不意味着对均匀白噪声的鲁棒性，反之亦然。因此，训练和测试之间噪声分布的变化对深度学习视觉系统构成了至关重要的挑战，可以在终身机器学习方法中系统地解决。我们的新数据集由83K精心测量的人类心理物理学试验组成，为人类视觉系统设定的图像质量降解提供了终身稳健性的有用参考。

##### URL
[http://arxiv.org/abs/1808.08750](http://arxiv.org/abs/1808.08750)

##### PDF
[http://arxiv.org/pdf/1808.08750](http://arxiv.org/pdf/1808.08750)

