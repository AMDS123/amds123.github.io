---
layout: post
title: "Long-Term Memory Networks for Question Answering"
date: 2017-07-06 20:48:42
categories: arXiv_CV
tags: arXiv_CV Face Inference RNN Memory_Networks
author: Fenglong Ma, Radha Chitta, Saurabh Kataria, Jing Zhou, Palghat Ramesh, Tong Sun, Jing Gao
mathjax: true
---

* content
{:toc}

##### Abstract
Question answering is an important and difficult task in the natural language processing domain, because many basic natural language processing tasks can be cast into a question answering task. Several deep neural network architectures have been developed recently, which employ memory and inference components to memorize and reason over text information, and generate answers to questions. However, a major drawback of many such models is that they are capable of only generating single-word answers. In addition, they require large amount of training data to generate accurate answers. In this paper, we introduce the Long-Term Memory Network (LTMN), which incorporates both an external memory module and a Long Short-Term Memory (LSTM) module to comprehend the input data and generate multi-word answers. The LTMN model can be trained end-to-end using back-propagation and requires minimal supervision. We test our model on two synthetic data sets (based on Facebook's bAbI data set) and the real-world Stanford question answering data set, and show that it can achieve state-of-the-art performance.

##### Abstract (translated by Google)
问答在自然语言处理领域是一个重要而艰巨的任务，因为许多基本的自然语言处理任务都可以投入到问答任务中。最近开发了几种深度神经网络架构，它们使用记忆和推理组件来记忆和推理文本信息，并产生问题的答案。然而，许多这样的模型的一个主要缺点是它们只能产生单字的答案。另外，他们需要大量的训练数据来生成准确的答案。在本文中，我们介绍了长期记忆网络（LTMN），它包含一个外部存储器模块和一个长期短期记忆（LSTM）模块来理解输入数据并生成多词的答案。 LTMN模型可以使用反向传播进行端对端训练，并且需要最少的监督。我们使用两个合成数据集（基于Facebook的bAbI数据集）和真实的斯坦福问答数据集来测试我们的模型，并显示它可以实现最先进的性能。

##### URL
[https://arxiv.org/abs/1707.01961](https://arxiv.org/abs/1707.01961)

##### PDF
[https://arxiv.org/pdf/1707.01961](https://arxiv.org/pdf/1707.01961)

