---
layout: post
title: "Squeeze-SegNet: A new fast Deep Convolutional Neural Network for Semantic Segmentation"
date: 2017-11-15 10:41:52
categories: arXiv_CV
tags: arXiv_CV Object_Detection Segmentation Attention CNN Semantic_Segmentation Classification Deep_Learning Detection
author: Geraldin Nanfack, Azeddine Elhassouny, Rachid Oulad Haj Thami
mathjax: true
---

* content
{:toc}

##### Abstract
The recent researches in Deep Convolutional Neural Network have focused their attention on improving accuracy that provide significant advances. However, if they were limited to classification tasks, nowadays with contributions from Scientific Communities who are embarking in this field, they have become very useful in higher level tasks such as object detection and pixel-wise semantic segmentation. Thus, brilliant ideas in the field of semantic segmentation with deep learning have completed the state of the art of accuracy, however this architectures become very difficult to apply in embedded systems as is the case for autonomous driving. We present a new Deep fully Convolutional Neural Network for pixel-wise semantic segmentation which we call Squeeze-SegNet. The architecture is based on Encoder-Decoder style. We use a SqueezeNet-like encoder and a decoder formed by our proposed squeeze-decoder module and upsample layer using downsample indices like in SegNet and we add a deconvolution layer to provide final multi-channel feature map. On datasets like Camvid or City-states, our net gets SegNet-level accuracy with less than 10 times fewer parameters than SegNet.

##### Abstract (translated by Google)
深度卷积神经网络最近的研究集中在提高准确度方面，提供了重要的进展。然而，如果仅仅局限于分类任务，现在正在进行这一领域的科学界的贡献，他们已经变得非常有用，比如对象检测和像素语义分割等高级任务。因此，深度学习的语义分割领域的出色思想已经完成了精确的技术发展水平，但是这种体系结构在自主驾驶的情况下变得非常难以应用在嵌入式系统中。我们提出了一个新的深层完全卷积神经网络像素的语义分割，我们称之为Squeeze-SegNet。该架构基于编码器 - 解码器风格。我们使用类似SqueezeNet的编码器和由我们提出的挤压 - 解码器模块形成的解码器，并使用像SegNet中的下采样索引的上采样层，并且增加一个解卷积层以提供最终的多通道特征映射。在像Camvid或City-states这样的数据集上，我们的网络获得SegNet级别的精度，参数比SegNet少10倍。

##### URL
[https://arxiv.org/abs/1711.05491](https://arxiv.org/abs/1711.05491)

##### PDF
[https://arxiv.org/pdf/1711.05491](https://arxiv.org/pdf/1711.05491)

