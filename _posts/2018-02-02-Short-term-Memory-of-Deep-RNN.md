---
layout: post
title: "Short-term Memory of Deep RNN"
date: 2018-02-02 16:14:59
categories: arXiv_AI
tags: arXiv_AI GAN RNN Deep_Learning
author: Claudio Gallicchio
mathjax: true
---

* content
{:toc}

##### Abstract
The extension of deep learning towards temporal data processing is gaining an increasing research interest. In this paper we investigate the properties of state dynamics developed in successive levels of deep recurrent neural networks (RNNs) in terms of short-term memory abilities. Our results reveal interesting insights that shed light on the nature of layering as a factor of RNN design. Noticeably, higher layers in a hierarchically organized RNN architecture results to be inherently biased towards longer memory spans even prior to training of the recurrent connections. Moreover, in the context of Reservoir Computing framework, our analysis also points out the benefit of a layered recurrent organization as an efficient approach to improve the memory skills of reservoir models.

##### Abstract (translated by Google)
深度学习向时态数据处理的延伸越来越受到研究者的关注。在本文中，我们研究了在深度递归神经网络（RNNs）的短期记忆能力方面发展的状态动力学的性质。我们的研究结果揭示了有趣的见解，揭示了作为RNN设计因素的分层性质。值得注意的是，在分级组织的RNN体系结构中的更高层，即使在重复连接的训练之前，其本身也会偏向于更长的存储器跨度。此外，在油藏计算框架下，我们的分析也指出了分层递归组织作为提高油藏模型记忆能力的有效方法的好处。

##### URL
[https://arxiv.org/abs/1802.00748](https://arxiv.org/abs/1802.00748)

##### PDF
[https://arxiv.org/pdf/1802.00748](https://arxiv.org/pdf/1802.00748)

