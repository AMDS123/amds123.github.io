---
layout: post
title: "Cross-modal Hallucination for Few-shot Fine-grained Recognition"
date: 2018-06-13 17:06:10
categories: arXiv_CV
tags: arXiv_CV GAN Embedding Deep_Learning Recognition
author: Frederik Pahde, Patrick J&#xe4;hnichen, Tassilo Klein, Moin Nabi
mathjax: true
---

* content
{:toc}

##### Abstract
State-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance, particularly in scenarios with fine-grained boundaries between categories. To this end, we propose a multimodal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multimodal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multimodal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose a framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strat- egy for sample selection. We show the results of our pro- posed discriminative hallucinated method for 1-, 2-, and 5- shot learning on the CUB dataset, where the accuracy is improved by employing multimodal data.

##### Abstract (translated by Google)
最先进的深度学习算法通常需要大量的数据进行模型训练。其缺乏会严重损害性能，特别是在类别之间具有细粒度边界的情况下。为此，我们提出了一种多模式方法，通过有意义的联合嵌入来促进弥​​合信息鸿沟。具体而言，我们提出了一个基准，它是训练期间的多模态（即图像和文本）和单模态测试时间（即图像），以及相关任务，利用基础类中的多模态数据（包含许多样本），学习显式视觉用于新类别的分类器（具有少量样本）。接下来，我们提出一个基于跨模式数据幻觉概念的框架。在这方面，我们引入了一个有区别的文本条件GAN，用于样本选择的简单自定步骤策略。我们展示了我们在CUB数据集上进行1-，2-和5-射击学习的有差别的幻觉方法的结果，其中通过使用多模态数据来提高准确度。

##### URL
[http://arxiv.org/abs/1806.05147](http://arxiv.org/abs/1806.05147)

##### PDF
[http://arxiv.org/pdf/1806.05147](http://arxiv.org/pdf/1806.05147)

