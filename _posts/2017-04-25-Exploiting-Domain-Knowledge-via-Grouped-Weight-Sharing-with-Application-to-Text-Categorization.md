---
layout: post
title: "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization"
date: 2017-04-25 16:33:52
categories: arXiv_SD
tags: arXiv_SD Knowledge Classification
author: Ye Zhang, Matthew Lease, Byron C. Wallace
mathjax: true
---

* content
{:toc}

##### Abstract
A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.

##### Abstract (translated by Google)
NLP的神经模型的一个基本优势是他们从零开始学习表示的能力。然而，实际上这通常意味着忽略现有的外部语言资源，例如WordNet或诸如统一医疗语言系统（UMLS）等领域特定的本体。我们提出了一种通过权重共享来开发这种资源的通用的新颖方法。之前有关神经网络中权重共享的研究主要将其视为模型压缩的手段。相比之下，我们将权重共享作为将先前知识纳入神经模型的灵活机制。我们证明，与不利用权重分享的基准策略相比，这种方法在分类任务上始终如一地提高了性能。

##### URL
[https://arxiv.org/abs/1702.02535](https://arxiv.org/abs/1702.02535)

##### PDF
[https://arxiv.org/pdf/1702.02535](https://arxiv.org/pdf/1702.02535)

