---
layout: post
title: "Skipping Word: A Character-Sequential Representation based Framework for Question Answering"
date: 2016-09-02 11:57:46
categories: arXiv_CL
tags: arXiv_CL Embedding CNN
author: Lingxun Meng, Yan Li, Mengyi Liu, Peng Shu
mathjax: true
---

* content
{:toc}

##### Abstract
Recent works using artificial neural networks based on word distributed representation greatly boost the performance of various natural language learning tasks, especially question answering. Though, they also carry along with some attendant problems, such as corpus selection for embedding learning, dictionary transformation for different learning tasks, etc. In this paper, we propose to straightforwardly model sentences by means of character sequences, and then utilize convolutional neural networks to integrate character embedding learning together with point-wise answer selection training. Compared with deep models pre-trained on word embedding (WE) strategy, our character-sequential representation (CSR) based method shows a much simpler procedure and more stable performance across different benchmarks. Extensive experiments on two benchmark answer selection datasets exhibit the competitive performance compared with the state-of-the-art methods.

##### Abstract (translated by Google)
最近使用基于词分布表示的人工神经网络的工作极大地提高了各种自然语言学习任务的性能，尤其是问题回答。但是，它们也伴随着一些相关的问题，如嵌入式学习的语料库选择，针对不同学习任务的字典变换等。本文提出了通过字符序列直接建模句子，然后利用卷积神经网络将字符嵌入学习与点式答案选择训练相结合。与预先训练词嵌入（WE）策略的深层模型相比，基于字符序列表示（CSR）的方法在不同的基准测试中表现出更简单的过程和更稳定的性能。在两个基准答案选择数据集上的广泛实验显示了与最先进的方法相比的竞争性表现。

##### URL
[https://arxiv.org/abs/1609.00565](https://arxiv.org/abs/1609.00565)

##### PDF
[https://arxiv.org/pdf/1609.00565](https://arxiv.org/pdf/1609.00565)

