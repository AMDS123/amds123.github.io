---
layout: post
title: "Multi-Label Zero-Shot Human Action Recognition via Joint Latent Embedding"
date: 2017-09-15 08:44:02
categories: arXiv_CV
tags: arXiv_CV Knowledge Action_Recognition Embedding Relation Recognition
author: Qian Wang, Ke Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Human action recognition refers to automatic recognizing human actions from a video clip, which is one of the most challenging tasks in computer vision. In reality, a video stream is often weakly-annotated with a set of relevant human action labels at a global level rather than assigning each label to a specific video episode corresponding to a single action, which leads to a multi-label learning problem. Furthermore, there are a great number of meaningful human actions in reality but it would be extremely difficult, if not impossible, to collect/annotate video clips regarding all of various human actions, which leads to a zero-shot learning scenario. To the best of our knowledge, there is no work that has addressed all the above issues together in human action recognition. In this paper, we formulate a real-world human action recognition task as a multi-label zero-shot learning problem and propose a framework to tackle this problem. Our framework simultaneously tackles the issue of unknown temporal boundaries between different actions for multi-label learning and exploits the side information regarding the semantic relationship between different human actions for zero-shot learning. As a result, our framework leads to a joint latent embedding representation for multi-label zero-shot human action recognition. The joint latent embedding is learned with two component models by exploring temporal coherence underlying video data and the intrinsic relationship between visual and semantic domain. We evaluate our framework with different settings, including a novel data split scheme designed especially for evaluating multi-label zero-shot learning, on two weakly annotated multi-label human action datasets: Breakfast and Charades. The experimental results demonstrate the effectiveness of our framework in multi-label zero-shot human action recognition.

##### Abstract (translated by Google)
人类行为识别是指从视频剪辑中自动识别人类行为，这是计算机视觉中最具挑战性的任务之一。实际上，视频流通常在全球层面上用一组相关的人类活动标签进行弱注释，而不是将每个标签分配给对应于单个动作的特定视频片段，这导致多标签学习问题。此外，在现实中存在大量有意义的人类行为，但是收集/注释关于所有各种人类行为的视频剪辑（如果不是不可能的话）是非常困难的，这导致了零点学习场景。据我们所知，在人类行为识别方面，没有任何工作能够解决所有上述问题。在本文中，我们将一个真实世界的人类行为识别任务作为一个多标签零点学习问题，并提出一个框架来解决这个问题。我们的框架同时解决了多标签学习中不同动作之间未知时间边界的问题，并利用不同人类动作之间语义关系的零信息进行零点学习。结果，我们的框架导致了多标签零点人类动作识别的联合潜在嵌入表示。通过探索基于视频数据的时间相关性以及视觉和语义领域之间的内在联系，利用两个组件模型来学习联合潜在嵌入。我们用不同的设置来评估我们的框架，包括一个专门为评估多标签零点学习而设计的新型数据分割方案，在两个带有注释的多标签人体动作数据集上：早餐和Charades。实验结果证明了我们的框架在多标签零点人类动作识别中的有效性。

##### URL
[https://arxiv.org/abs/1709.05107](https://arxiv.org/abs/1709.05107)

##### PDF
[https://arxiv.org/pdf/1709.05107](https://arxiv.org/pdf/1709.05107)

