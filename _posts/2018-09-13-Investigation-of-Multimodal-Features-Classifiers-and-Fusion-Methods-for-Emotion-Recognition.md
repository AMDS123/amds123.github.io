---
layout: post
title: "Investigation of Multimodal Features, Classifiers and Fusion Methods for Emotion Recognition"
date: 2018-09-13 08:56:25
categories: arXiv_AI
tags: arXiv_AI Transfer_Learning Classification Recognition
author: Zheng Lian, Ya Li, Jianhua Tao, Jian Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Automatic emotion recognition is a challenging task. In this paper, we present our effort for the audio-video based sub-challenge of the Emotion Recognition in the Wild (EmotiW) 2018 challenge, which requires participants to assign a single emotion label to the video clip from the six universal emotions (Anger, Disgust, Fear, Happiness, Sad and Surprise) and Neutral. The proposed multimodal emotion recognition system takes audio, video and text information into account. Except for handcraft features, we also extract bottleneck features from deep neutral networks (DNNs) via transfer learning. Both temporal classifiers and non-temporal classifiers are evaluated to obtain the best unimodal emotion classification result. Then possibilities are extracted and passed into the Beam Search Fusion (BS-Fusion). We test our method in the EmotiW 2018 challenge and we gain promising results. Compared with the baseline system, there is a significant improvement. We achieve 60.34% accuracy on the testing dataset, which is only 1.5% lower than the winner. It shows that our method is very competitive.

##### Abstract (translated by Google)
自动情绪识别是一项具有挑战性的任在本文中，我们展示了我们为基于音频视频的野外情绪识别（EmotiW）2018挑战的子挑战所做的努力，该挑战要求参与者从六种普遍情绪中为视频剪辑分配单个情感标签（Anger ，厌恶，恐惧，幸福，悲伤和惊喜）和中立。所提出的多模态情感识别系统考虑了音频，视频和文本信息。除手工功能外，我们还通过传输学习从深度中性网络（DNN）中提取瓶颈功能。评估时间分类器和非时间分类器以获得最佳单峰情绪分类结果。然后提取可能性并将其传递到光束搜索融合（BS-Fusion）。我们在EmotiW 2018挑战中测试了我们的方法，我们获得了有希望的结果。与基线系统相比，有了显着的改进。我们在测试数据集上实现了60.34％的准确率，仅比获胜者低1.5％。它表明我们的方法非常有竞争力。

##### URL
[http://arxiv.org/abs/1809.06225](http://arxiv.org/abs/1809.06225)

##### PDF
[http://arxiv.org/pdf/1809.06225](http://arxiv.org/pdf/1809.06225)

