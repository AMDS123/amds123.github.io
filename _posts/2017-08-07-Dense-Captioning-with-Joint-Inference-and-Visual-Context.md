---
layout: post
title: 'Dense Captioning with Joint Inference and Visual Context'
date: 2017-08-07 23:17:34
categories: arXiv_CV
tags: arXiv_CV Caption
author: Linjie Yang, Kevin Tang, Jianchao Yang, Li-Jia Li
---

* content
{:toc}

##### Abstract
Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model, compact and efficient, achieves state-of-the-art accuracy on Visual Genome for dense captioning with a relative gain of 73\% compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning.

##### Abstract (translated by Google)
密集字幕是一种新兴的计算机视觉主题，用于理解密集的语言描述的图像。目标是从图像中密集地检测视觉概念（例如，对象，对象部分和它们之间的交互），用短的描述性短语来标记每个概念。我们确定密集字幕在解决问题时需要妥善处理的两个关键挑战。首先，每个图像中密集的视觉概念注释与高度重叠的目标区域相关联，使得每个视觉概念的准确定位具有挑战性。其次，大量的视觉概念使得难以仅凭外观来识别它们中的每一个。我们提出了一个基于两个新思想，联合推理和上下文融合的新模型管道，以缓解这两个挑战。我们以有条不紊的方式设计我们的模型架构，并彻底评估架构的变化。我们的最后一个模型紧凑而高效，与以前的最佳算法相比，在密集字幕的Visual Genome上实现了最高的精度，相对增益为73％。定性实验也揭示了我们的模型在密集字幕中的语义能力。

##### URL
[https://arxiv.org/abs/1611.06949](https://arxiv.org/abs/1611.06949)

