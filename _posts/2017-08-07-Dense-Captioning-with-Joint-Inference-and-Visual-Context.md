---
layout: post
title: "Dense Captioning with Joint Inference and Visual Context"
date: 2017-08-07 23:17:34
categories: arXiv_CV
tags: arXiv_CV Caption Inference
author: Linjie Yang, Kevin Tang, Jianchao Yang, Li-Jia Li
mathjax: true
---

* content
{:toc}

##### Abstract
Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model, compact and efficient, achieves state-of-the-art accuracy on Visual Genome for dense captioning with a relative gain of 73\% compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning.

##### Abstract (translated by Google)
密集字幕是一种新兴的计算机视觉主题，用于理解具有密集语言描述的图像。目标是从图像中密集地检测视觉概念（例如，对象，对象部分和它们之间的交互），用简短的描述性短语标记每个概念。我们确定了在解决问题时需要妥善解决的密集字幕的两个关键挑战。首先，每个图像中的密集视觉概念注释与高度重叠的目标区域相关联，使得每个视觉概念的精确定位具有挑战性。其次，大量的视觉概念使得很难仅通过外观来识别它们。我们提出了一个新的模型管道，基于两个新的想法，联合推理和上下文融合，以缓解这两个挑战。我们以有条不紊的方式设计我们的模型架构，并彻底评估架构的变化。我们的最终模型，紧凑而高效，在Visual Genome上实现了最先进的精确度，用于密集字幕，与之前的最佳算法相比，相对增益为73％。定性实验还揭示了我们的模型在密集字幕中的语义能力。

##### URL
[https://arxiv.org/abs/1611.06949](https://arxiv.org/abs/1611.06949)

##### PDF
[https://arxiv.org/pdf/1611.06949](https://arxiv.org/pdf/1611.06949)

