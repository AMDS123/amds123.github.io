---
layout: post
title: "CrossNets: Cross-Information Flow in Deep Learning Architectures"
date: 2018-07-15 23:57:53
categories: arXiv_CV
tags: arXiv_CV Image_Classification Classification Deep_Learning
author: Chirag Agarwal, Joe Klobusicky, Mehdi Sharifzhadeh, Dan Schonfeld
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a novel neural network structure called CrossNets, which considers architectures on directed acyclic graphs. This structure builds on previous generalization of sequential feed-forward models, such as ResNets, by allowing for all forward cross-connections between both adjacent and non-adjacent layers. The addition of cross-connections within the network increases the information flow across the whole network, leading to better training and testing performances. The superior performance of the network is tested against both image classification and compression tasks using various datasets, such as MNIST, FER, CIFAR-10, CIFAR-100, and SVHN. We conclude with a proof of convergence for CrossNets to a local minimum for error, where weights for connections are chosen through backpropagation with momentum.

##### Abstract (translated by Google)
我们提出了一种名为CrossNets的新型神经网络结构，它考虑了有向无环图的结构。该结构建立在先前对顺序前馈模型（例如ResNets）的推广的基础上，允许相邻和非相邻层之间的所有前向交叉连接。在网络中增加交叉连接增加了整个网络的信息流，从而带来更好的培训和测试性能。使用各种数据集（例如MNIST，FER，CIFAR-10，CIFAR-100和SVHN）针对图像分类和压缩任务测试网络的卓越性能。最后，我们得出了CrossNets收敛到局部最小误差的证明，其中连接的权重是通过动量的反向传播来选择的。

##### URL
[http://arxiv.org/abs/1705.07404](http://arxiv.org/abs/1705.07404)

##### PDF
[http://arxiv.org/pdf/1705.07404](http://arxiv.org/pdf/1705.07404)

