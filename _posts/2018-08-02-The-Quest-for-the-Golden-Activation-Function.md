---
layout: post
title: "The Quest for the Golden Activation Function"
date: 2018-08-02 12:44:09
categories: arXiv_CV
tags: arXiv_CV Image_Classification Classification
author: Mina Basirat, Peter M. Roth
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks have been shown to be beneficial for a variety of tasks, in particular allowing for end-to-end learning and reducing the requirement for manual design decisions. However, still many parameters have to be chosen in advance, also raising the need to optimize them. One important, but often ignored system parameter is the selection of a proper activation function. Thus, in this paper we target to demonstrate the importance of activation functions in general and show that for different tasks different activation functions might be meaningful. To avoid the manual design or selection of activation functions, we build on the idea of genetic algorithms to learn the best activation function for a given task. In addition, we introduce two new activation functions, ELiSH and HardELiSH, which can easily be incorporated in our framework. In this way, we demonstrate for three different image classification benchmarks that different activation functions are learned, also showing improved results compared to typically used baselines.

##### Abstract (translated by Google)
深度神经网络已被证明对各种任务有益，特别是允许端到端学习并减少对手动设计决策的要求。但是，仍然需要提前选择许多参数，这也增加了对它们进行优化的需要。一个重要但经常被忽略的系统参数是选择适当的激活函数。因此，在本文中，我们的目标是一般地展示激活函数的重要性，并表明对于不同的任务，不同的激活函数可能是有意义的。为了避免手动设计或激活功能的选择，我们建立了遗传算法的思想，以学习给定任务的最佳激活函数。此外，我们还介绍了两个新的激活函数，ELiSH和HardELiSH，它们可以很容易地合并到我们的框架中。通过这种方式，我们展示了三种不同的图像分类基准，即学习了不同的激活函数，并且与通常使用的基线相比也显示出改进的结果。

##### URL
[http://arxiv.org/abs/1808.00783](http://arxiv.org/abs/1808.00783)

##### PDF
[http://arxiv.org/pdf/1808.00783](http://arxiv.org/pdf/1808.00783)

