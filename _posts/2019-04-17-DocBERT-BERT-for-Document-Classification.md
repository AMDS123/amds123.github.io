---
layout: post
title: "DocBERT: BERT for Document Classification"
date: 2019-04-17 17:55:18
categories: arXiv_CL
tags: arXiv_CL Sentiment Attention Sentiment_Classification Inference Classification
author: Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin
mathjax: true
---

* content
{:toc}

##### Abstract
Pre-trained language representation models achieve remarkable state of the art across a wide range of tasks in natural language processing. One of the latest advancements is BERT, a deep pre-trained transformer that yields much better results than its predecessors do. Despite its burgeoning popularity, however, BERT has not yet been applied to document classification. This task deserves attention, since it contains a few nuances: first, modeling syntactic structure matters less for document classification than for other problems, such as natural language inference and sentiment classification. Second, documents often have multiple labels across dozens of classes, which is uncharacteristic of the tasks that BERT explores. In this paper, we describe fine-tuning BERT for document classification. We are the first to demonstrate the success of BERT on this task, achieving state of the art across four popular datasets.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.08398](http://arxiv.org/abs/1904.08398)

##### PDF
[http://arxiv.org/pdf/1904.08398](http://arxiv.org/pdf/1904.08398)

