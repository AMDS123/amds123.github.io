---
layout: post
title: "Microsoft's Submission to the WMT2018 News Translation Task: How I Learned to Stop Worrying and Love the Data"
date: 2018-09-01 14:33:11
categories: arXiv_CL
tags: arXiv_CL
author: Marcin Junczys-Dowmunt
mathjax: true
---

* content
{:toc}

##### Abstract
This paper describes the Microsoft submission to the WMT2018 news translation shared task. We participated in one language direction -- English-German. Our system follows current best-practice and combines state-of-the-art models with new data filtering (dual conditional cross-entropy filtering) and sentence weighting methods. We trained fairly standard Transformer-big models with an updated version of Edinburgh's training scheme for WMT2017 and experimented with different filtering schemes for Paracrawl. According to automatic metrics (BLEU) we reached the highest score for this subtask with a nearly 2 BLEU point margin over the next strongest system. Based on human evaluation we ranked first among constrained systems. We believe this is mostly caused by our data filtering/weighting regime.

##### Abstract (translated by Google)
本文介绍了Microsoft向WMT2018提交的新闻翻译共享任务。我们参加了一个语言方向 - 英语 - 德语。我们的系统遵循当前的最佳实践，并将最先进的模型与新的数据过滤（双条件交叉熵过滤）和句子加权方法相结合。我们使用更新版本的爱丁堡WMT2017培训计划培训了相当标准的Transformer-large模型，并为Paracrawl试验了不同的过滤方案。根据自动指标（BLEU），我们在这个子任务中达到了最高分，在下一个最强系统上有近2个BLEU点差。基于人类评估，我们在受约束系统中排名第一。我们认为这主要是由我们的数据过滤/加权制度引起的。

##### URL
[http://arxiv.org/abs/1809.00196](http://arxiv.org/abs/1809.00196)

##### PDF
[http://arxiv.org/pdf/1809.00196](http://arxiv.org/pdf/1809.00196)

