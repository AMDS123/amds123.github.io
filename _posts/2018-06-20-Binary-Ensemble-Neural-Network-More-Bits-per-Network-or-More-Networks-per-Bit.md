---
layout: post
title: "Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?"
date: 2018-06-20 04:48:18
categories: arXiv_AI
tags: arXiv_AI
author: Shilin Zhu, Xin Dong, Hao Su
mathjax: true
---

* content
{:toc}

##### Abstract
Binary neural networks (BNN) have been studied extensively since they run dramatically faster at lower memory and power consumption than floating-point networks, thanks to the efficiency of bit operations. However, contemporary BNNs whose weights and activations are both single bits suffer from severe accuracy degradation. To understand why, we investigate the representation ability, speed and bias/variance of BNNs through extensive experiments. We conclude that the error of BNNs is predominantly caused by the intrinsic instability (training time) and non-robustness (train \&amp; test time). Inspired by this investigation, we propose the Binary Ensemble Neural Network (BENN) which leverages ensemble methods to improve the performance of BNNs with limited efficiency cost. While ensemble techniques have been broadly believed to be only marginally helpful for strong classifiers such as deep neural networks, our analyses and experiments show that they are naturally a perfect fit to boost BNNs. We find that our BENN, which is faster and much more robust than state-of-the-art binary networks, can even surpass the accuracy of the full-precision floating number network with the same architecture.

##### Abstract (translated by Google)
二进制神经网络（BNN）已经得到了广泛的研究，因为比特操作的效率，它们在低内存和低功耗的情况下比浮点网络运行速度更快。然而，当前BNN的权重和激活都是单比特，严重精度下降。为了理解为什么，我们通过广泛的实验来研究BNN的表示能力，速度和偏差/方差。我们得出结论，BNN的误差主要是由固有的不稳定性（训练时间）和非鲁棒性（训练和测试时间）造成的。受此调查的启发，我们提出二元集成神经网络（BENN），它利用集成方法以有限的效率成本提高BNN的性能。虽然集合技术被广泛认为对强分类器（如深度神经网络）仅有一点帮助，但我们的分析和实验表明，它们自然地完美适合于提升BNN。我们发现我们的BENN比现有技术的二进制网络更快，更稳健，甚至可以超过具有相同架构的全精度浮点数网络的精度。

##### URL
[http://arxiv.org/abs/1806.07550](http://arxiv.org/abs/1806.07550)

##### PDF
[http://arxiv.org/pdf/1806.07550](http://arxiv.org/pdf/1806.07550)

