---
layout: post
title: "Graph Neural Networks: A Review of Methods and Applications"
date: 2019-07-10 14:50:01
categories: arXiv_AI
tags: arXiv_AI Review Attention Face Survey CNN Optimization Relation
author: Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network (GCN), graph attention network (GAT), gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.08434](http://arxiv.org/abs/1812.08434)

##### PDF
[http://arxiv.org/pdf/1812.08434](http://arxiv.org/pdf/1812.08434)

