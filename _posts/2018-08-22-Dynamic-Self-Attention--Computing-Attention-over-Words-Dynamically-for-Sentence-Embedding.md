---
layout: post
title: "Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding"
date: 2018-08-22 14:30:03
categories: arXiv_CL
tags: arXiv_CL Sentiment Attention Embedding Inference
author: Deunsol Yoon, Dongbok Lee, SangKeun Lee
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention mechanism for sentence embedding. We design DSA by modifying dynamic routing in capsule network (Sabouretal.,2017) for natural language processing. DSA attends to informative words with a dynamic weight vector. We achieve new state-of-the-art results among sentence encoding methods in Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, while showing comparative results in Stanford Sentiment Treebank (SST) dataset.

##### Abstract (translated by Google)
在本文中，我们提出动态自注意（DSA），一种新的句子嵌入自我注意机制。我们通过修改胶囊网络（Sabouretal。，2017）中的动态路由来设计DSA以进行自然语言处理。 DSA通过动态权重向量来处理信息性词语。我们在具有最少参数的斯坦福自然语言推理（SNLI）数据集中的句子编码方法中实现了新的最新结果，同时在斯坦福情感树库（SST）数据集中显示了比较结果。

##### URL
[http://arxiv.org/abs/1808.07383](http://arxiv.org/abs/1808.07383)

##### PDF
[http://arxiv.org/pdf/1808.07383](http://arxiv.org/pdf/1808.07383)

