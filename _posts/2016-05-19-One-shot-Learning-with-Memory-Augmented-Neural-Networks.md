---
layout: post
title: "One-shot Learning with Memory-Augmented Neural Networks"
date: 2016-05-19 17:44:51
categories: arXiv_CV
tags: arXiv_CV Prediction
author: Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap
mathjax: true
---

* content
{:toc}

##### Abstract
Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.

##### Abstract (translated by Google)
尽管最近在深度神经网络的应用方面取得了突破，但是一个持续的挑战是“一次学习”。传统的基于梯度的网络需要大量的数据来学习，通常需要大量的迭代训练。当遇到新的数据时，模型必须低效地重新学习他们的参数，以充分地结合新的信息而没有灾难性的干扰。具有增强存储容量的体系结构（如神经图灵机（NTM））提供了快速编码和检索新信息的能力，因此可以避免传统模型的缺点。在这里，我们展示了一个记忆增强的神经网络能够快速吸收新的数据，并利用这些数据在仅仅几个样本之后做出准确的预测。我们还介绍了一种访问外部存储器的新方法，该方法专注于存储器内容，而不像以前使用基于存储器位置的聚焦机制的方法。

##### URL
[https://arxiv.org/abs/1605.06065](https://arxiv.org/abs/1605.06065)

##### PDF
[https://arxiv.org/pdf/1605.06065](https://arxiv.org/pdf/1605.06065)

