---
layout: post
title: "Relation-Aware Graph Attention Network for Visual Question Answering"
date: 2019-08-15 03:59:32
categories: arXiv_CV
tags: arXiv_CV QA Attention Relation VQA
author: Linjie Li, Zhe Gan, Yu Cheng, Jingjing Liu
mathjax: true
---

* content
{:toc}

##### Abstract
In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1903.12314](https://arxiv.org/abs/1903.12314)

##### PDF
[https://arxiv.org/pdf/1903.12314](https://arxiv.org/pdf/1903.12314)

