---
layout: post
title: "EV-Action: Electromyography-Vision Multi-Modal Action Dataset"
date: 2019-04-20 08:06:40
categories: arXiv_CV
tags: arXiv_CV Knowledge Action_Recognition Recognition
author: Lichen Wang, Bin Sun, Joseph Robinson, Taotao Jing, Yun Fu
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-modal human motion analysis is a critical and attractive research topic. Most existing multi-modal action datasets only provide visual modalities such as RGB, depth, or low quality skeleton data. In this paper, we introduce a new, large-scale dataset named EV-Action dataset. It consists RGB, depth, electromyography (EMG), and two skeleton modalities. Compared with others, our dataset has two major improvements: (1) we deploy a motion capturing system to obtain high quality skeleton modality, which provides more comprehensive motion information including skeleton, trajectory, and acceleration with higher accuracy, sample frequency, and more skeleton markers. (2) we include an EMG modality. While EMG is used as an effective indicator for biomechanics area, it has yet to be well explored in multimedia, computer vision, and machine learning areas. To the best of our knowledge, this is the first action dataset with EMG modality. In this paper, we introduce the details of EV-Action dataset. A simple yet effective framework for EMG-based action recognition is proposed. Moreover, we provide state-of-the-art baselines for each modality. The approaches achieve considerable improvements when EMG is involved, and it demonstrates the effectiveness of EMG modality in human action analysis tasks. We hope this dataset could make significant contributions to signal processing, multimedia, computer vision, machine learning, biomechanics, and other interdisciplinary fields.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.12602](http://arxiv.org/abs/1904.12602)

##### PDF
[http://arxiv.org/pdf/1904.12602](http://arxiv.org/pdf/1904.12602)

