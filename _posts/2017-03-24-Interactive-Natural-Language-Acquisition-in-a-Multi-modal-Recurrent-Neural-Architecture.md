---
layout: post
title: "Interactive Natural Language Acquisition in a Multi-modal Recurrent Neural Architecture"
date: 2017-03-24 17:13:08
categories: arXiv_SD
tags: arXiv_SD Knowledge GAN
author: Stefan Heinrich, Stefan Wermter
mathjax: true
---

* content
{:toc}

##### Abstract
The human brain is one of the most complex dynamic systems that enables us to communicate in natural language. We have a good understanding of some principles underlying natural languages and language processing, some knowledge about socio-cultural conditions framing acquisition, and some insights about where activity is occurring in the brain. However, we were not yet able to understand the behavioural and mechanistic characteristics for natural language and how mechanisms in the brain allow to acquire and process language. In an effort to bridge the gap between insights from behavioural psychology and neuroscience, the goal of this paper is to contribute a computational understanding of the appropriate characteristics that favour language acquisition, in a brain-inspired neural architecture. Accordingly, we provide concepts and refinements in cognitive modelling regarding principles and mechanisms in the brain - such as the hierarchical abstraction of context - in a plausible recurrent architecture. On this basis, we propose neurocognitively plausible model for embodied language acquisition from real world interaction of a humanoid robot with its environment. The model is capable of learning language production grounded in both, temporal dynamic somatosensation and vision. In particular, the architecture consists of a continuous time recurrent neural network, where parts have different leakage characteristics and thus operate on multiple timescales for every modality and the association of the higher level nodes of all modalities into cell assemblies. Thus, this model features hierarchical concept abstraction in sensation as well as concept decomposition in production, multi-modal integration, and self-organisation of latent representations.

##### Abstract (translated by Google)
人脑是使我们能够用自然语言进行交流的最复杂的动态系统之一。我们对自然语言和语言处理的一些基本原理有了很好的理解，对构建习得的社会文化条件有一定的了解，以及对大脑中活动发生的一些见解。然而，我们还不能理解自然语言的行为和机制特征，以及大脑中的机制如何获取和处理语言。为了弥合行为心理学和神经科学的见解之间的差距，本文的目标是提供一个有利于语言习得的合适特征的计算理解，在大脑启发的神经结构中。因此，我们在认知建模中提供关于大脑中的原理和机制的概念和改进 - 例如上下文的层次抽象 - 在一个似是而非的经常性架构中。在此基础上，本文提出了从仿人机器人与其环境的真实世界交互中获取具体语言的神经认知模型。这个模型能够学习基于时间动态的体感和视觉的语言生产。具体来说，该架构由连续时间递归神经网络组成，其中部分具有不同的泄漏特性，因此针对每种模式在多个时间尺度上运行，并且将所有模态的较高层节点关联到单元组件中。因此，该模型具有感觉的分层概念抽象，以及生产中的概念分解，多模态集成以及潜在表示的自组织。

##### URL
[https://arxiv.org/abs/1703.08513](https://arxiv.org/abs/1703.08513)

##### PDF
[https://arxiv.org/pdf/1703.08513](https://arxiv.org/pdf/1703.08513)

