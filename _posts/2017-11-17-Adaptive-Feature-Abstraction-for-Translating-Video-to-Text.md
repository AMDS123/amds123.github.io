---
layout: post
title: 'Adaptive Feature Abstraction for Translating Video to Text'
date: 2017-11-17 05:13:16
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption CNN
author: Yunchen Pu, Martin Renqiang Min, Zhe Gan, Lawrence Carin
---

* content
{:toc}

##### Abstract
Previous models for video captioning often use the output from a specific layer of a Convolutional Neural Network (CNN) as video features. However, the variable context-dependent semantics in the video may make it more appropriate to adaptively select features from the multiple CNN layers. We propose a new approach for generating adaptive spatiotemporal representations of videos for the captioning task. A novel attention mechanism is developed, that adaptively and sequentially focuses on different layers of CNN features (levels of feature "abstraction"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.

##### Abstract (translated by Google)
视频字幕的以前的模型经常使用来自卷积神经网络（CNN）的特定层的输出作为视频特征。然而，视频中的可变的依赖于上下文的语义可以使得从多个CNN层自适应地选择特征更适合。我们提出了一种新的方法来为字幕任务生成视频的自适应时空表示。开发了一种新颖的关注机制，自适应地依次关注CNN特征的不同层次（特征“抽象层次”）以及各层特征地图的局部时空区域。建议的方法在三个基准数据集上进行评估：YouTube2Text，M-VAD和MSR-VTT。除了可视化的结果和如何模型的工作，这些实验定量地展示了提出的自适应时空特征抽象的有效性，将视频翻译成丰富的语义句子。

##### URL
[https://arxiv.org/abs/1611.07837](https://arxiv.org/abs/1611.07837)

