---
layout: post
title: "Impossibility of deducing preferences and rationality from human policy"
date: 2018-09-06 16:36:54
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Stuart Armstrong, S&#xf6;ren Mindermann
mathjax: true
---

* content
{:toc}

##### Abstract
Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, there has been little analysis of the general problem of inferring the reward of a human of unknown rationality. The observed behavior can, in principle, be decomposed into two components: a reward function and a planning algorithm, both of which have to be inferred from behavior. This paper presents a No Free Lunch theorem, showing that, without making `normative' assumptions beyond the data, nothing about the human reward function can be deduced from human behavior. Unlike most No Free Lunch theorems, this cannot be alleviated by regularising with simplicity assumptions. We show that the simplest hypotheses which explain the data are generally degenerate.

##### Abstract (translated by Google)
反强化学习（IRL）试图从观察到的行为中推断出人类的奖励或偏好。由于人类规划系统地偏离了理性，因此已经尝试了几种方法来解释特定的人类缺点。然而，对于推断未知理性的人的奖赏的一般问题几乎没有分析。原则上，观察到的行为可以分解为两个部分：奖励函数和计划算法，两者都必须从行为中推断出来。本文提出了一个免费午餐定理，表明在没有超出数据的“规范”假设的情况下，人类行为可以推断出人类奖励函数的任何内容。与大多数免费午餐定理不同，这不能通过简单假设的正则化来缓解。我们证明解释数据的最简单假设通常是退化的。

##### URL
[http://arxiv.org/abs/1712.05812](http://arxiv.org/abs/1712.05812)

##### PDF
[http://arxiv.org/pdf/1712.05812](http://arxiv.org/pdf/1712.05812)

