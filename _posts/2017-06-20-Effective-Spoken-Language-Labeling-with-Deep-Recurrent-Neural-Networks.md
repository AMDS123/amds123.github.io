---
layout: post
title: "Effective Spoken Language Labeling with Deep Recurrent Neural Networks"
date: 2017-06-20 09:44:52
categories: arXiv_CL
tags: arXiv_CL Embedding RNN
author: Marco Dinarelli, Yoann Dupont, Isabelle Tellier
mathjax: true
---

* content
{:toc}

##### Abstract
Understanding spoken language is a highly complex problem, which can be decomposed into several simpler tasks. In this paper, we focus on Spoken Language Understanding (SLU), the module of spoken dialog systems responsible for extracting a semantic interpretation from the user utterance. The task is treated as a labeling problem. In the past, SLU has been performed with a wide variety of probabilistic models. The rise of neural networks, in the last couple of years, has opened new interesting research directions in this domain. Recurrent Neural Networks (RNNs) in particular are able not only to represent several pieces of information as embeddings but also, thanks to their recurrent architecture, to encode as embeddings relatively long contexts. Such long contexts are in general out of reach for models previously used for SLU. In this paper we propose novel RNNs architectures for SLU which outperform previous ones. Starting from a published idea as base block, we design new deep RNNs achieving state-of-the-art results on two widely used corpora for SLU: ATIS (Air Traveling Information System), in English, and MEDIA (Hotel information and reservation in France), in French.

##### Abstract (translated by Google)
了解口语是一个非常复杂的问题，可以分解成几个更简单的任务。在本文中，我们将重点放在口语语言理解（SLU）上，这个口语对话系统负责从用户话语中提取语义解释。该任务被视为标签问题。过去，SLU已经有了各种各样的概率模型。神经网络的兴起，在过去的几年中，开辟了这个领域新的有趣的研究方向。递归神经网络（RNN）尤其不仅能够将多条信息表示为嵌入，而且由于它们的经常性体系结构，因此将其编码为嵌入相对较长的上下文。对于之前用于SLU的模型而言，这种长期的背景一般遥遥无期。在本文中，我们提出了新的用于SLU的RNNs架构，其性能优于以前的架构。从已发表的基本模块出发，我们设计了新的深度RNN，在两个广泛使用的SLU语料库：ATIS（航空旅行信息系统），英语和MEDIA（酒店信息和预订法国）。

##### URL
[https://arxiv.org/abs/1706.06896](https://arxiv.org/abs/1706.06896)

##### PDF
[https://arxiv.org/pdf/1706.06896](https://arxiv.org/pdf/1706.06896)

