---
layout: post
title: "Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks"
date: 2018-02-12 13:37:09
categories: arXiv_CV
tags: arXiv_CV Adversarial Prediction
author: Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama
mathjax: true
---

* content
{:toc}

##### Abstract
High sensitivity of neural networks against malicious perturbations on inputs causes security concerns. We aim to ensure perturbation invariance in their predictions. However, prior work requires strong assumptions on network structures and massive computational costs, and thus their applications are limited. In this paper, based on Lipschitz constants and prediction margins, we present a widely applicable and computationally efficient method to lower-bound the size of adversarial perturbations that networks can never be deceived. Moreover, we propose an efficient training procedure to strengthen perturbation invariance. In experimental evaluations, our method showed its ability to provide a strong guarantee for even large networks.

##### Abstract (translated by Google)
神经网络对输入恶意扰动的高度敏感性引起安全问题。我们的目标是确保其预测中的摄动不变性。然而，之前的工作需要对网络结构和大量计算成本进行强有力的假设，因此它们的应用受到限制。在本文中，基于Lipschitz常数和预测余量，我们提出了一种广泛适用和计算有效的方法来降低对抗扰动的大小，网络永远不会被欺骗。此外，我们提出一个有效的训练程序来加强摄动不变性。在实验评估中，我们的方法显示了其为大型网络提供有力保证的能力。

##### URL
[http://arxiv.org/abs/1802.04034](http://arxiv.org/abs/1802.04034)

##### PDF
[http://arxiv.org/pdf/1802.04034](http://arxiv.org/pdf/1802.04034)

