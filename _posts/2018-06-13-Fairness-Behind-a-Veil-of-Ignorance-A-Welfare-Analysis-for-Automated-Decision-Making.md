---
layout: post
title: "Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making"
date: 2018-06-13 11:36:05
categories: arXiv_AI
tags: arXiv_AI Attention Face Prediction
author: Hoda Heidari, Claudio Ferrari, Krishna P. Gummadi, Andreas Krause
mathjax: true
---

* content
{:toc}

##### Abstract
We draw attention to an important, yet largely overlooked aspect of evaluating fairness for automated decision making systems---namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics. We come to this proposal by taking the perspective of a rational, risk-averse individual who is going to be subject to algorithmic decision making and is faced with the task of choosing between several algorithmic alternatives behind a Rawlsian veil of ignorance. The convex formulation of our measures allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of individual fairness. Furthermore and perhaps most importantly, our work provides both theoretical and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level (un)fairness.

##### Abstract (translated by Google)
我们将注意力集中在评估自动化决策制度公平性的一个重要的但很大程度上被忽略的方面---即风险和福利考虑。我们提出的措施系列符合经济学中基本社会福利的长期制定。我们通过采用理性的，规避风险的个人的角度来看待这个提议，他们将接受算法决策，并且面临着在罗尔斯式无知背后的几种算法选择之间进行选择的任务。我们的措施的凸形式允许我们将它们作为约束集成到任何凸损失最小化管线中。我们的实证分析揭示了我们的建议与（a）预测准确性，（b）群体歧视，以及（c）Dwork等人关于个人公平的概念之间有趣的折衷。此外，也许最重要的是，我们的工作提供了理论和经验证据，表明我们的措施的下限常常导致算法结果中的有界不平等;因此提出了用于界定个人层面（非）公平的第一个计算上可行的机制。

##### URL
[http://arxiv.org/abs/1806.04959](http://arxiv.org/abs/1806.04959)

##### PDF
[http://arxiv.org/pdf/1806.04959](http://arxiv.org/pdf/1806.04959)

