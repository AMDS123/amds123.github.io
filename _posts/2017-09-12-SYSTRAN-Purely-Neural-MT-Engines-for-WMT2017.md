---
layout: post
title: "SYSTRAN Purely Neural MT Engines for WMT2017"
date: 2017-09-12 12:47:05
categories: arXiv_CL
tags: arXiv_CL Attention NMT RNN
author: Yongchao Deng, Jungi Kim, Guillaume Klein, Catherine Kobus, Natalia Segal, Christophe Servan, Bo Wang, Dakun Zhang, Josep Crego, Jean Senellart
mathjax: true
---

* content
{:toc}

##### Abstract
This paper describes SYSTRAN's systems submitted to the WMT 2017 shared news translation task for English-German, in both translation directions. Our systems are built using OpenNMT, an open-source neural machine translation system, implementing sequence-to-sequence models with LSTM encoder/decoders and attention. We experimented using monolingual data automatically back-translated. Our resulting models are further hyper-specialised with an adaptation technique that finely tunes models according to the evaluation test sentences.

##### Abstract (translated by Google)
本文介绍了SYSTRAN系统在翻译方向上提交给WMT 2017共享英德新闻翻译任务。我们的系统使用OpenNMT（一种开源的神经机器翻译系统）构建，实现了带有LSTM编码器/解码器和注意力的序列 - 序列模型。我们尝试使用单语数据自动反向翻译。我们所得到的模型是进一步超专用的一种适应技术，根据评估测试句子对模型进行精细调整。

##### URL
[https://arxiv.org/abs/1709.03814](https://arxiv.org/abs/1709.03814)

##### PDF
[https://arxiv.org/pdf/1709.03814](https://arxiv.org/pdf/1709.03814)

