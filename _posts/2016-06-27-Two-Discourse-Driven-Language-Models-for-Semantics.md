---
layout: post
title: "Two Discourse Driven Language Models for Semantics"
date: 2016-06-27 06:20:52
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Language_Model
author: Haoruo Peng, Dan Roth
mathjax: true
---

* content
{:toc}

##### Abstract
Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a "standard" N-gram language model and three discriminatively trained "neural" language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.

##### Abstract (translated by Google)
自然语言理解通常需要深入的语义知识。扩展以前的建议，我们建议，语义知识的一些重要方面可以建模为一个语言模型，如果在适当的抽象层次上完成。我们开发了两个截然不同的模型，用于捕获语义框架链和语篇信息，同时抽象谓词和实体的具体提及。对于每个模型，我们调查了四个实现：一个“标准的”N-gram语言模型和三个歧义训练的“神经”语言模型，为语义框架生成嵌入。语义语言模型（SemLM）的质量本质上是使用困惑性和叙述性完整性测试和外在性来评估的 - 我们展示了SemLM有助于提高语义自然语言处理任务的性能，如共同参考解析和话语解析。

##### URL
[https://arxiv.org/abs/1606.05679](https://arxiv.org/abs/1606.05679)

##### PDF
[https://arxiv.org/pdf/1606.05679](https://arxiv.org/pdf/1606.05679)

