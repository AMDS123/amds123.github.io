---
layout: post
title: "Unsupervised Segmentation of Action Segments in Egocentric Videos using Gaze"
date: 2017-09-30 12:19:41
categories: arXiv_CV
tags: arXiv_CV Segmentation Tracking Recognition
author: I. Hipiny, H. Ujir, J.L. Minoi, S.F. Samson Juan, M.A. Khairuddin, M.S. Sunar
mathjax: true
---

* content
{:toc}

##### Abstract
Unsupervised segmentation of action segments in egocentric videos is a desirable feature in tasks such as activity recognition and content-based video retrieval. Reducing the search space into a finite set of action segments facilitates a faster and less noisy matching. However, there exist a substantial gap in machine understanding of natural temporal cuts during a continuous human activity. This work reports on a novel gaze-based approach for segmenting action segments in videos captured using an egocentric camera. Gaze is used to locate the region-of-interest inside a frame. By tracking two simple motion-based parameters inside successive regions-of-interest, we discover a finite set of temporal cuts. We present several results using combinations (of the two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains egocentric videos depicting several daily-living activities. The quality of the temporal cuts is further improved by implementing two entropy measures.

##### Abstract (translated by Google)
在以自我为中心的视频中，无监督地分割动作片段是活动识别和基于内容的视频检索等任务中理想的特征。将搜索空间缩减为一组有限的动作片段有助于更快和更少噪声的匹配。然而，在连续的人类活动中，机器对自然时间切割的认识存在很大差距。这项工作报告了一种新颖的基于注视的方法，用于分割使用自我中心相机拍摄的视频中的动作片段。凝视是用来定位一个框架内的感兴趣的区域。通过在连续的感兴趣区域内跟踪两个简单的基于运动的参数，我们发现有限的一组时间切割。我们使用数据集上的（两个参数的）组合，即BRISGAZE-ACTIONS，提出了几个结果。该数据集包含描述几个日常生活活动的以自我为中心的视频。通过实施两个熵度量来进一步改善时间削减的质量。

##### URL
[https://arxiv.org/abs/1710.00187](https://arxiv.org/abs/1710.00187)

##### PDF
[https://arxiv.org/pdf/1710.00187](https://arxiv.org/pdf/1710.00187)

