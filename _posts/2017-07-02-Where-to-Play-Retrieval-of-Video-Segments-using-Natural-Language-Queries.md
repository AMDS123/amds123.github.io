---
layout: post
title: "Where to Play: Retrieval of Video Segments using Natural-Language Queries"
date: 2017-07-02 07:56:06
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption
author: Sangkuk Lee, Daesik Kim, Myunggi Lee, Jihye Hwang, Nojun Kwak
---

* content
{:toc}

##### Abstract
In this paper, we propose a new approach for retrieval of video segments using natural language queries. Unlike most previous approaches such as concept-based methods or rule-based structured models, the proposed method uses image captioning model to construct sentential queries for visual information. In detail, our approach exploits multiple captions generated by visual features in each image with `Densecap'. Then, the similarities between captions of adjacent images are calculated, which is used to track semantically similar captions over multiple frames. Besides introducing this novel idea of 'tracking by captioning', the proposed method is one of the first approaches that uses a language generation model learned by neural networks to construct semantic query describing the relations and properties of visual information. To evaluate the effectiveness of our approach, we have created a new evaluation dataset, which contains about 348 segments of scenes in 20 movie-trailers. Through quantitative and qualitative evaluation, we show that our method is effective for retrieval of video segments using natural language queries.

##### Abstract (translated by Google)
在本文中，我们提出了一种使用自然语言查询来检索视频片段的新方法。与以前的大多数方法（如基于概念的方法或基于规则的结构化模型）不同，所提出的方法使用图像字幕模型来构建视觉信息的语义查询。具体来说，我们的方法利用每个图像中具有“Densecap”的视觉特征产生的多个字幕。然后，计算相邻图像的字幕之间的相似性，其用于跟踪多个帧上的语义上相似的字幕。提出的方法除了引入“字幕跟踪”这一新颖的思想外，还是第一种使用神经网络学习的语言生成模型来构建描述视觉信息关系和属性的语义查询的方法之一。为了评估我们的方法的有效性，我们创建了一个新的评估数据集，其中包含20部电影预告片中的约348个场景片段。通过定性和定量的评估，我们表明，我们的方法是有效的检索使用自然语言查询视频片段。

##### URL
[https://arxiv.org/abs/1707.00251](https://arxiv.org/abs/1707.00251)

