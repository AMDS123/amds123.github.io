---
layout: post
title: "Where to Play: Retrieval of Video Segments using Natural-Language Queries"
date: 2017-07-02 07:56:06
categories: arXiv_CV
tags: arXiv_CV Image_Caption Tracking Caption Quantitative Relation
author: Sangkuk Lee, Daesik Kim, Myunggi Lee, Jihye Hwang, Nojun Kwak
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a new approach for retrieval of video segments using natural language queries. Unlike most previous approaches such as concept-based methods or rule-based structured models, the proposed method uses image captioning model to construct sentential queries for visual information. In detail, our approach exploits multiple captions generated by visual features in each image with `Densecap'. Then, the similarities between captions of adjacent images are calculated, which is used to track semantically similar captions over multiple frames. Besides introducing this novel idea of 'tracking by captioning', the proposed method is one of the first approaches that uses a language generation model learned by neural networks to construct semantic query describing the relations and properties of visual information. To evaluate the effectiveness of our approach, we have created a new evaluation dataset, which contains about 348 segments of scenes in 20 movie-trailers. Through quantitative and qualitative evaluation, we show that our method is effective for retrieval of video segments using natural language queries.

##### Abstract (translated by Google)
在本文中，我们提出了一种使用自然语言查询检索视频片段的新方法。与大多数先前的方法（例如基于概念的方法或基于规则的结构化模型）不同，所提出的方法使用图像字幕模型来构建用于视觉信息的句子查询。详细地说，我们的方法利用“Densecap”在每个图像中利用视觉特征生成的多个字幕。然后，计算相邻图像的字幕之间的相似性，其用于在多个帧上跟踪语义上相似的字幕。除了引入这种“通过字幕跟踪”的新思想外，该方法还是首先采用神经网络学习的语言生成模型构建描述视觉信息关系和属性的语义查询的方法之一。为了评估我们的方法的有效性，我们创建了一个新的评估数据集，其中包含20个电影预告片中的大约348个场景片段。通过定量和定性评估，我们表明我们的方法对于使用自然语言查询检索视频片段是有效的。

##### URL
[https://arxiv.org/abs/1707.00251](https://arxiv.org/abs/1707.00251)

##### PDF
[https://arxiv.org/pdf/1707.00251](https://arxiv.org/pdf/1707.00251)

