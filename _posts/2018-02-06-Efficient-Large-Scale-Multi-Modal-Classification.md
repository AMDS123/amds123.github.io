---
layout: post
title: "Efficient Large-Scale Multi-Modal Classification"
date: 2018-02-06 20:30:59
categories: arXiv_AI
tags: arXiv_AI CNN Classification
author: D. Kiela, E. Grave, A. Joulin, T. Mikolov
mathjax: true
---

* content
{:toc}

##### Abstract
While the incipient internet was largely text-based, the modern digital world is becoming increasingly multi-modal. Here, we examine multi-modal classification where one modality is discrete, e.g. text, and the other is continuous, e.g. visual representations transferred from a convolutional neural network. In particular, we focus on scenarios where we have to be able to classify large quantities of data quickly. We investigate various methods for performing multi-modal fusion and analyze their trade-offs in terms of classification accuracy and computational efficiency. Our findings indicate that the inclusion of continuous information improves performance over text-only on a range of multi-modal classification tasks, even with simple fusion methods. In addition, we experiment with discretizing the continuous features in order to speed up and simplify the fusion process even further. Our results show that fusion with discretized features outperforms text-only classification, at a fraction of the computational cost of full multi-modal fusion, with the additional benefit of improved interpretability.

##### Abstract (translated by Google)
虽然初期的互联网基本上是基于文本的，但现代数字世界正变得越来越多元化。在这里，我们考察一种形式是离散的多模式分类，例如，文本，另一个是连续的，例如从卷积神经网络转移的视觉表示。特别是，我们专注于需要快速分类大量数据的场景。我们研究了多种方式进行多模式融合，并在分类精度和计算效率方面进行权衡分析。我们的研究结果表明，即使使用简单的融合方法，包含连续的信息也可以提高整个文本的性能，而不仅仅局限于多种分类任务。此外，我们尝试对连续特征进行离散化处理，以加速和简化融合过程。我们的研究结果表明，离散化特征的融合优于纯文本分类，仅占全多模式融合计算成本的一小部分，并具有提高解释性的额外好处。

##### URL
[http://arxiv.org/abs/1802.02892](http://arxiv.org/abs/1802.02892)

##### PDF
[http://arxiv.org/pdf/1802.02892](http://arxiv.org/pdf/1802.02892)

