---
layout: post
title: "Temporal Saliency Adaptation in Egocentric Videos"
date: 2018-08-28 22:24:10
categories: arXiv_CV
tags: arXiv_CV Salient GAN CNN RNN Prediction
author: Panagiotis Linardos, Eva Mohedano, Monica Cherto, Cathal Gurrin, Xavier Giro-i-Nieto
mathjax: true
---

* content
{:toc}

##### Abstract
This work adapts a deep neural model for image saliency prediction to the temporal domain of egocentric video. We compute the saliency map for each video frame, firstly with an off-the-shelf model trained from static images, secondly by adding a a convolutional or conv-LSTM layers trained with a dataset for video saliency prediction. We study each configuration on EgoMon, a new dataset made of seven egocentric videos recorded by three subjects in both free-viewing and task-driven set ups. Our results indicate that the temporal adaptation is beneficial when the viewer is not moving and observing the scene from a narrow field of view. Encouraged by this observation, we compute and publish the saliency maps for the EPIC Kitchens dataset, in which viewers are cooking. Source code and models available at https://imatge-upc.github.io/saliency-2018-videosalgan/

##### Abstract (translated by Google)
这项工作将图像显着性预测的深度神经模型适应于以自我为中心的视频的时间域。我们计算每个视频帧的显着性图，首先是使用静态图像训练的现成模型，然后通过添加使用数据集训练的卷积或卷积LSTM层进行视频显着性预测。我们研究了EgoMon上的每个配置，这是一个新的数据集，由三个以自由观看和任务驱动的设置中的三个主题记录的自我中心视频组成。我们的结果表明，当观察者不移动并从狭窄的视野观察场景时，时间适应是有益的。受此观察的鼓舞，我们计算并发布了EPIC Kitchens数据集的显着性图，其中观众正在烹饪。源代码和模型可在https://imatge-upc.github.io/saliency-2018-videosalgan/获得

##### URL
[http://arxiv.org/abs/1808.09559](http://arxiv.org/abs/1808.09559)

##### PDF
[http://arxiv.org/pdf/1808.09559](http://arxiv.org/pdf/1808.09559)

