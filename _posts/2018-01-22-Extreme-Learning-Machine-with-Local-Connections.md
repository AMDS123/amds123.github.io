---
layout: post
title: "Extreme Learning Machine with Local Connections"
date: 2018-01-22 06:54:22
categories: arXiv_AI
tags: arXiv_AI Regularization
author: Feng Li, Sibo Yang, Huanhuan Huang, Wei Wu
mathjax: true
---

* content
{:toc}

##### Abstract
This paper is concerned with the sparsification of the input-hidden weights of ELM (Extreme Learning Machine). For ordinary feedforward neural networks, the sparsification is usually done by introducing certain regularization technique into the learning process of the network. But this strategy can not be applied for ELM, since the input-hidden weights of ELM are supposed to be randomly chosen rather than to be learned. To this end, we propose a modified ELM, called ELM-LC (ELM with local connections), which is designed for the sparsification of the input-hidden weights as follows: The hidden nodes and the input nodes are divided respectively into several corresponding groups, and an input node group is fully connected with its corresponding hidden node group, but is not connected with any other hidden node group. As in the usual ELM, the hidden-input weights are randomly given, and the hidden-output weights are obtained through a least square learning. In the numerical simulations on some benchmark problems, the new ELM-CL behaves better than the traditional ELM.

##### Abstract (translated by Google)
本文关注ELM（极限学习机）的输入隐藏权重的稀疏性。对于普通的前向神经网络，稀疏化通常是通过在网络的学习过程中引入一定的正则化技术来完成的。但是这个策略不能应用于ELM，因为ELM的输入隐藏权重应该是随机选择的而不是学习的。为此，我们提出了一个改进的ELM，称为ELM-LC（ELM与局部连接），它被设计用于输入隐藏权重的稀疏化，如下所示：隐藏节点和输入节点分别被分成几个相应的组，输入节点组与其对应的隐藏节点组完全连接，但不与任何其他隐藏节点组连接。与通常的ELM一样，隐式输入权重是随机给定的，隐式输出权重是通过最小二乘学习获得的。在一些基准问题的数值模拟中，新的ELM-CL比传统的ELM表现更好。

##### URL
[https://arxiv.org/abs/1801.06975](https://arxiv.org/abs/1801.06975)

##### PDF
[https://arxiv.org/pdf/1801.06975](https://arxiv.org/pdf/1801.06975)

