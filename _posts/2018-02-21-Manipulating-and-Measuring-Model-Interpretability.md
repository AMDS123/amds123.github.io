---
layout: post
title: "Manipulating and Measuring Model Interpretability"
date: 2018-02-21 21:11:36
categories: arXiv_AI
tags: arXiv_AI Prediction
author: Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wortman Vaughan, Hanna Wallach
mathjax: true
---

* content
{:toc}

##### Abstract
Despite a growing body of research focused on creating interpretable machine learning methods, there have been few empirical studies verifying whether interpretable methods achieve their intended effects on end users. We present a framework for assessing the effects of model interpretability on users via pre-registered experiments in which participants are shown functionally identical models that vary in factors thought to influence interpretability. Using this framework, we ran a sequence of large-scale randomized experiments, varying two putative drivers of interpretability: the number of features and the model transparency (clear or black-box). We measured how these factors impact trust in model predictions, the ability to simulate a model, and the ability to detect a model's mistakes. We found that participants who were shown a clear model with a small number of features were better able to simulate the model's predictions. However, we found no difference in multiple measures of trust and found that clear models did not improve the ability to correct mistakes. These findings suggest that interpretability research could benefit from more emphasis on empirically verifying that interpretable models achieve all their intended effects.

##### Abstract (translated by Google)
尽管越来越多的研究专注于创建可解释的机器学习方法，但很少有实证研究证实可解释的方法是否能够实现其对最终用户的预期效果。我们提出了一个框架，用于通过预先注册的实验评估模型可解释性对用户的影响，在这些实验中参与者显示功能相同的模型，这些模型在影响可解释性的因素上有所不同。使用这个框架，我们进行了一系列大规模的随机实验，改变了两种推定的解释性驱动因素：特征数量和模型透明度（透明或黑框）。我们测量了这些因素如何影响模型预测中的信任，模拟模型的能力以及检测模型错误的能力。我们发现参与者被证明具有少量特征的清晰模型能够更好地模拟模型的预测。然而，我们没有发现多重信任度量方面的差异，并发现明确的模型并没有提高纠正错误的能力。这些研究结果表明，可解释性研究可以受益于更多重视经验性验证可解释模型实现其所有预期效果。

##### URL
[http://arxiv.org/abs/1802.07810](http://arxiv.org/abs/1802.07810)

##### PDF
[http://arxiv.org/pdf/1802.07810](http://arxiv.org/pdf/1802.07810)

