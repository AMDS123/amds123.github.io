---
layout: post
title: "Learning to Make Predictions on Graphs with Autoencoders"
date: 2018-02-23 00:02:59
categories: arXiv_AI
tags: arXiv_AI Knowledge Represenation_Learning Classification Prediction
author: Phi Vu Tran
mathjax: true
---

* content
{:toc}

##### Abstract
We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a densely connected autoencoder architecture capable of learning a joint representation of both local graph structure and available external node features for the multi-task learning of link prediction and node classification. To the best of our knowledge, this is the first architecture that can be efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification. We provide comprehensive empirical evaluation of our models on a range of challenging benchmark graph-structured datasets, and demonstrate significant improvement in accuracy over related methods for graph representation learning. Code implementation is available at https://github.com/vuptran/graph-representation-learning

##### Abstract (translated by Google)
我们检查与图表示学习相关的两个基本任务：链接预测和半监督节点分类。我们提出了一种密集连接的自编码器体系结构，能够学习链接预测和节点分类的多任务学习的局部图结构和可用外部节点特征的联合表示。就我们所知，这是第一个可以在单个学习阶段端对端有效地进行训练以同时执行链接预测和节点分类的体系结构。我们在一系列具有挑战性的基准图结构化数据集上对我们的模型进行了全面的实证评估，并且与图形表示学习的相关方法相比，它们的准确性显着提高。代码实现可在https://github.com/vuptran/graph-representation-learning获得

##### URL
[http://arxiv.org/abs/1802.08352](http://arxiv.org/abs/1802.08352)

##### PDF
[http://arxiv.org/pdf/1802.08352](http://arxiv.org/pdf/1802.08352)

