---
layout: post
title: "Deep Learning for Lip Reading using Audio-Visual Information for Urdu Language"
date: 2018-02-15 13:28:19
categories: arXiv_CV
tags: arXiv_CV Knowledge Speech_Recognition CNN RNN Classification Deep_Learning Recognition
author: M Faisal, Sanaullah Manzoor
mathjax: true
---

* content
{:toc}

##### Abstract
Human lip-reading is a challenging task. It requires not only knowledge of underlying language but also visual clues to predict spoken words. Experts need certain level of experience and understanding of visual expressions learning to decode spoken words. Now-a-days, with the help of deep learning it is possible to translate lip sequences into meaningful words. The speech recognition in the noisy environments can be increased with the visual information [1]. To demonstrate this, in this project, we have tried to train two different deep-learning models for lip-reading: first one for video sequences using spatiotemporal convolution neural network, Bi-gated recurrent neural network and Connectionist Temporal Classification Loss, and second for audio that inputs the MFCC features to a layer of LSTM cells and output the sequence. We have also collected a small audio-visual dataset to train and test our model. Our target is to integrate our both models to improve the speech recognition in the noisy environment

##### Abstract (translated by Google)
人类的口头阅读是一项艰巨的任务。它不仅需要基础语言的知识，还需要视觉线索来预测口语词汇。专家需要一定程度的经验和理解视觉表达的学习来解读口语单词。现在，在深度学习的帮助下，可以将唇部序列翻译成有意义的单词。噪声环境中的语音识别可以通过视觉信息来增加[1]。为了证明这一点，在这个项目中，我们试图训练两种不同的用于唇读的深度学习模型：第一种是使用时空卷积神经网络，Bi-gated递归神经网络和Connectionist时间分类丢失的视频序列，第二种将MFCC功能输入到一层LSTM单元并输出序列的音频。我们还收集了一个小型视听数据集来训练和测试我们的模型。我们的目标是整合我们的两种模型来改善嘈杂环境中的语音识别

##### URL
[https://arxiv.org/abs/1802.05521](https://arxiv.org/abs/1802.05521)

##### PDF
[https://arxiv.org/pdf/1802.05521](https://arxiv.org/pdf/1802.05521)

