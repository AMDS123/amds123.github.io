---
layout: post
title: "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency"
date: 2018-09-05 05:12:34
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Richard Futrell, Ethan Wilcox, Takashi Morita, Roger Levy
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.

##### Abstract (translated by Google)
递归神经网络（RNN）是自然语言的序列建模的现有技术。然而，由于优化语言建模目标，他们隐含地学习和表示自然语言的语法特征仍然知之甚少。在这里，我们部署了受控心理语言学实验的方法，以阐明RNN行为在多大程度上反映了已知用于表征人类语言行为的增量句法状态和语法依赖性表征。我们广泛测试两种公开的长期短期记忆（LSTM）英语序列模型，并学习和测试一种新的日本LSTM。我们证明了这些模型代表并保持了增量的句法状态，但它们并不总是以与人类相同的方式推广。此外，我们的模型都没有学习适当的语法依赖配置许可反身代词或负极性项目。

##### URL
[http://arxiv.org/abs/1809.01329](http://arxiv.org/abs/1809.01329)

##### PDF
[http://arxiv.org/pdf/1809.01329](http://arxiv.org/pdf/1809.01329)

