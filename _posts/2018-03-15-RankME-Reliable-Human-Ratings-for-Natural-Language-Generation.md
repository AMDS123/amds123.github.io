---
layout: post
title: "RankME: Reliable Human Ratings for Natural Language Generation"
date: 2018-03-15 18:10:45
categories: arXiv_CL
tags: arXiv_CL
author: Jekaterina Novikova, Ondřej Dušek, Verena Rieser
mathjax: true
---

* content
{:toc}

##### Abstract
Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems.

##### Abstract (translated by Google)
自然语言生成（NLG）的人类评估经常遭受不一致的用户评级。虽然以前的研究倾向于将这个问题归因于个人用户偏好，但我们表明，人类判断的质量也可以通过实验设计来改善。我们提出了一种新颖的基于秩的量级估计方法（RankME），它结合了连续量表和相关评估的使用。我们证明RankME与传统评估方法相比，显着提高了人类评分的可靠性和一致性。另外，我们表明可以根据多个不同的标准评估NLG系统，这对于错误分析很重要。最后，我们证明RankME与系统质量的贝叶斯估计相结合，是用于对多个NLG系统进行排名的一种经济有效的选择。

##### URL
[https://arxiv.org/abs/1803.05928](https://arxiv.org/abs/1803.05928)

##### PDF
[https://arxiv.org/pdf/1803.05928](https://arxiv.org/pdf/1803.05928)

