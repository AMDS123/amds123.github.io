---
layout: post
title: "Subregular Complexity and Deep Learning"
date: 2017-10-14 18:24:08
categories: arXiv_CL
tags: arXiv_CL Inference RNN Deep_Learning
author: Enes Avcu, Chihiro Shibata, Jeffrey Heinz
mathjax: true
---

* content
{:toc}

##### Abstract
This paper argues that the judicial use of formal language theory and grammatical inference are invaluable tools in understanding how deep neural networks can and cannot represent and learn long-term dependencies in temporal sequences. Learning experiments were conducted with two types of Recurrent Neural Networks (RNNs) on six formal languages drawn from the Strictly Local (SL) and Strictly Piecewise (SP) classes. The networks were Simple RNNs (s-RNNs) and Long Short-Term Memory RNNs (LSTMs) of varying sizes. The SL and SP classes are among the simplest in a mathematically well-understood hierarchy of subregular classes. They encode local and long-term dependencies, respectively. The grammatical inference algorithm Regular Positive and Negative Inference (RPNI) provided a baseline. According to earlier research, the LSTM architecture should be capable of learning long-term dependencies and should outperform s-RNNs. The results of these experiments challenge this narrative. First, the LSTMs' performance was generally worse in the SP experiments than in the SL ones. Second, the s-RNNs out-performed the LSTMs on the most complex SP experiment and performed comparably to them on the others.

##### Abstract (translated by Google)
本文认为，形式语言理论和语法推理的司法使用是理解深度神经网络如何能够并且不能代表和学习时间序列中的长期依赖关系的宝贵工具。学习实验是用严格本地（SL）和严格分段（SP）类的六种形式语言的两种递归神经网络（RNN）进行的。网络是大小不等的简单RNN（s-RNN）和长期短期记忆RNN（LSTM）。 SL和SP类是数学上很好理解的子类中最简单的类。他们分别编码本地和长期的依赖关系。语法推理算法常规正反推理（RPNI）提供了一个基线。根据之前的研究，LSTM架构应该能够学习长期的依赖关系，并应该胜过s-RNNs。这些实验的结果挑战了这个叙述。首先，SP实验中LSTM的表现一般比SL更差。其次，s-RNNs在最复杂的SP实验上表现超过LSTMs，在其他实验上表现相当。

##### URL
[https://arxiv.org/abs/1705.05940](https://arxiv.org/abs/1705.05940)

##### PDF
[https://arxiv.org/pdf/1705.05940](https://arxiv.org/pdf/1705.05940)

