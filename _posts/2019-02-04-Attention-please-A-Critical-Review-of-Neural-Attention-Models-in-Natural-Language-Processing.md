---
layout: post
title: "Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing"
date: 2019-02-04 17:14:13
categories: arXiv_AI
tags: arXiv_AI Review Attention
author: Andrea Galassi, Marco Lippi, Paolo Torroni
mathjax: true
---

* content
{:toc}

##### Abstract
Attention is an increasingly popular mechanism used in a wide range of neural architectures. Because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures for natural language processing, with a focus on architectures designed to work with vector representation of the textual data. We discuss the dimensions along which proposals differ, the possible uses of attention, and chart the major research activities and open challenges in the area.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.02181](http://arxiv.org/abs/1902.02181)

##### PDF
[http://arxiv.org/pdf/1902.02181](http://arxiv.org/pdf/1902.02181)

