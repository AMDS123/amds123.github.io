---
layout: post
title: "Per-Pixel Feedback for improving Semantic Segmentation"
date: 2017-12-07 21:12:53
categories: arXiv_CV
tags: arXiv_CV Segmentation CNN Semantic_Segmentation Recognition
author: Aditya Ganeshan
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic segmentation is the task of assigning a label to each pixel in the image.In recent years, deep convolutional neural networks have been driving advances in multiple tasks related to cognition. Although, DCNNs have resulted in unprecedented visual recognition performances, they offer little transparency. To understand how DCNN based models work at the task of semantic segmentation, we try to analyze the DCNN models in semantic segmentation. We try to find the importance of global image information for labeling pixels. 
 Based on the experiments on discriminative regions, and modeling of fixations, we propose a set of new training loss functions for fine-tuning DCNN based models. The proposed training regime has shown improvement in performance of DeepLab Large FOV(VGG-16) Segmentation model for PASCAL VOC 2012 dataset. However, further test remains to conclusively evaluate the benefits due to the proposed loss functions across models, and data-sets. 
 Submitted in part fulfillment of the requirements for the degree of Integrated Masters of Science in Applied Mathematics. 
 Update: Further Experiment showed minimal benefits. 
 Code Available [here](https://github.com/BardOfCodes/Seg-Unravel).

##### Abstract (translated by Google)
语义分割是为图像中的每个像素分配一个标签的任务。近年来，深度卷积神经网络一直在推动与认知有关的多个任务。虽然DCNN导致了前所未有的视觉识别性能，但是它们的透明度很低。为了理解基于DCNN的模型如何工作在语义分割的任务，我们试图在语义分割中分析DCNN模型。我们试图找出全局图像信息对标记像素的重要性。
 基于判别区域的实验和注意建模，提出了一套新的训练损失函数，用于微调DCNN模型。所提议的培训体系已经显示了PASCAL VOC 2012数据集的DeepLab大型FOV（VGG-16）分段模型的性能改进。然而，进一步的测试仍然可以确定地评估模型和数据集中提出的损失函数带来的收益。
 部分完成了应用数学综合硕士学位的要求。
 更新：进一步实验显示最小的好处。
 可用代码[这里]（https://github.com/BardOfCodes/Seg-Unravel）。

##### URL
[http://arxiv.org/abs/1712.02861](http://arxiv.org/abs/1712.02861)

##### PDF
[http://arxiv.org/pdf/1712.02861](http://arxiv.org/pdf/1712.02861)

