---
layout: post
title: "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones"
date: 2017-07-20 12:46:09
categories: arXiv_CL
tags: arXiv_CL Segmentation RNN Language_Model
author: Zhenisbek Assylbekov, Rustem Takhanov, Bagdat Myrzakhmetov, Jonathan N. Washington
mathjax: true
---

* content
{:toc}

##### Abstract
Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.

##### Abstract (translated by Google)
与基于字符的分割相比，Syllabification似乎不能提高单词级别的RNN语言建模质量。然而，我们最好的音节感知语言模型，其性能与竞争性特征模型相比，参数少18％-33％，训练速度提高了1.2-2.2倍。

##### URL
[https://arxiv.org/abs/1707.06480](https://arxiv.org/abs/1707.06480)

##### PDF
[https://arxiv.org/pdf/1707.06480](https://arxiv.org/pdf/1707.06480)

