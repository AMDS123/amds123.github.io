---
layout: post
title: "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks"
date: 2018-05-25 23:01:01
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Sansiri Tarnpradab, Fei Liu, Kien A. Hua
mathjax: true
---

* content
{:toc}

##### Abstract
Forum threads are lengthy and rich in content. Concise thread summaries will benefit both newcomers seeking information and those who participate in the discussion. Few studies, however, have examined the task of forum thread summarization. In this work we make the first attempt to adapt the hierarchical attention networks for thread summarization. The model draws on the recent development of neural attention mechanisms to build sentence and thread representations and use them for summarization. Our results indicate that the proposed approach can outperform a range of competitive baselines. Further, a redundancy removal step is crucial for achieving outstanding results.

##### Abstract (translated by Google)
论坛主题冗长且内容丰富。简明的线程摘要将有利于新手寻求信息和参与讨论的人。然而，很少有研究检验了论坛话题摘要的任务。在这项工作中，我们首次尝试为线程摘要调整分层关注网络。该模型借鉴了神经关注机制的最新发展，以建立句子和线索表示并将其用于摘要。我们的结果表明，所提出的方法可以超越一系列竞争基线。此外，冗余消除步骤对于实现出色结果至关重要。

##### URL
[http://arxiv.org/abs/1805.10390](http://arxiv.org/abs/1805.10390)

##### PDF
[http://arxiv.org/pdf/1805.10390](http://arxiv.org/pdf/1805.10390)

