---
layout: post
title: "Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings"
date: 2018-02-05 02:42:40
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Relation
author: Gabriel Grand, Idan Asher Blank, Francisco Pereira, Evelina Fedorenko
mathjax: true
---

* content
{:toc}

##### Abstract
The words of a language reflect the structure of the human mind, allowing us to transmit thoughts between individuals. However, language can represent only a subset of our rich and detailed cognitive architecture. Here, we ask what kinds of common knowledge (semantic memory) are captured by word meanings (lexical semantics). We examine a prominent computational model that represents words as vectors in a multidimensional space, such that proximity between word-vectors approximates semantic relatedness. Because related words appear in similar con-texts, such spaces - called "word embeddings" - can be learned from patterns of lexical co-occurrences in natural language. Despite their popularity, a fundamental concern about word embeddings is that they appear to be semantically "rigid": inter-word proximity captures only overall similarity, yet human judgments about object similarities are highly context-dependent and involve multiple, distinct semantic features. For example, dolphins and alligators appear similar in size, but differ in intelligence and aggressiveness. Could such context-dependent relationships be recovered from word embeddings? To address this issue, we introduce a powerful, domain-general solution: "semantic projection" of word-vectors onto lines that represent various object features, like size (the line extending from the word "small" to "big"), intelligence (from "dumb" to "smart"), or danger (from "safe" to "dangerous"). This method, which is intuitively analogous to placing objects "on a mental scale" between two extremes, recovers human judgments across a range of object categories and properties. We thus show that word embeddings inherit a wealth of common knowledge from word co-occurrence statistics and can be flexibly manipulated to express context-dependent meanings.

##### Abstract (translated by Google)
一种语言的词语反映了人类思维的结构，使我们能够在个人之间传递思想。但是，语言只能代表我们丰富而详细的认知架构的一个子集。在这里，我们问什么样的常识（语义记忆）是由词义（词汇语义）来捕捉的。我们研究了一个显着的计算模型，在多维空间中将单词表示为向量，使得单词向量之间的接近度接近语义相关性。由于相关词汇出现在相似的文本中，这种空间称为“词汇嵌入”，可以从自然语言中的词汇共现模式中学习到。尽管它们很受欢迎，但是对于词嵌入的基本关注在于它们看起来在语义上是“刚性的”：词间接近只捕获整体相似性，但是关于对象相似性的人类判断是高度依赖于上下文的并且涉及多个不同的语义特征。例如，海豚和短吻鳄在尺寸上看起来相似，但智力和侵略性不同。这种依赖于上下文的关系可以从文字嵌入中恢复吗？为了解决这个问题，我们引入了一个强大的，领域一般的解决方案：将词向量“语义投影”到表示各种对象特征的线上，如尺寸（从“小”到“大”这个词的延伸线） （从“哑”到“聪明”）或危险（从“安全”到“危险”）。这种方法直观地类似于在两个极端之间放置“精神尺度”的对象，在一系列对象类别和属性上恢复了人类的判断。因此，我们表明，词嵌入从词共现统计继承丰富的常识，并可以灵活地操纵来表达与上下文相关的含义。

##### URL
[http://arxiv.org/abs/1802.01241](http://arxiv.org/abs/1802.01241)

##### PDF
[http://arxiv.org/pdf/1802.01241](http://arxiv.org/pdf/1802.01241)

