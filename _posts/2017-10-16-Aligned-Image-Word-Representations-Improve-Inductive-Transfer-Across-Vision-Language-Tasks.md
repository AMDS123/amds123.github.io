---
layout: post
title: "Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks"
date: 2017-10-16 05:34:24
categories: arXiv_CV
tags: arXiv_CV QA Embedding VQA Recognition
author: Tanmay Gupta, Kevin Shih, Saurabh Singh, Derek Hoiem
mathjax: true
---

* content
{:toc}

##### Abstract
An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.

##### Abstract (translated by Google)
计算机视觉的一个重要目标是建立学习随着时间的视觉表示的系统，可以应用于许多任务。在本文中，我们调查视觉语言嵌入作为核心代表，并表明它导致更好的跨任务转移比标准的多任务学习。具体来说，视觉识别的任务是通过迫使每个使用相同的词区嵌入来与视觉问题解答的任务相对应。我们表明，这导致从识别到VQA比标准的多任务学习更大的归纳转移。视觉识别也得到了改善，特别是对于识别训练标签相对较少但在VQA环境中经常出现的类别。因此，我们的论文通过展示可解释的，灵活的，可训练的核心陈述的好处，朝着创造更多一般的视觉系统迈出了一小步。

##### URL
[https://arxiv.org/abs/1704.00260](https://arxiv.org/abs/1704.00260)

##### PDF
[https://arxiv.org/pdf/1704.00260](https://arxiv.org/pdf/1704.00260)

