---
layout: post
title: "Provably Minimally-Distorted Adversarial Examples"
date: 2018-02-20 05:35:36
categories: arXiv_AI
tags: arXiv_AI Adversarial
author: Nicholas Carlini, Guy Katz, Clark Barrett, David L. Dill
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken. We propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.

##### Abstract (translated by Google)
在现实世界中安全关键系统中部署神经网络的能力受到对抗性例子的严重限制：网络误分类的轻微扰动输入。近年来，已经提出了几种技术来提高对抗性实例的鲁棒性 - 然而其中大多数已经很快显示出容易受到未来攻击的影响。例如，在2019年ICLR接受的论文中提出的防御措施中，有一半以上已被打破。我们建议通过形式验证技术来解决这个难题。我们展示了如何构造可证明最低限度扭曲的对抗性例子：给定一个任意的神经网络和输入样本，我们可以构造我们证明失真最小的对抗性例子。使用这种方法，我们证明最近的ICLR防御提案之一是对抗性再培训，可以成功地将构建敌对案例所需的扭曲增加4.2倍。

##### URL
[http://arxiv.org/abs/1709.10207](http://arxiv.org/abs/1709.10207)

##### PDF
[http://arxiv.org/pdf/1709.10207](http://arxiv.org/pdf/1709.10207)

