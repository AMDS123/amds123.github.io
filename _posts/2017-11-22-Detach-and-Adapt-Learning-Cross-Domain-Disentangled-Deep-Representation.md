---
layout: post
title: "Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation"
date: 2017-11-22 15:40:35
categories: arXiv_CV
tags: arXiv_CV Represenation_Learning Classification Deep_Learning
author: Yen-Cheng Liu, Yu-Ying Yeh, Tzu-Chien Fu, Wei-Chen Chiu, Sheng-De Wang, Yu-Chiang Frank Wang
mathjax: true
---

* content
{:toc}

##### Abstract
While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.

##### Abstract (translated by Google)
表示学习的目的是为了描述视觉数据而获得可解释的特征，表示解开进一步导致这样的特征，从而可以识别和操纵特定的图像属性。但是，如果不对训练数据进行实际的注释，就不能轻易解决这个问题。为了解决这个问题，我们提出了一个新的跨域表示分析器（CDRD）的深度学习模型。通过观察完全注释的源域数据和感兴趣的未标记目标域数据，我们的模型跨越数据域桥接信息并相应地传输属性信息。因此，可以共同执行跨域联合特征解缠和自适应。在实验中，我们提供定性结果来验证我们的解缠能力。此外，我们进一步确认，我们的模型可以应用于解决无监督领域适应的分类任务，并且有利地抵制最先进的图像解开和翻译方法。

##### URL
[https://arxiv.org/abs/1705.01314](https://arxiv.org/abs/1705.01314)

##### PDF
[https://arxiv.org/pdf/1705.01314](https://arxiv.org/pdf/1705.01314)

