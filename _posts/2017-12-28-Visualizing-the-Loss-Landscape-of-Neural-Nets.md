---
layout: post
title: "Visualizing the Loss Landscape of Neural Nets"
date: 2017-12-28 16:15:42
categories: arXiv_CV
tags: arXiv_CV
author: Hao Li, Zheng Xu, Gavin Taylor, Tom Goldstein
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.

##### Abstract (translated by Google)
神经网络训练依赖于我们找到高度非凸损失函数的“好”极小值的能力。众所周知，某些网络体系结构设计（例如，跳过连接）产生更容易训练的损失函数，并且良好选择的训练参数（批量大小，学习速率，优化器）产生更好地概括的最小化。然而，这些差异的原因，以及它们对潜在损失景观的影响，目前尚不清楚。在本文中，我们使用一系列的可视化方法，探讨神经损失函数的结构，以及损失景观对泛化的影响。首先，我们介绍一个简单的“滤波器归一化”方法，它帮助我们可视化损失函数曲率，并在损失函数之间进行有意义的并行比较。然后，使用各种可视化，我们探索网络架构如何影响损失情况，以及训练参数如何影响最小化器的形状。

##### URL
[http://arxiv.org/abs/1712.09913](http://arxiv.org/abs/1712.09913)

##### PDF
[http://arxiv.org/pdf/1712.09913](http://arxiv.org/pdf/1712.09913)

