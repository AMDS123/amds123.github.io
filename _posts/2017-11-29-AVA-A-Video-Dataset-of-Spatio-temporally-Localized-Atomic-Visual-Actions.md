---
layout: post
title: "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions"
date: 2017-11-29 07:58:40
categories: arXiv_CV
tags: arXiv_CV Video_Caption Sparse Action_Recognition Recognition
author: Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik
mathjax: true
---

* content
{:toc}

##### Abstract
This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 192 15-minute video clips, where actions are localized in space and time, resulting in 740k action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. We will release the dataset publicly. AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 16.2% mAP, underscoring the need for developing new approaches for video understanding.

##### Abstract (translated by Google)
本文介绍了时空局部原子视觉行为（AVA）的视频数据集。 AVA数据集在192个15分钟的视频剪辑中密集地标注了80个原子视觉动作，其中动作在空间和时间上被本地化，导致每个人经常出现多个标签的740k动作标签。我们的数据集的关键特征是：（1）原子视觉行为的定义，而不是复合行为; （2）对于每个人可能具有多个注释的精确的时空注释; （3）通过15分钟的视频剪辑对这些原子动作进行详尽的注释; （4）人们在时间上连贯的连续段;和（5）使用电影来收集一组不同的动作表示。这与现有的空间 - 时间动作识别数据集不同，其通常为短视频剪辑中的复合动作提供稀疏注释。我们将公开发布数据集。 AVA具有逼真的场景和复杂的行为，揭示了动作识别的内在困难。以此为基准，我们提出了一种基于当前最先进方法的动作本地化的新方法，并且在JHMDB和UCF101-24类别上展示了更好的性能。在为现有数据集设定新的技术水平的同时，AVA的总体结果是低至16.2％的mAP，这凸显了开发新的视频理解方法的必要性。

##### URL
[https://arxiv.org/abs/1705.08421](https://arxiv.org/abs/1705.08421)

##### PDF
[https://arxiv.org/pdf/1705.08421](https://arxiv.org/pdf/1705.08421)

