---
layout: post
title: "Nearly optimal exploration-exploitation decision thresholds"
date: 2018-06-04 18:17:32
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Christos Dimitrakakis
mathjax: true
---

* content
{:toc}

##### Abstract
While in general trading off exploration and exploitation in reinforcement learning is hard, under some formulations relatively simple solutions exist. In this paper, we first derive upper bounds for the utility of selecting different actions in the multi-armed bandit setting. Unlike the common statistical upper confidence bounds, these explicitly link the planning horizon, uncertainty and the need for exploration explicit. The resulting algorithm can be seen as a generalisation of the classical Thompson sampling algorithm. We experimentally test these algorithms, as well as $\epsilon$-greedy and the value of perfect information heuristics. Finally, we also introduce the idea of bagging for reinforcement learning. By employing a version of online bootstrapping, we can efficiently sample from an approximate posterior distribution.

##### Abstract (translated by Google)
尽管通常在强化学习中进行勘探和开发交易很困难，但在一些配方下存在相对简单的解决方案。在本文中，我们首先推导出在多武装强盗背景下选择不同行为的效用的上界。与常见统计上置信区间不同，这些明确地将计划范围，不确定性和对探索的需求联系起来。得到的算法可以看作是经典汤普森采样算法的推广。我们通过实验测试这些算法，以及$ \ epsilon $ -greedy和完美信息启发式的价值。最后，我们还介绍了用于强化学习的装袋理念。通过使用在线引导版本，我们可以从近似后验分布有效地采样。

##### URL
[http://arxiv.org/abs/cs/0604010](http://arxiv.org/abs/cs/0604010)

##### PDF
[http://arxiv.org/pdf/cs/0604010](http://arxiv.org/pdf/cs/0604010)

