---
layout: post
title: "Explainable Neural Computation via Stack Neural Module Networks"
date: 2018-07-23 12:18:18
categories: arXiv_CV
tags: arXiv_CV Prediction
author: Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko
mathjax: true
---

* content
{:toc}

##### Abstract
In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.

##### Abstract (translated by Google)
在诸如问答的复杂推理任务中，机器学习模型必须面对两个挑战：实现组合推理过程的需要，以及在许多应用中，需要解释这个推理过程以帮助用户进行开发和预测。旨在产生可解释的决策过程痕迹的现有模型通常要求在训练时监督这些痕迹。在本文中，我们提出了一种新颖的神经模块化方法，通过自动引入所需的子任务分解来执行组合推理，而不依赖于强有力的监督。我们的模型允许通过共享模块链接不同的推理任务，这些模块处理跨任务的常见例程。实验表明，与其他最先进的模型相比，该模型对人类评估者更具解释性：用户可以更好地理解模型的基础推理过程，并根据观察其中间输出来预测何时成功或失败。

##### URL
[http://arxiv.org/abs/1807.08556](http://arxiv.org/abs/1807.08556)

##### PDF
[http://arxiv.org/pdf/1807.08556](http://arxiv.org/pdf/1807.08556)

