---
layout: post
title: "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition"
date: 2015-05-11 02:23:06
categories: arXiv_CV
tags: arXiv_CV Speech_Recognition RNN Recognition
author: Xiangang Li, Xihong Wu
mathjax: true
---

* content
{:toc}

##### Abstract
Long short-term memory (LSTM) based acoustic modeling methods have recently been shown to give state-of-the-art performance on some speech recognition tasks. To achieve a further performance improvement, in this research, deep extensions on LSTM are investigated considering that deep hierarchical model has turned out to be more efficient than a shallow one. Motivated by previous research on constructing deep recurrent neural networks (RNNs), alternative deep LSTM architectures are proposed and empirically evaluated on a large vocabulary conversational telephone speech recognition task. Meanwhile, regarding to multi-GPU devices, the training process for LSTM networks is introduced and discussed. Experimental results demonstrate that the deep LSTM networks benefit from the depth and yield the state-of-the-art performance on this task.

##### Abstract (translated by Google)
基于长期短期记忆（LSTM）的声学建模方法最近已被证明在某些语音识别任务上给出了最先进的性能。为了进一步提高绩效，在本研究中，考虑到深层次模型比浅层模型效率更高，深入研究了LSTM。在以往关于构建深度递归神经网络（RNNs）的研究的启发下，提出了可选的深度LSTM体系结构，并在一个大型词汇会话电话语音识别任务上进行了实证评估。同时，针对多GPU设备，介绍并讨论了LSTM网络的训练过程。实验结果表明，深度LSTM网络受益于深度，并在此任务中获得最先进的性能。

##### URL
[https://arxiv.org/abs/1410.4281](https://arxiv.org/abs/1410.4281)

##### PDF
[https://arxiv.org/pdf/1410.4281](https://arxiv.org/pdf/1410.4281)

