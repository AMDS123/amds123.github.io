---
layout: post
title: "Explaining Recurrent Neural Network Predictions in Sentiment Analysis"
date: 2017-08-04 20:01:33
categories: arXiv_CL
tags: arXiv_CL Sentiment RNN Classification Prediction Quantitative
author: Leila Arras, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.

##### Abstract (translated by Google)
最近，被称为分层相关传播（LRP）的技术被证明能够以输入空间相关的形式提供洞察力的解释来理解前馈神经网络分类决策。在目前的工作中，我们将LRP的用法扩展到递归神经网络。我们提出了一个特定的传播规则，适用于像LSTM和GRU这样的经常性网络体系结构中出现的乘法连接。我们将这一技术应用于基于单词的双向LSTM模型上，对五类情感预测任务进行定性和定量评估，得到比以前工作中使用的基于梯度的相关方法更好的结果。

##### URL
[https://arxiv.org/abs/1706.07206](https://arxiv.org/abs/1706.07206)

##### PDF
[https://arxiv.org/pdf/1706.07206](https://arxiv.org/pdf/1706.07206)

