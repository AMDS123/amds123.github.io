---
layout: post
title: "Max-Pooling Dropout for Regularization of Convolutional Neural Networks"
date: 2015-12-04 13:18:37
categories: arXiv_CV
tags: arXiv_CV Regularization CNN Deep_Learning
author: Haibing Wu, Xiaodong Gu
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.

##### Abstract (translated by Google)
最近，辍学越来越多地用于深度学习。对于深卷积神经网络，已知退出在完全连接的层中工作良好。但是，其在汇聚层的效果仍不清楚。本文证明了最大池化丢失相当于在训练时间基于多项式分布的随机挑选激活。鉴于这种见解，我们主张使用我们提出的概率加权池，而不是常用的最大池，在测试时间作为模型平均。经验证据验证了概率加权池的优越性。我们还比较了最大池辍学和随机池，这两者都引入了基于池式阶段多项分布的随机性。

##### URL
[https://arxiv.org/abs/1512.01400](https://arxiv.org/abs/1512.01400)

##### PDF
[https://arxiv.org/pdf/1512.01400](https://arxiv.org/pdf/1512.01400)

