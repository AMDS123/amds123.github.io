---
layout: post
title: "Entropy Maximization for Markov Decision Processes Under Temporal Logic Constraints"
date: 2018-07-09 15:19:15
categories: arXiv_AI
tags: arXiv_AI Optimization
author: Yagiz Savas, Melkior Ornik, Murat Cubuktepe, Ufuk Topcu
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of synthesizing a policy that maximizes the entropy of a Markov decision process (MDP) subject to a temporal logic constraint. Such a policy minimizes the predictability of the paths it generates, or dually, maximizes the continual exploration of different paths in an MDP while ensuring the satisfaction of a temporal logic specification. We first show that the maximum entropy of an MDP can be finite, infinite or unbounded. We provide necessary and sufficient conditions under which the maximum entropy of an MDP is finite, infinite or unbounded. We then present an algorithm to synthesize a policy that maximizes the entropy of an MDP. The proposed algorithm is based on a convex optimization problem and runs in time polynomial in the size of the MDP. We also show that maximizing the entropy of an MDP is equivalent to maximizing the entropy of the paths that reach a certain set of states in the MDP. Finally, we extend the algorithm to an MDP subject to a temporal logic specification. In numerical examples, we demonstrate the proposed method on different motion planning scenarios and illustrate that as the restrictions imposed on the paths by a specification increase, the maximum entropy decreases, which in turn, increases the predictability of paths.

##### Abstract (translated by Google)
我们研究了综合一种策略的问题，该策略最大化了受时间逻辑约束的马尔可夫决策过程（MDP）的熵。这样的策略最小化其生成的路径的可预测性，或者双重地最大化MDP中的不同路径的连续探索，同时确保满足时间逻辑规范。我们首先表明MDP的最大熵可以是有限的，无限的或无界的。我们提供了必要和充分的条件，在这些条件下，MDP的最大熵是有限的，无限的或无限的。然后，我们提出一种算法来合成一种最大化MDP熵的策略。所提出的算法基于凸优化问题并且以MDP的大小运行时间多项式。我们还表明，最大化MDP的熵等同于最大化到达MDP中某一组状态的路径的熵。最后，我们将算法扩展到MDP主题的时间逻辑规范。在数值例子中，我们在不同的运动规划场景中演示了所提出的方法，并说明随着规范增加对路径的限制，最大熵减小，这反过来增加了路径的可预测性。

##### URL
[http://arxiv.org/abs/1807.03223](http://arxiv.org/abs/1807.03223)

##### PDF
[http://arxiv.org/pdf/1807.03223](http://arxiv.org/pdf/1807.03223)

