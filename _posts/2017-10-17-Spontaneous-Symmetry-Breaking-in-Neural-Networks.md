---
layout: post
title: "Spontaneous Symmetry Breaking in Neural Networks"
date: 2017-10-17 04:55:14
categories: arXiv_CV
tags: arXiv_CV Relation
author: Ricky Fok, Aijun An, Xiaogang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a framework to understand the unprecedented performance and robustness of deep neural networks using field theory. Correlations between the weights within the same layer can be described by symmetries in that layer, and networks generalize better if such symmetries are broken to reduce the redundancies of the weights. Using a two parameter field theory, we find that the network can break such symmetries itself towards the end of training in a process commonly known in physics as spontaneous symmetry breaking. This corresponds to a network generalizing itself without any user input layers to break the symmetry, but by communication with adjacent layers. In the layer decoupling limit applicable to residual networks (He et al., 2015), we show that the remnant symmetries that survive the non-linear layers are spontaneously broken. The Lagrangian for the non-linear and weight layers together has striking similarities with the one in quantum field theory of a scalar. Using results from quantum field theory we show that our framework is able to explain many experimentally observed phenomena,such as training on random labels with zero error (Zhang et al., 2017), the information bottleneck, the phase transition out of it and gradient variance explosion (Shwartz-Ziv & Tishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.

##### Abstract (translated by Google)
我们提出了一个框架来理解使用场理论的深度神经网络的前所未有的性能和鲁棒性。相同层内的权重之间的相关性可以通过该层中的对称来描述，并且如果这样的对称性被破坏以减少权重的冗余，则网络更好地概括。使用双参数场理论，我们发现，网络可以打破这种对称性本身就是在自然对称性破缺的过程中，在物理学中一直被称为训练的结束。这对应于网络的概括，没有任何用户输入层来打破对称性，而是通过与相邻层的通信。在适用于剩余网络的层次解耦限制（He et al。，2015）中，我们证明非线性层的残余对称性是自发破坏的。拉格朗日的非线性和加权层与标量的量子场论中的拉格朗日非常相似。利用量子场论的结果，我们发现我们的框架能够解释许多实验观察到的现象，如随机标签上的零误差训练（Zhang et al。，2017），信息瓶颈，相变以及梯度（Shwartz-Ziv＆Tishby，2017），破碎的梯度（Balduzzi等，2017）等等。

##### URL
[https://arxiv.org/abs/1710.06096](https://arxiv.org/abs/1710.06096)

##### PDF
[https://arxiv.org/pdf/1710.06096](https://arxiv.org/pdf/1710.06096)

