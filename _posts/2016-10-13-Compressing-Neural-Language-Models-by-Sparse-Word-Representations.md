---
layout: post
title: "Compressing Neural Language Models by Sparse Word Representations"
date: 2016-10-13 06:55:54
categories: arXiv_CL
tags: arXiv_CL Sparse Embedding Language_Model
author: Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, Zhi Jin
mathjax: true
---

* content
{:toc}

##### Abstract
Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.

##### Abstract (translated by Google)
神经网络是最先进的语言建模技术之一。现有的神经语言模型通常将离散的单词映射到分布密集的矢量表示。在通过隐藏层对前面的上下文单词进行信息处理之后，输出层估计下一个单词的概率。由于字嵌入和输出层的大量参数，这种方法是时间和内存密集型的。在本文中，我们建议通过稀疏词表示来压缩神经语言模型。在实验中，随着词汇大小的增长，模型中的参数数量增长非常缓慢，几乎不可察觉。此外，我们的方法不仅在很大程度上减少了参数空间，而且在困惑度量方面也提高了性能。

##### URL
[https://arxiv.org/abs/1610.03950](https://arxiv.org/abs/1610.03950)

##### PDF
[https://arxiv.org/pdf/1610.03950](https://arxiv.org/pdf/1610.03950)

