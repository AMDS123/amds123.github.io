---
layout: post
title: "Improved Image Captioning via Policy Gradient optimization of SPIDEr"
date: 2018-03-12 18:53:06
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption Optimization
author: Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, Kevin Murphy
mathjax: true
---

* content
{:toc}

##### Abstract
Current image captioning methods are usually trained via (penalized) maximum likelihood estimation. However, the log-likelihood score of a caption does not correlate well with human assessments of quality. Standard syntactic evaluation metrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The newer SPICE and CIDEr metrics are better correlated, but have traditionally been hard to optimize for. In this paper, we show how to use a policy gradient (PG) method to directly optimize a linear combination of SPICE and CIDEr (a combination we call SPIDEr): the SPICE score ensures our captions are semantically faithful to the image, while CIDEr score ensures our captions are syntactically fluent. The PG method we propose improves on the prior MIXER approach, by using Monte Carlo rollouts instead of mixing MLE training with PG. We show empirically that our algorithm leads to easier optimization and improved results compared to MIXER. Finally, we show that using our PG method we can optimize any of the metrics, including the proposed SPIDEr metric which results in image captions that are strongly preferred by human raters compared to captions generated by the same model but trained to optimize MLE or the COCO metrics.

##### Abstract (translated by Google)
当前的图像字幕方法通常通过（惩罚的）最大似然估计来训练。然而，标题的对数似然得分与人类对质量的评估并不相关。标准的句法评估指标，如BLEU，METEOR和ROUGE，也没有很好的相关性。较新的SPICE和CIDEr指标具有更好的相关性，但传统上很难进行优化。在本文中，我们展示了如何使用策略梯度（PG）方法直接优化SPICE和CIDEr的线性组合（我们称之为SPIDEr的组合）：SPICE分数确保我们的字幕在语义上忠实于图像，而CIDEr得分确保我们的字幕在语法上流畅。我们提出的PG方法通过使用蒙特卡罗推出而不是将MLE训练与PG混合来改进先前的MIXER方法。我们凭经验证明，与MIXER相比，我们的算法可以更轻松地优化和改进结果。最后，我们展示了使用我们的PG方法，我们可以优化任何指标，包括建议的SPIDEr指标，导致人类评价者强烈偏好的图像标题与同一模型生成的标题相比，但经过培训可以优化MLE或COCO指标。

##### URL
[https://arxiv.org/abs/1612.00370](https://arxiv.org/abs/1612.00370)

##### PDF
[https://arxiv.org/pdf/1612.00370](https://arxiv.org/pdf/1612.00370)

