---
layout: post
title: "Universal Style Transfer via Feature Transforms"
date: 2017-11-17 18:30:43
categories: arXiv_CV
tags: arXiv_CV Style_Transfer Optimization Inference
author: Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang
mathjax: true
---

* content
{:toc}

##### Abstract
Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.

##### Abstract (translated by Google)
通用样式转换旨在将任意视觉样式转换为内容图像。现有的基于前馈的方法在享受推理效率的同时，主要受到无法推广到不可见样式或影响视觉质量的限制。在本文中，我们提出了一个简单而有效的方法，可以解决这些限制，而无需对任何预定义的样式进行培训。我们的方法的关键要素是一对特征变换，美白和着色，嵌入到图像重建网络。白化和着色变换反映了内容图像的特征协方差与给定样式图像的直接匹配，与神经样式转移中基于格拉姆矩阵的成本优化具有相似的精神。我们证明了我们的算法的有效性，通过生成高质量的程式化的图像与最近的一些方法进行比较。我们还通过可视化白化特征和通过简单特征着色来合成纹理来分析我们的方法。

##### URL
[https://arxiv.org/abs/1705.08086](https://arxiv.org/abs/1705.08086)

##### PDF
[https://arxiv.org/pdf/1705.08086](https://arxiv.org/pdf/1705.08086)

