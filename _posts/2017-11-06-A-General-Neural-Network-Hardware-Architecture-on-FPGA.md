---
layout: post
title: "A General Neural Network Hardware Architecture on FPGA"
date: 2017-11-06 19:17:58
categories: arXiv_CV
tags: arXiv_CV Recognition
author: Yufeng Hao
mathjax: true
---

* content
{:toc}

##### Abstract
Field Programmable Gate Arrays (FPGAs) plays an increasingly important role in data sampling and processing industries due to its highly parallel architecture, low power consumption, and flexibility in custom algorithms. Especially, in the artificial intelligence field, for training and implement the neural networks and machine learning algorithms, high energy efficiency hardware implement and massively parallel computing capacity are heavily demanded. Therefore, many global companies have applied FPGAs into AI and Machine learning fields such as autonomous driving and Automatic Spoken Language Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3]. Considering the FPGAs great potential in these fields, we tend to implement a general neural network hardware architecture on XILINX ZU9CG System On Chip (SOC) platform [4], which contains abundant hardware resource and powerful processing capacity. The general neural network architecture on the FPGA SOC platform can perform forward and backward algorithms in deep neural networks (DNN) with high performance and easily be adjusted according to the type and scale of the neural networks.

##### Abstract (translated by Google)
现场可编程门阵列（FPGA）由于其高度并行的架构，低功耗和定制算法的灵活性，在数据采样和处理行业中扮演着越来越重要的角色。特别是在人工智能领域，为了训练和实现神经网络和机器学习算法，高能效硬件工具和大规模并行计算能力是非常需要的。因此，很多全球公司已经将FPGA应用到了自动驾驶和自动语言识别（Baidu）[1] [2]和Bing搜索（Microsoft）[3]等AI和机器学习领域。考虑到FPGA在这些领域的巨大潜力，我们倾向于在XILINX ZU9CG片上系统（SOC）平台上实现一个通用的神经网络硬件体系结构[4]，它拥有丰富的硬件资源和强大的处理能力。 FPGA SOC平台上的通用神经网络结构可以在深度神经网络（DNN）中执行前向和后向算法，性能高，并且可以根据神经网络的类型和规模进行调整。

##### URL
[https://arxiv.org/abs/1711.05860](https://arxiv.org/abs/1711.05860)

##### PDF
[https://arxiv.org/pdf/1711.05860](https://arxiv.org/pdf/1711.05860)

