---
layout: post
title: "Defending Against Adversarial Examples with K-Nearest Neighbor"
date: 2019-06-23 00:38:07
categories: arXiv_AI
tags: arXiv_AI Adversarial
author: Chawin Sitawarin, David Wagner
mathjax: true
---

* content
{:toc}

##### Abstract
Robustness is an increasingly important property of machine learning models as they become more and more prevalent. We propose a defense against adversarial examples based on a k-nearest neighbor (kNN) on the intermediate activation of neural networks. Our scheme surpasses state-of-the-art defenses on MNIST and CIFAR-10 against l2-perturbation by a significant margin. With our models, the mean perturbation norm required to fool our MNIST model is 3.07 and 2.30 on CIFAR-10. Additionally, we propose a simple certifiable lower bound on the l2-norm of the adversarial perturbation using a more specific version of our scheme, a 1-NN on representations learned by a Lipschitz network. Our model provides a nontrivial average lower bound of the perturbation norm, comparable to other schemes on MNIST with similar clean accuracy.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.09525](http://arxiv.org/abs/1906.09525)

##### PDF
[http://arxiv.org/pdf/1906.09525](http://arxiv.org/pdf/1906.09525)

