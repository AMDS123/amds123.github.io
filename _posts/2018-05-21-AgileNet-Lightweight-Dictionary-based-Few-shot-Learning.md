---
layout: post
title: "AgileNet: Lightweight Dictionary-based Few-shot Learning"
date: 2018-05-21 22:36:11
categories: arXiv_AI
tags: arXiv_AI Knowledge Inference Deep_Learning
author: Mohammad Ghasemzadeh, Fang Lin, Bita Darvish Rouhani, Farinaz Koushanfar, Ke Huang
mathjax: true
---

* content
{:toc}

##### Abstract
The success of deep learning models is heavily tied to the use of massive amount of labeled data and excessively long training time. With the emergence of intelligent edge applications that use these models, the critical challenge is to obtain the same inference capability on a resource-constrained device while providing adaptability to cope with the dynamic changes in the data. We propose AgileNet, a novel lightweight dictionary-based few-shot learning methodology which provides reduced complexity deep neural network for efficient execution at the edge while enabling low-cost updates to capture the dynamics of the new data. Evaluations of state-of-the-art few-shot learning benchmarks demonstrate the superior accuracy of AgileNet compared to prior arts. Additionally, AgileNet is the first few-shot learning approach that prevents model updates by eliminating the knowledge obtained from the primary training. This property is ensured through the dictionaries learned by our novel end-to-end structured decomposition, which also reduces the memory footprint and computation complexity to match the edge device constraints.

##### Abstract (translated by Google)
深度学习模式的成功与使用大量标记数据和训练时间过长密切相关。随着使用这些模型的智能边缘应用程序的出现，关键的挑战是在资源受限的设备上获得相同的推理能力，同时提供适应性以应对数据的动态变化。我们提出AgileNet，这是一种基于轻量级词典的新颖的少量学习方法，它提供了降低复杂度的深度神经网络，以便在边缘高效执​​行，同时实现低成本更新以捕捉新数据的动态。最先进的少量学习基准评估表明，与现有技术相比，AgileNet具有更高的准确性。此外，AgileNet是第一个通过消除从主要培训获得的知识来防止模型更新的少量学习方法。通过我们新颖的端到端结构化分解学习的字典保证了这一特性，这也降低了内存占用面积和计算复杂性以匹配边缘设备约束。

##### URL
[https://arxiv.org/abs/1805.08311](https://arxiv.org/abs/1805.08311)

##### PDF
[https://arxiv.org/pdf/1805.08311](https://arxiv.org/pdf/1805.08311)

