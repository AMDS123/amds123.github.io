---
layout: post
title: "A Simple Approach to Learn Polysemous Word Embeddings"
date: 2017-08-14 18:44:20
categories: arXiv_CL
tags: arXiv_CL Embedding Represenation_Learning Language_Model Quantitative Detection
author: Yifan Sun, Nikhil Rao, Weicong Ding
mathjax: true
---

* content
{:toc}

##### Abstract
Many NLP applications require disambiguating polysemous words. Existing methods that learn polysemous word vector representations involve first detecting various senses and optimizing the sense-specific embeddings separately, which are invariably more involved than single sense learning methods such as word2vec. Evaluating these methods is also problematic, as rigorous quantitative evaluations in this space is limited, especially when compared with single-sense embeddings. In this paper, we propose a simple method to learn a word representation, given any context. Our method only requires learning the usual single sense representation, and coefficients that can be learnt via a single pass over the data. We propose several new test sets for evaluating word sense induction, relevance detection, and contextual word similarity, significantly supplementing the currently available tests. Results on these and other tests show that while our method is embarrassingly simple, it achieves excellent results when compared to the state of the art models for unsupervised polysemous word representation learning.

##### Abstract (translated by Google)
许多NLP应用程序需要消歧多义词。学习多义词向量表示的现有方法涉及到首先检测各种意义，并分别优化特定于意义的嵌入，这些嵌入总是比单向感知学习方法（如word2vec）更为复杂。评估这些方法也是有问题的，因为在这个空间中严格的定量评估是有限的，尤其是与单义嵌入相比。在本文中，我们提出了一个简单的方法来学习一个词的表示，给定任何上下文。我们的方法只需要学习通常的单一意义表示和可以通过一次遍历数据学习的系数。我们提出了几个新的测试集，用于评估词义感应，相关性检测和上下文词汇相似性，大大补充了当前可用的测试。这些和其他测试的结果表明，虽然我们的方法是令人尴尬的简单，但与无监督多义词表示学习的现有技术模型相比，它取得了优异的结果。

##### URL
[https://arxiv.org/abs/1707.01793](https://arxiv.org/abs/1707.01793)

##### PDF
[https://arxiv.org/pdf/1707.01793](https://arxiv.org/pdf/1707.01793)

