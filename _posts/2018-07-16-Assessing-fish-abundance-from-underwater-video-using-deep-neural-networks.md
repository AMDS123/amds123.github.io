---
layout: post
title: "Assessing fish abundance from underwater video using deep neural networks"
date: 2018-07-16 13:13:37
categories: arXiv_CV
tags: arXiv_CV Object_Detection Classification Deep_Learning Detection Recognition
author: Ranju Mandal, Rod M. Connolly, Thomas A. Schlacherz, Bela Stantic
mathjax: true
---

* content
{:toc}

##### Abstract
Uses of underwater videos to assess diversity and abundance of fish are being rapidly adopted by marine biologists. Manual processing of videos for quantification by human analysts is time and labour intensive. Automatic processing of videos can be employed to achieve the objectives in a cost and time-efficient way. The aim is to build an accurate and reliable fish detection and recognition system, which is important for an autonomous robotic platform. However, there are many challenges involved in this task (e.g. complex background, deformation, low resolution and light propagation). Recent advancement in the deep neural network has led to the development of object detection and recognition in real time scenarios. An end-to-end deep learning-based architecture is introduced which outperformed the state of the art methods and first of its kind on fish assessment task. A Region Proposal Network (RPN) introduced by an object detector termed as Faster R-CNN was combined with three classification networks for detection and recognition of fish species obtained from Remote Underwater Video Stations (RUVS). An accuracy of 82.4% (mAP) obtained from the experiments are much higher than previously proposed methods.

##### Abstract (translated by Google)
海洋生物学家正在迅速采用水下视频来评估鱼类的多样性和丰富程度。人工分析师手动处理视频以进行量化是耗费时间和人力的。可以采用视频的自动处理以成本和时间有效的方式实现目标。目的是建立一个准确可靠的鱼类检测和识别系统，这对于自动机器人平台非常重要。然而，该任务涉及许多挑战（例如复杂的背景，变形，低分辨率和光传播）。深度神经网络的最新进展已经导致在实时场景中开发对象检测和识别。引入了基于端到端深度学习的体系结构，该体系结构优于最先进的方法，并且首先在鱼类评估任务方面表现出色。由称为快速R-CNN的物体检测器引入的区域提议网络（RPN）与用于检测和识别从远程水下视频站（RUVS）获得的鱼类的三个分类网络组合。从实验中获得的82.4％（mAP）的准确度远高于先前提出的方法。

##### URL
[http://arxiv.org/abs/1807.05838](http://arxiv.org/abs/1807.05838)

##### PDF
[http://arxiv.org/pdf/1807.05838](http://arxiv.org/pdf/1807.05838)

