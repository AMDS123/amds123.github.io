---
layout: post
title: "Leveraging Intra-User and Inter-User Representation Learning for Automated Hate Speech Detection"
date: 2018-09-14 02:31:25
categories: arXiv_AI
tags: arXiv_AI Represenation_Learning RNN Detection
author: Jing Qian, Mai ElSherief, Elizabeth M. Belding, William Yang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Hate speech detection is a critical, yet challenging problem in Natural Language Processing (NLP). Despite the existence of numerous studies dedicated to the development of NLP hate speech detection approaches, the accuracy is still poor. The central problem is that social media posts are short and noisy, and most existing hate speech detection solutions take each post as an isolated input instance, which is likely to yield high false positive and negative rates. In this paper, we radically improve automated hate speech detection by presenting a novel model that leverages intra-user and inter-user representation learning for robust hate speech detection on Twitter. In addition to the target Tweet, we collect and analyze the user's historical posts to model intra-user Tweet representations. To suppress the noise in a single Tweet, we also model the similar Tweets posted by all other users with reinforced inter-user representation learning techniques. Experimentally, we show that leveraging these two representations can significantly improve the f-score of a strong bidirectional LSTM baseline model by 10.1%.

##### Abstract (translated by Google)
仇恨语音检测是自然语言处理（NLP）中的一个关键但具有挑战性的问题。尽管存在大量致力于开发NLP仇恨语音检测方法的研究，但准确性仍然很差。中心问题是社交媒体帖子短而嘈杂，并且大多数现有的仇恨语音检测解决方案将每个帖子作为孤立的输入实例，这可能产生高的假阳性和阴性率。在本文中，我们通过提出一种新型模型从根本上改善自动仇恨语音检测，该模型利用用户内和用户间表示学习来在Twitter上进行强大的仇恨语音检测。除了目标推文之外，我们还收集并分析用户的历史帖子，以模拟用户内部的推文表示。为了抑制单个推文中的噪音，我们还使用强化的用户间表示学习技术对所有其他用户发布的类似推文进行建模。在实验上，我们证明利用这两种表示可以显着提高强双向LSTM基线模型的f值得分10.1％。

##### URL
[http://arxiv.org/abs/1804.03124](http://arxiv.org/abs/1804.03124)

##### PDF
[http://arxiv.org/pdf/1804.03124](http://arxiv.org/pdf/1804.03124)

