---
layout: post
title: "Are good local minima wide in sparse recovery?"
date: 2018-06-21 15:39:22
categories: arXiv_CV
tags: arXiv_CV Sparse Optimization
author: Michael Moeller, Otmar Loffeld, Juergen Gall, Felix Krahmer
mathjax: true
---

* content
{:toc}

##### Abstract
The idea of compressed sensing is to exploit representations in suitable (overcomplete) dictionaries that allow to recover signals far beyond the Nyquist rate provided that they admit a sparse representation in the respective dictionary. The latter gives rise to the sparse recovery problem of finding the best sparse linear approximation of given data in a given generating system. In this paper we analyze the iterative hard thresholding (IHT) algorithm as one of the most popular greedy methods for solving the sparse recovery problem, and demonstrate that systematically perturbing the IHT algorithm by adding noise to intermediate iterates yields improved results. Further improvements can be obtained by entirely rephrasing the problem as a parametric deep-learning-type of optimization problem. By introducing perturbations via dropout, we demonstrate to significantly outperform the classical IHT algorithm, obtaining $3$ to $6$ times lower average objective errors.

##### Abstract (translated by Google)
压缩感知的思想是利用合适的（过完备的）字典中的表示，其允许恢复远远超出奈奎斯特速率的信号，只要它们在相应字典中承认稀疏表示。后者引起稀疏恢复问题，即在给定的生成系统中找到给定数据的最佳稀疏线性逼近。在本文中，我们分析了迭代硬阈值（IHT）算法作为解决稀疏恢复问题的最流行的贪心方法之一，并且证明了通过向中间迭代添加噪声来系统扰动IHT算法产生了改进的结果。通过将问题完全改写为参数化深度学习类型的优化问题，可以获得进一步的改进。通过引入辍学干扰，我们证明其显着优于传统的IHT算法，从而获得3美元至6美元的平均客观误差。

##### URL
[http://arxiv.org/abs/1806.08296](http://arxiv.org/abs/1806.08296)

##### PDF
[http://arxiv.org/pdf/1806.08296](http://arxiv.org/pdf/1806.08296)

