---
layout: post
title: "SoundNet: Learning Sound Representations from Unlabeled Video"
date: 2016-10-27 20:23:39
categories: arXiv_CV
tags: arXiv_CV Knowledge Classification Recognition
author: Yusuf Aytar, Carl Vondrick, Antonio Torralba
mathjax: true
---

* content
{:toc}

##### Abstract
We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.

##### Abstract (translated by Google)
我们通过利用野外收集的大量无标签声音数据来学习丰富的自然声音表现形式。我们利用视觉和声音之间的自然同步来学习使用200万个未标记视频的声音表示。未标记的视频具有可以在大规模上经济地获得的优点，但是包含有关自然声音的有用信号。我们提出了一个学生教师培训程序，将有识别性的视觉知识从完善的视觉识别模型转化为使用无标签视频作为桥梁的声音模态。我们的声音表现比声学场景/物体分类标准基准的最新成果显着提高了性能。可视化表明，一些高级语义会自动出现在声音网络中，即使它没有经过地面实况标签的训练。

##### URL
[https://arxiv.org/abs/1610.09001](https://arxiv.org/abs/1610.09001)

##### PDF
[https://arxiv.org/pdf/1610.09001](https://arxiv.org/pdf/1610.09001)

