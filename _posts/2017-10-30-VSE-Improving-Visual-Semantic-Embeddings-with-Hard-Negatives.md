---
layout: post
title: "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives"
date: 2017-10-30 15:55:21
categories: arXiv_CV
tags: arXiv_CV Caption
author: Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, Sanja Fidler
mathjax: true
---

* content
{:toc}

##### Abstract
We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings. That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance. We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).

##### Abstract (translated by Google)
我们提出了一种新的跨语义检索视觉语义嵌入技术。受结构化预测中使用硬性否定以及检索中使用的排序损失函数的启发，我们引入对用于学习多模式嵌入的常用损失函数的简单改变。结合微调和扩展数据的使用，在检索性能方面会有显着的提高。我们在MS-COCO和Flickr30K数据集上展示了我们的方法，称为VSE ++，使用消融研究和现有方法进行比较。在MS-COCO上，我们的方法在字幕检索方面比现有技术方法提高了8.8％，在图像检索方面（基于R1）提高了11.3％。

##### URL
[https://arxiv.org/abs/1707.05612](https://arxiv.org/abs/1707.05612)

