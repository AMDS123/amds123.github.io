---
layout: post
title: "Unifying Identification and Context Learning for Person Recognition"
date: 2018-06-08 11:05:05
categories: arXiv_CV
tags: arXiv_CV Attention Face Relation Recognition Face_Recognition
author: Qingqiu Huang, Yu Xiong, Dahua Lin
mathjax: true
---

* content
{:toc}

##### Abstract
Despite the great success of face recognition techniques, recognizing persons under unconstrained settings remains challenging. Issues like profile views, unfavorable lighting, and occlusions can cause substantial difficulties. Previous works have attempted to tackle this problem by exploiting the context, e.g. clothes and social relations. While showing promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from people identities. In this work, we aim to move beyond such limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also develop a unified formulation, where the social contexts are learned along with the reasoning of people identities. These models substantially improve the robustness when working with the complex contextual relations in unconstrained environments. On two large datasets, PIPA and Cast In Movies (CIM), a new dataset proposed in this work, our method consistently achieves state-of-the-art performance under multiple evaluation policies.

##### Abstract (translated by Google)
尽管人脸识别技术取得了巨大的成功，但在不受约束的环境下识别人员依然具有挑战性。诸如配置文件视图，不利照明和遮挡等问题可能会造成很大的困难。以前的作品试图通过利用上下文来解决这个问题，例如，衣服和社会关系。虽然显示出有希望的改进，但它们通常局限于两个重要方面，依靠简单的启发式方法来结合不同的线索，并将情境的构建与人的身份分开。在这项工作中，我们的目标是超越这些限制，并提出一个新的框架来利用上下文进行人员识别。特别是，我们提出了一个区域关注网络，该网络学习将视觉线索与实例相关的权重进行自适应组合。我们也制定了一个统一的表述，其中社会背景与人身份的推理一起学习。当处理复杂的上下文关系时，这些模型在不受约束的环境中大大提高了鲁棒性。在两个大型数据集中，PIPA和Cast In Movies（CIM）是一项在本文中提出的新数据集，我们的方法在多种评估策略下始终如一地实现了最先进的性能。

##### URL
[http://arxiv.org/abs/1806.03084](http://arxiv.org/abs/1806.03084)

##### PDF
[http://arxiv.org/pdf/1806.03084](http://arxiv.org/pdf/1806.03084)

