---
layout: post
title: "Generative Adversarial Perturbations"
date: 2017-12-06 18:52:12
categories: arXiv_CV
tags: arXiv_CV Adversarial Segmentation Semantic_Segmentation Inference Classification
author: Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.

##### Abstract (translated by Google)
在本文中，我们提出了创造对抗性的例子，微扰图像类似于自然图像，但恶意制作愚弄预训练模型的新型生成模型。我们提出了可训练的深度神经网络转换图像敌对扰动。我们提出的模型可以产生针对性和非针对性攻击的图像不可知和图像相关干扰。我们还证明，类似的体系结构可以在愚弄分类和语义分割模型方面取得令人印象深刻的结果，从而避免了每个任务的手工攻击方法。在挑战高分辨率数据集（如ImageNet和Cityscapes）的广泛实验中，我们证明了我们的扰动在小的扰动规范下达到了高的愚弄率。而且，我们的攻击在推理时比目前的迭代方法快得多。

##### URL
[http://arxiv.org/abs/1712.02328](http://arxiv.org/abs/1712.02328)

##### PDF
[http://arxiv.org/pdf/1712.02328](http://arxiv.org/pdf/1712.02328)

