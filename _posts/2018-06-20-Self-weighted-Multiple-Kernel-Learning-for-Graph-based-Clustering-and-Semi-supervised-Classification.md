---
layout: post
title: "Self-weighted Multiple Kernel Learning for Graph-based Clustering and Semi-supervised Classification"
date: 2018-06-20 12:46:43
categories: arXiv_AI
tags: arXiv_AI Classification
author: Zhao Kang, Xiao Lu, Jinfeng Yi, Zenglin Xu
mathjax: true
---

* content
{:toc}

##### Abstract
Multiple kernel learning (MKL) method is generally believed to perform better than single kernel method. However, some empirical studies show that this is not always true: the combination of multiple kernels may even yield an even worse performance than using a single kernel. There are two possible reasons for the failure: (i) most existing MKL methods assume that the optimal kernel is a linear combination of base kernels, which may not hold true; and (ii) some kernel weights are inappropriately assigned due to noises and carelessly designed algorithms. In this paper, we propose a novel MKL framework by following two intuitive assumptions: (i) each kernel is a perturbation of the consensus kernel; and (ii) the kernel that is close to the consensus kernel should be assigned a large weight. Impressively, the proposed method can automatically assign an appropriate weight to each kernel without introducing additional parameters, as existing methods do. The proposed framework is integrated into a unified framework for graph-based clustering and semi-supervised classification. We have conducted experiments on multiple benchmark datasets and our empirical results verify the superiority of the proposed framework.

##### Abstract (translated by Google)
一般认为多核学习（MKL）方法比单核方法更好。然而，一些经验研究表明，这并非总是如此：多核的组合甚至可能会产生比使用单个内核更差的性能。这种失败有两种可能的原因：（i）大多数现有的MKL方法都假定最优内核是基本内核的线性组合，这可能不适用; （ii）由于噪声和不小心设计的算法，某些内核权重被不恰当地分配。在本文中，我们通过遵循两个直观的假设提出了一个新的MKL框架：（i）每个核是一致核的扰动;和（ii）接近共识内核的核心应该被分配很大的权重。令人印象深刻的是，所提出的方法可以自动为每个内核分配适当的权重，而不像现有方法那样引入额外的参数。所提出的框架被集成到基于图的聚类和半监督分类的统一框架中。我们已经对多个基准数据集进行了实验，并且我们的实证结果验证了所提出的框架的优越性。

##### URL
[http://arxiv.org/abs/1806.07697](http://arxiv.org/abs/1806.07697)

##### PDF
[http://arxiv.org/pdf/1806.07697](http://arxiv.org/pdf/1806.07697)

