---
layout: post
title: "Multi-task Maximum Entropy Inverse Reinforcement Learning"
date: 2018-05-22 21:57:34
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Adam Gleave, Oliver Habryka
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes the first formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms require hundreds of demonstrations to solve. Furthermore, we outline how our formulation can be applied to state-of-the-art MCE IRL algorithms such as Guided Cost Learning. This extension, based on meta-learning, could enable multi-task IRL to be performed for the first time in high-dimensional, continuous state MDPs with unknown dynamics as commonly arise in robotics.

##### Abstract (translated by Google)
多任务反向强化学习（IRL）是从专家演示中推断多种奖励功能的问题。由于计算限制，以前的工作基于贝叶斯IRL，无法扩展到复杂的环境。本文为计算效率更高的最大因果熵（MCE）IRL框架提出了多任务IRL的第一个表达式。实验表明我们的方法可以在单一任务IRL算法需要数百个演示来解决的网格世界环境中执行一次性模仿学习。此外，我们概述了我们的公式如何应用于最先进的MCE IRL算法，如引导成本学习。这种基于元学习的扩展可以使多任务IRL首次在机器人学中常见的具有未知动态的高维连续状态MDP中执行。

##### URL
[http://arxiv.org/abs/1805.08882](http://arxiv.org/abs/1805.08882)

##### PDF
[http://arxiv.org/pdf/1805.08882](http://arxiv.org/pdf/1805.08882)

