---
layout: post
title: "Fixed-point Factorized Networks"
date: 2017-08-29 09:46:41
categories: arXiv_CV
tags: arXiv_CV Classification
author: Peisong Wang, Jian Cheng
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, Deep Neural Networks (DNN) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) for pretrained models to reduce the computational complexity as well as the storage requirement of networks. The resulting networks have only weights of -1, 0 and 1, which significantly eliminates the most resource-consuming multiply-accumulate operations (MACs). Extensive experiments on large-scale ImageNet classification task show the proposed FFN only requires one-thousandth of multiply operations with comparable accuracy.

##### Abstract (translated by Google)
近年来，基于深度神经网络（DNN）的方法在广泛的任务中取得了显着的成绩，并且一直是计算机视觉领域中功能最强大，应用最广泛的技术之一。然而，基于DNN的方法既是计算密集型的又是资源消耗型的，这阻碍了这些方法在智能手机等嵌入式系统中的应用。为了缓解这个问题，我们引入了一种新的定点因子分解网络（FFN）用于预训练模型，以降低计算复杂度以及网络的存储需求。由此产生的网络只有权重-1,0和1，这显着消除了最耗费资源的乘法累加操作（MAC）。大规模ImageNet分类任务的大量实验表明，所提出的FFN仅需要千分之一的乘法运算，精确度相当。

##### URL
[https://arxiv.org/abs/1611.01972](https://arxiv.org/abs/1611.01972)

##### PDF
[https://arxiv.org/pdf/1611.01972](https://arxiv.org/pdf/1611.01972)

