---
layout: post
title: "Sketching Word Vectors Through Hashing"
date: 2017-05-11 15:53:00
categories: arXiv_CL
tags: arXiv_CL Sparse Embedding
author: Behrang QasemiZadeh, Laura Kallmeyer, Peyman Passban
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a new fast word embedding technique using hash functions. The method is a derandomization of a new type of random projections: By disregarding the classic constraint used in designing random projections (i.e., preserving pairwise distances in a particular normed space), our solution exploits extremely sparse non-negative random projections. Our experiments show that the proposed method can achieve competitive results, comparable to neural embedding learning techniques, however, with only a fraction of the computational complexity of these methods. While the proposed derandomization enhances the computational and space complexity of our method, the possibility of applying weighting methods such as positive pointwise mutual information (PPMI) to our models after their construction (and at a reduced dimensionality) imparts a high discriminatory power to the resulting embeddings. Obviously, this method comes with other known benefits of random projection-based techniques such as ease of update.

##### Abstract (translated by Google)
我们提出了一个新的使用散列函数的快速词嵌入技术。该方法是一种新型随机投影的去随机化：通过忽略用于设计随机投影的经典约束（即保留特定赋范空间中的成对距离），我们的解决方案利用非常稀疏的非负随机投影。我们的实验表明，所提出的方法可以实现竞争结果，但是与神经嵌入学习技术相比，只有这些方法的计算复杂度的一小部分。虽然所提出的去随机化增强了我们方法的计算和空间复杂性，但是在建模之后（和降维）对模型应用正向逐点互信息（PPMI）等加权方法的可能性赋予了最终的高判别能力的嵌入。很明显，这种方法带来了其他已知的随机投影技术的好处，例如易于更新。

##### URL
[https://arxiv.org/abs/1705.04253](https://arxiv.org/abs/1705.04253)

##### PDF
[https://arxiv.org/pdf/1705.04253](https://arxiv.org/pdf/1705.04253)

