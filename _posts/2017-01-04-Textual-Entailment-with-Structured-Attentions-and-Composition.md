---
layout: post
title: "Textual Entailment with Structured Attentions and Composition"
date: 2017-01-04 19:14:37
categories: arXiv_SD
tags: arXiv_SD Attention Deep_Learning Relation
author: Kai Zhao, Liang Huang, Mingbo Ma
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning techniques are increasingly popular in the textual entailment task, overcoming the fragility of traditional discrete models with hard alignments and logics. In particular, the recently proposed attention models (Rockt\"aschel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art accuracy by computing soft word alignments between the premise and hypothesis sentences. However, there remains a major limitation: this line of work completely ignores syntax and recursion, which is helpful in many traditional efforts. We show that it is beneficial to extend the attention model to tree nodes between premise and hypothesis. More importantly, this subtree-level attention reveals information about entailment relation. We study the recursive composition of this subtree-level entailment relation, which can be viewed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy.

##### Abstract (translated by Google)
深度学习技术在文本包含任务中越来越受欢迎，克服了传统离散模型的难度和难度。特别是，最近提出的关注模型（Rockt \'asch​​el等，2015; Wang and Jiang，2015）通过计算前提句和假设句之间的软词对齐来达到最高的准确性。主要局限在于：这一系列的工作完全忽略了语法和递归，这在许多传统的努力中是有帮助的，我们证明在假设和前提之间将注意模型扩展到树节点是有益的，更重要的是，关于蕴涵关系的信息，我们研究了这种子树级蕴涵关系的递归组合，可以看作是自然逻辑框架的一个软版本（MacCartney and Manning，2009）。实验表明，我们的结构化注意和蕴涵组合模型可以从下到上正确识别和推断需求关系，并提高准确度。

##### URL
[https://arxiv.org/abs/1701.01126](https://arxiv.org/abs/1701.01126)

##### PDF
[https://arxiv.org/pdf/1701.01126](https://arxiv.org/pdf/1701.01126)

