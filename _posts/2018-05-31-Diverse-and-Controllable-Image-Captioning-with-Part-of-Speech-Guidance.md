---
layout: post
title: "Diverse and Controllable Image Captioning with Part-of-Speech Guidance"
date: 2018-05-31 17:56:17
categories: arXiv_CV
tags: arXiv_CV Image_Caption Adversarial Caption RNN Prediction
author: Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander Schwing, D. A. Forsyth
mathjax: true
---

* content
{:toc}

##### Abstract
Automatically describing an image is an important capability for virtual assistants. Significant progress has been achieved in recent years on this task of image captioning. However, classical prediction techniques based on maximum likelihood trained LSTM nets don't embrace the inherent ambiguity of image captioning. To address this concern, recent variational auto-encoder and generative adversarial network based methods produce a set of captions by sampling from an abstract latent space. But, this latent space has limited interpretability and therefore, a control mechanism for captioning remains an open problem. This paper proposes a captioning technique conditioned on part-of-speech. Our method provides human interpretable control in form of part-of-speech. Importantly, part-of-speech is a language prior, and conditioning on it provides: (i) more diversity as evaluated by counting n-grams and the novel sentences generated, (ii) achieves high accuracy for the diverse captions on standard captioning metrics.

##### Abstract (translated by Google)
自动描述图像是虚拟助手的重要功能。近年来，在图像字幕的这一任务上取得了重大进展。然而，基于最大似然训练的LSTM网络的经典预测技术不包含图像字幕的固有模糊性。为了解决这个问题，最近的变分自动编码器和基于生成对抗网络的方法通过从抽象潜在空间中抽样产生一组字幕。但是，这种潜在空间的可解释性有限，因此，字幕控制机制仍然是一个悬而未决的问题。本文提出了一种以词性为条件的字幕技术。我们的方法以词性的形式提供人类可解释的控制。重要的是，词性是先前的语言，并且对它的调节提供了：（i）通过计算n-gram和产生的新句子来评估更多的多样性，（ii）对标准字幕指标的不同字幕实现高准确度。

##### URL
[https://arxiv.org/abs/1805.12589](https://arxiv.org/abs/1805.12589)

##### PDF
[https://arxiv.org/pdf/1805.12589](https://arxiv.org/pdf/1805.12589)

