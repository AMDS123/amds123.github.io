---
layout: post
title: "Diverse and Controllable Image Captioning with Part-of-Speech Guidance"
date: 2018-05-31 17:56:17
categories: arXiv_CV
tags: arXiv_CV Image_Caption Adversarial Caption RNN Prediction
author: Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander Schwing, D. A. Forsyth
mathjax: true
---

* content
{:toc}

##### Abstract
Automatically describing an image is an important capability for virtual assistants. Significant progress has been achieved in recent years on this task of image captioning. However, classical prediction techniques based on maximum likelihood trained LSTM nets don't embrace the inherent ambiguity of image captioning. To address this concern, recent variational auto-encoder and generative adversarial network based methods produce a set of captions by sampling from an abstract latent space. But, this latent space has limited interpretability and therefore, a control mechanism for captioning remains an open problem. This paper proposes a captioning technique conditioned on part-of-speech. Our method provides human interpretable control in form of part-of-speech. Importantly, part-of-speech is a language prior, and conditioning on it provides: (i) more diversity as evaluated by counting n-grams and the novel sentences generated, (ii) achieves high accuracy for the diverse captions on standard captioning metrics.

##### Abstract (translated by Google)
自动描述图像是虚拟助手的重要功能。近年来，图像字幕这项任务取得了重大进展。然而，基于最大似然训练LSTM网络的经典预测技术并不包含图像字幕的固有模糊性。为了解决这个问题，最近的变分自动编码器和生成对抗网络的方法通过从抽象的潜在空间抽样来产生一组字幕。但是，这个潜在的空间解释能力有限，因此字幕控制机制仍然是一个悬而未决的问题。本文提出了一种以词性为条件的字幕技术。我们的方法以词性的形式提供人类可解释的控制。重要的是，词性是一种先前的语言，并且其条件提供：（i）通过计数n-gram和生成的新语句来评估更多的多样性，（ii）针对标准字幕指标的不同标题实现高准确度。

##### URL
[http://arxiv.org/abs/1805.12589](http://arxiv.org/abs/1805.12589)

##### PDF
[http://arxiv.org/pdf/1805.12589](http://arxiv.org/pdf/1805.12589)

