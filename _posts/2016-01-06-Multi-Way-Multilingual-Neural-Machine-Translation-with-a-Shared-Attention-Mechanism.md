---
layout: post
title: "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism"
date: 2016-01-06 04:00:50
categories: arXiv_SD
tags: arXiv_SD Attention
author: Orhan Firat, Kyunghyun Cho, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.

##### Abstract (translated by Google)
我们提出多途径，多语言的神经机器翻译。所提出的方法使得单个神经翻译模型能够在多种语言之间进行翻译，其中许多参数只与语言的数量成线性增长。这可以通过在所有语言对之间共享单个关注机制来实现。我们同时对来自WMT'15的10对语言对提出的多途径，多语言模型进行了训练，并且观察到只有一对语言训练的模型有明显的性能改进。特别是，我们观察到，所提出的模型显着提高了低资源语言对的翻译质量。

##### URL
[https://arxiv.org/abs/1601.01073](https://arxiv.org/abs/1601.01073)

##### PDF
[https://arxiv.org/pdf/1601.01073](https://arxiv.org/pdf/1601.01073)

