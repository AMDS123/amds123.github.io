---
layout: post
title: "Making Neural QA as Simple as Possible but not Simpler"
date: 2017-06-08 14:12:35
categories: arXiv_SD
tags: arXiv_SD QA RNN
author: Dirk Weissenborn, Georg Wiese, Laura Seiffe
mathjax: true
---

* content
{:toc}

##### Abstract
Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.

##### Abstract (translated by Google)
最近大规模问答（QA）数据集的发展触发了大量关于QA的端到端神经架构的研究。越来越复杂的系统被认为没有比较简单的神经基线系统，将证明其复杂性。在这项工作中，我们提出了一个简单的启发式，指导神经基线系统的开发QA任务。我们发现，建立一个高性能的神经质量分析系统需要两个要素：一是在处理上下文时对疑问词的意识;二是超越简单的词袋建模的复合函数，如循环神经网络。我们的研究结果表明，与现有模型相比，满足这两个要求的系统FastQA可以实现非常有竞争力的性能。我们认为，这个令人惊讶的发现把先前系统的结果和近期QA数据集的复杂性放在了一起。

##### URL
[https://arxiv.org/abs/1703.04816](https://arxiv.org/abs/1703.04816)

##### PDF
[https://arxiv.org/pdf/1703.04816](https://arxiv.org/pdf/1703.04816)

