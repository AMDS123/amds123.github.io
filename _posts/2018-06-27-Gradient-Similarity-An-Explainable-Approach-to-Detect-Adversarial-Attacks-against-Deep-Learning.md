---
layout: post
title: "Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning"
date: 2018-06-27 22:47:37
categories: arXiv_CV
tags: arXiv_CV Adversarial Object_Detection Knowledge Deep_Learning Detection
author: Jasjeet Dhaliwal, Saurabh Shintre
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks are susceptible to small-but-specific adversarial perturbations capable of deceiving the network. This vulnerability can lead to potentially harmful consequences in security-critical applications. To address this vulnerability, we propose a novel metric called \emph{Gradient Similarity} that allows us to capture the influence of training data on test inputs. We show that \emph{Gradient Similarity} behaves differently for normal and adversarial inputs, and enables us to detect a variety of adversarial attacks with a near perfect ROC-AUC of 95-100\%. Even white-box adversaries equipped with perfect knowledge of the system cannot bypass our detector easily. On the MNIST dataset, white-box attacks are either detected with a high ROC-AUC of 87-96\%, or require very high distortion to bypass our detector.

##### Abstract (translated by Google)
深度神经网络容易受到能够欺骗网络的小而特定的敌对扰动。此漏洞可能导致安全关键应用程序可能产生的有害后果。为了解决这个漏洞，我们提出了一种叫做\ emph {Gradient Similarity}的新颖度量，它可以让我们捕捉训练数据对测试输入的影响。我们证明\ emph {梯度相似}对于正常和敌对投入具有不同的表现，并且使我们能够以近乎完美的95-100％的ROC-AUC检测各种敌对攻击。即使配备完善的系统知识的白盒对手也无法轻松绕过我们的探测器。在MNIST数据集上，白盒攻击或者以87-96％的高ROC-AUC检测，或者需要非常高的失真来绕过我们的检测器。

##### URL
[http://arxiv.org/abs/1806.10707](http://arxiv.org/abs/1806.10707)

##### PDF
[http://arxiv.org/pdf/1806.10707](http://arxiv.org/pdf/1806.10707)

