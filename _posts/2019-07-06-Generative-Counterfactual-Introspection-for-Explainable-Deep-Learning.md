---
layout: post
title: "Generative Counterfactual Introspection for Explainable Deep Learning"
date: 2019-07-06 04:30:13
categories: arXiv_AI
tags: arXiv_AI Salient Deep_Learning Prediction
author: Shusen Liu, Bhavya Kailkhura, Donald Loveland, Yong Han
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we propose an introspection technique for deep neural networks that relies on a generative model to instigate salient editing of the input image for model interpretation. Such modification provides the fundamental interventional operation that allows us to obtain answers to counterfactual inquiries, i.e., what meaningful change can be made to the input image in order to alter the prediction. We demonstrate how to reveal interesting properties of the given classifiers by utilizing the proposed introspection approach on both the MNIST and the CelebA dataset.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.03077](http://arxiv.org/abs/1907.03077)

##### PDF
[http://arxiv.org/pdf/1907.03077](http://arxiv.org/pdf/1907.03077)

