---
layout: post
title: "Textually Enriched Neural Module Networks for Visual Question Answering"
date: 2018-09-23 23:45:54
categories: arXiv_CV
tags: arXiv_CV Image_Caption Knowledge QA Attention Caption VQA Recognition
author: Khyathi Raghavi Chandu, Mary Arpita Pyreddy, Matthieu Felix, Narendra Nath Joshi
mathjax: true
---

* content
{:toc}

##### Abstract
Problems at the intersection of language and vision, like visual question answering, have recently been gaining a lot of attention in the field of multi-modal machine learning as computer vision research moves beyond traditional recognition tasks. There has been recent success in visual question answering using deep neural network models which use the linguistic structure of the questions to dynamically instantiate network layouts. In the process of converting the question to a network layout, the question is simplified, which results in loss of information in the model. In this paper, we enrich the image information with textual data using image captions and external knowledge bases to generate more coherent answers. We achieve 57.1% overall accuracy on the test-dev open-ended questions from the visual question answering (VQA 1.0) real image dataset.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1809.08697](https://arxiv.org/abs/1809.08697)

##### PDF
[https://arxiv.org/pdf/1809.08697](https://arxiv.org/pdf/1809.08697)

