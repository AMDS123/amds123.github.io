---
layout: post
title: "Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach"
date: 2019-08-16 05:48:41
categories: arXiv_CL
tags: arXiv_CL Knowledge Transfer_Learning
author: Igor Shalyminov, Sungjin Lee, Arash Eshghi, Oliver Lemon
mathjax: true
---

* content
{:toc}

##### Abstract
Learning with minimal data is one of the key challenges in the development of practical, production-ready goal-oriented dialogue systems. In a real-world enterprise setting where dialogue systems are developed rapidly and are expected to work robustly for an ever-growing variety of domains, products, and scenarios, efficient learning from a limited number of examples becomes indispensable. In this paper, we introduce a technique to achieve state-of-the-art dialogue generation performance in a few-shot setup, without using any annotated data. We do this by leveraging background knowledge from a larger, more highly represented dialogue source --- namely, the MetaLWOz dataset. We evaluate our model on the Stanford Multi-Domain Dialogue Dataset, consisting of human-human goal-oriented dialogues in in-car navigation, appointment scheduling, and weather information domains. We show that our few-shot approach achieves state-of-the art results on that dataset by consistently outperforming the previous best model in terms of BLEU and Entity F1 scores, while being more data-efficient by not requiring any data annotation.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.05854](https://arxiv.org/abs/1908.05854)

##### PDF
[https://arxiv.org/pdf/1908.05854](https://arxiv.org/pdf/1908.05854)

