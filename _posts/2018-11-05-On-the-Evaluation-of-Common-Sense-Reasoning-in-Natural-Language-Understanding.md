---
layout: post
title: "On the Evaluation of Common-Sense Reasoning in Natural Language Understanding"
date: 2018-11-05 15:11:24
categories: arXiv_AI
tags: arXiv_AI
author: Paul Trichelair, Ali Emami, Jackie Chi Kit Cheung, Adam Trischler, Kaheer Suleman, Fernando Diaz
mathjax: true
---

* content
{:toc}

##### Abstract
The NLP and ML communities have long been interested in developing models capable of common-sense reasoning, and recent works have significantly improved the state of the art on benchmarks like the Winograd Schema Challenge (WSC). Despite these advances, the complexity of tasks designed to test common-sense reasoning remains under-analyzed. In this paper, we make a case study of the Winograd Schema Challenge and, based on two new measures of instance-level complexity, design a protocol that both clarifies and qualifies the results of previous work. Our protocol accounts for the WSC's limited size and variable instance difficulty, properties common to other common-sense benchmarks. Accounting for these properties when assessing model results may prevent unjustified conclusions.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.01778](http://arxiv.org/abs/1811.01778)

##### PDF
[http://arxiv.org/pdf/1811.01778](http://arxiv.org/pdf/1811.01778)

