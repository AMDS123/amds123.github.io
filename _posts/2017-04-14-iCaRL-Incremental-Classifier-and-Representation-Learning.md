---
layout: post
title: "iCaRL: Incremental Classifier and Representation Learning"
date: 2017-04-14 16:41:02
categories: arXiv_CV
tags: arXiv_CV Represenation_Learning Deep_Learning
author: Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, Christoph H. Lampert
mathjax: true
---

* content
{:toc}

##### Abstract
A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.

##### Abstract (translated by Google)
在人工智能之路上面临的一个主要的开放性问题是逐步学习系统的发展，这些系统随着时间的推移从数据流中学习越来越多的概念。在这项工作中，我们引入了一个新的培训策略iCaRL，它允许以类增量的方式进行学习：只有少数类别的培训数据必须同时出现，并且可以逐渐添加新的类别。 iCaRL同时学习强分类器和数据表示。这与之前的作品是有区别的，这些作品从根本上局限于固定数据表示，因此与深度学习架构不兼容。我们通过CIFAR-100和ImageNet ILSVRC 2012数据的实验显示，iCaRL可以在很长一段时间内逐渐学习许多类，其他策略很快就会失败。

##### URL
[https://arxiv.org/abs/1611.07725](https://arxiv.org/abs/1611.07725)

##### PDF
[https://arxiv.org/pdf/1611.07725](https://arxiv.org/pdf/1611.07725)

