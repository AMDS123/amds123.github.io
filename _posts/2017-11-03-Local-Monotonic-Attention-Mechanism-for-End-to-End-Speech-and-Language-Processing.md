---
layout: post
title: "Local Monotonic Attention Mechanism for End-to-End Speech and Language Processing"
date: 2017-11-03 15:34:00
categories: arXiv_CL
tags: arXiv_CL Attention Summarization Speech_Recognition Recognition
author: Andros Tjandra, Sakriani Sakti, Satoshi Nakamura
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, encoder-decoder neural networks have shown impressive performance on many sequence-related tasks. The architecture commonly uses an attentional mechanism which allows the model to learn alignments between the source and the target sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in several tasks, such as automatic speech recognition (ASR), grapheme-to-phoneme (G2P), etc. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results on ASR, G2P and machine translation between two languages with similar sentence structures, demonstrate that the proposed encoder-decoder model with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.

##### Abstract (translated by Google)
最近，编码器 - 解码器神经网络在许多与序列相关的任务中表现出令人印象深刻的性能该体系结构通常使用注意力机制，允许模型学习源和目标序列之间的对齐。今天使用的大多数注意力机制是基于全局注意力属性，其需要计算由编码器状态生成的整个输入序列的加权汇总。然而，这在计算上是昂贵的，并且经常在较长的输入序列上产生未对准。此外，在自动语音识别（ASR），字形音素（G2P）等几个任务中，它不适合单调或从左到右的性质。本文提出了一种新颖的注意机制，具有局部性和单调性。还探讨了各种控制这些特性的方法。在ASR，G2P和具有相似句子结构的两种语言之间的机器翻译的实验结果表明，所提出的具有局部单调注意力的编码器 - 解码器模型与使用全球标准注意力的编码器 - 解码器模型相比，可以实现显着的性能改进并降低计算复杂度建筑。

##### URL
[https://arxiv.org/abs/1705.08091](https://arxiv.org/abs/1705.08091)

##### PDF
[https://arxiv.org/pdf/1705.08091](https://arxiv.org/pdf/1705.08091)

