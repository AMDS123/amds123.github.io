---
layout: post
title: "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles"
date: 2016-10-05 17:12:00
categories: arXiv_CL
tags: arXiv_CL Prediction Gradient_Descent
author: Stefan Lee, Senthil Purushwalkam, Michael Cogswell, Viresh Ranjan, David Crandall, Dhruv Batra
mathjax: true
---

* content
{:toc}

##### Abstract
Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity.

##### Abstract (translated by Google)
在较大的流程中存在许多实际的感知系统，包括与用户或能够评估预测解决方案质量的附加组件的交互。在这些背景下，为这些预言机制提供多种高度可能的假设而不是单一的预测是有益的。在这项工作中，我们将产生多个输出的任务作为一个深层网络集成的学习问题 - 引入一种新的随机梯度下降的方法，以最大限度地减少关于神谕的损失。我们的方法实现起来很简单，对架构和丢失功能都是不可知的，而且无参数。与广泛的任务和深层架构的现有方法相比，我们的方法可以实现更低的预言误差。我们还定性地表明，所产生的多样化解决方案通常提供可解释的任务模糊表示。

##### URL
[https://arxiv.org/abs/1606.07839](https://arxiv.org/abs/1606.07839)

##### PDF
[https://arxiv.org/pdf/1606.07839](https://arxiv.org/pdf/1606.07839)

