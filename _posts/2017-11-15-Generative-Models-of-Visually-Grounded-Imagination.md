---
layout: post
title: "Generative Models of Visually Grounded Imagination"
date: 2017-11-15 04:58:10
categories: arXiv_CV
tags: arXiv_CV Inference
author: Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, Kevin Murphy
mathjax: true
---

* content
{:toc}

##### Abstract
It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C's). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of Suzuki et.al. and the BiVCCA method of Wang et.al.) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset.

##### Abstract (translated by Google)
人们很容易想象一个粉红色头发的男人是什么样的，即使他们以前从来没有见过这样的人。我们称之为创造新颖的语义概念图像的能力。在本文中，我们展示了如何修改变分自编码器来执行这个任务。我们的方法使用了一个新颖的训练目标和一个新颖的专家推理网络，它能够以一种有原则和有效的方式处理部分指定的（抽象的）概念。我们还提出了一套易于计算的评估指标，这些评估指标能够捕捉到我们对具有良好视觉想象力，即正确性，覆盖范围和组合性（3 C）意味着什么的直观概念。最后，我们将这种方法与两个现有的联合图像属性VAE方法（Suzuki等人的JMVAE方法和Wang等人的BiVCCA方法）进行了详细的比较，将其应用于两个数据集：MNIST-带有属性的数据集（我们在这里介绍）和CelebA数据集。

##### URL
[https://arxiv.org/abs/1705.10762](https://arxiv.org/abs/1705.10762)

##### PDF
[https://arxiv.org/pdf/1705.10762](https://arxiv.org/pdf/1705.10762)

