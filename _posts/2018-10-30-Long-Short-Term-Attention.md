---
layout: post
title: "Long Short-Term Attention"
date: 2018-10-30 14:08:30
categories: arXiv_CL
tags: arXiv_CL Attention RNN Deep_Learning
author: Guoqiang Zhong, Xin Lin, Kang Chen
mathjax: true
---

* content
{:toc}

##### Abstract
In order to learn effective features from temporal sequences, the long short-term memory (LSTM) network is widely applied. A critical component of LSTM is the memory cell, which is able to extract, process and store temporal information. Nevertheless, in LSTM, the memory cell is not directly enforced to pay attention to a part of the sequence. Alternatively, the attention mechanism can help to pay attention to specific information of data. In this paper, we present a novel neural model, called long short-term attention (LSTA), which seamlessly merges the attention mechanism into LSTM. More than processing long short term sequences, it can distill effective and valuable information from the sequences with the attention mechanism. Experiments show that LSTA achieves promising learning performance in various deep learning tasks.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.12752](http://arxiv.org/abs/1810.12752)

##### PDF
[http://arxiv.org/pdf/1810.12752](http://arxiv.org/pdf/1810.12752)

