---
layout: post
title: "Evaluation of Automatic Video Captioning Using Direct Assessment"
date: 2017-10-29 09:37:02
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption
author: Yvette Graham, George Awad, Alan Smeaton
mathjax: true
---

* content
{:toc}

##### Abstract
We present Direct Assessment, a method for manually assessing the quality of automatically-generated captions for video. Evaluating the accuracy of video captions is particularly difficult because for any given video clip there is no definitive ground truth or correct answer against which to measure. Automatic metrics for comparing automatic video captions against a manual caption such as BLEU and METEOR, drawn from techniques used in evaluating machine translation, were used in the TRECVid video captioning task in 2016 but these are shown to have weaknesses. The work presented here brings human assessment into the evaluation by crowdsourcing how well a caption describes a video. We automatically degrade the quality of some sample captions which are assessed manually and from this we are able to rate the quality of the human assessors, a factor we take into account in the evaluation. Using data from the TRECVid video-to-text task in 2016, we show how our direct assessment method is replicable and robust and should scale to where there many caption-generation techniques to be evaluated.

##### Abstract (translated by Google)
我们提供直接评估，这是一种手动评估视频自动生成字幕质量的方法。评估视频字幕的准确性特别困难，因为对于任何给定的视频剪辑，没有确定的基本事实或正确的答案来衡量。自动视频字幕与用于评估机器翻译的技术中的BLEU和METEOR等手动字幕进行比较的自动指标在2016年的TRECVid视频字幕任务中使用，但这些显示有缺点。这里介绍的工作通过众包方式将人工评估纳入评估标题描述视频的程度。我们会自动降低一些手动评估的样本字幕的质量，因此我们能够评估人类评估员的质量，这是我们在评估中考虑的一个因素。使用2016年TRECVid视频到文本任务的数据，我们展示了我们的直接评估方法如何可复制和可靠，并且应该扩展到需要评估多种字幕生成技术的位置。

##### URL
[https://arxiv.org/abs/1710.10586](https://arxiv.org/abs/1710.10586)

##### PDF
[https://arxiv.org/pdf/1710.10586](https://arxiv.org/pdf/1710.10586)

