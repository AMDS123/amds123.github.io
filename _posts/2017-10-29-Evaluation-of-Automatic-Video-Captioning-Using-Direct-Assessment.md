---
layout: post
title: "Evaluation of Automatic Video Captioning Using Direct Assessment"
date: 2017-10-29 09:37:02
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption
author: Yvette Graham, George Awad, Alan Smeaton
mathjax: true
---

* content
{:toc}

##### Abstract
We present Direct Assessment, a method for manually assessing the quality of automatically-generated captions for video. Evaluating the accuracy of video captions is particularly difficult because for any given video clip there is no definitive ground truth or correct answer against which to measure. Automatic metrics for comparing automatic video captions against a manual caption such as BLEU and METEOR, drawn from techniques used in evaluating machine translation, were used in the TRECVid video captioning task in 2016 but these are shown to have weaknesses. The work presented here brings human assessment into the evaluation by crowdsourcing how well a caption describes a video. We automatically degrade the quality of some sample captions which are assessed manually and from this we are able to rate the quality of the human assessors, a factor we take into account in the evaluation. Using data from the TRECVid video-to-text task in 2016, we show how our direct assessment method is replicable and robust and should scale to where there many caption-generation techniques to be evaluated.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1710.10586](https://arxiv.org/abs/1710.10586)

##### PDF
[https://arxiv.org/pdf/1710.10586](https://arxiv.org/pdf/1710.10586)

