---
layout: post
title: "Evaluation of Automatic Video Captioning Using Direct Assessment"
date: 2017-10-29 09:37:02
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption
author: Yvette Graham, George Awad, Alan Smeaton
mathjax: true
---

* content
{:toc}

##### Abstract
We present Direct Assessment, a method for manually assessing the quality of automatically-generated captions for video. Evaluating the accuracy of video captions is particularly difficult because for any given video clip there is no definitive ground truth or correct answer against which to measure. Automatic metrics for comparing automatic video captions against a manual caption such as BLEU and METEOR, drawn from techniques used in evaluating machine translation, were used in the TRECVid video captioning task in 2016 but these are shown to have weaknesses. The work presented here brings human assessment into the evaluation by crowdsourcing how well a caption describes a video. We automatically degrade the quality of some sample captions which are assessed manually and from this we are able to rate the quality of the human assessors, a factor we take into account in the evaluation. Using data from the TRECVid video-to-text task in 2016, we show how our direct assessment method is replicable and robust and should scale to where there many caption-generation techniques to be evaluated.

##### Abstract (translated by Google)
我们提供直接评估，一种手动评估视频自动生成字幕质量的方法。评估视频字幕的准确性是特别困难的，因为对于任何给定的视频剪辑，没有明确的基本事实或针对其进行测量的正确答案。 2016年TRECVid视频字幕任务中使用了用于比较自动视频字幕和BLEU和METEOR等手动字幕的自动指标，这些指标来自用于评估机器翻译的技术，但这些指标都显示出其缺点。这里介绍的工作通过众包将人的评估带入评估中，描述视频的标题。我们会自动降低一些手动评估的样本字幕的质量，由此我们可以评估人类评估者的质量，这是我们在评估中考虑的一个因素。使用2016年TRECVid视频到文本任务的数据，我们展示了我们的直接评估方法是如何可复制和强大的，并且应该扩展到需要评估许多字幕生成技术的地方。

##### URL
[https://arxiv.org/abs/1710.10586](https://arxiv.org/abs/1710.10586)

##### PDF
[https://arxiv.org/pdf/1710.10586](https://arxiv.org/pdf/1710.10586)

