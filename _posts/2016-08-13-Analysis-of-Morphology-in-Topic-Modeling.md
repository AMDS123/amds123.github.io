---
layout: post
title: "Analysis of Morphology in Topic Modeling"
date: 2016-08-13 15:54:10
categories: arXiv_CL
tags: arXiv_CL Quantitative
author: Chandler May, Ryan Cotterell, Benjamin Van Durme
mathjax: true
---

* content
{:toc}

##### Abstract
Topic models make strong assumptions about their data. In particular, different words are implicitly assumed to have different meanings: topic models are often used as human-interpretable dimensionality reductions and a proliferation of words with identical meanings would undermine the utility of the top-$m$ word list representation of a topic. Though a number of authors have added preprocessing steps such as lemmatization to better accommodate these assumptions, the effects of such data massaging have not been publicly studied. We make first steps toward elucidating the role of morphology in topic modeling by testing the effect of lemmatization on the interpretability of a latent Dirichlet allocation (LDA) model. Using a word intrusion evaluation, we quantitatively demonstrate that lemmatization provides a significant benefit to the interpretability of a model learned on Wikipedia articles in a morphologically rich language.

##### Abstract (translated by Google)
主题模型对他们的数据做出了强有力的假设。特别是，不同的词被隐含地认为具有不同的含义：主题模型经常被用作人类可解释的降维，具有相同含义的词的扩散将破坏主题的最高$ m $词表表示的效用。尽管许多作者已经加入了预处理步骤，如更好地适应这些假设，但是这种数据按摩的效果尚未被公开研究。我们通过检验词形化对潜在Dirichlet分配（LDA）模型的解释性的影响来阐明形态学在主题建模中的作用的第一步。使用词语入侵评估，我们从数量上证明，词形化为以形态丰富的语言在维基百科文章上学习的模型的可解释性提供了显着的益处。

##### URL
[https://arxiv.org/abs/1608.03995](https://arxiv.org/abs/1608.03995)

##### PDF
[https://arxiv.org/pdf/1608.03995](https://arxiv.org/pdf/1608.03995)

