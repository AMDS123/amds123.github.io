---
layout: post
title: "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation"
date: 2018-08-05 09:50:47
categories: arXiv_CL
tags: arXiv_CL Salient RNN Classification Prediction Relation
author: Pankaj Gupta, Hinrich Sch&#xfc;tze
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) LISA: "How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response" (2) Example2pattern: "How the saliency patterns look like for each category in the data according to the network in decision making". We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.

##### Abstract (translated by Google)
递归神经网络（RNN）是时间网络，并且在本质上是累积的，已经在各种自然语言处理任务中显示出有希望的结果。尽管他们取得了成功，但了解他们隐藏的行为仍然是一个挑战。在这项工作中，我们通过一种名为Layer-wIse-Semantic-Accumulation（LISA）的提议技术来分析和解释RNN的累积性质，用于解释决策并检测网络在决策时依赖的最可能（即显着性）模式。制造。我们演示了（1）LISA：“RNN在给定文本示例和预期响应的顺序处理期间如何累积或构建语义”（2）Example2pattern：“根据网络，数据中每个类别的显着性模式如何在决策中“。我们分析了RNN对不同输入的敏感性，以检查预测分数的增加或减少，并进一步提取网络学习的显着性模式。我们使用两个关系分类数据集：SemEval 10 Task 8和TAC KBP Slot Filling来解释通过LISA和example2pattern的RNN预测。

##### URL
[http://arxiv.org/abs/1808.01591](http://arxiv.org/abs/1808.01591)

##### PDF
[http://arxiv.org/pdf/1808.01591](http://arxiv.org/pdf/1808.01591)

