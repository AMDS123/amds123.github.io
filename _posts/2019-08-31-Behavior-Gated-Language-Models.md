---
layout: post
title: "Behavior Gated Language Models"
date: 2019-08-31 02:10:32
categories: arXiv_CL
tags: arXiv_CL Language_Model Prediction
author: Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan
mathjax: true
---

* content
{:toc}

##### Abstract
Most current language modeling techniques only exploit co-occurrence, semantic and syntactic information from the sequence of words. However, a range of information such as the state of the speaker and dynamics of the interaction might be useful. In this work we derive motivation from psycholinguistics and propose the addition of behavioral information into the context of language modeling. We propose the augmentation of language models with an additional module which analyzes the behavioral state of the current context. This behavioral information is used to gate the outputs of the language model before the final word prediction output. We show that the addition of behavioral context in language models achieves lower perplexities on behavior-rich datasets. We also confirm the validity of the proposed models on a variety of model architectures and improve on previous state-of-the-art models with generic domain Penn Treebank Corpus.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1909.00107](http://arxiv.org/abs/1909.00107)

##### PDF
[http://arxiv.org/pdf/1909.00107](http://arxiv.org/pdf/1909.00107)

