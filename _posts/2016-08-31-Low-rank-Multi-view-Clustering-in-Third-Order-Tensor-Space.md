---
layout: post
title: "Low-rank Multi-view Clustering in Third-Order Tensor Space"
date: 2016-08-31 23:37:37
categories: arXiv_CV
tags: arXiv_CV Sparse Classification Relation
author: Ming Yin, Junbin Gao, Shengli Xie, Yi Guo
mathjax: true
---

* content
{:toc}

##### Abstract
The plenty information from multiple views data as well as the complementary information among different views are usually beneficial to various tasks, e.g., clustering, classification, de-noising. Multi-view subspace clustering is based on the fact that the multi-view data are generated from a latent subspace. To recover the underlying subspace structure, the success of the sparse and/or low-rank subspace clustering has been witnessed recently. Despite some state-of-the-art subspace clustering approaches can numerically handle multi-view data, by simultaneously exploring all possible pairwise correlation within views, the high order statistics is often disregarded which can only be captured by simultaneously utilizing all views. As a consequence, the clustering performance for multi-view data is compromised. To address this issue, in this paper, a novel multi-view clustering method is proposed by using \textit{t-product} in third-order tensor space. Based on the circular convolution operation, multi-view data can be effectively represented by a \textit{t-linear} combination with sparse and low-rank penalty using "self-expressiveness". Our extensive experimental results on facial, object, digits image and text data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of many criteria.

##### Abstract (translated by Google)
来自多个视图数据的丰富信息以及不同视图间的互补信息通常有利于各种任务，例如聚类，分类，去噪。多视图子空间聚类基于多视图数据是从潜在子空间生成的事实。为了恢复潜在的子空间结构，最近已经见证了稀疏和/或低秩子空间聚类的成功。尽管一些最先进的子空间聚类方法可以在数值上处理多视图数据，但是通过同时探索视图内的所有可能的成对相关性，高阶统计量通常被忽视，这只能通过同时利用所有视图来捕获。因此，多视图数据的聚类性能会受到影响。针对这个问题，本文提出了一种在三阶张量空间中使用\ textit {t-product}的多视图聚类方法。基于循环卷积运算，多视图数据可以用“自我表达”的稀疏低秩惩罚有效表示为\ textit {t-线性}组合。我们在面部，对象，数字图像和文本数据上的广泛的实验结果表明，所提出的方法在许多标准方面优于最先进的方法。

##### URL
[https://arxiv.org/abs/1608.08336](https://arxiv.org/abs/1608.08336)

##### PDF
[https://arxiv.org/pdf/1608.08336](https://arxiv.org/pdf/1608.08336)

