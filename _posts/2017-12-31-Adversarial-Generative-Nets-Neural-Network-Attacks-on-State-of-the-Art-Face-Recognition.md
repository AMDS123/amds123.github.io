---
layout: post
title: "Adversarial Generative Nets: Neural Network Attacks on State-of-the-Art Face Recognition"
date: 2017-12-31 20:17:45
categories: arXiv_CV
tags: arXiv_CV Adversarial Face Classification Recognition Face_Recognition
author: Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K. Reiter
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we show that misclassification attacks against face-recognition systems based on deep neural networks (DNNs) are more dangerous than previously demonstrated, even in contexts where the adversary can manipulate only her physical appearance (versus directly manipulating the image input to the DNN). Specifically, we show how to create eyeglasses that, when worn, can succeed in targeted (impersonation) or untargeted (dodging) attacks while improving on previous work in one or more of three facets: (i) inconspicuousness to onlooking observers, which we test through a user study; (ii) robustness of the attack against proposed defenses; and (iii) scalability in the sense of decoupling eyeglass creation from the subject who will wear them, i.e., by creating "universal" sets of eyeglasses that facilitate misclassification. Central to these improvements are adversarial generative nets, a method we propose to generate physically realizable attack artifacts (here, eyeglasses) automatically.

##### Abstract (translated by Google)
在本文中，我们表明，基于深度神经网络（DNNs）的面部识别系统的错误分类攻击比以前所示的更加危险，即使在对手只能操纵她的外表的情况下（与直接操纵图像输入到DNN ）。具体来说，我们展示了如何制作眼镜，当佩戴时可以成功进行有针对性（模仿）或无目标（躲避）的攻击，同时改善以前在三个方面中的一个或多个方面的工作：（i）观察者的不显眼性，我们测试通过用户学习; （二）针对拟议的防御措施的稳健性; （iii）从将穿戴眼镜的对象脱离的意义上的可伸缩性，即通过创建便于错误分类的“通用”眼镜组。这些改进的核心是对抗生成网络，我们建议自动生成物理上可实现的攻击伪像（这里是眼镜）。

##### URL
[http://arxiv.org/abs/1801.00349](http://arxiv.org/abs/1801.00349)

##### PDF
[http://arxiv.org/pdf/1801.00349](http://arxiv.org/pdf/1801.00349)

