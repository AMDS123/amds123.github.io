---
layout: post
title: "DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer"
date: 2017-07-05 05:47:11
categories: arXiv_CV
tags: arXiv_CV Re-identification Knowledge
author: Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision, natural language processing, etc. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton etal. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge -- cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the learning to rank technique into deep metric learning formulation. We test our proposed DarkRank on the pedestrian re-identification task. The results are quite encouraging. Our DarkRank can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted.

##### Abstract (translated by Google)
近年来，我们见证了深度神经网络架构设计的快速发展。这些最新进展极大地促进了计算机视觉，自然语言处理等各个领域的发展。但是，这些先进的模型伴随着非凡的性能也带来了昂贵的计算成本。直接将这些模型部署到具有实时需求的应用程序中仍然是不可行的。最近，Hinton等人。已经表明，强大的教师模型中的黑暗知识可以显着帮助训练更小，更快的学生网络。这些知识对于提高学生模型的泛化能力是非常有益的。受到他们工作的启发，我们引入了一种新型的知识 - 跨模型压缩和加速的样本相似性。这种知识可以自然地从深度度量学习模型中导出。为了转移他们，我们把学习排名技术纳入深度学习的学习制定。我们测试我们建议的DarkRank上的行人再识别任务。结果是相当令人鼓舞的。我们的DarkRank可以大幅度提高基线方法。而且，它与其他现有的方法完全兼容。综合起来，表现可以进一步提升。

##### URL
[https://arxiv.org/abs/1707.01220](https://arxiv.org/abs/1707.01220)

##### PDF
[https://arxiv.org/pdf/1707.01220](https://arxiv.org/pdf/1707.01220)

