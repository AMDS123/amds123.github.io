---
layout: post
title: "Local Contrast Learning"
date: 2018-02-10 01:54:44
categories: arXiv_AI
tags: arXiv_AI Classification Deep_Learning
author: Chuanyun Xu, Yang Zhang, Xin Feng, YongXing Ge, Yihao Zhang, Jianwu Long
mathjax: true
---

* content
{:toc}

##### Abstract
Learning a deep model from small data is yet an opening and challenging problem. We focus on one-shot classification by deep learning approach based on a small quantity of training samples. We proposed a novel deep learning approach named Local Contrast Learning (LCL) based on the key insight about a human cognitive behavior that human recognizes the objects in a specific context by contrasting the objects in the context or in her/his memory. LCL is used to train a deep model that can contrast the recognizing sample with a couple of contrastive samples randomly drawn and shuffled. On one-shot classification task on Omniglot, the deep model based LCL with 122 layers and 1.94 millions of parameters, which was trained on a tiny dataset with only 60 classes and 20 samples per class, achieved the accuracy 97.99% that outperforms human and state-of-the-art established by Bayesian Program Learning (BPL) trained on 964 classes. LCL is a fundamental idea which can be applied to alleviate parametric model's overfitting resulted by lack of training samples.

##### Abstract (translated by Google)
从小数据中学习深度模型仍然是一个开放和具有挑战性的问题。我们专注于基于少量训练样本的深度学习方法的一次分类。我们提出了一种名为局部对比学习（LCL）的新型深度学习方法，该方法基于对人类认知行为的关键洞察力，人类通过对比上下文或其记忆中的对象来识别特定环境中的对象。 LCL用于训练一个深层模型，该模型可以将识别样本与随机抽取和洗牌的一对对比样本进行对比。在Omniglot的一次性分类任务中，基于深层模型的LCL具有122层和1.94百万个参数，在仅有60个类别和每个类别20个样本的小型数据集上进行训练，其准确率达到97.99％，优于人和国家由贝叶斯计划学习（BPL）建立的最先进的964课程培训。 LCL是一个基本思想，可以用来缓解缺乏训练样本导致的参数模型的过度拟合。

##### URL
[http://arxiv.org/abs/1802.03499](http://arxiv.org/abs/1802.03499)

##### PDF
[http://arxiv.org/pdf/1802.03499](http://arxiv.org/pdf/1802.03499)

