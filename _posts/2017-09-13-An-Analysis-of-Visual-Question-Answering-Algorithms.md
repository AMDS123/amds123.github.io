---
layout: post
title: "An Analysis of Visual Question Answering Algorithms"
date: 2017-09-13 18:56:45
categories: arXiv_CV
tags: arXiv_CV QA Attention GAN VQA
author: Kushal Kafle, Christopher Kanan
mathjax: true
---

* content
{:toc}

##### Abstract
In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.

##### Abstract (translated by Google)
在视觉问答（VQA）中，算法必须回答关于图像的基于文本的问题。尽管自2014年底以来已经创建了多个VQA数据集，但它们的内容和评估算法的方式都存在缺陷。因此，评估分数是膨胀的，主要是通过回答更容易的问题来确定，使得难以比较不同的方法。在本文中，我们使用新的数据集分析现有的VQA算法。它包含超过160万个问题，分为12个不同的类别。我们还介绍了对于给定的图像没有意义的问题，以强制VQA系统推理图像内容。我们提出了新的评估方案来弥补过度代表的问题类型，并且更容易研究算法的优缺点。我们分析了基线和最先进的VQA模型的性能，包括多模式紧凑双线性汇聚（MCB），神经模块网络和循环应答单元。我们的实验确定注意力如何帮助某些类别比其他类型更有效，确定哪些模型比其他模型更有效，并通过简单地学习回答大型，简单的问题类别来解释简单模型（如MLP）能够超越更复杂的模型（MCB）。

##### URL
[https://arxiv.org/abs/1703.09684](https://arxiv.org/abs/1703.09684)

##### PDF
[https://arxiv.org/pdf/1703.09684](https://arxiv.org/pdf/1703.09684)

