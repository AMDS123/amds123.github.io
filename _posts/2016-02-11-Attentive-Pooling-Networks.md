---
layout: post
title: "Attentive Pooling Networks"
date: 2016-02-11 03:06:33
categories: arXiv_SD
tags: arXiv_SD Attention CNN Represenation_Learning RNN Classification
author: Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.

##### Abstract (translated by Google)
在这项工作中，我们提出了注意池（AP），一个双向注意机制的歧视模型训练。在神经网络的成对排序或者分类的情况下，AP使池层知道当前的输入对，以这种方式来自两个输入项的信息可以直接影响彼此的表示的计算。除了配对输入的这种表示之外，AP共同学习该对的投影段（例如，三元组）的相似性度量，并且随后为每个输入导出对应的关注度矢量以指导合并。我们的双向关注机制是一个独立于底层表示学习的通用框架，在我们的研究中已经被应用于卷积神经网络（CNN）和递归神经网络（RNN）。从三个不同的问题回答/答案选择的基准任务的实证结果表明，我们提出的模型胜过了各种强大的基线，并在所有的基准测试中达到了最先进的性能。

##### URL
[https://arxiv.org/abs/1602.03609](https://arxiv.org/abs/1602.03609)

##### PDF
[https://arxiv.org/pdf/1602.03609](https://arxiv.org/pdf/1602.03609)

