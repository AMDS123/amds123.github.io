---
layout: post
title: "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms"
date: 2017-11-05 12:18:24
categories: arXiv_CV
tags: arXiv_CV Adversarial Sparse Deep_Learning Gradient_Descent Relation
author: Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, Shie Mannor
mathjax: true
---

* content
{:toc}

##### Abstract
The question why deep learning algorithms generalize so well has attracted increasing research interest. However, most of the well-established approaches, such as hypothesis capacity, stability or sparseness, have not provided complete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus on the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis will not change much due to perturbations of its training examples, then it will also generalize well. As most deep learning algorithms are stochastic (e.g., Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness arguments of Xu & Mannor, and introduce a new approach, ensemble robustness, that concerns the robustness of a population of hypotheses. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbations is bounded in average over training examples. Moreover, an algorithm may be sensitive to some adversarial examples (Goodfellow et al., 2015) but still generalize well. To support our claims, we provide extensive simulations for different deep learning algorithms and different network architectures exhibiting a strong correlation between ensemble robustness and the ability to generalize.

##### Abstract (translated by Google)
深度学习算法推广的问题引起了越来越多的研究兴趣。然而，大多数已有的假设能力，稳定性或者稀疏性的方法并没有提供完整的解释（Zhang等，2016; Kawaguchi等，2017）。在这项工作中，我们专注于鲁棒性方法（Xu＆Mannor，2012），即如果假设的误差由于其训练样本的扰动而不会有太大变化，那么它也将得到很好的推广。由于大多数深度学习算法是随机的（例如，随机梯度下降，丢失和贝叶斯逐回），我们重新审视了徐和曼诺的鲁棒性论证，并引入了一种新的方法，集成稳健性，涉及到一个总体的鲁棒性的假设。通过集合稳健性的研究，我们发现随机学习算法只要训练样本对敌对扰动的敏感性平均有界，就可以很好地推广。此外，一个算法可能对一些敌对的例子很敏感（Goodfellow等，2015），但仍然很好地推广。为了支持我们的观点，我们为不同的深度学习算法和不同的网络架构提供了广泛的仿真，展示了集成稳健性与泛化能力之间的强相关性。

##### URL
[https://arxiv.org/abs/1602.02389](https://arxiv.org/abs/1602.02389)

##### PDF
[https://arxiv.org/pdf/1602.02389](https://arxiv.org/pdf/1602.02389)

