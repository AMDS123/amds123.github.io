---
layout: post
title: "A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization"
date: 2018-05-09 16:56:41
categories: arXiv_CL
tags: arXiv_CL Summarization CNN Optimization Inference Deep_Learning
author: Li Wang, Junlin Yao, Yunzhe Tao, Li Zhong, Wei Liu, Qiang Du
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a deep learning approach to tackle the automatic summarization tasks by incorporating topic information into the convolutional sequence-to-sequence (ConvS2S) model and using self-critical sequence training (SCST) for optimization. Through jointly attending to topics and word-level alignment, our approach can improve coherence, diversity, and informativeness of generated summaries via a biased probability generation mechanism. On the other hand, reinforcement training, like SCST, directly optimizes the proposed model with respect to the non-differentiable metric ROUGE, which also avoids the exposure bias during inference. We carry out the experimental evaluation with state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets. The empirical results demonstrate the superiority of our proposed method in the abstractive summarization.

##### Abstract (translated by Google)
在本文中，我们提出了一种深度学习方法，通过将主题信息纳入卷积序列到序列（ConvS2S）模型并使用自我关键序列训练（SCST）进行优化来处理自动总结任务。通过共同参与主题和词级对齐，我们的方法可以通过有偏倚的概率生成机制来提高生成的摘要的一致性，多样性和信息性。另一方面，像SCST这样的强化训练直接针对不可微分度量ROUGE优化所提出的模型，这也避免了推理期间的暴露偏差。我们在Gigaword，DUC-2004和LCSTS数据集上采用最先进的方法进行实验评估。实证结果证明了我们提出的方法在抽象概括中的优越性。

##### URL
[http://arxiv.org/abs/1805.03616](http://arxiv.org/abs/1805.03616)

##### PDF
[http://arxiv.org/pdf/1805.03616](http://arxiv.org/pdf/1805.03616)

