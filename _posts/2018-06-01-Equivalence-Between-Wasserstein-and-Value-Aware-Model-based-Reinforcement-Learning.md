---
layout: post
title: "Equivalence Between Wasserstein and Value-Aware Model-based Reinforcement Learning"
date: 2018-06-01 21:54:18
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Kavosh Asadi, Evan Cater, Dipendra Misra, Michael L. Littman
mathjax: true
---

* content
{:toc}

##### Abstract
Learning a generative model is a key component of model-based reinforcement learning. Though learning a good model in the tabular setting is a simple task, learning a useful model in the approximate setting is challenging. Recently Farahmand et al. (2017) proposed a value-aware (VAML) objective that captures the structure of value function during model learning. Using tools from Lipschitz continuity, we show that minimizing the VAML objective is in fact equivalent to minimizing the Wasserstein metric.

##### Abstract (translated by Google)
学习生成模型是基于模型的强化学习的关键组成部分。虽然在表格设置中学习好模型是一项简单的任务，但在大致设置中学习一个有用的模型是具有挑战性的。最近Farahmand等人。 （2017）提出了一个价值意识（VAML）目标，该目标捕捉模型学习过程中价值函数的结构。使用来自Lipschitz连续性的工具，我们证明最小化VAML目标实际上相当于最小化Wasserstein度量。

##### URL
[http://arxiv.org/abs/1806.01265](http://arxiv.org/abs/1806.01265)

##### PDF
[http://arxiv.org/pdf/1806.01265](http://arxiv.org/pdf/1806.01265)

