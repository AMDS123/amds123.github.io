---
layout: post
title: "Extractive Summarization: Limits, Compression, Generalized Model and Heuristics"
date: 2017-04-18 22:21:22
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Rakesh Verma, Daniel Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data.

##### Abstract (translated by Google)
由于其承诺缓解信息过载，文本摘要已经引起了许多研究者的关注。但是，这仍然是一个严峻的挑战。在这里，我们首先证明了对单个文档和多个文档摘要任务的ROUGE评估下的DUC数据集上的抽取摘要的回忆（和F1分数）的经验限制。接下来我们定义一个文档的可压缩性的概念，并提出一个新的概括模型，它概括了现有的文献模型，并且综合了摘要的几个维度，即抽象与抽象，单个与多个文档，句法与语义。最后，我们在单个框架中检查一些新的和现有的单文档摘要算法，并与DUC数据的现有技术摘要进行比较。

##### URL
[https://arxiv.org/abs/1704.05550](https://arxiv.org/abs/1704.05550)

##### PDF
[https://arxiv.org/pdf/1704.05550](https://arxiv.org/pdf/1704.05550)

