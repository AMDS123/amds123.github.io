---
layout: post
title: "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings"
date: 2016-03-30 13:43:38
categories: arXiv_SD
tags: arXiv_SD Image_Caption Image_Retrieval Embedding
author: Spandana Gella, Mirella Lapata, Frank Keller
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at: this https URL

##### Abstract (translated by Google)
我们引入了一个新的任务，动词的视觉消歧：给定一个图像和一个动词，分配动词的正确含义，即描述图像中所描绘动作的那个动词。正如文本词义消歧对于广泛的NLP任务是有用的一样，视觉消歧可以用于多模式任务，如图像检索，图像描述和文本插图。我们介绍VerSe，一个新的数据集，用现有的标签来扩充现有的多模态数据集（COCO和TUHOI）。我们提出一种基于Lesk的无监督算法，它使用文本，视觉或多模式嵌入来执行视觉消歧。我们发现，当黄金标准的文本注释（对象标签和图像描述）可用时，文本嵌入表现良好，而多模式嵌入在未注释的图像上表现良好。我们还通过使用文本和多模式嵌入作为监督设置中的特征来验证我们的发现，并分析视觉消歧任务的性能。 VerSe已公开发布，可以通过以下网址下载

##### URL
[https://arxiv.org/abs/1603.09188](https://arxiv.org/abs/1603.09188)

##### PDF
[https://arxiv.org/pdf/1603.09188](https://arxiv.org/pdf/1603.09188)

