---
layout: post
title: "Global Encoding for Abstractive Summarization"
date: 2018-05-10 14:11:51
categories: arXiv_CL
tags: arXiv_CL Summarization CNN
author: Junyang Lin, Xu Sun, Shuming Ma, Qi Su
mathjax: true
---

* content
{:toc}

##### Abstract
In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of reducing repetition.

##### Abstract (translated by Google)
在神经抽象概括中，传统的序列到序列（seq2seq）模型经常遭受重复和语义不相关。为了解决这个问题，我们提出了一个全局编码框架，它根据源上下文的全局信息控制从编码器到解码器的信息流。它由卷积门控单元组成，执行全局编码以改善源端信息的表示。对LCSTS和英文Gigaword的评估都表明，我们的模型优于基准模型，分析表明我们的模型能够减少重复。

##### URL
[http://arxiv.org/abs/1805.03989](http://arxiv.org/abs/1805.03989)

##### PDF
[http://arxiv.org/pdf/1805.03989](http://arxiv.org/pdf/1805.03989)

