---
layout: post
title: "Long Short-Term Memory Over Tree Structures"
date: 2015-03-16 23:59:02
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition RNN Recognition
author: Xiaodan Zhu, Parinaz Sobhani, Hongyu Guo
mathjax: true
---

* content
{:toc}

##### Abstract
The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.

##### Abstract (translated by Google)
链式结构的长时短记忆（LSTM）已经证明在诸如语音识别和机器翻译等广泛的问题上是有效的。在本文中，我们建议将其扩展到树结构，其中一个存储单元可以在递归过程中反映多个子单元或多个子单元的历史记忆。我们称之为S-LSTM模型，它提供了一个考虑到层次结构（如语言或图像解析结构）的长距离交互的原则性方法。我们利用语义组合的模型来理解文本的含义，这是自然语言理解中的一个基本问题，并且表明，它通过用S-LSTM内存块替换其组成层而超越了最先进的递归模型。我们还表明，利用给定的结构有助于实现比没有考虑结构更好的性能。

##### URL
[https://arxiv.org/abs/1503.04881](https://arxiv.org/abs/1503.04881)

##### PDF
[https://arxiv.org/pdf/1503.04881](https://arxiv.org/pdf/1503.04881)

