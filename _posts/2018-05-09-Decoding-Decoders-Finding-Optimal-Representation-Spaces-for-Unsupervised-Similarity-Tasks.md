---
layout: post
title: "Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks"
date: 2018-05-09 09:41:51
categories: arXiv_AI
tags: arXiv_AI Embedding
author: Vitalii Zhelezniak, Dan Busbridge, April Shen, Samuel L. Smith, Nils Y. Hammerla
mathjax: true
---

* content
{:toc}

##### Abstract
Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. We provide a simple yet rigorous explanation for this behaviour by introducing the concept of an optimal representation space, in which semantically close symbols are mapped to representations that are close under a similarity measure induced by the model's objective function. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.

##### Abstract (translated by Google)
实验证据表明，简单模型在许多无监督相似任务上胜过复杂深度网络。我们通过引入最优表示空间的概念为这种行为提供了简单而严格的解释，其中语义上接近的符号被映射到在由模型的目标函数引起的相似性度量下接近的表示。此外，我们提出了一个简单的过程，在没有任何再培训或体系结构修改的情况下，与浅层模型相比，可以使深度复发模型的表现相当好（有时更好）。为了验证我们的分析，我们进行了一系列一致的实证评估，并在此过程中引入了几个新的句子嵌入模型。尽管这项工作是在自然语言处理的背景下提出的，但这些见解很容易适用于其他依赖分布式表示进行传输任务的领域。

##### URL
[http://arxiv.org/abs/1805.03435](http://arxiv.org/abs/1805.03435)

##### PDF
[http://arxiv.org/pdf/1805.03435](http://arxiv.org/pdf/1805.03435)

