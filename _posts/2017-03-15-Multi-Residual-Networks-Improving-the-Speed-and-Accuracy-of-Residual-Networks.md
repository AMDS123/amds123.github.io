---
layout: post
title: "Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks"
date: 2017-03-15 08:50:00
categories: arXiv_CV
tags: arXiv_CV CNN Classification
author: Masoud Abdi, Saeid Nahavandi
mathjax: true
---

* content
{:toc}

##### Abstract
In this article, we take one step toward understanding the learning behavior of deep residual networks, and supporting the observation that deep residual networks behave like ensembles. We propose a new convolutional neural network architecture which builds upon the success of residual networks by explicitly exploiting the interpretation of very deep networks as an ensemble. The proposed multi-residual network increases the number of residual functions in the residual blocks. Our architecture generates models that are wider, rather than deeper, which significantly improves accuracy. We show that our model achieves an error rate of 3.73% and 19.45% on CIFAR-10 and CIFAR-100 respectively, that outperforms almost all of the existing models. We also demonstrate that our model outperforms very deep residual networks by 0.22% (top-1 error) on the full ImageNet 2012 classification dataset. Additionally, inspired by the parallel structure of multi-residual networks, a model parallelism technique has been investigated. The model parallelism method distributes the computation of residual blocks among the processors, yielding up to 15% computational complexity improvement.

##### Abstract (translated by Google)
在本文中，我们向理解深度残差网络的学习行为迈出了一步，并且支持深度残余网络像集合的行为。我们提出了一个新的卷积神经网络架构，建立在残差网络成功的基础上，明确地利用深度网络作为一个整体的解释。所提出的多残差网络增加了残差块中残差函数的数量。我们的架构产生的模型更宽，而不是更深，这显着提高了准确性。我们表明，我们的模型在CIFAR-10和CIFAR-100上分别达到了3.73％和19.45％的错误率，几乎比所有现有的模型都要好。我们还证明，我们的模型在整个ImageNet 2012分类数据集上的表现优于非常深的残留网络，误差为0.22％（前1个错误）。另外，受多残余网络并行结构的启发，研究了一种模型并行技术。模型并行方法在处理器之间分配残余块的计算，产生高达15％的计算复杂度提高。

##### URL
[https://arxiv.org/abs/1609.05672](https://arxiv.org/abs/1609.05672)

##### PDF
[https://arxiv.org/pdf/1609.05672](https://arxiv.org/pdf/1609.05672)

