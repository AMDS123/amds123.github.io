---
layout: post
title: "On the Universal Approximability of Quantized ReLU Neural Networks"
date: 2018-02-10 19:43:42
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: Yukun Ding, Jinglan Liu, Yiyu Shi
mathjax: true
---

* content
{:toc}

##### Abstract
Compression is a key step to deploy large neural networks on resource-constrained platforms. As a popular compression technique, quantization constrains the number of distinct weight values and thus reducing the number of bits required to represent and store each weight. In this paper, we study the representation power of quantized neural networks. First, we prove the universal approximability of quantized ReLU networks. Then we provide upper bounds of storage size given the approximation error bound and the bit-width of weights for function-independent and function-dependent structures. To the best of the authors' knowledge, this is the first work on the universal approximability as well as the associated storage size bound of quantized neural networks.

##### Abstract (translated by Google)
压缩是在资源受限平台上部署大型神经网络的关键步骤。作为流行的压缩技术，量化限制了不同权重值的数量，因此减少了表示和存储每个权重所需的位数。在本文中，我们研究了量化神经网络的表示能力。首先，我们证明了量化ReLU网络的通用逼近性。然后，我们给出存储大小的上限，给出了近似误差界限和函数独立结构和函数相关结构的权重位宽。据作者所知，这是关于量子化神经网络的通用逼近性和相关存储大小界的第一项工作。

##### URL
[http://arxiv.org/abs/1802.03646](http://arxiv.org/abs/1802.03646)

##### PDF
[http://arxiv.org/pdf/1802.03646](http://arxiv.org/pdf/1802.03646)

