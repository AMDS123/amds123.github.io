---
layout: post
title: "Deep Image Demosaicking using a Cascade of Convolutional Residual Denoising Networks"
date: 2018-03-14 11:44:08
categories: arXiv_CV
tags: arXiv_CV Regularization CNN Optimization Deep_Learning
author: Filippos Kokkinos, Stamatios Lefkimmiatis
mathjax: true
---

* content
{:toc}

##### Abstract
Demosaicking and denoising are among the most crucial steps of modern digital camera pipelines. Meanwhile, joint image denoising-demosaicking is a highly ill-posed inverse problem where at-least two-thirds of the information are missing and the rest are corrupted by noise. This poses a great challenge in obtaining meaningful reconstructions and a special care for the efficient treatment of the problem is required. While there are several machine learning approaches that have been recently introduced to solve this problem, in this work we propose a novel deep learning architecture which is inspired by powerful classical image regularization methods and large-scale convex optimization techniques. Consequently, our derived network is more transparent and has a clear interpretation compared to alternative competitive deep learning approaches. Our extensive experiments demonstrate that our network outperforms any previous approaches on both noisy and noise-free data. This improvement in reconstruction quality is attributed to the principled way we design our network architecture, which also requires fewer trainable parameters than the current state-of-the-art deep network solution. Finally, we show that our network has the ability to generalize well even when it is trained on small datasets, while keeping the overall number of parameters low.

##### Abstract (translated by Google)
Demosaicking和去噪是现代数码相机管线中最关键的步骤之一。同时，联合图像去噪 - 去马赛克是一种非常不适合的反演问题，其中至少有三分之二的信息丢失，其余的信息被噪声破坏。这对于获得有意义的重建提出了巨大的挑战，并且需要特别注意有效处理该问题。虽然最近有几种机器学习方法被用来解决这个问题，但在这项工作中，我们提出了一种新颖的深度学习体系结构，它受到强大的经典图像正则化方法和大规模凸面优化技术的启发。因此，与其他竞争深度学习方法相比，我们派生的网络更加透明并且有着明确的解释。我们广泛的实验表明，我们的网络在噪声和无噪数据方面都优于以前的任何方法。重建质量的这种提高归功于我们设计网络架构的原则性方式，与当前最先进的深度网络解决方案相比，这种方法所需的可训练参数也更少。最后，我们表明，即使在小数据集上进行训练时，我们的网络仍具有良好的泛化能力，同时保持低参数总数。

##### URL
[https://arxiv.org/abs/1803.05215](https://arxiv.org/abs/1803.05215)

##### PDF
[https://arxiv.org/pdf/1803.05215](https://arxiv.org/pdf/1803.05215)

