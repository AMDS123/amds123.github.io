---
layout: post
title: "Document Informed Neural Autoregressive Topic Models"
date: 2018-08-11 12:16:09
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Pankaj Gupta, Florian Buettner, Hinrich Sch&#xfc;tze
mathjax: true
---

* content
{:toc}

##### Abstract
Context information around words helps in determining their actual meaning, for example "networks" used in contexts of artificial neural networks or biological neuron networks. Generative topic models infer topic-word distributions, taking no or only little context into account. Here, we extend a neural autoregressive topic model to exploit the full context information around words in a document in a language modeling fashion. This results in an improved performance in terms of generalization, interpretability and applicability. We apply our modeling approach to seven data sets from various domains and demonstrate that our approach consistently outperforms stateof-the-art generative topic models. With the learned representations, we show on an average a gain of 9.6% (0.57 Vs 0.52) in precision at retrieval fraction 0.02 and 7.2% (0.582 Vs 0.543) in F1 for text categorization.

##### Abstract (translated by Google)
围绕单词的上下文信息有助于确定它们的实际含义，例如在人工神经网络或生物神经网络的上下文中使用的“网络”。生成主题模型推断主题 - 单词分布，不考虑或仅考虑很少的上下文。在这里，我们扩展神经自回归主题模型，以语言建模方式利用文档中单词周围的完整上下文信息。这导致在泛化，可解释性和适用性方面的改进性能。我们将建模方法应用于来自不同领域的七个数据集，并证明我们的方法始终优于最先进的生成主题模型。通过所学习的表示，我们平均显示在文本分类的F1中检索分数0.02和7.2％（0.582 Vs 0.543）的精度增益为9.6％（0.57 Vs 0.52）。

##### URL
[http://arxiv.org/abs/1808.03793](http://arxiv.org/abs/1808.03793)

##### PDF
[http://arxiv.org/pdf/1808.03793](http://arxiv.org/pdf/1808.03793)

