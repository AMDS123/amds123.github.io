---
layout: post
title: "Attention-over-Attention Neural Networks for Reading Comprehension"
date: 2017-06-06 02:51:54
categories: arXiv_CL
tags: arXiv_CL Attention GAN Prediction
author: Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, Guoping Hu
mathjax: true
---

* content
{:toc}

##### Abstract
Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this paper, we present a novel model called attention-over-attention reader for the Cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention, and induces "attended attention" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.

##### Abstract (translated by Google)
填空式查询是阅读理解中的代表性问题。在过去的几个月里，我们已经看到利用神经网络方法解决Cloze式的问题已经有了很多的进展。在本文中，我们提出了一种称为注意力过度注意阅读器的新模式，用于填空式阅读理解任务。我们的模型旨在将另一种关注机制置于文档层面的关注之上，并引发“最受关注”的最终预测。与以前的作品不同，我们的神经网络模型需要较少的预定义超参数，并使用优雅的架构进行建模。实验结果表明，在CNN和儿童图书测试数据集等公共数据集中，所提出的注意过分关注模型显着优于各种最先进的系统。

##### URL
[https://arxiv.org/abs/1607.04423](https://arxiv.org/abs/1607.04423)

##### PDF
[https://arxiv.org/pdf/1607.04423](https://arxiv.org/pdf/1607.04423)

