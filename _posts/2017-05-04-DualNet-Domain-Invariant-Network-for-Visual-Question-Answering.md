---
layout: post
title: "DualNet: Domain-Invariant Network for Visual Question Answering"
date: 2017-05-04 07:54:06
categories: arXiv_CV
tags: arXiv_CV QA Attention Embedding VQA
author: Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada
mathjax: true
---

* content
{:toc}

##### Abstract
Visual question answering (VQA) task not only bridges the gap between images and language, but also requires that specific contents within the image are understood as indicated by linguistic context of the question, in order to generate the accurate answers. Thus, it is critical to build an efficient embedding of images and texts. We implement DualNet, which fully takes advantage of discriminative power of both image and textual features by separately performing two operations. Building an ensemble of DualNet further boosts the performance. Contrary to common belief, our method proved effective in both real images and abstract scenes, in spite of significantly different properties of respective domain. Our method was able to outperform previous state-of-the-art methods in real images category even without explicitly employing attention mechanism, and also outperformed our own state-of-the-art method in abstract scenes category, which recently won the first place in VQA Challenge 2016.

##### Abstract (translated by Google)
视觉问答（VQA）任务不仅弥合了图像和语言之间的差距，而且还要求图像中的特定内容被理解为问题的语言上下文，以便生成准确的答案。因此，建立有效的图像和文本嵌入至关重要。我们实施DualNet，通过分别执行两个操作，充分利用图像和文本功能的辨别力。构建DualNet集合可进一步提升性能。与普遍看法相反，我们的方法在真实图像和抽象场景中都证明是有效的，尽管各个域的属性明显不同。即使没有明确地使用注意机制，我们的方法也能够超越真实图像类别中先前的最先进方法，并且在抽象场景类别中也超越了我们自己最先进的方法，最近赢得了第一名参加2016年VQA挑战赛。

##### URL
[https://arxiv.org/abs/1606.06108](https://arxiv.org/abs/1606.06108)

##### PDF
[https://arxiv.org/pdf/1606.06108](https://arxiv.org/pdf/1606.06108)

