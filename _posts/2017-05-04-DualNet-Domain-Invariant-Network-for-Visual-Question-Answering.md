---
layout: post
title: "DualNet: Domain-Invariant Network for Visual Question Answering"
date: 2017-05-04 07:54:06
categories: arXiv_CV
tags: arXiv_CV VQA
author: Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada
mathjax: true
---

* content
{:toc}

##### Abstract
Visual question answering (VQA) task not only bridges the gap between images and language, but also requires that specific contents within the image are understood as indicated by linguistic context of the question, in order to generate the accurate answers. Thus, it is critical to build an efficient embedding of images and texts. We implement DualNet, which fully takes advantage of discriminative power of both image and textual features by separately performing two operations. Building an ensemble of DualNet further boosts the performance. Contrary to common belief, our method proved effective in both real images and abstract scenes, in spite of significantly different properties of respective domain. Our method was able to outperform previous state-of-the-art methods in real images category even without explicitly employing attention mechanism, and also outperformed our own state-of-the-art method in abstract scenes category, which recently won the first place in VQA Challenge 2016.

##### Abstract (translated by Google)
视觉问答（VQA）任务不仅弥合了图像和语言之间的差距，而且要求图像中的特定内容被理解为问题的语言环境所指示的，以便产生准确的答案。因此，建立一个有效的图像和文本嵌入是至关重要的。我们实现DualNet，通过分别执行两个操作，充分利用图像和文本特征的判别力。构建DualNet整体进一步提升了性能。与普遍的看法相反，我们的方法在真实图像和抽象场景中被证明是有效的，尽管各个域的性质有很大不同。即使没有明确地使用关注机制，我们的方法也能胜过先前的真实图像类别的先进方法，而且在最近获得第一名的抽象场景类别中也超越了我们自己的最新方法在2016年VQA挑战赛中。

##### URL
[https://arxiv.org/abs/1606.06108](https://arxiv.org/abs/1606.06108)

