---
layout: post
title: "Off-Policy Policy Gradient with State Distribution Correction"
date: 2019-04-17 19:46:02
categories: arXiv_AI
tags: arXiv_AI Optimization
author: Yao Liu, Adith Swaminathan, Alekh Agarwal, Emma Brunskill
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of off-policy policy optimization in Markov decision processes, and develop a novel off-policy policy gradient method. Prior off-policy policy gradient approaches have generally ignored the mismatch between the distribution of states visited under the behavior policy used to collect data, and what would be the distribution of states under the learned policy. Here we build on recent progress for estimating the ratio of the Markov chain stationary distribution of states in policy evaluation, and presentan off-policy policy gradient optimization technique that can account for this mismatch in distributions.We present an illustrative example of why this is important, theoretical convergence guarantee for our approach and empirical simulations that highlight the benefits of correcting this distribution mismatch.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.08473](http://arxiv.org/abs/1904.08473)

##### PDF
[http://arxiv.org/pdf/1904.08473](http://arxiv.org/pdf/1904.08473)

