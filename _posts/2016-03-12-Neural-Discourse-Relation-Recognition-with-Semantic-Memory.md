---
layout: post
title: "Neural Discourse Relation Recognition with Semantic Memory"
date: 2016-03-12 08:54:16
categories: arXiv_SD
tags: arXiv_SD Knowledge Attention Face Embedding Relation Recognition
author: Biao Zhang, Deyi Xiong, Jinsong Su
mathjax: true
---

* content
{:toc}

##### Abstract
Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56\% on average over current state-of-the-art baselines in terms of F1-score.

##### Abstract (translated by Google)
人们在很大程度上依赖于他们的语义记忆来理解话语的含义和关系，这些记忆编码有关概念和事实的一般知识。受此启发，我们提出了一种用于隐式话语关系分析的神经识别器，其基于以分布式方式存储知识的语义记忆。我们把这个识别器称为SeMDER。从话语参数的词嵌入开始，SeMDER使用浅编码器来产生话语的分布表面表示。注意语义存储矩阵的语义编码器进一步建立在表面表示之上。它可以从记忆中为话语检索深层的语义表征。使用表面和语义表示作为输入，SeMDER最终通过神经识别器预测隐含的语篇关系。基准数据集上的实验表明，SeMDER从语义记忆中受益，在F1分数方面平均超过当前最先进的基线，实现了2.56％的实质性提高。

##### URL
[https://arxiv.org/abs/1603.03873](https://arxiv.org/abs/1603.03873)

##### PDF
[https://arxiv.org/pdf/1603.03873](https://arxiv.org/pdf/1603.03873)

