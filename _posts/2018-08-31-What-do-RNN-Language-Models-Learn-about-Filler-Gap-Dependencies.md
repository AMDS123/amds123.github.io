---
layout: post
title: "What do RNN Language Models Learn about Filler-Gap Dependencies?"
date: 2018-08-31 20:04:42
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model Relation
author: Ethan Wilcox, Roger Levy, Takashi Morita, Richard Futrell
mathjax: true
---

* content
{:toc}

##### Abstract
RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance filler-gap dependencies and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler-gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler-gap dependencies, known as island constraints: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions.

##### Abstract (translated by Google)
RNN语言模型已经实现了最先进的困惑结果，并且在一系列NLP任务中证明是有用的，但目前还不清楚他们学习的语法概括。在这里，我们研究最先进的RNN语言模型是否代表长距离填充间隙依赖性和对它们的约束。通过检验实验控制句子上的RNN行为来揭示填充间隙依赖性，我们证明了RNN可以表示多个句法位置和大跨度文本之间的关系。此外，我们表明RNN学习填充间隙依赖性的已知限制的子集，称为岛约束：RNN显示wh岛，附属岛和复杂NP岛的证据。这些研究表明，最先进的RNN模型能够学习和概括空句法位置。

##### URL
[http://arxiv.org/abs/1809.00042](http://arxiv.org/abs/1809.00042)

##### PDF
[http://arxiv.org/pdf/1809.00042](http://arxiv.org/pdf/1809.00042)

