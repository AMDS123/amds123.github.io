---
layout: post
title: "Towards Universal Representation for Unseen Action Recognition"
date: 2018-03-22 17:02:45
categories: arXiv_AI
tags: arXiv_AI Action_Recognition Recognition
author: Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao
mathjax: true
---

* content
{:toc}

##### Abstract
Unseen Action Recognition (UAR) aims to recognise novel action categories without training examples. While previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale training source to achieve a Universal Representation (UR) that can generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover 'building-blocks' from the large-scale ActivityNet dataset using distribution kernels. Essential visual and semantic components are preserved in a shared space to achieve the UR that can efficiently generalise to new datasets. Predicted UR exemplars can be improved by a simple semantic adaptation, and then an unseen action can be directly recognised using UR during the test. Without further training, extensive experiments manifest significant improvements over the UCF101 and HMDB51 benchmarks.

##### Abstract (translated by Google)
看不见的行动识别（UAR）旨在识别新的行动类别，没有训练的例子。虽然以前的方法专注于内部数据集看到/看不见的分裂，但本文提出了一种使用大规模训练源来实现通用表示（UR）的流水线，该泛化表示可推广到更现实的跨数据集UAR（CD-UAR）场景。我们首先将UAR称为广义多实例学习（GMIL）问题，并使用分布式内核从大规模ActivityNet数据集中发现“构建块”。必要的视觉和语义组件被保存在一个共享空间中，以实现可以高效地推广到新数据集的UR。通过简单的语义适应可以改进预测的UR范例，然后在测试过程中使用UR直接识别出看不见的动作。在没有进一步培训的情况下，广泛的实验显示出UCF101和HMDB51基准测试的显着改进。

##### URL
[https://arxiv.org/abs/1803.08460](https://arxiv.org/abs/1803.08460)

##### PDF
[https://arxiv.org/pdf/1803.08460](https://arxiv.org/pdf/1803.08460)

