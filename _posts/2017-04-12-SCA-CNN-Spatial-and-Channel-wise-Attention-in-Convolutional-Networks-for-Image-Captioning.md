---
layout: post
title: "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning"
date: 2017-04-12 05:48:44
categories: arXiv_CV
tags: arXiv_CV Image_Caption Attention Caption CNN Prediction
author: Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, Tat-Seng Chua
mathjax: true
---

* content
{:toc}

##### Abstract
Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.

##### Abstract (translated by Google)
视觉注意已成功应用于结构预测任务，如视觉字幕和问答。现有的视觉注意模型通常是空间的，即，注意力被建模为空间概率，其重新加权编码输入图像的CNN的最后一个转换层特征图。然而，我们认为这种空间注意力并不一定符合注意机制 - 一种动态特征提取器，它结合了上下文固定，随着时间的推移，因为CNN特征自然是空间的，通道的和多层的。在本文中，我们介绍了一种新的卷积神经网络，称为SCA-CNN，它在CNN中结合了空间和信道方向的注意。在图像字幕的任务中，SCA-CNN动态地调制多层特征图中的句子生成上下文，编码视觉注意的位置（即，多层的注意空间位置）和什么（即，注意信道）。我们在三个基准图像字幕数据集上评估建议的SCA-CNN架构：Flickr8K，Flickr30K和MSCOCO。始终如一地观察到SCA-CNN明显优于最先进的基于视觉注意的图像字幕方法。

##### URL
[https://arxiv.org/abs/1611.05594](https://arxiv.org/abs/1611.05594)

##### PDF
[https://arxiv.org/pdf/1611.05594](https://arxiv.org/pdf/1611.05594)

