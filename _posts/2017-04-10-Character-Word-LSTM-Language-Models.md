---
layout: post
title: "Character-Word LSTM Language Models"
date: 2017-04-10 11:42:09
categories: arXiv_CL
tags: arXiv_CL Embedding RNN Language_Model
author: Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq
mathjax: true
---

* content
{:toc}

##### Abstract
We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.

##### Abstract (translated by Google)
我们提出了一个字 - 字长短期记忆语言模型，它既减少了相对于基线词级语言模型的困惑，又减少了模型参数的数量。字符信息可以揭示单词之间的结构（非）相似性，甚至可以在单词不在词汇表时使用，从而改进罕见和未知单词的建模。通过连接词和字符嵌入，相比于具有类似参数的基线模型和荷兰语的4.57％，我们实现了英语的相对改进高达2.77％。而且，我们也比大量参数的基准字级模型更胜一筹。

##### URL
[https://arxiv.org/abs/1704.02813](https://arxiv.org/abs/1704.02813)

##### PDF
[https://arxiv.org/pdf/1704.02813](https://arxiv.org/pdf/1704.02813)

