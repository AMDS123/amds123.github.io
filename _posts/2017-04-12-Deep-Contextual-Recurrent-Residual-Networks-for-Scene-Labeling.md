---
layout: post
title: "Deep Contextual Recurrent Residual Networks for Scene Labeling"
date: 2017-04-12 01:52:06
categories: arXiv_CV
tags: arXiv_CV Represenation_Learning
author: T. Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Marios Savvides, Dipan Pal
mathjax: true
---

* content
{:toc}

##### Abstract
Designed as extremely deep architectures, deep residual networks which provide a rich visual representation and offer robust convergence behaviors have recently achieved exceptional performance in numerous computer vision problems. Being directly applied to a scene labeling problem, however, they were limited to capture long-range contextual dependence, which is a critical aspect. To address this issue, we propose a novel approach, Contextual Recurrent Residual Networks (CRRN) which is able to simultaneously handle rich visual representation learning and long-range context modeling within a fully end-to-end deep network. Furthermore, our proposed end-to-end CRRN is completely trained from scratch, without using any pre-trained models in contrast to most existing methods usually fine-tuned from the state-of-the-art pre-trained models, e.g. VGG-16, ResNet, etc. The experiments are conducted on four challenging scene labeling datasets, i.e. SiftFlow, CamVid, Stanford background and SUN datasets, and compared against various state-of-the-art scene labeling methods.

##### Abstract (translated by Google)
作为极其深厚的体系结构设计，深度残留网络提供丰富的视觉表现并提供稳健的收敛行为，最近在许多计算机视觉问题中取得了卓越的性能。然而，直接应用于场景标记问题，它们仅限于捕获远距离情景依赖，这是一个关键方面。为了解决这个问题，我们提出了一种新颖的方法，能够在一个完全端到端的深度网络中同时处理丰富的视觉表示学习和远程上下文建模的情境递归残留网络（CRRN）。此外，我们提出的端对端CRRN是完全从头开始训练的，而不使用任何预先训练的模型，而大多数现有的方法通常是从现有技术的预训练模型（例如， VGG-16，ResNet等。在四个具有挑战性的场景标记数据集（即SiftFlow，CamVid，Stanford背景和SUN数据集）上进行实验，并与各种最先进的场景标记方法进行比较。

##### URL
[https://arxiv.org/abs/1704.03594](https://arxiv.org/abs/1704.03594)

##### PDF
[https://arxiv.org/pdf/1704.03594](https://arxiv.org/pdf/1704.03594)

