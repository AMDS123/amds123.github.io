---
layout: post
title: "Incidental Supervision from Question-Answering Signals"
date: 2019-09-01 06:30:57
categories: arXiv_CL
tags: arXiv_CL QA Language_Model
author: Hangfeng He, Qiang Ning, Dan Roth
mathjax: true
---

* content
{:toc}

##### Abstract
Human annotations are costly for many natural language processing (NLP) tasks, especially for those requiring NLP expertise. One promising solution is to use natural language to annotate natural language. However, it remains an open problem how to get supervision signals or learn representations from natural language annotations. This paper studies the case where the annotations are in the format of question-answering (QA) and proposes an effective way to learn useful representations for other tasks. We also find that the representation retrieved from question-answer meaning representation (QAMR) data can almost universally improve on a wide range of tasks, suggesting that such kind of natural language annotations indeed provide unique information on top of modern language models.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1909.00333](http://arxiv.org/abs/1909.00333)

##### PDF
[http://arxiv.org/pdf/1909.00333](http://arxiv.org/pdf/1909.00333)

