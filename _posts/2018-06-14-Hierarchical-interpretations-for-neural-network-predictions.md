---
layout: post
title: "Hierarchical interpretations for neural network predictions"
date: 2018-06-14 02:41:03
categories: arXiv_CV
tags: arXiv_CV Sentiment Adversarial Prediction Relation
author: Chandan Singh, W. James Murdoch, Bin Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.

##### Abstract (translated by Google)
深度神经网络（DNN）由于能够学习变量之间复杂的非线性关系而获得了令人印象深刻的预测性能。然而，无法有效地将这些关系可视化导致DNN被定性为黑盒子，并因此限制了它们的应用。为了改善这个问题，我们通过我们提出的方法 - 凝聚上下文分解（ACD），引入层次解释来解释DNN预测。给定来自训练的DNN的预测，ACD产生输入要素的层次聚类，以及每个聚类对最终预测的贡献。此层次结构经过优化，可识别DNN获知的功能集群是预测性的。使用Stanford Sentiment Treebank和ImageNet的实例，我们证明ACD在诊断不正确的预测和识别数据集偏倚方面是有效的。通过人体实验，我们证明ACD使用户既能够识别更准确的两个DNN，又能更好地信任DNN的输出。我们还发现，ACD的层次结构对对抗性扰动具有很强的鲁棒性，这意味着它捕捉输入的基本方面并忽略寄生噪声。

##### URL
[http://arxiv.org/abs/1806.05337](http://arxiv.org/abs/1806.05337)

##### PDF
[http://arxiv.org/pdf/1806.05337](http://arxiv.org/pdf/1806.05337)

