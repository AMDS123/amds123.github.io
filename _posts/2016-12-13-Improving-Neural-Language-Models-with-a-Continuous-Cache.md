---
layout: post
title: "Improving Neural Language Models with a Continuous Cache"
date: 2016-12-13 23:09:49
categories: arXiv_CL
tags: arXiv_CL Language_Model Prediction
author: Edouard Grave, Armand Joulin, Nicolas Usunier
mathjax: true
---

* content
{:toc}

##### Abstract
We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.

##### Abstract (translated by Google)
我们建议对神经网络语言模型进行扩展，以使其预测适应最近的历史。我们的模型是内存增强网络的简化版本，它将过去隐藏的激活存储为内存，并通过当前隐藏激活的点积访问它们。这种机制非常有效，可以扩展到非常大的内存大小。我们还在神经网络中使用外部存储器和基于计数的语言模型使用缓存模型之间建立了联系。我们演示了几种语言模型数据集，我们的方法比最近的记忆增强网络有更好的表现。

##### URL
[https://arxiv.org/abs/1612.04426](https://arxiv.org/abs/1612.04426)

##### PDF
[https://arxiv.org/pdf/1612.04426](https://arxiv.org/pdf/1612.04426)

