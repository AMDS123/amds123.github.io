---
layout: post
title: "Low-Precision Batch-Normalized Activations"
date: 2017-02-27 11:10:54
categories: arXiv_CV
tags: arXiv_CV
author: Benjamin Graham
mathjax: true
---

* content
{:toc}

##### Abstract
Artificial neural networks can be trained with relatively low-precision floating-point and fixed-point arithmetic, using between one and 16 bits. Previous works have focused on relatively wide-but-shallow, feed-forward networks. We introduce a quantization scheme that is compatible with training very deep neural networks. Quantizing the network activations in the middle of each batch-normalization module can greatly reduce the amount of memory and computational power needed, with little loss in accuracy.

##### Abstract (translated by Google)
人造神经网络可以用相对低精度的浮点和定点运算来训练，使用1到16比特。以前的作品集中在相对宽泛的前馈网络上。我们引入一个与训练非常深的神经网络兼容的量化方案。量化每个批量归一化模块中间的网络激活可以大大减少所需的内存和计算能力，而精度损失很小。

##### URL
[https://arxiv.org/abs/1702.08231](https://arxiv.org/abs/1702.08231)

##### PDF
[https://arxiv.org/pdf/1702.08231](https://arxiv.org/pdf/1702.08231)

