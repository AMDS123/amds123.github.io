---
layout: post
title: "Analysis of the Impact of Negative Sampling on Link Prediction in Knowledge Graphs"
date: 2018-03-02 12:27:10
categories: arXiv_AI
tags: arXiv_AI Knowledge_Graph Knowledge Embedding Prediction Relation
author: Bhushan Kotnis, Vivi Nastase
mathjax: true
---

* content
{:toc}

##### Abstract
Knowledge graphs are large, useful, but incomplete knowledge repositories. They encode knowledge through entities and relations which define each other through the connective structure of the graph. This has inspired methods for the joint embedding of entities and relations in continuous low-dimensional vector spaces, that can be used to induce new edges in the graph, i.e., link prediction in knowledge graphs. Learning these representations relies on contrasting positive instances with negative ones. Knowledge graphs include only positive relation instances, leaving the door open for a variety of methods for selecting negative examples. In this paper we present an empirical study on the impact of negative sampling on the learned embeddings, assessed through the task of link prediction. We use state-of-the-art knowledge graph embeddings -- \rescal , TransE, DistMult and ComplEX -- and evaluate on benchmark datasets -- FB15k and WN18. We compare well known methods for negative sampling and additionally propose embedding based sampling methods. We note a marked difference in the impact of these sampling methods on the two datasets, with the "traditional" corrupting positives method leading to best results on WN18, while embedding based methods benefiting the task on FB15k.

##### Abstract (translated by Google)
知识图是大的，有用的，但是知识库不完整。他们通过实体和关系来编码知识，这些实体和关系通过图的连接结构相互定义。这激发了实体和关系在连续的低维矢量空间中联合嵌入的方法，其可以用于在图中引起新的边缘，即知识图中的链接预测。学习这些表述依赖于对比积极的事例与消极的事例。知识图只包括正面关系实例，为选择负面实例的各种方法敞开大门。在本文中，我们提出了一个负面抽样对学习嵌入影响的实证研究，通过链接预测的任务进行评估。我们使用最先进的知识图嵌入 -  rescal，TransE，DistMult和ComplEX  - 并在基准数据集上评估--FB15k和WN18。我们比较了众所周知的负面采样方法，并提出了基于嵌入的采样方法。我们注意到这些采样方法对两个数据集的影响存在显着差异，“传统”腐败阳性方法导致WN18获得最佳结果，而基于嵌入的方法则有利于FB15k的任务。

##### URL
[http://arxiv.org/abs/1708.06816](http://arxiv.org/abs/1708.06816)

##### PDF
[http://arxiv.org/pdf/1708.06816](http://arxiv.org/pdf/1708.06816)

