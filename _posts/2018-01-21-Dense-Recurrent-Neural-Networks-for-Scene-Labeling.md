---
layout: post
title: "Dense Recurrent Neural Networks for Scene Labeling"
date: 2018-01-21 14:43:22
categories: arXiv_CV
tags: arXiv_CV Attention CNN RNN
author: Heng Fan, Haibin Ling
mathjax: true
---

* content
{:toc}

##### Abstract
Recently recurrent neural networks (RNNs) have demonstrated the ability to improve scene labeling through capturing long-range dependencies among image units. In this paper, we propose dense RNNs for scene labeling by exploring various long-range semantic dependencies among image units. In comparison with existing RNN based approaches, our dense RNNs are able to capture richer contextual dependencies for each image unit via dense connections between each pair of image units, which significantly enhances their discriminative power. Besides, to select relevant and meanwhile restrain irrelevant dependencies for each unit from dense connections, we introduce an attention model into dense RNNs. The attention model enables automatically assigning more importance to helpful dependencies while less weight to unconcerned dependencies. Integrating with convolutional neural networks (CNNs), our method achieves state-of-the-art performances on the PASCAL Context, MIT ADE20K and SiftFlow benchmarks.

##### Abstract (translated by Google)
最近，递归神经网络（RNN）已经证明了通过捕获图像单元之间的长距离依赖性来改善场景标记的能力。在本文中，我们通过探索图像单元之间的各种远程语义依赖，提出了用于场景标注的稠密RNN。与现有的基于RNN的方法相比，我们的稠密RNN能够通过每对图像单元之间的密集连接捕获每个图像单元更丰富的上下文相关性，这显着增强了它们的判别能力。此外，为了从密集连接中选择相关的，同时抑制每个单元的不相关的依赖关系，我们引入一个注意模型到密集的RNN中。注意模型使自动分配更重要的是有用的依赖关系，而不重视不重要的依赖关系。与卷积神经网络（CNN）相结合，我们的方法在PASCAL上下文，MIT ADE20K和SiftFlow基准测试中达到了最先进的性能。

##### URL
[https://arxiv.org/abs/1801.06831](https://arxiv.org/abs/1801.06831)

##### PDF
[https://arxiv.org/pdf/1801.06831](https://arxiv.org/pdf/1801.06831)

