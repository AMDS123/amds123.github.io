---
layout: post
title: "CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes"
date: 2018-02-27 18:39:31
categories: arXiv_CV
tags: arXiv_CV Knowledge CNN Deep_Learning Recognition
author: Yuhong Li, Xiaofan Zhang, Deming Chen
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. To our best acknowledge, CSRNet is the first implementation using dilated CNNs for crowd counting tasks. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance on all the datasets. In the ShanghaiTech Part_B dataset, we significantly achieve the MAE which is 47.3% lower than the previous state-of-the-art method. We extend the applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.

##### Abstract (translated by Google)
我们提出了一个名为CSRNet的拥塞场景识别网络，它提供了一种数据驱动和深度学习方法，可以理解高度拥挤的场景并执行准确的计数估计以及呈现高质量密度的地图。提议的CSRNet由两个主要部分组成：作为2D特征提取前端的卷积神经网络（CNN）和用于后端的扩展CNN，其使用扩大的内核来提供更大的接收场并取代池化操作。由于其纯粹的卷积结构，CSRNet是一个易于训练的模型。最值得肯定的是，CSRNet是第一个使用扩张型CNN进行人群统计任务的实施方案。我们在四个数据集（ShanghaiTech数据集，UCF_CC_50数据集，WorldEXPO'10数据集和UCSD数据集）上展示CSRNet，并在所有数据集上提供最先进的性能。在ShanghaiTech Part_B数据集中，我们显着地实现了比先前的先进方法低47.3％的MAE。我们扩展了计算其他对象的应用程序，例如TRANCOS数据集中的车辆。结果表明，CSRNet比以前的最新方法显着提高了MAE的输出质量，降低了15.4％。

##### URL
[https://arxiv.org/abs/1802.10062](https://arxiv.org/abs/1802.10062)

##### PDF
[https://arxiv.org/pdf/1802.10062](https://arxiv.org/pdf/1802.10062)

