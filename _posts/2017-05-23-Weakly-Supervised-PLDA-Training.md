---
layout: post
title: "Weakly Supervised PLDA Training"
date: 2017-05-23 10:19:15
categories: arXiv_CL
tags: arXiv_CL Weakly_Supervised
author: Lantian Li, Yixiang Chen, Dong Wang, Chenghui Zhao
mathjax: true
---

* content
{:toc}

##### Abstract
PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in `weak labels' which are not fully accurate but cheap, leading to a weak PLDA training. Our experimental results on real-life large-scale telephony customer service achieves demonstrated that the weak training can offer good performance when human-labelled data are limited. More interestingly, the weak training can be employed as a discriminative adaptation approach, which is more efficient than the prevailing unsupervised method when human-labelled data are insufficient.

##### Abstract (translated by Google)
PLDA是用于i矢量模型的流行的标准化方法，并且已经在说话者验证中提供了最新的性能。但是，PLDA培训需要大量标签化的开发数据，这在大多数情况下是非常昂贵的。我们提出一种便宜的PLDA训练方法，假设同一场会议的发言人可以很容易地分开，而且不同会议的发言人是完全不同的。这导致“弱标签”不完全准确但便宜，导致PLDA训练较弱。我们对实际大规模电话客户服务的实验结果表明，当人标数据有限时，弱培训可以提供良好的性能。更有意思的是，弱训练可以作为一种有区别的适应方法，当人标记的数据不足时，它比当前的无监督方法更有效。

##### URL
[https://arxiv.org/abs/1609.08441](https://arxiv.org/abs/1609.08441)

##### PDF
[https://arxiv.org/pdf/1609.08441](https://arxiv.org/pdf/1609.08441)

