---
layout: post
title: "AWE-CM Vectors: Augmenting Word Embeddings with a Clinical Metathesaurus"
date: 2017-12-05 03:11:07
categories: arXiv_CL
tags: arXiv_CL
author: Willie Boag, Hassan Kan&#xe9;
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, word embeddings have been surprisingly effective at capturing intuitive characteristics of the words they represent. These vectors achieve the best results when training corpora are extremely large, sometimes billions of words. Clinical natural language processing datasets, however, tend to be much smaller. Even the largest publicly-available dataset of medical notes is three orders of magnitude smaller than the dataset of the oft-used "Google News" word vectors. In order to make up for limited training data sizes, we encode expert domain knowledge into our embeddings. Building on a previous extension of word2vec, we show that generalizing the notion of a word's "context" to include arbitrary features creates an avenue for encoding domain knowledge into word embeddings. We show that the word vectors produced by this method outperform their text-only counterparts across the board in correlation with clinical experts.

##### Abstract (translated by Google)
近年来，单词嵌入在捕捉他们所代表的单词的直观特征方面出人意料地有效。当训练语料库非常大，有时数十亿字时，这些载体达到最好的结果。然而，临床自然语言处理数据集往往小得多。即使是医疗记录最大的公开可用数据集，也比经常使用的“Google新闻”单词向量的数据集小三个数量级。为了弥补有限的训练数据量，我们将专家领域知识编码到我们的嵌入中。基于word2vec以前的扩展，我们展示了将单词“上下文”的概念概括为包含任意特征创建了将领域知识编码为单词嵌入的途径。我们表明，这种方法产生的单词向量比临床专家的相关文本表现更好。

##### URL
[http://arxiv.org/abs/1712.01460](http://arxiv.org/abs/1712.01460)

