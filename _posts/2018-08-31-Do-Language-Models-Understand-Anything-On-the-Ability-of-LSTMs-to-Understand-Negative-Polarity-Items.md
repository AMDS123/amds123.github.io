---
layout: post
title: "Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items"
date: 2018-08-31 08:21:45
categories: arXiv_CL
tags: arXiv_CL RNN Deep_Learning Language_Model Relation
author: Jaap Jumelet, Dieuwke Hupkes
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we attempt to link the inner workings of a neural language model to linguistic theory, focusing on a complex phenomenon well discussed in formal linguis- tics: (negative) polarity items. We briefly discuss the leading hypotheses about the licensing contexts that allow negative polarity items and evaluate to what extent a neural language model has the ability to correctly process a subset of such constructions. We show that the model finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence. With this research, we hope to pave the way for other studies linking formal linguistics to deep learning.

##### Abstract (translated by Google)
在本文中，我们试图将神经语言模型的内部运作与语言理论联系起来，重点关注在形式语言学中充分讨论的复杂现象:(负）极性项。我们简要讨论关于允许负极性项目的许可上下文的主要假设，并评估神经语言模型在多大程度上能够正确处理此类构造的子集。我们证明模型找到了许可上下文和负极性项之间的关系，并且似乎意识到了这个上下文的范围，我们从句子的解析树中提取。通过这项研究，我们希望为将正式语言学与深度学习联系起来的其他研究铺平道路。

##### URL
[http://arxiv.org/abs/1808.10627](http://arxiv.org/abs/1808.10627)

##### PDF
[http://arxiv.org/pdf/1808.10627](http://arxiv.org/pdf/1808.10627)

