---
layout: post
title: "Embedding Words and Senses Together via Joint Knowledge-Enhanced Training"
date: 2017-06-21 10:16:35
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Quantitative
author: Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci, Roberto Navigli
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings are widely used in Natural Language Processing, mainly due to their success in capturing semantic information from massive corpora. However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector. We address this issue by proposing a new model which learns word and sense embeddings jointly. Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to state-of-the-art word- and sense-based models.

##### Abstract (translated by Google)
自动语言处理中广泛使用了词嵌入，主要是由于它们成功捕获大量语料库的语义信息。然而，他们的创作过程不允许一个单词的不同含义被自动分开，因为它将它们混合成一个单独的矢量。我们通过提出一个联合学习单词和感知嵌入的新模型来解决这个问题。我们的模型利用来自语义网络的大型语料库和知识来产生统一的词义嵌入的向量空间。我们评估我们的方法在各种任务中的定性和定量的主要特征，突出所提出的方法的优点与最先进的基于单词和基于感觉的模型相比较。

##### URL
[https://arxiv.org/abs/1612.02703](https://arxiv.org/abs/1612.02703)

##### PDF
[https://arxiv.org/pdf/1612.02703](https://arxiv.org/pdf/1612.02703)

