---
layout: post
title: "AGIL: Learning Attention from Human for Visuomotor Tasks"
date: 2018-06-01 18:36:36
categories: arXiv_AI
tags: arXiv_AI Attention Prediction
author: Ruohan Zhang, Zhuode Liu, Luxin Zhang, Jake A. Whritner, Karl S. Muller, Mary M. Hayhoe, Dana H. Ballard
mathjax: true
---

* content
{:toc}

##### Abstract
When intelligent agents learn visuomotor behaviors from human demonstrations, they may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agents' performance. With this motivation, we propose the AGIL (Attention Guided Imitation Learning) framework. We collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another network to predict human actions (the policy network). Incorporating the learned attention model from the gaze network into the policy network significantly improves the action prediction accuracy and task performance.

##### Abstract (translated by Google)
当智能代理人从人类示威中学习视觉运动行为时，他们可能从知道人类分配视觉注意力的位置受益，这可以从他们的视线中推断出来。关于智能决策的丰富信息通过人类注视分配来传达;因此，利用这些信息有可能提高代理商的业绩。有了这个动机，我们提出了AGIL（注意指导模仿学习）框架。我们在仔细控制的实验环境中玩Atari游戏时收集高质量的人体动作和注视数据。使用这些数据，我们首先训练一个深度神经网络，可以高准确度地预测人眼注视位置和视觉注意力（注视网络），然后训练另一个网络来预测人的行为（政策网络）。将来自注视网络的学习关注模型并入策略网络中可显着提高动作预测准确性和任务性能。

##### URL
[http://arxiv.org/abs/1806.03960](http://arxiv.org/abs/1806.03960)

##### PDF
[http://arxiv.org/pdf/1806.03960](http://arxiv.org/pdf/1806.03960)

