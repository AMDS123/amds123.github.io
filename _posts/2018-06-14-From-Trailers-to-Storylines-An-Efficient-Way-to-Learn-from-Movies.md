---
layout: post
title: "From Trailers to Storylines: An Efficient Way to Learn from Movies"
date: 2018-06-14 02:52:09
categories: arXiv_CV
tags: arXiv_CV
author: Qingqiu Huang, Yuanjun Xiong, Yu Xiong, Yuqi Zhang, Dahua Lin
mathjax: true
---

* content
{:toc}

##### Abstract
The millions of movies produced in the human history are valuable resources for computer vision research. However, learning a vision model from movie data would meet with serious difficulties. A major obstacle is the computational cost -- the length of a movie is often over one hour, which is substantially longer than the short video clips that previous study mostly focuses on. In this paper, we explore an alternative approach to learning vision models from movies. Specifically, we consider a framework comprised of a visual module and a temporal analysis module. Unlike conventional learning methods, the proposed approach learns these modules from different sets of data -- the former from trailers while the latter from movies. This allows distinctive visual features to be learned within a reasonable budget while still preserving long-term temporal structures across an entire movie. We construct a large-scale dataset for this study and define a series of tasks on top. Experiments on this dataset showed that the proposed method can substantially reduce the training time while obtaining highly effective features and coherent temporal structures.

##### Abstract (translated by Google)
在人类历史上生产的数百万部电影是计算机视觉研究的宝贵资源。但是，从电影数据中学习视觉模型会遇到严重的困难。一个主要的障碍是计算成本 - 电影的长度通常超过一个小时，这比以前的研究主要关注的短视频剪辑长得多。在本文中，我们探讨了从电影中学习视觉模型的另一种方法。具体而言，我们考虑一个由可视化模块和时间分析模块组成的框架。与传统的学习方法不同，所提出的方法从不同的数据集中学习这些模块 - 前者来自预告片，而后者来自电影。这允许在合理的预算内学习独特的视觉特征，同时仍保持整个电影的长期时间结构。我们为这项研究构建了一个大规模的数据集，并在上面定义了一系列的任务。在这个数据集上的实验表明，该方法可以大大减少训练时间，同时获得高效特征和相干时间结构。

##### URL
[http://arxiv.org/abs/1806.05341](http://arxiv.org/abs/1806.05341)

##### PDF
[http://arxiv.org/pdf/1806.05341](http://arxiv.org/pdf/1806.05341)

