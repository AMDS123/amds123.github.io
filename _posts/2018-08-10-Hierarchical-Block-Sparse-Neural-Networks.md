---
layout: post
title: "Hierarchical Block Sparse Neural Networks"
date: 2018-08-10 05:53:12
categories: arXiv_AI
tags: arXiv_AI Sparse
author: Dharma Teja Vooturi, Dheevatsa Mudigree, Sasikanth Avancha
mathjax: true
---

* content
{:toc}

##### Abstract
Sparse deep neural networks(DNNs) are efficient in both memory and compute when compared to dense DNNs. But due to irregularity in computation of sparse DNNs, their efficiencies are much lower than that of dense DNNs on general purpose hardwares. This leads to poor/no performance benefits for sparse DNNs. Performance issue for sparse DNNs can be alleviated by bringing structure to the sparsity and leveraging it for improving runtime efficiency. But such structural constraints often lead to sparse models with suboptimal accuracies. In this work, we jointly address both accuracy and performance of sparse DNNs using our proposed class of neural networks called HBsNN ( Hierarchical Block Sparse Neural Networks).

##### Abstract (translated by Google)
与密集DNN相比，稀疏深度神经网络（DNN）在存储器和计算中都是有效的。但由于稀疏DNN计算的不规则性，其效率远低于通用硬件上的密集DNN。这导致稀疏DNN的差/无效益。稀疏DNN的性能问题可以通过将结构引入稀疏性并利用它来提高运行时效率来缓解。但是这种结构约束常常导致稀疏模型具有次优的精度。在这项工作中，我们使用我们提出的称为HBsNN（分层块稀疏神经网络）的神经网络类共同解决稀疏DNN的准确性和性能。

##### URL
[http://arxiv.org/abs/1808.03420](http://arxiv.org/abs/1808.03420)

##### PDF
[http://arxiv.org/pdf/1808.03420](http://arxiv.org/pdf/1808.03420)

