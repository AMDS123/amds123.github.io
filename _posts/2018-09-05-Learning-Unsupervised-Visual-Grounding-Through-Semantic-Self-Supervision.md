---
layout: post
title: "Learning Unsupervised Visual Grounding Through Semantic Self-Supervision"
date: 2018-09-05 14:51:14
categories: arXiv_CV
tags: arXiv_CV Quantitative
author: Syed Ashar Javed, Shreyas Saxena, Vineet Gandhi
mathjax: true
---

* content
{:toc}

##### Abstract
Localizing natural language phrases in images is a challenging problem that requires joint understanding of both the textual and visual modalities. In the unsupervised setting, lack of supervisory signals exacerbate this difficulty. In this paper, we propose a novel framework for unsupervised visual grounding which uses concept learning as a proxy task to obtain self-supervision. The simple intuition behind this idea is to encourage the model to localize to regions which can explain some semantic property in the data, in our case, the property being the presence of a concept in a set of images. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our approach and show a 5.6% improvement over the current state of the art on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and comparable to state-of-art performance on the Flickr30k dataset.

##### Abstract (translated by Google)
在图像中定位自然语言短语是一个具有挑战性的问题，需要共同理解文本和视觉模态。在无监督的环境中，缺乏监督信号会加剧这种困难。在本文中，我们提出了一种新的无监督视觉基础框架，它使用概念学习作为代理任务来获得自我监督。这个想法背后的简单直觉是鼓励模型本地化到可以解释数据中某些语义属性的区域，在我们的例子中，属性是在一组图像中存在概念。我们提供了彻底的定量和定性实验来证明我们的方法的有效性，并且显示出比Visual Genome数据集当前最新技术水平提高5.6％，在ReferItGame数据集上有5.8％的改进，并且与最新的性能相当。 Flickr30k数据集。

##### URL
[http://arxiv.org/abs/1803.06506](http://arxiv.org/abs/1803.06506)

##### PDF
[http://arxiv.org/pdf/1803.06506](http://arxiv.org/pdf/1803.06506)

