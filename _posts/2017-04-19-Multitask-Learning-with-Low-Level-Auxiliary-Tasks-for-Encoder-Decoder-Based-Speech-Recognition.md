---
layout: post
title: "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition"
date: 2017-04-19 16:01:53
categories: arXiv_CL
tags: arXiv_CL Knowledge Speech_Recognition Deep_Learning Recognition
author: Shubham Toshniwal, Hao Tang, Liang Lu, Karen Livescu
mathjax: true
---

* content
{:toc}

##### Abstract
End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set.

##### Abstract (translated by Google)
基于深度学习的模型的端到端训练允许基于最终任务损失隐式学习中间表示。然而，端到端的方法忽略了在明确的中间层监督下编码的有用领域知识。我们假设使用中间表示作为深层网络的较低层次的辅助监督可能是结合端到端培训的优点和更传统的管道方法的好方法。我们在会话式语音识别方面进行了实验，我们在使用编码器 - 解码器模型的多任务训练方法中使用底层任务（如音素识别）进行直接字符转录。我们比较多种类型的低级任务，并分析辅助任务的效果。我们在交换机语料库上的结果表明，这种方法提高了Eval2000测试装置上标准编码器 - 解码器模型的识别精度。

##### URL
[https://arxiv.org/abs/1704.01631](https://arxiv.org/abs/1704.01631)

##### PDF
[https://arxiv.org/pdf/1704.01631](https://arxiv.org/pdf/1704.01631)

