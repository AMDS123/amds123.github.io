---
layout: post
title: "Deep Learning with Convolutional Neural Network for Objective Skill Evaluation in Robot-assisted Surgery"
date: 2018-06-15 03:22:06
categories: arXiv_CV
tags: arXiv_CV Knowledge Segmentation CNN Deep_Learning
author: Ziheng Wang, Ann Majewicz Fey
mathjax: true
---

* content
{:toc}

##### Abstract
With the advent of robot-assisted surgery, the role of data-driven approaches to integrate statistics and machine learning is growing rapidly with prominent interests in objective surgical skill assessment. However, most existing work requires translating robot motion kinematics into intermediate features or gesture segments that are expensive to extract, lack efficiency, and require significant domain-specific knowledge. We propose an analytical deep learning framework for skill assessment in surgical training. A deep convolutional neural network is implemented to map multivariate time series data of the motion kinematics to individual skill levels. We perform experiments on the public minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our proposed learning model achieved a competitive accuracy of 94.1%, 90.3%, and 86.8%, in the standard training tasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need of engineered features or carefully-tuned gesture segmentation, our model can successfully decode skill information from raw motion profiles via end-to-end learning. Meanwhile, the proposed model is able to reliably interpret skills within 1-3 second window, without needing an observation of entire training trial. This study highlights the potentials of deep architectures for an proficient online skill assessment in modern surgical training.

##### Abstract (translated by Google)
随着机器人辅助手术的出现，数据驱动方法在整合统计学和机器学习方面的作用正在迅速增长，其对客观手术技能评估有着突出的兴趣。然而，大多数现有的工作要求将机器人运动学运动转换为中间特征或手势段，这些特征或手势段提取昂贵，缺乏效率并且需要明确的特定领域知识。我们提出了一个分析性的深度学习框架，用于外科培训的技能评估实施深卷积神经网络以将运动运动学的多元时间序列数据映射到个人技能水平。我们在公共微创手术机器人数据集JHU-ISI手势和技能评估工作集（JIGSAWS）上进行实验。我们提出的学习模型在标准培训任务中分别达到了94.1％，90.3％和86.8％的竞争准确率：缝合，穿针和打结。无需设计功能或经过仔细调整的手势分割，我们的模型可以通过端到端的学习，成功解码来自原始运动配置文件的技能信息。同时，所提出的模型能够在1-3秒内可靠地解释技能，而无需观察整个训练试验。这项研究突出了现代外科培训中精通在线技能评估的深层架构的潜力。

##### URL
[http://arxiv.org/abs/1806.05796](http://arxiv.org/abs/1806.05796)

##### PDF
[http://arxiv.org/pdf/1806.05796](http://arxiv.org/pdf/1806.05796)

