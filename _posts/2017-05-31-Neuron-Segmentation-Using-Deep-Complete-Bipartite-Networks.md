---
layout: post
title: "Neuron Segmentation Using Deep Complete Bipartite Networks"
date: 2017-05-31 12:15:59
categories: arXiv_CV
tags: arXiv_CV Segmentation Face CNN Deep_Learning Prediction Quantitative
author: Jianxu Chen, Sreya Banerjee, Abhinav Grama, Walter J. Scheirer, Danny Z. Chen
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we consider the problem of automatically segmenting neuronal cells in dual-color confocal microscopy images. This problem is a key task in various quantitative analysis applications in neuroscience, such as tracing cell genesis in Danio rerio (zebrafish) brains. Deep learning, especially using fully convolutional networks (FCN), has profoundly changed segmentation research in biomedical imaging. We face two major challenges in this problem. First, neuronal cells may form dense clusters, making it difficult to correctly identify all individual cells (even to human experts). Consequently, segmentation results of the known FCN-type models are not accurate enough. Second, pixel-wise ground truth is difficult to obtain. Only a limited amount of approximate instance-wise annotation can be collected, which makes the training of FCN models quite cumbersome. We propose a new FCN-type deep learning model, called deep complete bipartite networks (CB-Net), and a new scheme for leveraging approximate instance-wise annotation to train our pixel-wise prediction model. Evaluated using seven real datasets, our proposed new CB-Net model outperforms the state-of-the-art FCN models and produces neuron segmentation results of remarkable quality

##### Abstract (translated by Google)
在本文中，我们考虑在双色共焦显微镜图像中自动分割神经元细胞的问题。这个问题是在神经科学的各种定量分析应用中的关键任务，例如追踪斑马鱼（Danio rerio）（斑马鱼）脑中的细胞发生。深度学习，尤其是使用完全卷积网络（FCN），已经深刻地改变了生物医学成像领域的细分研究。在这个问题上我们面临两大挑战。首先，神经元细胞可能形成密集的簇，使得难以正确识别所有单个细胞（甚至对于人类专家）。因此，已知的FCN型模型的分割结果不够精确。其次，基于像素的基本事实很难获得。只能收集有限数量的近似实例注释，这使得FCN模型的训练相当麻烦。我们提出了一种称为深度完全双向网络（CB-Net）的新型FCN型深度学习模型，以及利用近似实例注释来训练我们的像素级预测模型的新方案。使用七个真实数据集进行评估，我们提出的新的CB-Net模型胜过最先进的FCN模型，并产生卓越质量的神经元分割结果

##### URL
[https://arxiv.org/abs/1705.11053](https://arxiv.org/abs/1705.11053)

##### PDF
[https://arxiv.org/pdf/1705.11053](https://arxiv.org/pdf/1705.11053)

