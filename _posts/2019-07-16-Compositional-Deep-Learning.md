---
layout: post
title: "Compositional Deep Learning"
date: 2019-07-16 10:21:15
categories: arXiv_AI
tags: arXiv_AI GAN Optimization Deep_Learning Gradient_Descent
author: Bruno Gavranovi&#x107;
mathjax: true
---

* content
{:toc}

##### Abstract
Neural networks have become an increasingly popular tool for solving many real-world problems. They are a general framework for differentiable optimization which includes many other machine learning approaches as special cases. In this thesis we build a category-theoretic formalism around a class of neural networks exemplified by CycleGAN. CycleGAN is a collection of neural networks, closed under composition, whose inductive bias is increased by enforcing composition invariants, i.e. cycle-consistencies. Inspired by Functorial Data Migration, we specify the interconnection of these networks using a categorical schema, and network instances as set-valued functors on this schema. We also frame neural network architectures, datasets, models, and a number of other concepts in a categorical setting and thus show a special class of functors, rather than functions, can be learned using gradient descent. We use the category-theoretic framework to conceive a novel neural network architecture whose goal is to learn the task of object insertion and object deletion in images with unpaired data. We test the architecture on three different datasets and obtain promising results.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.08292](http://arxiv.org/abs/1907.08292)

##### PDF
[http://arxiv.org/pdf/1907.08292](http://arxiv.org/pdf/1907.08292)

