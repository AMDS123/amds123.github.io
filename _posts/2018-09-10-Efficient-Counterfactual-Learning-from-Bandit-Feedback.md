---
layout: post
title: "Efficient Counterfactual Learning from Bandit Feedback"
date: 2018-09-10 02:08:14
categories: arXiv_AI
tags: arXiv_AI Optimization
author: Yusuke Narita, Shota Yasui, Kohei Yata
mathjax: true
---

* content
{:toc}

##### Abstract
What is the most statistically efficient way to do off-policy evaluation and optimization with batch data from bandit feedback? For log data generated by contextual bandit algorithms, we consider offline estimators for the expected reward from a counterfactual policy. Our estimators are shown to have lowest variance in a wide class of estimators, achieving variance reduction relative to standard estimators. We also apply our estimators to improve online advertisement design by a major advertisement company. Consistent with the theoretical result, our estimators allow us to improve on the existing bandit algorithm with more statistical confidence compared to a state-of-the-art benchmark.

##### Abstract (translated by Google)
使用来自强盗反馈的批量数据进行政策评估和优化的最具统计效率的方法是什么？对于由上下文强盗算法生成的日志数据，我们考虑离线估计器来获得来自反事实策略的预期奖励。我们的估计量在一大类估计量中具有最低的方差，相对于标准估计量，实现了方差减少。我们还应用我们的估算器来改进主要广告公司的在线广告设计。与理论结果一致，我们的估算器允许我们改进现有的强盗算法，与最先进的基准相比具有更高的统计可信度。

##### URL
[http://arxiv.org/abs/1809.03084](http://arxiv.org/abs/1809.03084)

##### PDF
[http://arxiv.org/pdf/1809.03084](http://arxiv.org/pdf/1809.03084)

