---
layout: post
title: "Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency"
date: 2018-05-10 15:58:09
categories: arXiv_CL
tags: arXiv_CL Regularization Summarization
author: Bingzhen Wei, Xuancheng Ren, Xu Sun, Yi Zhang, Xiaoyan Cai, Qi Su
mathjax: true
---

* content
{:toc}

##### Abstract
Abstractive text summarization is a highly difficult problem, and the sequence-to-sequence model has shown success in improving the performance on the task. However, the generated summaries are often inconsistent with the source content in semantics. In such cases, when generating summaries, the model selects semantically unrelated words with respect to the source content as the most probable output. The problem can be attributed to heuristically constructed training data, where summaries can be unrelated to the source content, thus containing semantically unrelated words and spurious word correspondence. In this paper, we propose a regularization approach for the sequence-to-sequence model and make use of what the model has learned to regularize the learning objective to alleviate the effect of the problem. In addition, we propose a practical human evaluation method to address the problem that the existing automatic evaluation method does not evaluate the semantic consistency with the source content properly. Experimental results demonstrate the effectiveness of the proposed approach, which outperforms almost all the existing models. Especially, the proposed approach improves the semantic consistency by 4\% in terms of human evaluation.

##### Abstract (translated by Google)
抽象文本摘要是一个非常困难的问题，序列到序列模型在改善任务性能方面取得了成功。但是，生成的摘要通常与语义中的源内容不一致。在这种情况下，当生成摘要时，模型选择与源内容相关的语义无关词作为最可能的输出。问题可归因于启发式构建的训练数据，其中摘要可与源内容无关，因此包含语义上不相关的词和虚假词对应。在本文中，我们提出了序列 - 序列模型的正则化方法，并利用模型学到的知识来规范学习目标以减轻问题的影响。另外，本文提出了一种实用的人力评估方法，以解决现有的自动评估方法无法正确评估与源内容语义一致性的问题。实验结果证明了所提出的方法的有效性，其胜过几乎所有现有的模型。特别是，所提出的方法在人类评估方面提高了4％的语义一致性。

##### URL
[http://arxiv.org/abs/1805.04033](http://arxiv.org/abs/1805.04033)

##### PDF
[http://arxiv.org/pdf/1805.04033](http://arxiv.org/pdf/1805.04033)

