---
layout: post
title: "Recurrent Orthogonal Networks and Long-Memory Tasks"
date: 2017-03-15 17:45:08
categories: arXiv_CV
tags: arXiv_CV Optimization RNN
author: Mikael Henaff, Arthur Szlam, Yann LeCun
mathjax: true
---

* content
{:toc}

##### Abstract
Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.

##### Abstract (translated by Google)
尽管RNN已经被证明是处理顺序数据的强大工具，但是寻找允许它们建模非常长期依赖性的架构或优化策略仍然是一个活跃的研究领域。在这项工作中，我们仔细分析了最初概述的两个合成数据集（Hochreiter和Schmidhuber，1997），这些数据集用于评估RNNs在多个时间步骤中存储信息的能力。我们明确地构建RNN解决方案来解决这些问题，并且使用这些构造来阐明问题本身以及RNN将不同类型的信息存储在其隐藏状态中的方式。这些构造进一步解释了最近方法的成功，这些方法指定了转换矩阵的单位初始化或约束。

##### URL
[https://arxiv.org/abs/1602.06662](https://arxiv.org/abs/1602.06662)

##### PDF
[https://arxiv.org/pdf/1602.06662](https://arxiv.org/pdf/1602.06662)

