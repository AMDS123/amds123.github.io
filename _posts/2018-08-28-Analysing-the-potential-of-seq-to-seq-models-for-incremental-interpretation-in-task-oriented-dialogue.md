---
layout: post
title: "Analysing the potential of seq-to-seq models for incremental interpretation in task-oriented dialogue"
date: 2018-08-28 09:00:37
categories: arXiv_CL
tags: arXiv_CL Attention
author: Dieuwke Hupkes, Sanne Bouwmeester, Raquel Fern&#xe1;ndez
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate how encoder-decoder models trained on a synthetic dataset of task-oriented dialogues process disfluencies, such as hesitations and self-corrections. We find that, contrary to earlier results, disfluencies have very little impact on the task success of seq-to-seq models with attention. Using visualisation and diagnostic classifiers, we analyse the representations that are incrementally built by the model, and discover that models develop little to no awareness of the structure of disfluencies. However, adding disfluencies to the data appears to help the model create clearer representations overall, as evidenced by the attention patterns the different models exhibit.

##### Abstract (translated by Google)
我们研究了在面向任务的对话的合成数据集上训练的编码器 - 解码器模型如何处理不流失，例如犹豫和自我纠正。我们发现，与早期的结果相反，不受影响对seq-to-seq模型的任务成功影响很小。使用可视化和诊断分类器，我们分析由模型逐步建立的表示，并发现模型很少或根本没有意识到不流畅的结构。然而，对数据添加不满意似乎有助于模型创建更清晰的表示，正如不同模型所展示的注意模式所证明的那样。

##### URL
[http://arxiv.org/abs/1808.09178](http://arxiv.org/abs/1808.09178)

##### PDF
[http://arxiv.org/pdf/1808.09178](http://arxiv.org/pdf/1808.09178)

