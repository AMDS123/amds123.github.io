---
layout: post
title: "Numerically Grounded Language Models for Semantic Error Correction"
date: 2016-08-14 22:34:22
categories: arXiv_CL
tags: arXiv_CL Knowledge Language_Model Detection
author: Georgios P. Spithourakis, Isabelle Augenstein, Sebastian Riedel
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.

##### Abstract (translated by Google)
语义错误检测和纠正是事实检查，语音到文本或语法纠错等应用的重要任务。目前的方法通常集中在相对较浅的语义上，而不考虑数量。我们的方法使用文本中的数字语言模型。对于经常性的神经语言模型体系结构，这样的基础很容易实现，这可以进一步以不完备的背景知识库为基础。我们对临床报告的评估显示，数字接地与未接地的方法相比，将困惑度提高了33％，对于语义错误纠正的提高了5个点。在知识基础上进行调整可以进一步改进。

##### URL
[https://arxiv.org/abs/1608.04147](https://arxiv.org/abs/1608.04147)

##### PDF
[https://arxiv.org/pdf/1608.04147](https://arxiv.org/pdf/1608.04147)

