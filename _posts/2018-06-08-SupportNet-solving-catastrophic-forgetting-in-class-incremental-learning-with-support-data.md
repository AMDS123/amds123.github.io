---
layout: post
title: "SupportNet: solving catastrophic forgetting in class incremental learning with support data"
date: 2018-06-08 01:58:51
categories: arXiv_AI
tags: arXiv_AI Review Knowledge Classification Deep_Learning Prediction
author: Yu Li, Zhongxiao Li, Lizhong Ding, Peng Yang, Yuhui Hu, Wei Chen, Xin Gao
mathjax: true
---

* content
{:toc}

##### Abstract
A plain well-trained deep learning model often does not have the ability to learn new knowledge without forgetting the previously learned knowledge, which is known as the catastrophic forgetting. Here we propose a novel method, SupportNet, to solve the catastrophic forgetting problem in class incremental learning scenario efficiently and effectively. SupportNet combines the strength of deep learning and support vector machine (SVM), where SVM is used to identify the support data from the old data, which are fed to the deep learning model together with the new data for further training so that the model can review the essential information of the old data when learning the new information. Two powerful consolidation regularizers are applied to ensure the robustness of the learned model. Comprehensive experiments on various tasks, including enzyme function prediction, subcellular structure classification and breast tumor classification, show that SupportNet drastically outperforms the state-of-the-art incremental learning methods and even reaches similar performance as the deep learning model trained from scratch on both old and new data. Our program is accessible at: https://github.com/lykaust15/SupportNet

##### Abstract (translated by Google)
纯粹的训练有素的深度学习模式通常不具备学习新知识的能力，而不会忘记以前学到的知识，这被称为灾难性遗忘。在这里，我们提出了一种新颖的方法，SupportNet，以有效地解决课堂增量学习场景中的灾难性遗忘问题。 SupportNet结合了深度学习和支持向量机（SVM）的优势，其中SVM用于识别来自旧数据的支持数据，将这些数据与新数据一起提供给深度学习模型，以便进一步培训，从而使模型可以在学习新信息时，检查旧数据的基本信息。应用两个强大的整合调整器来确保学习模型的稳健性。包括酶功能预测，亚细胞结构分类和乳腺肿瘤分类在内的各种任务的综合实验表明，SupportNet大大超越了最先进的增量式学习方法，甚至达到了与从零开始的从零开始训练的深度学习模型类似的性能新旧数据。我们的计划可通过以下网址访问：https：//github.com/lykaust15/SupportNet

##### URL
[http://arxiv.org/abs/1806.02942](http://arxiv.org/abs/1806.02942)

##### PDF
[http://arxiv.org/pdf/1806.02942](http://arxiv.org/pdf/1806.02942)

