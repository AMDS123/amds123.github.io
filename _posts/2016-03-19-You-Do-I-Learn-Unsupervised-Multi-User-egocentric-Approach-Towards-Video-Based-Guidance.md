---
layout: post
title: "You-Do, I-Learn: Unsupervised Multi-User egocentric Approach Towards Video-Based Guidance"
date: 2016-03-19 17:44:58
categories: arXiv_CV
tags: arXiv_CV Attention Tracking
author: Dima Damen, Teesid Leelasawassuk, Walterio Mayol-Cuevas
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents an unsupervised approach towards automatically extracting video-based guidance on object usage, from egocentric video and wearable gaze tracking, collected from multiple users while performing tasks. The approach i) discovers task relevant objects, ii) builds a model for each, iii) distinguishes different ways in which each discovered object has been used and iv) discovers the dependencies between object interactions. The work investigates using appearance, position, motion and attention, and presents results using each and a combination of relevant features. Moreover, an online scalable approach is presented and is compared to offline results. The paper proposes a method for selecting a suitable video guide to be displayed to a novice user indicating how to use an object, purely triggered by the user's gaze. The potential assistive mode can also recommend an object to be used next based on the learnt sequence of object interactions. The approach was tested on a variety of daily tasks such as initialising a printer, preparing a coffee and setting up a gym machine.

##### Abstract (translated by Google)
本文提出了一种无监督的方法来自动提取基于视频的对象使用指导，从以自己为中心的视频和可穿戴视线跟踪，在执行任务时从多个用户收集。 i）发现任务相关对象，ii）为每个对象建立一个模型，iii）区分使用每个发现对象的不同方式，以及iv）发现对象相互作用之间的依赖关系。这项工作使用外观，位置，运动和注意力进行调查，并使用每一个和相关特征的组合来呈现结果。此外，还提出了在线可扩展方法，并将其与离线结果进行比较。本文提出了一种用于选择合适的视频指南的方法，该视频指南被显示给新手用户，指示如何使用纯粹由用户凝视触发的对象。潜在的辅助模式也可以基于所学习的对象交互序列来推荐下一个要使用的对象。该方法经过各种日常任务的测试，如初始化打印机，准备咖啡和建立健身机。

##### URL
[https://arxiv.org/abs/1510.04862](https://arxiv.org/abs/1510.04862)

##### PDF
[https://arxiv.org/pdf/1510.04862](https://arxiv.org/pdf/1510.04862)

