---
layout: post
title: "Generating Natural Adversarial Examples"
date: 2017-10-31 06:22:26
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Image_Classification Classification Prediction
author: Zhengli Zhao, Dheeru Dua, Sameer Singh
mathjax: true
---

* content
{:toc}

##### Abstract
Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.

##### Abstract (translated by Google)
由于其复杂的性质，很难描述机器学习模型在部署时可能行为不当或被利用的方式。近期有关对抗性实例的研究，即导致实质性不同模型预测的微扰动输入，有助于评估这些模型的鲁棒性，揭露它们失败的对抗情景。然而，这些恶意扰乱往往是不自然的，没有语义上的意义，并不适用于复杂的领域，如语言。在本文中，我们提出了一个框架，生成自然和清晰的敌对的例子，通过搜索在密集和连续的数据表示的语义空间，利用生成敌对​​网络的最新进展。我们介绍了生成的对手，展示了黑盒分类器在图像分类，文本蕴含和机器翻译等广泛应用中所提出方法的潜力。我们包括实验来证明生成的对手是自然的，对于人类是清晰的，并且在评估和分析黑盒分类器时很有用。

##### URL
[https://arxiv.org/abs/1710.11342](https://arxiv.org/abs/1710.11342)

##### PDF
[https://arxiv.org/pdf/1710.11342](https://arxiv.org/pdf/1710.11342)

