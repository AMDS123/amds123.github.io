---
layout: post
title: "Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images"
date: 2018-08-23 09:41:47
categories: arXiv_CV
tags: arXiv_CV Regularization Segmentation CNN Semantic_Segmentation Relation
author: Michele Volpi, Devis Tuia
mathjax: true
---

* content
{:toc}

##### Abstract
When approaching the semantic segmentation of overhead imagery in the decimeter spatial resolution range, successful strategies usually combine powerful methods to learn the visual appearance of the semantic classes (e.g. convolutional neural networks) with strategies for spatial regularization (e.g. graphical models such as conditional random fields). In this paper, we propose a method to learn evidence in the form of semantic class likelihoods, semantic boundaries across classes and shallow-to-deep visual features, each one modeled by a multi-task convolutional neural network architecture. We combine this bottom-up information with top-down spatial regularization encoded by a conditional random field model optimizing the label space across a hierarchy of segments with constraints related to structural, spatial and data-dependent pairwise relationships between regions. Our results show that such strategy provide better regularization than a series of strong baselines reflecting state-of-the-art technologies. The proposed strategy offers a flexible and principled framework to include several sources of visual and structural information, while allowing for different degrees of spatial regularization accounting for priors about the expected output structures.

##### Abstract (translated by Google)
当在分米空间分辨率范围内接近顶部图像的语义分割时，成功策略通常结合强大的方法来学习语义类（例如卷积神经网络）的视觉外观与空间正则化策略（例如，诸如条件随机场的图形模型） ）。在本文中，我们提出了一种方法，以语义类可能性，跨类的语义边界和浅层到深层的视觉特征的形式学习证据，每种方法都由多任务卷积神经网络架构建模。我们将这种自下而上的信息与由条件随机场模型编码的自上而下的空间正则化相结合，优化跨越层级的标签空间，其中约束与区域之间的结构，空间和数据相关的成对关系相关。我们的研究结果表明，这种策略提供了比反映最先进技术的一系列强基线更好的正规化。拟议的战略提供了一个灵活和有原则的框架，包括若干视觉和结构信息来源，同时允许不同程度的空间正规化考虑先验的预期输出结构。

##### URL
[http://arxiv.org/abs/1808.07675](http://arxiv.org/abs/1808.07675)

##### PDF
[http://arxiv.org/pdf/1808.07675](http://arxiv.org/pdf/1808.07675)

