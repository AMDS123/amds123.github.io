---
layout: post
title: "Character-Level Neural Translation for Multilingual Media Monitoring in the SUMMA Project"
date: 2016-04-05 11:34:11
categories: arXiv_SD
tags: arXiv_SD Segmentation Attention Speech_Recognition Recognition
author: Guntis Barzdins, Steve Renals, Didzis Gosko
mathjax: true
---

* content
{:toc}

##### Abstract
The paper steps outside the comfort-zone of the traditional NLP tasks like automatic speech recognition (ASR) and machine translation (MT) to addresses two novel problems arising in the automated multilingual news monitoring: segmentation of the TV and radio program ASR transcripts into individual stories, and clustering of the individual stories coming from various sources and languages into storylines. Storyline clustering of stories covering the same events is an essential task for inquisitorial media monitoring. We address these two problems jointly by engaging the low-dimensional semantic representation capabilities of the sequence to sequence neural translation models. To enable joint multi-task learning for multilingual neural translation of morphologically rich languages we replace the attention mechanism with the sliding-window mechanism and operate the sequence to sequence neural translation model on the character-level rather than on the word-level. The story segmentation and storyline clustering problem is tackled by examining the low-dimensional vectors produced as a side-product of the neural translation process. The results of this paper describe a novel approach to the automatic story segmentation and storyline clustering problem.

##### Abstract (translated by Google)
为了解决在自动化多语言新闻监控中出现的两个新问题，本文在传统的NLP任务（如自动语音识别（ASR）和机器翻译（MT））的舒适区之外进行步骤：将电视和无线电节目ASR转录本分割成个体故事，以及将来自各种来源和语言的各个故事聚集成故事情节。故事情节聚集报道同一事件是审讯媒体监测的一项重要任务。我们通过使用序列的低维语义表示能力来对神经翻译模型进行排序来解决这两个问题。为了实现形态丰富的语言的多语言神经翻译的联合多任务学习，我们用滑动窗口机制替换了注意机制，并且操作该序列以在字符级而不是在字级上对神经翻译模型进行排序。通过检查作为神经翻译过程副产品产生的低维向量来解决故事分割和故事情节聚类问题。本文的结果描述了一种自动故事分割和故事情节聚类问题的新方法。

##### URL
[https://arxiv.org/abs/1604.01221](https://arxiv.org/abs/1604.01221)

##### PDF
[https://arxiv.org/pdf/1604.01221](https://arxiv.org/pdf/1604.01221)

