---
layout: post
title: "Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media"
date: 2018-01-22 20:41:50
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Classification Detection
author: Arkaitz Zubiaga
mathjax: true
---

* content
{:toc}

##### Abstract
As people increasingly use social media as a source for news consumption, its unmoderated nature enables the diffusion of hoaxes, which in turn jeopardises the credibility of information gathered from social media platforms. To mitigate this problem, we study the development of a hoax detection system that can distinguish true and false reports early on. We introduce a semi-automated approach that leverages the Wikidata knowledge base to build large-scale datasets for veracity classification, which enables us to create a dataset with 4,007 reports including over 13 million tweets, 15% of which are fake. We describe a method for learning class-specific word representations using word embeddings, which we call multiw2v. Our approach achieves competitive results with F1 scores over 72% within 10 minutes of the first tweet being posted, outperforming other baselines. Our dataset represents a realistic scenario with a real distribution of true and false stories, which we release for further use as a benchmark in future research.

##### Abstract (translated by Google)
随着人们越来越多地使用社交媒体作为新闻消费的来源，它的未被塑造的性质使得恶作剧的传播成为可能，从而破坏了从社交媒体平台收集到的信息的可信度。为了缓解这个问题，我们研究了一个可以在早期区分真假报告的恶作剧检测系统。我们引入了半自动化的方法，利用维基数据库知识库构建大规模数据集进行准确性分类，使得我们可以创建一个包含超过1300万条推特（其中15％为假）的报告数量为4,007个。我们描述了一种使用词嵌入来学习类特定词表示的方法，我们称其为multiw2v。我们的方法能够在第一条推文发布10分钟内获得超过72％的F1分数，并超越其他基线。我们的数据集代表了一个真实的假想故事情节，我们发布这个假想故事作为未来研究的基准。

##### URL
[http://arxiv.org/abs/1801.07311](http://arxiv.org/abs/1801.07311)

##### PDF
[http://arxiv.org/pdf/1801.07311](http://arxiv.org/pdf/1801.07311)

