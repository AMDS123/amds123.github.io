---
layout: post
title: "Entropy Controlled Non-Stationarity for Improving Performance of Independent Learners in Anonymous MARL Settings"
date: 2018-03-27 07:10:20
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Tanvi Verma, Pradeep Varakantham, Hoong Chuin Lau
mathjax: true
---

* content
{:toc}

##### Abstract
With the advent of sequential matching (of supply and demand) systems (uber, Lyft, Grab for taxis; ubereats, deliveroo, etc for food; amazon prime, lazada etc. for groceries) across many online and offline services, individuals (taxi drivers, delivery boys, delivery van drivers, etc.) earn more by being at the "right" place at the "right" time. We focus on learning techniques for providing guidance (on right locations to be at right times) to individuals in the presence of other "learning" individuals. Interactions between indivduals are anonymous, i.e, the outcome of an interaction (competing for demand) is independent of the identity of the agents and therefore we refer to these as Anonymous MARL settings. Existing research of relevance is on independent learning using Reinforcement Learning (RL) or on Multi-Agent Reinforcement Learning (MARL). The number of individuals in aggregation systems is extremely large and individuals have their own selfish interest (of maximising revenue). Therefore, traditional MARL approaches are either not scalable or assumptions of common objective or action coordination are not viable. In this paper, we focus on improving performance of independent reinforcement learners, specifically the popular Deep Q-Networks (DQN) and Advantage Actor Critic (A2C) approaches by exploiting anonymity. Specifically, we control non-stationarity introduced by other agents using entropy of agent density distribution. We demonstrate a significant improvement in revenue for individuals and for all agents together with our learners on a generic experimental set up for aggregation systems and a real world taxi dataset.

##### Abstract (translated by Google)
随着许多在线和离线服务的顺序匹配（供应和需求）系统（uber，Lyft，抓斗出租车;食宿，送货等用于食品;亚马逊总理，lazada等杂货）的出现，个人（出租车司机，送货员，送货车司机等）通过在“正确”时间在“正确”的位置获得更多收入。我们专注于学习技术，为其他“学习”个体提供指导（在正确的时间点在正确的位置）给个人。个体之间的交互是匿名的，即交互（争夺需求）的结果与代理的身份无关，因此我们将这些交互称为匿名MARL设置。现有的相关性研究是使用强化学习（RL）或多智能体强化学习（MARL）的独立学习。聚合系统中的个人数量非常大，个人有自己的私利（最大化收入）。因此，传统的MARL方法要么不是可扩展的，要么是共同目标或行动协调的假设是不可行的。在本文中，我们专注于通过利用匿名来提高独立强化学习者的性能，特别是深度Q网络（DQN）和优势演员评论（A2C）方法的性能。具体而言，我们使用代理密度分布的熵来控制其他代理引入的非平稳性。我们证明了个人和所有代理商与我们的学习者在聚合系统和真实世界出租车数据集的通用实验设置上的收入显着提高。

##### URL
[https://arxiv.org/abs/1803.09928](https://arxiv.org/abs/1803.09928)

##### PDF
[https://arxiv.org/pdf/1803.09928](https://arxiv.org/pdf/1803.09928)

