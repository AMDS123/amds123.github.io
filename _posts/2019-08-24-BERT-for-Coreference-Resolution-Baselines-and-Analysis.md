---
layout: post
title: "BERT for Coreference Resolution: Baselines and Analysis"
date: 2019-08-24 05:07:36
categories: arXiv_CL
tags: arXiv_CL Prediction
author: Mandar Joshi, Omer Levy, Daniel S. Weld, Luke Zettlemoyer
mathjax: true
---

* content
{:toc}

##### Abstract
We apply BERT to coreference resolution, achieving strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.09091](http://arxiv.org/abs/1908.09091)

##### PDF
[http://arxiv.org/pdf/1908.09091](http://arxiv.org/pdf/1908.09091)

