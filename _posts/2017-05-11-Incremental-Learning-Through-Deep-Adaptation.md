---
layout: post
title: "Incremental Learning Through Deep Adaptation"
date: 2017-05-11 15:04:10
categories: arXiv_CV
tags: arXiv_CV
author: Amir Rosenfeld, John K. Tsotsos
mathjax: true
---

* content
{:toc}

##### Abstract
Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20%) in the number of required parameters while performing on par with more costly fine-tuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.

##### Abstract (translated by Google)
给定一个现有的训练神经网络，往往希望能够添加新的功能，而不会妨碍已经学过的任务的执行。现有的方法要么学习次优的解决方案，要求联合训练，或者对于每个增加的任务参数的数目会大量增加，通常与原始网络一样多。我们提出了一种完全保留原始任务的性能的方法，其中所需参数的数量仅略微增加（约20％），同时执行成本更高的微调程序，这通常是参数数量的两倍。可以控制所学习的体系结构以在各种学习表示之间切换，使得单个网络能够解决来自多个不同领域的任务。我们进行广泛的实验，展示我们的方法的有效性，并探索其行为的不同方面。

##### URL
[https://arxiv.org/abs/1705.04228](https://arxiv.org/abs/1705.04228)

##### PDF
[https://arxiv.org/pdf/1705.04228](https://arxiv.org/pdf/1705.04228)

