---
layout: post
title: "Universal Language Model Fine-Tuning with Subword Tokenization for Polish"
date: 2018-10-24 07:34:45
categories: arXiv_CL
tags: arXiv_CL Transfer_Learning Language_Model
author: Piotr Czapla, Jeremy Howard, Marcin Kardas
mathjax: true
---

* content
{:toc}

##### Abstract
Universal Language Model for Fine-tuning [<a href="https://export.arxiv.org/abs/1801.06146">arXiv:1801.06146</a>] (ULMFiT) is one of the first NLP methods for efficient inductive transfer learning. Unsupervised pretraining results in improvements on many NLP tasks for English. In this paper, we describe a new method that uses subword tokenization to adapt ULMFiT to languages with high inflection. Our approach results in a new state-of-the-art for the Polish language, taking first place in Task 3 of PolEval'18. After further training, our final model outperformed the second best model by 35%. We have open-sourced our pretrained models and code.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.10222](http://arxiv.org/abs/1810.10222)

##### PDF
[http://arxiv.org/pdf/1810.10222](http://arxiv.org/pdf/1810.10222)

