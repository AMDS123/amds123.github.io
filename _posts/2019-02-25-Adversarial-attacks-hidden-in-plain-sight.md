---
layout: post
title: "Adversarial attacks hidden in plain sight"
date: 2019-02-25 14:27:05
categories: arXiv_CV
tags: arXiv_CV Adversarial CNN Classification
author: Jan Philip G&#xf6;pfert, Heiko Wersing, Barbara Hammer
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural networks have been used to achieve a string of successes during recent years, but their lack of interpretability remains a serious issue. Adversarial examples are designed to deliberately fool neural networks into making any desired incorrect classification, potentially with very high certainty. We underline the severity of the issue by presenting a technique that allows to hide such adversarial attacks in regions of high complexity, such that they are imperceptible even to an astute observer.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.09286](http://arxiv.org/abs/1902.09286)

##### PDF
[http://arxiv.org/pdf/1902.09286](http://arxiv.org/pdf/1902.09286)

