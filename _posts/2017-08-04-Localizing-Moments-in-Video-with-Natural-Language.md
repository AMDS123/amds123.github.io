---
layout: post
title: "Localizing Moments in Video with Natural Language"
date: 2017-08-04 18:57:52
categories: arXiv_CV
tags: arXiv_CV
author: Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, Bryan Russell
mathjax: true
---

* content
{:toc}

##### Abstract
We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language.

##### Abstract (translated by Google)
我们考虑从给定自然语言文字描述的视频中检索特定的时间段或时刻。设计用于以自然语言检索整个视频剪辑的方法决定了在视频中发生什么，而不是何时发生。为了解决这个问题，我们提出了通过整合本地和全球视频功能有效地定位视频中的自然语言查询的时刻上下文网络（MCN）。培训我们的MCN模型的一个主要障碍是当前的视频数据集不包括成对的本地化视频片段和引用表达式，或者是唯一标识相应时刻的文本描述。因此，我们收集了不同的视觉设置中包含超过10,000个未经编辑的个人视频的独特描述时刻（DiDeMo）数据集，这些视频带有一对本地化视频片段和引用表达。我们证明MCN胜过了几种基准方法，并相信我们最初的结果与DiDeMo的发布将激发进一步研究以自然语言对视频片段进行本地化。

##### URL
[https://arxiv.org/abs/1708.01641](https://arxiv.org/abs/1708.01641)

##### PDF
[https://arxiv.org/pdf/1708.01641](https://arxiv.org/pdf/1708.01641)

