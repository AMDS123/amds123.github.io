---
layout: post
title: "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data"
date: 2017-04-21 19:11:36
categories: arXiv_CL
tags: arXiv_CL RNN
author: Shikhar Sharma, Jing He, Kaheer Suleman, Hannes Schulz, Philip Bachman
mathjax: true
---

* content
{:toc}

##### Abstract
Natural language generation plays a critical role in spoken dialogue systems. We present a new approach to natural language generation for task-oriented dialogue using recurrent neural networks in an encoder-decoder framework. In contrast to previous work, our model uses both lexicalized and delexicalized components i.e. slot-value pairs for dialogue acts, with slots and corresponding values aligned together. This allows our model to learn from all available data including the slot-value pairing, rather than being restricted to delexicalized slots. We show that this helps our model generate more natural sentences with better grammar. We further improve our model's performance by transferring weights learnt from a pretrained sentence auto-encoder. Human evaluation of our best-performing model indicates that it generates sentences which users find more appealing.

##### Abstract (translated by Google)
自然语言生成在口语对话系统中起着至关重要的作用。我们提出了一种自然语言生成的新方法，用于在编码器 - 解码器框架中使用递归神经网络进行面向任务的对话。与以前的工作相反，我们的模型使用词法化和非灵活化的组件，即用于对话行为的时隙 - 值对，时隙和对应的值一起对齐。这使得我们的模型可以从所有可用的数据中学习，包括插槽值配对，而不是被限制在delexicalized插槽。我们表明，这有助于我们的模型生成更好的语法更自然的句子。通过传递从预训练句子自动编码器学习的权重，我们进一步提高了模型的性能。我们最好的表现模型的人类评估表明，它产生的用户觉得更有吸引力的句子。

##### URL
[https://arxiv.org/abs/1606.03632](https://arxiv.org/abs/1606.03632)

##### PDF
[https://arxiv.org/pdf/1606.03632](https://arxiv.org/pdf/1606.03632)

