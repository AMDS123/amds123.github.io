---
layout: post
title: "Joint Embedding of Words and Labels for Text Classification"
date: 2018-05-10 20:42:52
categories: arXiv_CL
tags: arXiv_CL Attention Text_Classification Embedding Classification
author: Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo Henao, Lawrence Carin
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed.

##### Abstract (translated by Google)
在学习文本序列的表示时，单词嵌入是有效的中间表示，用于捕获单词之间的语义规则。我们建议将文本分类看作标签词联合嵌入问题：每个标签都与词向量嵌入在同一个空间中。我们引入了一个关注框架来衡量文本序列和标签之间嵌入的兼容性。注意力是在标记样本的训练集上学习，以确保在给定文本序列的情况下，相关单词的权重高于不相关的单词。我们的方法保持了词嵌入的可解释性，并且除了输入文本序列之外，还具有利用替代信息来源的内置功能。对几个大型文本数据集的广泛研究结果表明，所提出的框架在准确性和速度方面均优于最先进的方法。

##### URL
[http://arxiv.org/abs/1805.04174](http://arxiv.org/abs/1805.04174)

##### PDF
[http://arxiv.org/pdf/1805.04174](http://arxiv.org/pdf/1805.04174)

