---
layout: post
title: "Multimodal Relational Tensor Network for Sentiment and Emotion Classification"
date: 2018-06-07 23:21:51
categories: arXiv_CL
tags: arXiv_CL Sentiment Knowledge Sentiment_Classification Classification Relation Recognition
author: Saurav Sahay, Shachi H Kumar, Rui Xia, Jonathan Huang, Lama Nachman
mathjax: true
---

* content
{:toc}

##### Abstract
Understanding Affect from video segments has brought researchers from the language, audio and video domains together. Most of the current multimodal research in this area deals with various techniques to fuse the modalities, and mostly treat the segments of a video independently. Motivated by the work of (Zadeh et al., 2017) and (Poria et al., 2017), we present our architecture, Relational Tensor Network, where we use the inter-modal interactions within a segment (intra-segment) and also consider the sequence of segments in a video to model the inter-segment inter-modal interactions. We also generate rich representations of text and audio modalities by leveraging richer audio and linguistic context alongwith fusing fine-grained knowledge based polarity scores from text. We present the results of our model on CMU-MOSEI dataset and show that our model outperforms many baselines and state of the art methods for sentiment classification and emotion recognition.

##### Abstract (translated by Google)
了解来自视频片段的影响将来自语言，音频和视频领域的研究人员聚集在一起。目前这一领域的多模式研究大多涉及融合模式的各种技术，主要是独立处理视频片段。受到（Zadeh等，2017）和（Poria等，2017）的工作的启发，我们提出了我们的架构 - 关系张量网络，我们使用段内（段内）的模式间相互作用，考虑视频中段的顺序来模拟段间模式间的相互作用。我们还通过利用更丰富的音频和语言环境以及融合来自文本的基于细粒度知识的极性分数，生成丰富的文本和音频模态表示。我们在CMU-MOSEI数据集上展示了我们的模型的结果，并且表明我们的模型胜过了情感分类和情感识别的许多基线和最先进的方法。

##### URL
[http://arxiv.org/abs/1806.02923](http://arxiv.org/abs/1806.02923)

##### PDF
[http://arxiv.org/pdf/1806.02923](http://arxiv.org/pdf/1806.02923)

