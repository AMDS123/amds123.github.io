---
layout: post
title: "Explainable Deterministic MDPs"
date: 2018-06-09 15:44:54
categories: arXiv_AI
tags: arXiv_AI
author: Josh Bertram, Peng Wei
mathjax: true
---

* content
{:toc}

##### Abstract
We present a method for a certain class of Markov Decision Processes (MDPs) that can relate the optimal policy back to one or more reward sources in the environment. For a given initial state, without fully computing the value function, q-value function, or the optimal policy the algorithm can determine which rewards will and will not be collected, whether a given reward will be collected only once or continuously, and which local maximum within the value function the initial state will ultimately lead to. We demonstrate that the method can be used to map the state space to identify regions that are dominated by one reward source and can fully analyze the state space to explain all actions. We provide a mathematical framework to show how all of this is possible without first computing the optimal policy or value function.

##### Abstract (translated by Google)
我们提出了某种类型的马尔可夫决策过程（MDP）的一种方法，它可以将最优策略与环境中的一个或多个回报源联系起来。对于给定的初始状态，如果没有完全计算价值函数，q值函数或最优策略，则算法可以确定哪些奖励将被收集，哪些奖励将被收集，给定的奖励是否仅被收集一次或连续收集，以及哪些本地在初始状态最终导致的值函数内最大。我们证明，该方法可用于映射状态空间以识别由一个回报源主宰的区域，并且可以充分分析状态空间来解释所有操作。我们提供了一个数学框架来展示如何在没有首先计算最优策略或价值函数的情况下实现所有这些。

##### URL
[http://arxiv.org/abs/1806.03492](http://arxiv.org/abs/1806.03492)

##### PDF
[http://arxiv.org/pdf/1806.03492](http://arxiv.org/pdf/1806.03492)

