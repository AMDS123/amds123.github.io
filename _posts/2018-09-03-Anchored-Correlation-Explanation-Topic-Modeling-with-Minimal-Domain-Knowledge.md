---
layout: post
title: "Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge"
date: 2018-09-03 15:23:40
categories: arXiv_CL
tags: arXiv_CL Knowledge Relation
author: Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg
mathjax: true
---

* content
{:toc}

##### Abstract
While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in topic modeling, they often require detailed assumptions and careful specification of hyperparameters. Such model complexity issues only compound when trying to generalize generative models to incorporate human input. We introduce Correlation Explanation (CorEx), an alternative approach to topic modeling that does not assume an underlying generative model, and instead learns maximally informative topics through an information-theoretic framework. This framework naturally generalizes to hierarchical and semi-supervised extensions with no additional modeling assumptions. In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and representation to be promoted with minimal human intervention. Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semi-supervised variants of LDA.

##### Abstract (translated by Google)
虽然Latent Dirichlet Allocation（LDA）等生成模型在主题建模方面已经证明富有成效，但它们通常需要详细的假设和超参数的仔细规范。这种模型复杂性问题只有在试图概括生成模型以结合人类输入时才会复杂化。我们引入了相关解释（CorEx），这是一种不采用基础生成模型的主题建模的替代方法，而是通过信息理论框架学习最大限度提供信息的主题。该框架自然地推广到分层和半监督扩展，没有额外的建模假设。特别是，单词级领域知识可以通过锚词灵活地结合到CorEx中，从而允许以最少的人为干预来促进主题可分性和表示。在各种数据集，指标和实验中，我们证明CorEx产生的质量与LDA的无监督和半监督变体产生的质量相当。

##### URL
[http://arxiv.org/abs/1611.10277](http://arxiv.org/abs/1611.10277)

##### PDF
[http://arxiv.org/pdf/1611.10277](http://arxiv.org/pdf/1611.10277)

