---
layout: post
title: "Multi-Fidelity Reinforcement Learning with Gaussian Processes"
date: 2017-12-18 16:05:50
categories: arXiv_RO
tags: arXiv_RO Reinforcement_Learning
author: Varun Suryan, Nahush Gondhalekar, Pratap Tokekar
mathjax: true
---

* content
{:toc}

##### Abstract
This paper studies the problem of Reinforcement Learning (RL) using as few real-world samples as possible. A naive application of RL algorithms can be inefficient in large and continuous state spaces. We present two versions of Multi-Fidelity Reinforcement Learning (MFRL) algorithm that leverage Gaussian Processes (GPs) to learn the optimal policy in a real-world environment. In MFRL framework, an agent uses multiple simulators of the real environment to perform actions. With increasing fidelity in a simulator chain, the number of samples used in successively higher simulators can be reduced. By incorporating GPs in MFRL framework, further reduction in the number of learning samples can be achieved as we move up the simulator chain. We examine the performance of our algorithms with the help of real-world experiments for navigation with a ground robot.

##### Abstract (translated by Google)
本文使用尽可能少的实际样本来研究强化学习（RL）的问题。在大型连续状态空间中，RL算法的天真应用可能是低效的。我们提出了两个版本的多保真增强学习（MFRL）算法，利用高斯过程（GP）来学习现实世界环境中的最优策略。在MFRL框架中，代理使用真实环境的多个模拟器来执行操作。随着仿真器链中保真度的增加，可以减少在较高仿真器中使用的样本数量。通过将GP纳入MFRL框架，随着我们向模拟器链上移，可以进一步减少学习样本的数量。我们用真实世界的地面机器人导航实验来检验算法的性能。

##### URL
[http://arxiv.org/abs/1712.06489](http://arxiv.org/abs/1712.06489)

##### PDF
[http://arxiv.org/pdf/1712.06489](http://arxiv.org/pdf/1712.06489)

