---
layout: post
title: "Explicablility as Minimizing Distance from Expected Behavior"
date: 2018-07-26 04:49:09
categories: arXiv_AI
tags: arXiv_AI
author: Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam Vadlamudi, Yu Zhang, Subbarao Kambhampati
mathjax: true
---

* content
{:toc}

##### Abstract
In order to have effective human-AI collaboration, it is necessary to address how the AI agent's behavior is being perceived by the humans-in-the-loop. When the agent's task plans are generated without such considerations, they may often demonstrate inexplicable behavior from the human's point of view. This problem may arise due to the human's partial or inaccurate understanding of the agent's planning model. This may have serious implications from increased cognitive load to more serious concerns of safety around a physical agent. In this paper, we address this issue by modeling plan explicability as a function of the distance between a plan that agent makes and the plan that human expects it to make. We learn a regression model for mapping the plan distances to explicability scores of plans and develop an anytime search algorithm that can use this model as a heuristic to come up with progressively explicable plans. We evaluate the effectiveness of our approach in a simulated autonomous car domain and a physical robot domain.

##### Abstract (translated by Google)
为了实现有效的人工智能协作，有必要解决人工在循环中如何感知AI代理的行为。当代理商的任务计划在没有这些考虑的情况下生成时，它们可能经常从人的角度展示出莫名其妙的行为。由于人员对代理人的计划模型的部分或不准确的理解，可能会出现此问题。这可能会从认知负荷的增加到对物理因素周围安全性的更严重关注产生严重影响。在本文中，我们通过将计划可解释性建模为代理人制定的计划与人们期望计划制定的计划之间的距离来解决这个问题。我们学习了一种回归模型，用于将计划距离映射到计划的可解释性得分，并开发一种随时可用的搜索算法，该算法可以使用该模型作为启发式算法来提出逐步解释的计划。我们评估了我们的方法在模拟自动驾驶汽车领域和物理机器人领域的有效性。

##### URL
[http://arxiv.org/abs/1611.05497](http://arxiv.org/abs/1611.05497)

##### PDF
[http://arxiv.org/pdf/1611.05497](http://arxiv.org/pdf/1611.05497)

