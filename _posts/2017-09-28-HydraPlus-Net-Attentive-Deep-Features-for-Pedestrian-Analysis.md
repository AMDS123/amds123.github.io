---
layout: post
title: "HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis"
date: 2017-09-28 13:06:55
categories: arXiv_CV
tags: arXiv_CV Re-identification Attention Person_Re-identification CNN Recognition
author: Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie Yan, Xiaogang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attention-based deep neural network, named as HydraPlus-Net (HP-net), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person re-identification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-the-art methods on various datasets.

##### Abstract (translated by Google)
行人分析在智能视频监控中起着至关重要的作用，是以安全为中心的计算机视觉系统的关键组成部分。尽管卷积神经网络在图像学习中具有显着的特征，但是对于细粒度任务的行人综合特征的学习仍然是一个悬而未决的问题。在这项研究中，我们提出了一种新的基于注意力的深度神经网络，命名为HydraPlus-Net（HP-net），它将多层关注映射多向地馈送到不同的要素层。从提出的HP网络中学习到的细致深入的特点带来了独特的优势：（1）该模型能够从低层次到语义层面捕捉到多重关注;（2）探索细致特征的多尺度选择性，丰富行人图像的最终特征表示。我们证明了所提出的HP-net的行人分析在两项任务中的有效性和一般性，即行人属性识别和人员重新识别。已经提供了强有力的实验结果来证明HP-net在各种数据集上的性能优于最先进的方法。

##### URL
[https://arxiv.org/abs/1709.09930](https://arxiv.org/abs/1709.09930)

##### PDF
[https://arxiv.org/pdf/1709.09930](https://arxiv.org/pdf/1709.09930)

