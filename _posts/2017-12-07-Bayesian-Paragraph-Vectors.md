---
layout: post
title: "Bayesian Paragraph Vectors"
date: 2017-12-07 20:37:31
categories: arXiv_CL
tags: arXiv_CL Sentiment Embedding Inference Language_Model Detection Relation
author: Geng Ji, Robert Bamler, Erik B. Sudderth, Stephan Mandt
mathjax: true
---

* content
{:toc}

##### Abstract
Word2vec (Mikolov et al., 2013) has proven to be successful in natural language processing by capturing the semantic relationships between different words. Built on top of single-word embeddings, paragraph vectors (Le and Mikolov, 2014) find fixed-length representations for pieces of text with arbitrary lengths, such as documents, paragraphs, and sentences. In this work, we propose a novel interpretation for neural-network-based paragraph vectors by developing an unsupervised generative model whose maximum likelihood solution corresponds to traditional paragraph vectors. This probabilistic formulation allows us to go beyond point estimates of parameters and to perform Bayesian posterior inference. We find that the entropy of paragraph vectors decreases with the length of documents, and that information about posterior uncertainty improves performance in supervised learning tasks such as sentiment analysis and paraphrase detection.

##### Abstract (translated by Google)
Word2vec（Mikolov et al。，2013）已经证明在自然语言处理方面是成功的，通过捕捉不同单词之间的语义关系。建立在单词嵌入的基础上，段落向量（Le和Mikolov，2014）为任意长度的文本（如文档，段落和句子）找到固定长度的表示。在这项工作中，我们提出了一种基于神经网络的段落矢量的新颖解释，通过开发一个无监督的生成模型，其最大似然解对应于传统的段落矢量。这个概率表达式允许我们超越参数的点估计，并执行贝叶斯后验推理。我们发现，段矢量的熵随着文档长度而减少，关于后验不确定性的信息提高了监督学习任务（如情感分析和释义检测）的性能。

##### URL
[http://arxiv.org/abs/1711.03946](http://arxiv.org/abs/1711.03946)

##### PDF
[http://arxiv.org/pdf/1711.03946](http://arxiv.org/pdf/1711.03946)

