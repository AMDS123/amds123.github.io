---
layout: post
title: "Transformationally Identical and Invariant Convolutional Neural Networks by Combining Symmetric Operations or Input Vectors"
date: 2018-07-30 03:14:50
categories: arXiv_CV
tags: arXiv_CV CNN
author: ShihChung B. Lo, Matthew T. Freedman, Seong K. Mun
mathjax: true
---

* content
{:toc}

##### Abstract
Transformationally invariant processors constructed by transformed input vectors or operators have been suggested and applied to many applications. In this study, transformationally identical processing based on combining results of all sub-processes with corresponding transformations either at the final processing step or at the beginning step were found to be equivalent through a special algebraical operation property. This technique can be applied to most convolutional neural network (CNN) systems. Specifically, a transformationally identical CNN system can be constructed by running internally symmetric operations in parallel with the same transformation family followed by a flatten layer with weights sharing among their corresponding transformation elements. Such a CNN can output the same result with any transformation version of the original input vector. Interestingly, we found that this type of transformationally identical CNN system by combining symmetric operations at the flatten layer is mathematically equivalent to an ordinary CNN but combining all transformation versions of the input vector at the input layer. Since the former is computationally demanding, its equivalent with greatly simplified implementation is suggested

##### Abstract (translated by Google)
已经提出了由变换的输入向量或运算符构造的变换不变处理器并将其应用于许多应用。在这项研究中，基于将所有子过程的结果与最终处理步骤或开始步骤中的相应变换相结合的变换相同处理被发现通过特殊的代数运算特性是等价的。该技术可以应用于大多数卷积神经网络（CNN）系统。具体地，可以通过与相同的变换族并行运行内部对称运算来构造变换相同的CNN系统，随后是具有在其相应变换元素之间共享权重的展平层。这样的CNN可以用原始输入矢量的任何变换版本输出相同的结果。有趣的是，我们发现通过在展平层上组合对称操作，这种类型的变换相同的CNN系统在数学上等同于普通CNN，但在输入层组合输入矢量的所有变换版本。由于前者在计算上要求很高，因此建议使用相当于大大简化的实现

##### URL
[http://arxiv.org/abs/1807.11156](http://arxiv.org/abs/1807.11156)

##### PDF
[http://arxiv.org/pdf/1807.11156](http://arxiv.org/pdf/1807.11156)

