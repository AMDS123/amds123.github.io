---
layout: post
title: "On Evaluating and Comparing Conversational Agents"
date: 2018-01-11 03:30:00
categories: arXiv_AI
tags: arXiv_AI Knowledge
author: Anu Venkatesh, Chandra Khatri, Ashwin Ram, Fenfei Guo, Raefer Gabriel, Ashish Nagar, Rohit Prasad, Ming Cheng, Behnam Hedayatnia, Angeliki Metallinou, Rahul Goel, Shaohua Yang, Anirudh Raju
mathjax: true
---

* content
{:toc}

##### Abstract
Conversational agents are exploding in popularity. However, much work remains in the area of non goal-oriented conversations, despite significant growth in research interest over recent years. To advance the state of the art in conversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar university competition where sixteen selected university teams built conversational agents to deliver the best social conversational experience. Alexa Prize provided the academic community with the unique opportunity to perform research with a live system used by millions of users. The subjectivity associated with evaluating conversations is key element underlying the challenge of building non-goal oriented dialogue systems. In this paper, we propose a comprehensive evaluation strategy with multiple metrics designed to reduce subjectivity by selecting metrics which correlate well with human judgement. The proposed metrics provide granular analysis of the conversational agents, which is not captured in human ratings. We show that these metrics can be used as a reasonable proxy for human judgment. We provide a mechanism to unify the metrics for selecting the top performing agents, which has also been applied throughout the Alexa Prize competition. To our knowledge, to date it is the largest setting for evaluating agents with millions of conversations and hundreds of thousands of ratings from users. We believe that this work is a step towards an automatic evaluation process for conversational AIs.

##### Abstract (translated by Google)
会话代理正在迅速普及。然而，尽管近年来研究兴趣的显着增长，非目标导向对话领域还有很多工作要做。为了提高对话式人工智能的艺术水平，亚马逊推出了一项价值250万美元的大学竞赛，其中16个选定的大学队伍建立了会话代理人，以提供最好的社交会话体验。 Alexa Prize为学术界提供了一个独特的机会，使用数百万用户使用的现场系统进行研究。与评估对话相关的主观性是构建非目标导向对话系统挑战的关键因素。在本文中，我们提出了一个综合评估策略，设计了多个指标，通过选择与人为判断相关的指标来降低主观性。所提出的指标提供了会话代理的细粒度分析，这在人员评分中不被捕获。我们表明，这些指标可以用作人为判断的合理代理。我们提供了一个机制来统一选择表现最佳的代理商的指标，这在整个Alexa Prize竞争中也得到了应用。据我们所知，到目前为止，这是评估代理商数百万次的谈话和数以万计的用户评级的最大设置。我们相信这项工作是对会话认证机构自动评估过程的一个步骤。

##### URL
[http://arxiv.org/abs/1801.03625](http://arxiv.org/abs/1801.03625)

##### PDF
[http://arxiv.org/pdf/1801.03625](http://arxiv.org/pdf/1801.03625)

