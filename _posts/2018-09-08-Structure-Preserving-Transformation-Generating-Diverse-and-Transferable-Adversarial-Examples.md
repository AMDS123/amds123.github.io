---
layout: post
title: "Structure-Preserving Transformation: Generating Diverse and Transferable Adversarial Examples"
date: 2018-09-08 10:26:50
categories: arXiv_AI
tags: arXiv_AI Adversarial Image_Classification Classification
author: Dan Peng, Zizhan Zheng, Xiaofeng Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Adversarial examples are perturbed inputs designed to fool machine learning models. Most recent works on adversarial examples for image classification focus on directly modifying pixels with minor perturbations. A common requirement in all these works is that the malicious perturbations should be small enough (measured by an $L_p$ norm for some $p$) so that they are imperceptible to humans. However, small perturbations can be unnecessarily restrictive and limit the diversity of adversarial examples generated. Further, an $L_p$ norm based distance metric ignores important structure patterns hidden in images that are important to human perception. Consequently, even the minor perturbation introduced in recent works often makes the adversarial examples less natural to humans. More importantly, they often do not transfer well and are therefore less effective when attacking black-box models especially for those protected by a defense mechanism. In this paper, we propose a structure-preserving transformation (SPT) for generating natural and diverse adversarial examples with extremely high transferability. The key idea of our approach is to allow perceptible deviation in adversarial examples while keeping structure patterns that are central to a human classifier. Empirical results on the MNIST and the fashion-MNIST datasets show that adversarial examples generated by our approach can easily bypass strong adversarial training. Further, they transfer well to other target models with no loss or little loss of successful attack rate.

##### Abstract (translated by Google)
对抗性示例是旨在欺骗机器学习模型的扰动输入。最近关于图像分类的对抗性示例的工作集中于直接修改具有微小扰动的像素。所有这些工作中的一个共同要求是，恶意扰动应该足够小（以$ L_p $范围衡量一些$ p $），以便人类察觉不到。然而，小的扰动可能会产生不必要的限制，并限制所产生的对抗性示例的多样性。此外，基于$ L_p $规范的距离度量忽略了隐藏在对人类感知重要的图像中的重要结构模式。因此，即使是最近作品中引入的微小扰动，也常常使对抗性的例子对人类来说不那么自然。更重要的是，它们通常不能很好地转移，因此在攻击黑盒模型时效率较低，特别是对于那些受到防御机制保护的模型。在本文中，我们提出了一种结构保持变换（SPT），用于生成具有极高可转移性的自然和多样的对抗性示例。我们的方法的关键思想是允许对抗性实例中的可察觉偏差，同时保持对人类分类器至关重要的结构模式。 MNIST和时尚MNIST数据集的实证结果表明，我们的方法产生的对抗性例子很容易绕过强大的对抗性训练。此外，它们很好地转移到其他目标模型，没有损失或成功攻击率几乎没有损失。

##### URL
[http://arxiv.org/abs/1809.02786](http://arxiv.org/abs/1809.02786)

##### PDF
[http://arxiv.org/pdf/1809.02786](http://arxiv.org/pdf/1809.02786)

