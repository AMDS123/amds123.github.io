---
layout: post
title: "Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization through Self-Concordance"
date: 2019-02-08 12:18:05
categories: arXiv_AI
tags: arXiv_AI Regularization
author: Ulysse Marteau-Ferey (DI-ENS), Dmitrii Ostrovskii (DI-ENS), Francis Bach (DI-ENS), Alessandro Rudi (PSL)
mathjax: true
---

* content
{:toc}

##### Abstract
We consider learning methods based on the regularization of a convex empirical risk by a squared Hilbertian norm, a setting that includes linear predictors and non-linear predictors through positive-definite kernels. In order to go beyond the generic analysis leading to convergence rates of the excess risk as $O(1/\sqrt{n})$ from $n$ observations, we assume that the individual losses are self-concordant, that is, their third-order derivatives are bounded by their second-order derivatives. This setting includes least-squares, as well as all generalized linear models such as logistic and softmax regression. For this class of losses, we provide a bias-variance decomposition and show that the assumptions commonly made in least-squares regression, such as the source and capacity conditions, can be adapted to obtain fast non-asymptotic rates of convergence by improving the bias terms, the variance terms or both.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.03046](http://arxiv.org/abs/1902.03046)

##### PDF
[http://arxiv.org/pdf/1902.03046](http://arxiv.org/pdf/1902.03046)

