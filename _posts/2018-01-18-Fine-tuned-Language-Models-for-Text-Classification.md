---
layout: post
title: "Fine-tuned Language Models for Text Classification"
date: 2018-01-18 17:54:52
categories: arXiv_CL
tags: arXiv_CL Text_Classification Transfer_Learning Classification Language_Model
author: Jeremy Howard, Sebastian Ruder
mathjax: true
---

* content
{:toc}

##### Abstract
Transfer learning has revolutionized computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Fine-tuned Language Models (FitLaM), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a state-of-the-art language model. Our method significantly outperforms the state-of-the-art on five text classification tasks, reducing the error by 18-24% on the majority of datasets. We open-source our pretrained models and code to enable adoption by the community.

##### Abstract (translated by Google)
转移学习已经彻底改变了计算机视觉，但现有的NLP方法仍然需要从头开始进行特定于任务的修改和培训。我们提出了精细调整语言模型（FitLaM），这是一种有效的转换学习方法，可以应用于NLP中的任何任务，并且引入一些关键技术来微调一个最新的语言模型。我们的方法在五个文本分类任务上明显优于现有技术，可将大多数数据集的误差减少18-24％。我们开放了我们的预训练模型和代码，以便社区采用。

##### URL
[http://arxiv.org/abs/1801.06146](http://arxiv.org/abs/1801.06146)

##### PDF
[http://arxiv.org/pdf/1801.06146](http://arxiv.org/pdf/1801.06146)

