---
layout: post
title: "MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation"
date: 2017-11-24 15:20:22
categories: arXiv_CV
tags: arXiv_CV Face
author: Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling
mathjax: true
---

* content
{:toc}

##### Abstract
Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze that contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves the state of the art by 22% percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation.

##### Abstract (translated by Google)
据信基于学习的方法适用于无约束的注视估计，即，来自单眼RGB相机的注视估计，而无需关于用户，环境或相机的假设。然而，当前的凝视数据集是在实验室条件下收集的，并且方法不在多个数据集中进行评估。我们的工作为解决这些限制做出了三个贡献。首先，我们介绍了在日常笔记本电脑使用过程中，在几个月内收集了来自15名用户的213,659张全脸图像和相应的地面真实注视位置的MPIIGaze。经验抽样方法确保持续的注视和头部姿势以及眼睛外观和照明的实际变化。为了便于交叉数据集评估，37,667个图像被手动标注了眼角，嘴角和瞳孔中心。其次，我们对三种当前数据集（包括MPIIGaze）进行了最先进的凝视估计方法的广泛评估。我们研究关键挑战，包括目标注视范围，照明条件和面部外观变化。我们表明，图像分辨率和双眼的使用影响凝视估计性能，而头部姿势和瞳孔中心信息较少信息。最后，我们提出GazeNet，这是第一个基于深度外观的注视估计方法。 GazeNet将最具挑战性的交叉数据集评估提高了22％的水平（从13.9度的平均误差到10.8度）。

##### URL
[https://arxiv.org/abs/1711.09017](https://arxiv.org/abs/1711.09017)

##### PDF
[https://arxiv.org/pdf/1711.09017](https://arxiv.org/pdf/1711.09017)

