---
layout: post
title: "Iterative Alternating Neural Attention for Machine Reading"
date: 2016-11-09 18:11:09
categories: arXiv_CL
tags: arXiv_CL Attention
author: Alessandro Sordoni, Philip Bachman, Adam Trischler, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.

##### Abstract (translated by Google)
我们提出了一种新颖的神经关注架构来解决机器理解任务，比如回答关于文档的填充式查询。与以前的模型不同，我们不会将查询合并到一个单独的向量中，而是部署一个迭代交替注意机制，允许对查询和文档进行精细的探索。我们的模型在标准的机器理解基准（比如CNN新闻文章和儿童书本测试（CBT）数据集）中胜过了最先进的基准。

##### URL
[https://arxiv.org/abs/1606.02245](https://arxiv.org/abs/1606.02245)

##### PDF
[https://arxiv.org/pdf/1606.02245](https://arxiv.org/pdf/1606.02245)

