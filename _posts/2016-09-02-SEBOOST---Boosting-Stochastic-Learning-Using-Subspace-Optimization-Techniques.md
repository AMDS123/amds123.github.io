---
layout: post
title: "SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques"
date: 2016-09-02 14:48:16
categories: arXiv_CV
tags: arXiv_CV Optimization Deep_Learning
author: Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky
mathjax: true
---

* content
{:toc}

##### Abstract
We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.

##### Abstract (translated by Google)
我们提出了SEBOOST，这是一种提高现有随机优化方法性能的技术。 SEBOOST在由最后的步骤和下降方向跨越的子空间中应用次级优化过程。该方法受SESOP优化方法的启发，适用于大规模问题，并已适应随机学习框架。它可以应用在任何现有的优化方法之上，而不需要调整内部算法。我们表明，该方法能够提高不同算法的性能，并使它们对超参数的变化更有力。由于SEBOOST的推进步骤被应用在大的下降步骤之间，附加的子空间优化几乎不会增加整体的计算负担。我们引入两个超参数来控制基线方法和二次优化过程之间的平衡。该方法进行了几项深度学习任务的评估，展示了令人鼓舞的成果。

##### URL
[https://arxiv.org/abs/1609.00629](https://arxiv.org/abs/1609.00629)

##### PDF
[https://arxiv.org/pdf/1609.00629](https://arxiv.org/pdf/1609.00629)

