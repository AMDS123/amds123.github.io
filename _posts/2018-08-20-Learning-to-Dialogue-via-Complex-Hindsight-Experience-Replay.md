---
layout: post
title: "Learning to Dialogue via Complex Hindsight Experience Replay"
date: 2018-08-20 15:04:30
categories: arXiv_AI
tags: arXiv_AI Sparse Reinforcement_Learning
author: Keting Lu, Shiqi Zhang, Xiaoping Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Reinforcement learning methods have been used for learning dialogue policies from the experience of conversations. However, learning an effective dialogue policy frequently requires prohibitively many conversations. This is partly because of the sparse rewards in dialogues, and the relatively small number of successful dialogues in early learning phase. Hindsight experience replay (HER) enables an agent to learn from failure, but the vanilla HER is inapplicable to dialogue domains due to dialogue goals being implicit (c.f., explicit goals in manipulation tasks). In this work, we develop two complex HER methods providing different trade-offs between complexity and performance. Experiments were conducted using a realistic user simulator. Results suggest that our HER methods perform better than standard and prioritized experience replay methods (as applied to deep Q-networks) in learning rate, and that our two complex HER methods can be combined to produce the best performance.

##### Abstract (translated by Google)
强化学习方法已被用于从对话经验中学习对话政策。然而，学习有效的对话政策经常需要过多的对话。这部分是因为对话中的奖励稀少，以及早期学习阶段成功对话的数量相对较少。事后体验重播（HER）使代理人能够从失败中学习，但由于对话目标是隐含的（参见操纵任务中的明确目标），因此香草HER不适用于对话领域。在这项工作中，我们开发了两种复杂的HER方法，在复杂性和性能之间提供了不同的权衡。使用真实的用户模拟器进行实验。结果表明，我们的HER方法在学习率方面比标准和优先经验重放方法（应用于深度Q网络）表现更好，并且我们的两种复杂HER方法可以组合以产生最佳性能。

##### URL
[http://arxiv.org/abs/1808.06497](http://arxiv.org/abs/1808.06497)

##### PDF
[http://arxiv.org/pdf/1808.06497](http://arxiv.org/pdf/1808.06497)

