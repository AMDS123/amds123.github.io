---
layout: post
title: "Understanding Convolutional Neural Networks with A Mathematical Model"
date: 2016-11-02 21:55:26
categories: arXiv_CV
tags: arXiv_CV CNN Relation
author: C.-C. Jay Kuo
mathjax: true
---

* content
{:toc}

##### Abstract
This work attempts to address two fundamental questions about the structure of the convolutional neural networks (CNN): 1) why a non-linear activation function is essential at the filter output of every convolutional layer? 2) what is the advantage of the two-layer cascade system over the one-layer system? A mathematical model called the "REctified-COrrelations on a Sphere" (RECOS) is proposed to answer these two questions. After the CNN training process, the converged filter weights define a set of anchor vectors in the RECOS model. Anchor vectors represent the frequently occurring patterns (or the spectral components). The necessity of rectification is explained using the RECOS model. Then, the behavior of a two-layer RECOS system is analyzed and compared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are used to illustrate discussion points. Finally, the RECOS model is generalized to a multi-layer system with the AlexNet as an example. Keywords: Convolutional Neural Network (CNN), Nonlinear Activation, RECOS Model, Rectified Linear Unit (ReLU), MNIST Dataset.

##### Abstract (translated by Google)
这项工作试图解决关于卷积神经网络（CNN）结构的两个基本问题：1）为什么非线性激活函数在每个卷积层的滤波器输出中是必不可少的？ 2）双层系统在单层系统上的优势是什么？提出了一个称为“球体上的相互作用 - 相关性”（RECOS）的数学模型来回答这两个问题。在CNN训练过程之后，收敛的滤波器权重在RECOS模型中定义一组锚向量。锚矢量代表频繁发生的模式（或频谱分量）。 RECOS模型解释了纠正的必要性。然后，分析一个双层RECOS系统的行为，并与其单层对应物进行比较。 LeNet-5和MNIST数据集用于说明讨论要点。最后，将RECOS模型推广到以AlexNet为例的多层系统。关键词：卷积神经网络（CNN），非线性激励，RECOS模型，整流线性单元（ReLU），MNIST数据集。

##### URL
[https://arxiv.org/abs/1609.04112](https://arxiv.org/abs/1609.04112)

##### PDF
[https://arxiv.org/pdf/1609.04112](https://arxiv.org/pdf/1609.04112)

