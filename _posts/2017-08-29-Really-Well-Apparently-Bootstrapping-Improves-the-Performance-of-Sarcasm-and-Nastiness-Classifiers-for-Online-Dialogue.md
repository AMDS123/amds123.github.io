---
layout: post
title: "Really? Well. Apparently Bootstrapping Improves the Performance of Sarcasm and Nastiness Classifiers for Online Dialogue"
date: 2017-08-29 02:05:14
categories: arXiv_CL
tags: arXiv_CL Face
author: Stephanie Lukin, Marilyn Walker
mathjax: true
---

* content
{:toc}

##### Abstract
More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for NLP. This paper tests a bootstrapping method, originally proposed in a monologic domain, to train classifiers to identify two different types of subjective language in dialogue: sarcasm and nastiness. We explore two methods of developing linguistic indicators to be used in a first level classifier aimed at maximizing precision at the expense of recall. The best performing classifier for the first phase achieves 54% precision and 38% recall for sarcastic utterances. We then use general syntactic patterns from previous work to create more general sarcasm indicators, improving precision to 62% and recall to 52%. To further test the generality of the method, we then apply it to bootstrapping a classifier for nastiness dialogic acts. Our first phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we bootstrap over the first level with generalized syntactic patterns.

##### Abstract (translated by Google)
网络上越来越多的信息是对话式的，从Facebook的新闻推送到论坛对话，评论新闻文章的线索。与传统的单一的自然语言处理资源（例如新闻）相比，社交媒体中的高度社交对话频繁发生，使其成为NLP的一个具有挑战性的环境。本文测试了最初在单调领域提出的引导方法，以训练分类器来识别对话中的两种不同类型的主观语言：讽刺和唠叨。我们探索了两种开发语言指标的方法，用于一级分类器，旨在以召回为代价来最大化精度。第一阶段表现最好的分类器，对于讽刺话语，精度达到54％，回忆达到38％。然后，我们使用以前工作的一般句法模式来创建更一般的讽刺指标，将精确度提高到62％，回忆到52％。为了进一步检验该方法的一般性，我们将其应用于自动对话行为的分类器中。我们的第一个阶段，使用众包的讨厌的指标，达到58％的精确度和49％的召回率，当我们用一般化的句法模式引导到第一级时，精度提高到75％，回忆率提高到62％。

##### URL
[https://arxiv.org/abs/1708.08572](https://arxiv.org/abs/1708.08572)

##### PDF
[https://arxiv.org/pdf/1708.08572](https://arxiv.org/pdf/1708.08572)

