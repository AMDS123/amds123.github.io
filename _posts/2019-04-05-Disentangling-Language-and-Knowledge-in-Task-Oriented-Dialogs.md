---
layout: post
title: "Disentangling Language and Knowledge in Task-Oriented Dialogs"
date: 2019-04-05 17:24:32
categories: arXiv_CL
tags: arXiv_CL Knowledge Language_Model
author: Dinesh Raghu, Nikhil Gupta, Mausam
mathjax: true
---

* content
{:toc}

##### Abstract
The Knowledge Base (KB) used for real-world applications, such as booking a movie or restaurant reservation, keeps changing over time. End-to-end neural networks trained for these task-oriented dialogs are expected to be immune to any changes in the KB. However, existing approaches breakdown when asked to handle such changes. We propose an encoder-decoder architecture (BoSsNet) with a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled learning of the response's language model and its knowledge incorporation. Consequently, the KB can be modified with new knowledge without a drop in interpretability. We find that BoSsNet outperforms state-of-the-art models, with considerable improvements (&gt; 10\%) on bAbI OOV test sets and other human-human datasets. We also systematically modify existing datasets to measure disentanglement and show BoSsNet to be robust to KB modifications.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1805.01216](http://arxiv.org/abs/1805.01216)

##### PDF
[http://arxiv.org/pdf/1805.01216](http://arxiv.org/pdf/1805.01216)

