---
layout: post
title: "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"
date: 2019-07-29 14:42:30
categories: arXiv_CL
tags: arXiv_CL Summarization
author: Sascha Rothe, Shashi Narayan, Aliaksei Severyn
mathjax: true
---

* content
{:toc}

##### Abstract
Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. Warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper, we present an extensive empirical study on the utility of initializing large Transformer-based sequence-to-sequence models with the publicly available pre-trained BERT and GPT-2 checkpoints for sequence generation. We have run over 300 experiments spending thousands of TPU hours to find the recipe that works best and demonstrate that it results in new state-of-the-art results on Machine Translation, Summarization, Sentence Splitting and Sentence Fusion.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.12461](http://arxiv.org/abs/1907.12461)

##### PDF
[http://arxiv.org/pdf/1907.12461](http://arxiv.org/pdf/1907.12461)

