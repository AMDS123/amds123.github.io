---
layout: post
title: "Relation extraction from clinical texts using domain invariant convolutional neural network"
date: 2016-06-30 07:10:07
categories: arXiv_CL
tags: arXiv_CL Knowledge Relation_Extraction CNN Prediction Relation
author: Sunil Kumar Sahu, Ashish Anand, Krishnadev Oruganty, Mahanandeeshwar Gattu
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years extracting relevant information from biomedical and clinical texts such as research articles, discharge summaries, or electronic health records have been a subject of many research efforts and shared challenges. Relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts. Existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector. These features are then fed to classifier for the prediction of the correct class. It turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality. In this work we focus on extracting relations from clinical discharge summaries. Our main objective is to exploit the power of convolution neural network (CNN) to learn features automatically and thus reduce the dependency on manual feature engineering. We evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset. Our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert's knowledge on defining quality features.

##### Abstract (translated by Google)
近年来，从生物医学和临床文献中提取相关信息，如研究文章，出院总结或电子健康记录已成为许多研究工作的主题和共同的挑战。关系抽取是对给定文本中实体之间的语义关系进行检测和分类的过程。现有的生物医学领域任务模型使用手动工程特征或内核方法来创建特征向量。这些特征然后被馈送到分类器以预测正确的类别。事实证明，这些方法的结果高度依赖于用户设计的特征的质量，也受到维度的诅咒。在这项工作中，我们专注于从临床出院摘要中提取关系。我们的主要目标是利用卷积神经网络（CNN）的功能自动学习特征，从而减少对手动特征工程的依赖。我们评估提出的模型在i2b2-2010临床关系提取挑战数据集的性能。我们的研究结果表明，卷积神经网络可以成为临床文本中关系提取的良好模型，而不依赖于专家对定义质量特征的知识。

##### URL
[https://arxiv.org/abs/1606.09370](https://arxiv.org/abs/1606.09370)

##### PDF
[https://arxiv.org/pdf/1606.09370](https://arxiv.org/pdf/1606.09370)

