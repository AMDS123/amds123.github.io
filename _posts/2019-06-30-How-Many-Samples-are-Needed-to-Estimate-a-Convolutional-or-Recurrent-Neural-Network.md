---
layout: post
title: "How Many Samples are Needed to Estimate a Convolutional or Recurrent Neural Network?"
date: 2019-06-30 00:24:50
categories: arXiv_AI
tags: arXiv_AI CNN RNN
author: Simon S. Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan Salakhutdinov, Aarti Singh
mathjax: true
---

* content
{:toc}

##### Abstract
It is widely believed that the practical success of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) owes to the fact that CNNs and RNNs use a more compact parametric representation than their Fully-Connected Neural Network (FNN) counterparts, and consequently require fewer training examples to accurately estimate their parameters. We initiate the study of rigorously characterizing the sample-complexity of estimating CNNs and RNNs. We show that the sample-complexity to learn CNNs and RNNs scales linearly with their intrinsic dimension and this sample-complexity is much smaller than for their FNN counterparts. For both CNNs and RNNs, we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main technical tools for deriving these results are a localized empirical process analysis and a new technical lemma characterizing the convolutional and recurrent structure. We believe that these tools may inspire further developments in understanding CNNs and RNNs.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1805.07883](http://arxiv.org/abs/1805.07883)

##### PDF
[http://arxiv.org/pdf/1805.07883](http://arxiv.org/pdf/1805.07883)

