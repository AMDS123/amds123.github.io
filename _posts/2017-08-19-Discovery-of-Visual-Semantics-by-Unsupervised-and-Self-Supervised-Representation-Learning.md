---
layout: post
title: "Discovery of Visual Semantics by Unsupervised and Self-Supervised Representation Learning"
date: 2017-08-19 06:38:53
categories: arXiv_CV
tags: arXiv_CV Segmentation Semantic_Segmentation Represenation_Learning Classification Deep_Learning
author: Gustav Larsson
mathjax: true
---

* content
{:toc}

##### Abstract
The success of deep learning in computer vision is rooted in the ability of deep networks to scale up model complexity as demanded by challenging visual tasks. As complexity is increased, so is the need for large amounts of labeled data to train the model. This is associated with a costly human annotation effort. To address this concern, with the long-term goal of leveraging the abundance of cheap unlabeled data, we explore methods of unsupervised "pre-training." In particular, we propose to use self-supervised automatic image colorization. We show that traditional methods for unsupervised learning, such as layer-wise clustering or autoencoders, remain inferior to supervised pre-training. In search for an alternative, we develop a fully automatic image colorization method. Our method sets a new state-of-the-art in revitalizing old black-and-white photography, without requiring human effort or expertise. Additionally, it gives us a method for self-supervised representation learning. In order for the model to appropriately re-color a grayscale object, it must first be able to identify it. This ability, learned entirely self-supervised, can be used to improve other visual tasks, such as classification and semantic segmentation. As a future direction for self-supervision, we investigate if multiple proxy tasks can be combined to improve generalization. This turns out to be a challenging open problem. We hope that our contributions to this endeavor will provide a foundation for future efforts in making self-supervision compete with supervised pre-training.

##### Abstract (translated by Google)
深度学习在计算机视觉领域的成功源于深层网络根据具有挑战性的视觉任务所要求的扩大模型复杂性的能力。随着复杂性的增加，需要大量的标记数据来训练模型。这与昂贵的人工注释工作相关联。为了解决这个问题，为了利用大量便宜的未标记数据的长期目标，我们探索了无监督“预培训”的方法。特别是，我们建议使用自我监督的自动图像着色。我们表明，传统的无监督学习方法，如分层聚类或autoencoders，仍然逊于有监督的预训练。为了寻找替代品，我们开发了一种全自动图像着色方法。我们的方法为振兴旧的黑白摄影设置了一种新的艺术，不需要人力或专业知识。此外，它给了我们一个自我监督表征学习的方法。为了使模型适当地重新着色灰度对象，它必须首先能够识别它。这种完全自我监督学习的能力可以用来改善其他视觉任务，如分类和语义分割。作为未来自我监督的方向，我们调查是否可以结合多个代理任务来改进泛化。这原来是一个具有挑战性的开放性问题。我们希望，我们对这一努力的贡献将为未来的自律监督与受监督的预培训竞争打下基础。

##### URL
[https://arxiv.org/abs/1708.05812](https://arxiv.org/abs/1708.05812)

##### PDF
[https://arxiv.org/pdf/1708.05812](https://arxiv.org/pdf/1708.05812)

