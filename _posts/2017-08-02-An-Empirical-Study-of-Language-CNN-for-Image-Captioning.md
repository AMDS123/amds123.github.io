---
layout: post
title: "An Empirical Study of Language CNN for Image Captioning"
date: 2017-08-02 12:33:50
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption RNN Language_Model
author: Jiuxiang Gu, Gang Wang, Jianfei Cai, Tsuhan Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Language Models based on recurrent neural networks have dominated recent image caption generation tasks. In this paper, we introduce a Language CNN model which is suitable for statistical language modeling tasks and shows competitive performance in image captioning. In contrast to previous models which predict next word based on one previous word and hidden state, our language CNN is fed with all the previous words and can model the long-range dependencies of history words, which are critical for image captioning. The effectiveness of our approach is validated on two datasets MS COCO and Flickr30K. Our extensive experimental results show that our method outperforms the vanilla recurrent neural network based language models and is competitive with the state-of-the-art methods.

##### Abstract (translated by Google)
基于递归神经网络的语言模型已经主导了最近的图像标题生成任务。在本文中，我们介绍了一种语言CNN模型，它适用于统计语言建模任务，并在图像字幕中显示出竞争性的表现。与先前基于前一个词和隐藏状态预测下一个词的模型相反，我们的语言CNN被提供了所有先前的词，并且可以模拟历史词的长程依赖性，这对于图像字幕是至关重要的。我们的方法的有效性在两个数据集MS COCO和Flickr30K上得到验证。我们广泛的实验结果表明，我们的方法优于基于vanilla recurrent神经网络的语言模型，并且与最先进的方法相比具有竞争力。

##### URL
[https://arxiv.org/abs/1612.07086](https://arxiv.org/abs/1612.07086)

##### PDF
[https://arxiv.org/pdf/1612.07086](https://arxiv.org/pdf/1612.07086)

