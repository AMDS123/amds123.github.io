---
layout: post
title: "Robust end-to-end deep audiovisual speech recognition"
date: 2016-11-21 20:08:51
categories: arXiv_CL
tags: arXiv_CL Sparse Speech_Recognition RNN Classification Recognition
author: Ramon Sanabria, Florian Metze, Fernando De La Torre
mathjax: true
---

* content
{:toc}

##### Abstract
Speech is one of the most effective ways of communication among humans. Even though audio is the most common way of transmitting speech, very important information can be found in other modalities, such as vision. Vision is particularly useful when the acoustic signal is corrupted. Multi-modal speech recognition however has not yet found wide-spread use, mostly because the temporal alignment and fusion of the different information sources is challenging. This paper presents an end-to-end audiovisual speech recognizer (AVSR), based on recurrent neural networks (RNN) with a connectionist temporal classification (CTC) loss function. CTC creates sparse "peaky" output activations, and we analyze the differences in the alignments of output targets (phonemes or visemes) between audio-only, video-only, and audio-visual feature representations. We present the first such experiments on the large vocabulary IBM ViaVoice database, which outperform previously published approaches on phone accuracy in clean and noisy conditions.

##### Abstract (translated by Google)
言语是人类交流最有效的方式之一。尽管音频是传输语音的最常见的方式，但是在其他形式（如视觉）中可以找到非常重要的信息。当声音信号被破坏时，视觉特别有用。然而，多模式语音识别尚未被广泛使用，主要是因为不同信息源的时间对齐和融合是具有挑战性的。本文提出了一种基于递归神经网络（RNN）和连接主义时态分类（CTC）损失函数的端到端视听语音识别器（AVSR）。 CTC创建了稀疏的“峰值”输出激活，我们分析了纯音频，纯视频和音视频特征表示之间输出目标（音位或视素）对齐的差异。我们在大型词汇IBM ViaVoice数据库上展示了第一个这样的实验，它在干净和嘈杂的条件下胜过了之前公布的手机准确性方法。

##### URL
[https://arxiv.org/abs/1611.06986](https://arxiv.org/abs/1611.06986)

##### PDF
[https://arxiv.org/pdf/1611.06986](https://arxiv.org/pdf/1611.06986)

