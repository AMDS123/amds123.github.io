---
layout: post
title: "Hierarchical RNN with Static Sentence-Level Attention for Text-Based Speaker Change Detection"
date: 2017-03-22 15:42:28
categories: arXiv_SD
tags: arXiv_SD Attention RNN Detection
author: Zhao Meng, Lili Mou, Zhi Jin
mathjax: true
---

* content
{:toc}

##### Abstract
Traditional speaker change detection in dialogues is typically based on audio input. In some scenarios, however, researchers can only obtain text, and do not have access to raw audio signals. Moreover, with the increasing need of deep semantic processing, text-based dialogue understanding is attracting more attention in the community. These raise the problem of text-based speaker change detection. In this paper, we formulate the task as a matching problem of utterances before and after a certain decision point; we propose a hierarchical recurrent neural network (RNN) with static sentence-level attention. Our model comprises three main components: a sentence encoder with a long short term memory (LSTM)-based RNN, a context encoder with another LSTM-RNN, and a static sentence-level attention mechanism, which allows rich information interaction. Experimental results show that neural networks consistently achieve better performance than feature-based approaches, and that our attention-based model significantly outperforms non-attention neural networks.

##### Abstract (translated by Google)
对话中传统的说话人变化检测通常基于音频输入。然而在某些情况下，研究人员只能获得文本，并且无法获得原始音频信号。而且，随着对语义深度处理的需求日益增加，基于文本的对话理解在社区中引起了越来越多的关注。这些引起了基于文本的说话者变化检测的问题。在本文中，我们将任务作为某个决策点前后的话语匹配问题，我们提出了一个静态句级关注的分层递归神经网络（RNN）。我们的模型包括三个主要组成部分：一个带有长期短期记忆（LSTM）的RNN的句子编码器，一个带有另一个LSTM-RNN的上下文编码器，以及一个允许丰富信息交互的静态句子级关注机制。实验结果表明，神经网络一直比基于特征的方法获得更好的性能，而且我们的基于注意的模型明显优于非注意神经网络。

##### URL
[https://arxiv.org/abs/1703.07713](https://arxiv.org/abs/1703.07713)

##### PDF
[https://arxiv.org/pdf/1703.07713](https://arxiv.org/pdf/1703.07713)

