---
layout: post
title: "Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning"
date: 2016-06-14 22:30:08
categories: arXiv_CV
tags: arXiv_CV Regularization CNN Prediction Gradient_Descent
author: Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen
mathjax: true
---

* content
{:toc}

##### Abstract
Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.

##### Abstract (translated by Google)
有效的卷积神经网络在大量的标记数据上进行训练。但是，创建大型标记数据集是一项非常昂贵和耗时的任务。半监督学习使用无标记的数据来训练具有更高精确度的模型，当有一组有限的标记数据可用时。在本文中，我们考虑卷积神经网络的半监督学习问题。诸如随机数据增强，丢失和随机最大汇集之类的技术为使用梯度下降训练的分类器提供更好的泛化和稳定性。由于这些技术的非确定性行为，单个样本通过网络的多次传递可能导致不同的预测。我们提出了一个无监督的损失函数，它利用了这些方法的随机特性，并最小化了训练样本通过网络的多次传递预测之间的差异。我们在几个基准数据集上评估所提出的方法。

##### URL
[https://arxiv.org/abs/1606.04586](https://arxiv.org/abs/1606.04586)

##### PDF
[https://arxiv.org/pdf/1606.04586](https://arxiv.org/pdf/1606.04586)

