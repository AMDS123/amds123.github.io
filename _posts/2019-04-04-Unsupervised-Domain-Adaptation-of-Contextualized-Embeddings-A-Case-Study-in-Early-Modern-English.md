---
layout: post
title: "Unsupervised Domain Adaptation of Contextualized Embeddings: A Case Study in Early Modern English"
date: 2019-04-04 23:05:45
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model
author: Xiaochuang Han, Jacob Eisenstein
mathjax: true
---

* content
{:toc}

##### Abstract
Contextualized word embeddings such as ELMo and BERT provide a foundation for strong performance across a range of natural language processing tasks, in part by pretraining on a large and topically-diverse corpus. However, the applicability of this approach is unknown when the target domain varies substantially from the text used during pretraining. Specifically, we are interested the scenario in which labeled data is available in only a canonical source domain such as newstext, and the target domain is distinct from both the labeled corpus and the pretraining data. To address this scenario, we propose domain-adaptive fine-tuning, in which the contextualized embeddings are adapted by masked language modeling on the target domain. We test this approach on the challenging domain of Early Modern English, which differs substantially from existing pretraining corpora. Domain-adaptive fine-tuning yields an improvement of 4\% in part-of-speech tagging accuracy over a BERT baseline, substantially improving on prior work on this task.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1904.02817](https://arxiv.org/abs/1904.02817)

##### PDF
[https://arxiv.org/pdf/1904.02817](https://arxiv.org/pdf/1904.02817)

