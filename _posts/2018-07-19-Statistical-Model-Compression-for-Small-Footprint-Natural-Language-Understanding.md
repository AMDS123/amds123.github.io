---
layout: post
title: "Statistical Model Compression for Small-Footprint Natural Language Understanding"
date: 2018-07-19 16:23:35
categories: arXiv_CL
tags: arXiv_CL Regularization
author: Grant P. Strimel, Kanthashree Mysore Sathyendra, Stanislav Peshterliev
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we investigate statistical model compression applied to natural language understanding (NLU) models. Small-footprint NLU models are important for enabling offline systems on hardware restricted devices, and for decreasing on-demand model loading latency in cloud-based systems. To compress NLU models, we present two main techniques, parameter quantization and perfect feature hashing. These techniques are complementary to existing model pruning strategies such as L1 regularization. We performed experiments on a large scale NLU system. The results show that our approach achieves 14-fold reduction in memory usage compared to the original models with minimal predictive performance impact.

##### Abstract (translated by Google)
在本文中，我们研究应用于自然语言理解（NLU）模型的统计模型压缩。小尺寸NLU模型对于在硬件受限设备上启用离线系统以及减少基于云的系统中的按需模型加载延迟非常重要。为了压缩NLU模型，我们提出了两种主要技术，参数量化和完美的特征散列。这些技术是对现有模型修剪策略（例如L1正则化）的补充。我们在大规模NLU系统上进行了实验。结果表明，与原始模型相比，我们的方法将内存使用量降低了14倍，并且对预测性能的影响最小。

##### URL
[https://arxiv.org/abs/1807.07520](https://arxiv.org/abs/1807.07520)

##### PDF
[https://arxiv.org/pdf/1807.07520](https://arxiv.org/pdf/1807.07520)

