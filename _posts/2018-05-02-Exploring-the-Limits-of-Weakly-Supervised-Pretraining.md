---
layout: post
title: "Exploring the Limits of Weakly Supervised Pretraining"
date: 2018-05-02 17:57:16
categories: arXiv_CV
tags: arXiv_CV Object_Detection Weakly_Supervised CNN Image_Classification Transfer_Learning Classification Prediction Detection Relation
author: Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten
mathjax: true
---

* content
{:toc}

##### Abstract
State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.

##### Abstract (translated by Google)
用于各种任务的最先进的视觉感知模型依赖于受监督的预训练。 ImageNet分类是这些模型事实上的训练任务。然而，ImageNet现在已经有近十年的历史了，现在的标准是“小”。即使如此，对数据集预训练的行为相对较少，数据集的数量级要大几个数量级。原因很明显：这些数据集很难收集和注释。在本文中，我们提出了一个独特的转移学习研究，用大型卷积网络训练预测数十亿社交媒体图像的主题标签。我们的实验证明，大规模标签预测的培训可以带来出色的结果。我们展示了几种图像分类和对象检测任务的改进，并报告迄今为止最高的ImageNet-1k单作物，前1精度：85.4％（97.6％前5）。我们还进行广泛的实验，提供关于大规模预训练和转移学习表现之间关系的新颖经验数据。

##### URL
[https://arxiv.org/abs/1805.00932](https://arxiv.org/abs/1805.00932)

##### PDF
[https://arxiv.org/pdf/1805.00932](https://arxiv.org/pdf/1805.00932)

