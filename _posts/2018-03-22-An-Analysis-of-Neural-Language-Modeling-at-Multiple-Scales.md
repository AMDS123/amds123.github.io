---
layout: post
title: "An Analysis of Neural Language Modeling at Multiple Scales"
date: 2018-03-22 06:25:47
categories: arXiv_AI
tags: arXiv_AI RNN Language_Model
author: Stephen Merity, Nitish Shirish Keskar, Richard Socher
mathjax: true
---

* content
{:toc}

##### Abstract
Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.

##### Abstract (translated by Google)
语言建模中的许多领先方法引入了新颖，复杂和专业的体系结构。我们采用基于LSTM和QRNN的现有最先进的词级语言模型，并将它们扩展到更大的词汇表以及字符级粒度。经过适当调整后，LSTM和QRNN分别在字符级别（Penn Treebank，enwik8）和单词级别（WikiText-103）数据集上获得最新的结果。使用单个现代GPU仅需12小时（WikiText-103）至2天（enwik8）即可获得结果。

##### URL
[https://arxiv.org/abs/1803.08240](https://arxiv.org/abs/1803.08240)

##### PDF
[https://arxiv.org/pdf/1803.08240](https://arxiv.org/pdf/1803.08240)

