---
layout: post
title: "Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation"
date: 2018-08-25 15:03:59
categories: arXiv_CL
tags: arXiv_CL
author: Zhong Zhou, Matthias Sperber, Alex Waibel
mathjax: true
---

* content
{:toc}

##### Abstract
We work on translation from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a translation system that addresses these challenges using eight European language families as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6% accuracy in qualitative evaluation where our translations are akin to human translations in a preliminary study.

##### Abstract (translated by Google)
我们致力于从丰富资源语言到低资源语言的翻译。我们确定的主要挑战是缺乏低资源语言数据，有效的跨语言转移方法以及神经系统中常见的变量约束问题。我们建立了一个翻译系统，使用八个欧洲语系列作为我们的试验场来应对这些挑战。首先，我们添加源和目标家族标签，研究家庭内和家庭间的影响，以实现有效的跨语言转移。与单家庭多源多目标基线相比，使用8个家庭的英语 - 瑞典语翻译的BLEU得分提高了+9.9。此外，我们发现对最接近低资源语言的两个邻近家庭进行培训通常就足够了。其次，我们构建了一个消融研究，发现即使目标数据少得多，也可以取得相当好的结果。第三，我们通过构建一个保留命令的命名实体转换模型来解决变量绑定问题。我们在定性评估中获得60.6％的准确性，其中我们的翻译类似于初步研究中的人工翻译。

##### URL
[http://arxiv.org/abs/1804.07878](http://arxiv.org/abs/1804.07878)

##### PDF
[http://arxiv.org/pdf/1804.07878](http://arxiv.org/pdf/1804.07878)

