---
layout: post
title: "Utility of General and Specific Word Embeddings for Classifying Translational Stages of Research"
date: 2018-07-09 23:40:23
categories: arXiv_CL
tags: arXiv_CL Text_Classification Embedding Classification Language_Model
author: Vincent Major, Alisa Surkis, Yindalon Aphinyanaphongs
mathjax: true
---

* content
{:toc}

##### Abstract
Conventional text classification models make a bag-of-words assumption reducing text into word occurrence counts per document. Recent algorithms such as word2vec are capable of learning semantic meaning and similarity between words in an entirely unsupervised manner using a contextual window and doing so much faster than previous methods. Each word is projected into vector space such that similar meaning words such as "strong" and "powerful" are projected into the same general Euclidean space. Open questions about these embeddings include their utility across classification tasks and the optimal properties and source of documents to construct broadly functional embeddings. In this work, we demonstrate the usefulness of pre-trained embeddings for classification in our task and demonstrate that custom word embeddings, built in the domain and for the tasks, can improve performance over word embeddings learnt on more general data including news articles or Wikipedia.

##### Abstract (translated by Google)
传统的文本分类模型使得词袋假设将文本减少为每个文档的单词出现计数。诸如word2vec之类的最新算法能够使用上下文窗口以完全无监督的方式学习单词之间的语义和相似性，并且比以前的方法更快地完成。每个单词都被投射到向量空间中，使得类似的含义词如“强”和“强大”被投射到相同的一般欧几里德空间中。关于这些嵌入的开放式问题包括它们在分类任务中的实用性以及构建广泛功能嵌入的文档的最佳属性和来源。在这项工作中，我们展示了预训练嵌入在我们的任务中进行分类的有用性，并证明了在域和任务中构建的自定义词嵌入可以提高在包括新闻文章或维基百科在内的更一般数据上学习的词嵌入的性能。 。

##### URL
[http://arxiv.org/abs/1705.06262](http://arxiv.org/abs/1705.06262)

##### PDF
[http://arxiv.org/pdf/1705.06262](http://arxiv.org/pdf/1705.06262)

