---
layout: post
title: "Improving Efficiency in Convolutional Neural Network with Multilinear Filters"
date: 2017-10-23 15:42:11
categories: arXiv_CV
tags: arXiv_CV CNN
author: Dat Thanh Tran, Alexandros Iosifidis, Moncef Gabbouj
mathjax: true
---

* content
{:toc}

##### Abstract
The excellent performance of deep neural networks has enabled us to solve several automatization problems, opening an era of autonomous devices. However, current deep net architectures are heavy with millions of parameters and require billions of floating point operations. Several works have been developed to compress a pre-trained deep network to reduce memory footprint and, possibly, computation. Instead of compressing a pre-trained network, in this work, we propose a generic neural network layer structure employing multilinear projection as the primary feature extractor. The proposed architecture requires several times less memory as compared to the traditional Convolutional Neural Networks (CNN), while inherits the similar design principles of a CNN. In addition, the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability. Experimental results show the effectiveness of our compact projection that outperforms traditional CNN, while requiring far fewer parameters.

##### Abstract (translated by Google)
深度神经网络的优异性能使我们解决了一些自动化问题，开创了自主设备的时代。然而，目前的深层网络体系结构非常繁重，有数百万个参数，需要数十亿次浮点运算。已经开发了一些工作来压缩预先训练好的深度网络，以减少内存占用和可能的计算。在这项工作中，我们没有压缩预先训练好的网络，而是提出了一个采用多线性投影作为主要特征提取器的通用神经网络层结构。与传统的卷积神经网络（CNN）相比，所提出的架构需要少数倍的内存，同时继承了CNN的类似设计原则。另外，所提出的架构配备有两个计算方案，其使计算简化或可伸缩性成为可能。实验结果表明，我们的紧凑型投影的性能优于传统的CNN，而要求的参数要少得多。

##### URL
[https://arxiv.org/abs/1709.09902](https://arxiv.org/abs/1709.09902)

##### PDF
[https://arxiv.org/pdf/1709.09902](https://arxiv.org/pdf/1709.09902)

