---
layout: post
title: "Virtuously Safe Reinforcement Learning"
date: 2018-05-29 13:34:39
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Henrik Aslund, El Mahdi El Mhamdi, Rachid Guerraoui, Alexandre Maurer
mathjax: true
---

* content
{:toc}

##### Abstract
We show that when a third party, the adversary, steps into the two-party setting (agent and operator) of safely interruptible reinforcement learning, a trade-off has to be made between the probability of following the optimal policy in the limit, and the probability of escaping a dangerous situation created by the adversary. So far, the work on safely interruptible agents has assumed a perfect perception of the agent about its environment (no adversary), and therefore implicitly set the second probability to zero, by explicitly seeking a value of one for the first probability. We show that (1) agents can be made both interruptible and adversary-resilient, and (2) the interruptibility can be made safe in the sense that the agent itself will not seek to avoid it. We also solve the problem that arises when the agent does not go completely greedy, i.e. issues with safe exploration in the limit. Resilience to perturbed perception, safe exploration in the limit, and safe interruptibility are the three pillars of what we call \emph{virtuously safe reinforcement learning}.

##### Abstract (translated by Google)
我们证明，当第三方即对手进入安全可中断的强化学习的双方设置（代理和操作员）时，必须权衡在遵循最优策略的概率和逃避对手造成危险情况的可能性。到目前为止，关于安全可中断代理的工作已经假设代理关于其环境的完美感知（没有对手），因此通过明确地为第一个概率寻求一个值，隐含地将第二概率设置为零。我们证明：（1）代理人既可以被中断又可以被对抗，并且（2）代理人本身不会试图避免它，这种可破坏性就可以变得安全。我们还解决了当代理人不完全贪婪时出现的问题，即在极限情况下安全探索的问题。韧性知觉，极限安全探索和安全可破坏性是我们称之为“绝对安全的强化学习”的三大支柱。

##### URL
[http://arxiv.org/abs/1805.11447](http://arxiv.org/abs/1805.11447)

##### PDF
[http://arxiv.org/pdf/1805.11447](http://arxiv.org/pdf/1805.11447)

