---
layout: post
title: "Do Neural Nets Learn Statistical Laws behind Natural Language?"
date: 2017-11-28 07:36:25
categories: arXiv_CL
tags: arXiv_CL RNN Deep_Learning Language_Model Relation
author: Shuntaro Takahashi, Kumiko Tanaka-Ishii
mathjax: true
---

* content
{:toc}

##### Abstract
The performance of deep learning in natural language processing has been spectacular, but the reasons for this success remain unclear because of the inherent complexity of deep learning. This paper provides empirical evidence of its effectiveness and of a limitation of neural networks for language engineering. Precisely, we demonstrate that a neural language model based on long short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law, two representative statistical properties underlying natural language. We discuss the quality of reproducibility and the emergence of Zipf's law and Heaps' law as training progresses. We also point out that the neural language model has a limitation in reproducing long-range correlation, another statistical property of natural language. This understanding could provide a direction for improving the architectures of neural networks.

##### Abstract (translated by Google)
深度学习在自然语言处理中的表现是惊人的，但由于深度学习固有的复杂性，成功的原因还不清楚。本文提供了经验证据的有效性和神经网络的语言工程的局限性。准确地说，我们证明基于长期短期记忆（LSTM）的神经语言模型有效地再现了Zipf定律和堆法（Heaps'law）这两个有代表性的统计属性。我们讨论重现性的质量以及Zipf法则和堆法则随着培训的进展而出现。我们还指出，神经语言模型在再现自然语言的另一个统计特性 - 长程相关性方面有其局限性。这种理解可以为改善神经网络的结构提供一个方向。

##### URL
[https://arxiv.org/abs/1707.04848](https://arxiv.org/abs/1707.04848)

##### PDF
[https://arxiv.org/pdf/1707.04848](https://arxiv.org/pdf/1707.04848)

