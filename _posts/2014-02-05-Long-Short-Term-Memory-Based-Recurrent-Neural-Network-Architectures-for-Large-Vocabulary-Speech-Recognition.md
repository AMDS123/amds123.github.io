---
layout: post
title: "Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition"
date: 2014-02-05 19:01:51
categories: arXiv_CV
tags: arXiv_CV Speech_Recognition RNN Language_Model Prediction Recognition
author: Haşim Sak, Andrew Senior, Françoise Beaufays
mathjax: true
---

* content
{:toc}

##### Abstract
Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.

##### Abstract (translated by Google)
长期短期记忆（LSTM）是一种经常性神经网络（RNN）架构，旨在解决传统RNN的消失和爆炸梯度问题。与前馈神经网络不同，RNN具有循环连接，使得它们对于建模序列而言是强大的。它们已被成功地用于序列标签和序列预测任务，如手写识别，语言建模，声音帧的语音标注等。然而，与深度神经网络相比，在语音识别中使用RNN仅限于小规模任务中的电话识别。在本文中，我们提出了新型的基于LSTM的RNN体系结构，使得更有效地利用模型参数来训练用于大词汇量语音识别的声学模型。我们在各种参数和配置下对LSTM，RNN和DNN模型进行了培训和比较。我们展示LSTM模型快速收敛，并为相对较小尺寸的模型提供最先进的语音识别性能。

##### URL
[https://arxiv.org/abs/1402.1128](https://arxiv.org/abs/1402.1128)

##### PDF
[https://arxiv.org/pdf/1402.1128](https://arxiv.org/pdf/1402.1128)

