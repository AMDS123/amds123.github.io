---
layout: post
title: "Parallel Long Short-Term Memory for Multi-stream Classification"
date: 2017-02-11 09:50:40
categories: arXiv_SD
tags: arXiv_SD RNN Classification
author: Mohamed Bouaziz, Mohamed Morchid, Richard Dufour, Georges Linarès, Renato De Mori
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-the-art architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach.

##### Abstract (translated by Google)
最近，机器学习方法提供了广泛的基于深度神经网络（DNN）的原始和高效的算法，以自动预测关于输入序列的结果。循环隐藏单元允许这些基于DNN的模型管理长期相关性，例如递归神经网络（RNN）和长期短期记忆（LSTM）。尽管如此，这些RNN在一个（LSTM）或两个（双向LSTM）方向上处理单个输入流。但是现在大部分可用的信息是来自多流或多媒体文档，并且要求RNN在训练期间同步处理这些信息。本文提出了一个原始的基于LSTM的架构，称为并行LSTM（PLSTM），执行多个并行同步输入序列，以预测一个共同的输出。所提出的PLSTM方法可以用于并行序列分类的目的。 PLSTM方法在自动电视广播体裁序列分类任务上进行评估，并与不同的最先进的体系结构进行比较。结果显示，所提出的PLSTM方法优于基线n-gram模型以及最先进的LSTM方法。

##### URL
[https://arxiv.org/abs/1702.03402](https://arxiv.org/abs/1702.03402)

##### PDF
[https://arxiv.org/pdf/1702.03402](https://arxiv.org/pdf/1702.03402)

