---
layout: post
title: 'Data Dropout in Arbitrary Basis for Deep Network Regularization'
date: 2017-12-05 21:10:17
categories: arXiv_CV
[arXiv_CV,Regularization]
author: Mostafa Rahmani, George Atia
---

* content
{:toc}

##### Abstract
An important problem in training deep networks with high capacity is to ensure that the trained network works well with new inputs. Dropout is an effective regularization technique to boost the network generalization in which a random subset of the elements of given data and the extracted features are set to zero during the training process. In this paper, a new randomized regularization technique is proposed in which we withhold a random part of data without necessarily turning off the neurons/data-elements. In the proposed method, of which the conventional dropout is shown to be a special case, random data dropout is performed in an arbitrary basis. In addition, we present a framework to efficiently apply the proposed technique to the convolutional neural networks. The presented numerical experiments show that the proposed technique yields notable performance gain. The proposed approach, dubbed Generalized Dropout, provides a deep insight into the idea of dropout, shows that we can achieve different performance gains using different basis matrices, and opens up a new research question as of how to choose optimal basis matrices that achieve maximal performance gain.

##### Abstract (translated by Google)
培养高容量深度网络的一个重要问题就是要保证训练好的网络能够很好地适应新的输入。压差是一种有效的正则化技术，用于提高网络泛化能力，在训练过程中，将给定数据的元素的随机子集和提取的特征设置为零。在本文中，我们提出了一种新的随机正则化技术，在这种技术中，我们不需要关闭神经元/数据元素就可以保留一部分随机数据。在所提出的方法中，传统的丢失被表示为特殊情况，随机数据丢失是在任意的基础上执行的。此外，我们提出了一个框架，有效地将所提出的技术应用于卷积神经网络。所提出的数值实验表明，所提出的技术产生显着的性能增益。所提出的方法被称为广义丢失，它提供了对丢失概念的深入认识，表明我们可以使用不同的基矩阵来获得不同的性能增益，并且开辟了一个新的研究问题，就如何选择最优的基矩阵来实现最大的性能获得。

##### URL
[http://arxiv.org/abs/1712.00891](http://arxiv.org/abs/1712.00891)

