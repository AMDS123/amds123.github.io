---
layout: post
title: "Isolated and Ensemble Audio Preprocessing Methods for Detecting Adversarial Examples against Automatic Speech Recognition"
date: 2018-09-11 05:12:15
categories: arXiv_CL
tags: arXiv_CL Adversarial Speech_Recognition Recognition
author: Krishan Rajaratnam, Kunal Shah, Jugal Kalita
mathjax: true
---

* content
{:toc}

##### Abstract
An adversarial attack is an exploitative process in which minute alterations are made to natural inputs, causing the inputs to be misclassified by neural models. In the field of speech recognition, this has become an issue of increasing significance. Although adversarial attacks were originally introduced in computer vision, they have since infiltrated the realm of speech recognition. In 2017, a genetic attack was shown to be quite potent against the Speech Commands Model. Limited-vocabulary speech classifiers, such as the Speech Commands Model, are used in a variety of applications, particularly in telephony; as such, adversarial examples produced by this attack pose as a major security threat. This paper explores various methods of detecting these adversarial examples with combinations of audio preprocessing. One particular combined defense incorporating compressions, speech coding, filtering, and audio panning was shown to be quite effective against the attack on the Speech Commands Model, detecting audio adversarial examples with 93.5% precision and 91.2% recall.

##### Abstract (translated by Google)
对抗性攻击是一种剥削过程，其中对自然输入进行微小的改变，导致输入被神经模型错误分类。在语音识别领域，这已成为一个日益重要的问题。虽然最初在计算机视觉中引入了对抗性攻击，但它们已经渗透到语音识别领域。 2017年，基因攻击被证明对语音命令模型非常有效。有限词汇量语音分类器，例如语音命令模型，用于各种应用，特别是在电话中;因此，这次攻击产生的对抗性例子构成了一个主要的安全威胁。本文探讨了使用音频预处理组合检测这些对抗性示例的各种方法。结合压缩，语音编码，过滤和音频平移的一个特殊组合防御被证明对语音命令模型的攻击是非常有效的，检测音频对抗性示例具有93.5％的精度和91.2％的召回率。

##### URL
[http://arxiv.org/abs/1809.04397](http://arxiv.org/abs/1809.04397)

##### PDF
[http://arxiv.org/pdf/1809.04397](http://arxiv.org/pdf/1809.04397)

