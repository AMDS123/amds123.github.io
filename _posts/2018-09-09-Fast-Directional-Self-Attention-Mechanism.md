---
layout: post
title: "Fast Directional Self-Attention Mechanism"
date: 2018-09-09 06:58:09
categories: arXiv_AI
tags: arXiv_AI Attention
author: Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a self-attention mechanism, dubbed "fast directional self-attention (Fast-DiSA)", which is a fast and light extension of "directional self-attention (DiSA)". The proposed Fast-DiSA performs as expressively as the original DiSA but only uses much less computation time and memory, in which 1) both token2token and source2token dependencies are modeled by a joint compatibility function designed for a hybrid of both dot-product and multi-dim ways; 2) both multi-head and multi-dim attention combined with bi-directional temporal information captured by multiple positional masks are in consideration without heavy time and memory consumption appearing in the DiSA. The experiment results show that the proposed Fast-DiSA can achieve state-of-the-art performance as fast and memory-friendly as CNNs. The code for Fast-DiSA is released at \url{https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA}.

##### Abstract (translated by Google)
在本文中，我们提出了一种自我关注机制，称为“快速定向自我关注（Fast-DiSA）”，它是“定向自我关注（DiSA）”的快速而轻微的延伸。所提出的Fast-DiSA与原始DiSA一样表现，但仅使用更少的计算时间和内存，其中1）token2token和source2token依赖性都是通过为点积和多点混合设计的联合兼容函数建模的。昏暗的方式; 2）考虑到多头和多暗注意力以及由多个位置掩模捕获的双向时间信息，并且在DiSA中不会出现繁重的时间和存储器消耗。实验结果表明，所提出的Fast-DiSA可以实现与CNN一样快速且对内存友好的最新性能。 Fast-DiSA的代码发布在\ url {https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA}。

##### URL
[http://arxiv.org/abs/1805.00912](http://arxiv.org/abs/1805.00912)

##### PDF
[http://arxiv.org/pdf/1805.00912](http://arxiv.org/pdf/1805.00912)

