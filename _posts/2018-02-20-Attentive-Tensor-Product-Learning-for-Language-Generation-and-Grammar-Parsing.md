---
layout: post
title: "Attentive Tensor Product Learning for Language Generation and Grammar Parsing"
date: 2018-02-20 12:42:07
categories: arXiv_CV
tags: arXiv_CV Image_Caption Attention Caption RNN Deep_Learning
author: Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, Xiaodong He
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a new architecture - Attentive Tensor Product Learning (ATPL) - to represent grammatical structures in deep learning models. ATPL is a new architecture to bridge this gap by exploiting Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, aiming to integrate deep learning with explicit language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via TPR-based deep neural network; 2) employing attention modules to compute TPR; and 3) integration of TPR with typical deep learning architectures including Long Short-Term Memory (LSTM) and Feedforward Neural Network (FFNN). The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. This ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a sentence. Experimental results demonstrate the effectiveness of the proposed approach.

##### Abstract (translated by Google)
本文提出了一种新的体系结构 -  Attentive Tensor Product Learning（ATPL） - 来表示深度学习模型中的语法结构。 ATPL是一种通过利用Tensor Product Representations（TPR）来弥补这一差距的新架构，TPR是一种在认知科学中开发的结构化神经符号模型，旨在将深度学习与明确的语言结构和规则相结合。 ATPL的关键思想是：1）通过基于TPR的深度神经网络对角色解除单词向量进行无监督学习; 2）使用注意模块来计算TPR; 3）TPR与典型的深度学习架构的集成，包括长短期记忆（LSTM）和前馈神经网络（FFNN）。我们方法的新颖之处在于它能够通过使用以无人监督的方式获得的角色解除向量来提取句子的语法结构。该ATPL方法应用于1）图像字幕，2）词性（POS）标记，以及3）句子的选区解析。实验结果证明了该方法的有效性。

##### URL
[https://arxiv.org/abs/1802.07089](https://arxiv.org/abs/1802.07089)

##### PDF
[https://arxiv.org/pdf/1802.07089](https://arxiv.org/pdf/1802.07089)

