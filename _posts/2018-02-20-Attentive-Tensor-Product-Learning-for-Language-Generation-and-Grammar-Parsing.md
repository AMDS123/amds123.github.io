---
layout: post
title: "Attentive Tensor Product Learning for Language Generation and Grammar Parsing"
date: 2018-02-20 12:42:07
categories: arXiv_CL
tags: arXiv_CL Image_Caption Attention Caption RNN Deep_Learning
author: Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, Xiaodong He
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a new architecture - Attentive Tensor Product Learning (ATPL) - to represent grammatical structures in deep learning models. ATPL is a new architecture to bridge this gap by exploiting Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, aiming to integrate deep learning with explicit language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via TPR-based deep neural network; 2) employing attention modules to compute TPR; and 3) integration of TPR with typical deep learning architectures including Long Short-Term Memory (LSTM) and Feedforward Neural Network (FFNN). The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. This ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a sentence. Experimental results demonstrate the effectiveness of the proposed approach.

##### Abstract (translated by Google)
本文提出了一种新的架构 - 专注张量产品学习（ATPL） - 来表示深度学习模型中的语法结构。 ATPL是一种新的架构，通过利用张量产品表示（TPR）这一结构化的神经 - 符号模型在认知科学中发展起来，旨在将深度学习与显式语言结构和规则相结合，从而缩小这一差距。 ATPL的主要思想是：1）通过基于TPR的深度神经网络对单词的角色解绑向量进行无监督学习; 2）使用注意模块计算TPR;以及3）TPR与典型深度学习架构（包括长期短期记忆（LSTM）和前馈神经网络（FFNN））的集成。我们方法的新颖之处在于它能够通过使用以无监督方式获得的角色解除绑定向量来提取句子的语法结构。这种ATPL方法适用于1）图像字幕，2）词性（POS）标记，以及3）句子的选区分析。实验结果证明了所提出方法的有效性。

##### URL
[http://arxiv.org/abs/1802.07089](http://arxiv.org/abs/1802.07089)

##### PDF
[http://arxiv.org/pdf/1802.07089](http://arxiv.org/pdf/1802.07089)

