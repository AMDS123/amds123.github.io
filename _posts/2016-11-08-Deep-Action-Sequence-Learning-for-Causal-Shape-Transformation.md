---
layout: post
title: "Deep Action Sequence Learning for Causal Shape Transformation"
date: 2016-11-08 20:48:47
categories: arXiv_CV
tags: arXiv_CV CNN RNN Deep_Learning Prediction
author: Kin Gwn Lore, Daniel Stoecklein, Michael Davies, Baskar Ganapathysubramanian, Soumik Sarkar
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning became the method of choice in recent year for solving a wide variety of predictive analytics tasks. For sequence prediction, recurrent neural networks (RNN) are often the go-to architecture for exploiting sequential information where the output is dependent on previous computation. However, the dependencies of the computation lie in the latent domain which may not be suitable for certain applications involving the prediction of a step-wise transformation sequence that is dependent on the previous computation only in the visible domain. We propose that a hybrid architecture of convolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient to learn a sequence of actions that nonlinearly transforms an input shape or distribution into a target shape or distribution with the same support. While such a framework can be useful in a variety of problems such as robotic path planning, sequential decision-making in games, and identifying material processing pathways to achieve desired microstructures, the application of the framework is exemplified by the control of fluid deformations in a microfluidic channel by deliberately placing a sequence of pillars. Learning of a multistep topological transform has significant implications for rapid advances in material science and biomedical applications.

##### Abstract (translated by Google)
深度学习成为近年来解决各种预测分析任务的首选方法。对于序列预测，递归神经网络（RNN）通常是利用顺序信息的前提架构，其中输出取决于先前的计算。然而，计算的依赖性在于潜在的领域，其可能不适合涉及依赖于仅在可见域中的先前计算的逐步转换序列的预测的某些应用。我们提出，卷积神经网络（CNN）和堆叠自动编码器（SAE）的混合体系结构足以学习一系列的动作，将输入形状或分布非线性地转换为具有相同支持的目标形状或分布。虽然这样的框架可用于各种问题，例如机器人路径规划，游戏中的顺序决策以及识别材料处理路径以实现期望的微结构，但框架的应用通过控制流体变形微流体通道故意放置一系列的支柱。学习多步拓扑变换对材料科学和生物医学应用的快速发展具有重要意义。

##### URL
[https://arxiv.org/abs/1605.05368](https://arxiv.org/abs/1605.05368)

##### PDF
[https://arxiv.org/pdf/1605.05368](https://arxiv.org/pdf/1605.05368)

