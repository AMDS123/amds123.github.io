---
layout: post
title: "Boosting Neural Machine Translation"
date: 2017-10-03 11:46:04
categories: arXiv_CL
tags: arXiv_CL NMT
author: Dakun Zhang, Jungi Kim, Josep Crego, Jean Senellart
---

* content
{:toc}

##### Abstract
Training efficiency is one of the main problems for Neural Machine Translation (NMT). Deep networks need for very large data as well as many training iterations to achieve state-of-the-art performance. This results in very high computation cost, slowing down research and industrialisation. In this paper, we propose to alleviate this problem with several training methods based on data boosting and bootstrap with no modifications to the neural network. It imitates the learning process of humans, which typically spend more time when learning "difficult" concepts than easier ones. We experiment on an English-French translation task showing accuracy improvements of up to 1.63 BLEU while saving 20% of training time.

##### Abstract (translated by Google)
训练效率是神经机器翻译（NMT）的主要问题之一。深度网络需要非常大的数据以及许多训练迭代来实现最先进的性能。这导致了非常高的计算成本，减慢了研究和工业化。在本文中，我们提出了一些基于数据增强和自举的训练方法来缓解这个问题，而不需要修改神经网络。它模仿人类的学习过程，通常在学习“困难”的概念时花费更多的时间。我们在英语 - 法语翻译任务上进行实验，显示精度提高了1.63 BLEU，同时节省了20％的培训时间。

##### URL
[https://arxiv.org/abs/1612.06138](https://arxiv.org/abs/1612.06138)

