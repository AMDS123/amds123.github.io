---
layout: post
title: "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection"
date: 2017-07-08 19:45:47
categories: arXiv_CL
tags: arXiv_CL Weakly_Supervised Embedding Recognition
author: Jian Ni, Georgiana Dinu, Radu Florian
mathjax: true
---

* content
{:toc}

##### Abstract
The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.

##### Abstract (translated by Google)
最先进的命名实体识别（NER）系统是有监督的机器学习模型，需要大量的手动注释数据来实现高精度。然而，用人注释NER数据是昂贵和耗时的，并且对于新语言来说可能相当困难。在本文中，我们提出了两种针对目标语言中没有人类注释的跨语言NER的弱监督方法。第一种方法是通过在可比较的语料库上进行注解投影来为目标语言创建自动标记的NER数据，在那里我们开发了一种启发式方案，从噪音数据中有效地选择高质量的投影标记数据。第二种方法是将从目标语言到源语言的单词（单词嵌入）的分布式表示投影，以便源语言NER系统可以被应用于目标语言而不需要重新训练。我们还设计了两种有效结合两种基于投影的方法输出的共同解码方案。我们评估所提议的方法在内部和开放NER数据的几种目标语言的表现。结果表明，联合系统在CoNLL数据上优于其他三个弱监督方法。

##### URL
[https://arxiv.org/abs/1707.02483](https://arxiv.org/abs/1707.02483)

##### PDF
[https://arxiv.org/pdf/1707.02483](https://arxiv.org/pdf/1707.02483)

