---
layout: post
title: "Planning and Learning with Stochastic Action Sets"
date: 2018-05-07 06:48:41
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Craig Boutilier, Alon Cohen, Amit Daniely, Avinatan Hassidim, Yishay Mansour, Ofer Meshi, Martin Mladenov, Dale Schuurmans
mathjax: true
---

* content
{:toc}

##### Abstract
In many practical uses of reinforcement learning (RL) the set of actions available at a given state is a random variable, with realizations governed by an exogenous stochastic process. Somewhat surprisingly, the foundations for such sequential decision processes have been unaddressed. In this work, we formalize and investigate MDPs with stochastic action sets (SAS-MDPs) to provide these foundations. We show that optimal policies and value functions in this model have a structure that admits a compact representation. From an RL perspective, we show that Q-learning with sampled action sets is sound. In model-based settings, we consider two important special cases: when individual actions are available with independent probabilities; and a sampling-based model for unknown distributions. We develop poly-time value and policy iteration methods for both cases; and in the first, we offer a poly-time linear programming solution.

##### Abstract (translated by Google)
在强化学习（RL）的许多实际应用中，给定状态下可用的一组行为是一个随机变量，实现由外生随机过程控制。有点令人惊讶的是，这种顺序决策过程的基础并没有得到解决。在这项工作中，我们使用随机动作集（SAS-MDP）对MDP进行形式化和调查，以提供这些基础。我们表明，在这个模型中的最优策略和价值函数有一个承认紧凑表示的结构。从RL的角度来看，我们表明采样动作集的Q学习是正确的。在基于模型的设置中，我们考虑两个重要的特殊情况：当个体行为具有独立概率时;以及针对未知分布的基于抽样的模型。我们针对这两种情况开发多时间值和策略迭代方法;首先，我们提供多时间线性编程解决方案。

##### URL
[https://arxiv.org/abs/1805.02363](https://arxiv.org/abs/1805.02363)

##### PDF
[https://arxiv.org/pdf/1805.02363](https://arxiv.org/pdf/1805.02363)

