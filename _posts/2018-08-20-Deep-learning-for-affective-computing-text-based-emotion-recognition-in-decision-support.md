---
layout: post
title: "Deep learning for affective computing: text-based emotion recognition in decision support"
date: 2018-08-20 17:10:52
categories: arXiv_CL
tags: arXiv_CL Regularization Sentiment Knowledge Transfer_Learning RNN Deep_Learning Recognition
author: Bernhard Kratzwald, Suzana Ilic, Mathias Kraus, Stefan Feuerriegel, Helmut Prendinger
mathjax: true
---

* content
{:toc}

##### Abstract
Emotions widely affect the decision-making of humans. This is taken into account by affective computing with the goal of tailoring decision support to the emotional states of individuals. However, the accurate recognition of emotions within narrative documents presents a challenging undertaking due to the complexity and ambiguity of language. Performance improvements can be achieved by deep learning, yet, as demonstrated in this paper, the specific nature of this task requires customizations of recurrent neural networks with regard to bidirectional processing, dropout layers as a means of regularization, and weighted loss functions. In addition, we propose sent2affect a tailored form of transfer learning for emotion recognition: the network is pre-trained with sentiment analysis in order to induce knowledge from a different task (as opposed to the commonly merely different dataset) and is afterwards tuned together with an exchanged output layer. The resulting performance is evaluated in a holistic setting across 6 benchmark datasets, where we find that both recurrent neural networks and transfer learning consistently outperforms traditional machine learning. Altogether, the findings have considerable implications for the use of affective computing.

##### Abstract (translated by Google)
情绪广泛影响人类的决策。这通过情感计算来考虑，其目的是为个人的情绪状态定制决策支持。然而，由于语言的复杂性和模糊性，在叙事文档中准确识别情绪是一项具有挑战性的工作。通过深度学习可以实现性能改进，但是，正如本文所示，该任务的特定性质需要定制关于双向处理的递归神经网络，作为正则化手段的丢失层和加权损失函数。此外，我们建议发送用于情感识别的定制形式的转移学习：网络通过情感分析进行预训练，以便从不同的任务（而不是通常仅仅是不同的数据集）中获取知识，然后与之一起调整交换的输出层。在6个基准数据集的整体设置中评估得到的性能，我们发现循环神经网络和传递学习始终优于传统的机器学习。总而言之，这些发现对于情感计算的使用具有相当大的意义。

##### URL
[http://arxiv.org/abs/1803.06397](http://arxiv.org/abs/1803.06397)

##### PDF
[http://arxiv.org/pdf/1803.06397](http://arxiv.org/pdf/1803.06397)

