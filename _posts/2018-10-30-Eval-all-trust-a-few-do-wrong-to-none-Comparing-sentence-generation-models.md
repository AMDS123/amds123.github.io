---
layout: post
title: "Eval all, trust a few, do wrong to none: Comparing sentence generation models"
date: 2018-10-30 20:29:16
categories: arXiv_CL
tags: arXiv_CL Attention Text_Generation
author: Ond&#x159;ej C&#xed;fka, Aliaksei Severyn, Enrique Alfonseca, Katja Filippova
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we study recent neural generative models for text generation related to variational autoencoders. Previous works have employed various techniques to control the prior distribution of the latent codes in these models, which is important for sampling performance, but little attention has been paid to reconstruction error. In our study, we follow a rigorous evaluation protocol using a large set of previously used and novel automatic and human evaluation metrics, applied to both generated samples and reconstructions. We hope that it will become the new evaluation standard when comparing neural generative models for text.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1804.07972](http://arxiv.org/abs/1804.07972)

##### PDF
[http://arxiv.org/pdf/1804.07972](http://arxiv.org/pdf/1804.07972)

