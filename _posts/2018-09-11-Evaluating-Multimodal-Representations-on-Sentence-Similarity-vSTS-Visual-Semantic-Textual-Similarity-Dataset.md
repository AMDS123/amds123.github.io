---
layout: post
title: "Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset"
date: 2018-09-11 06:40:36
categories: arXiv_AI
tags: arXiv_AI Caption Quantitative
author: Oier Lopez de Lacalle, Aitor Soroa, Eneko Agirre
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we introduce vSTS, a new dataset for measuring textual similarity of sentences using multimodal information. The dataset is comprised by images along with its respectively textual captions. We describe the dataset both quantitatively and qualitatively, and claim that it is a valid gold standard for measuring automatic multimodal textual similarity systems. We also describe the initial experiments combining the multimodal information.

##### Abstract (translated by Google)
在本文中，我们介绍了vSTS，这是一个使用多模态信息测量句子文本相似性的新数据集。数据集由图像及其各自的文本字幕组成。我们定量和定性地描述了数据集，并声称它是用于测量自动多模态文本相似性系统的有效金标准。我们还描述了结合多模态信息的初始实验。

##### URL
[http://arxiv.org/abs/1809.03695](http://arxiv.org/abs/1809.03695)

##### PDF
[http://arxiv.org/pdf/1809.03695](http://arxiv.org/pdf/1809.03695)

