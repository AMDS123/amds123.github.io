---
layout: post
title: "Detecting Engagement in Egocentric Video"
date: 2016-04-04 15:21:16
categories: arXiv_CV
tags: arXiv_CV Salient Summarization
author: Yu-Chuan Su, Kristen Grauman
mathjax: true
---

* content
{:toc}

##### Abstract
In a wearable camera video, we see what the camera wearer sees. While this makes it easy to know roughly what he chose to look at, it does not immediately reveal when he was engaged with the environment. Specifically, at what moments did his focus linger, as he paused to gather more information about something he saw? Knowing this answer would benefit various applications in video summarization and augmented reality, yet prior work focuses solely on the "what" question (estimating saliency, gaze) without considering the "when" (engagement). We propose a learning-based approach that uses long-term egomotion cues to detect engagement, specifically in browsing scenarios where one frequently takes in new visual information (e.g., shopping, touring). We introduce a large, richly annotated dataset for ego-engagement that is the first of its kind. Our approach outperforms a wide array of existing methods. We show engagement can be detected well independent of both scene appearance and the camera wearer's identity.

##### Abstract (translated by Google)
在可穿戴摄像机视频中，我们可以看到佩戴者看到的是什么。虽然这可以很容易地知道他选择看什么，但是当他与环境有关时，并不能立即显示出来。具体来说，当他停下来收集关于他所看到的东西的更多信息时，他的焦点在什么时候停滞不前？知道这个答案将有利于视频摘要和增强现实中的各种应用，但是之前的工作仅仅关注“什么”问题（估计显着性，凝视）而不考虑“何时”（参与）。我们提出一种基于学习的方法，使用长期的运动提示来检测参与，特别是在浏览经常需要新视觉信息（例如购物，旅游）的场景中。我们引入了一个大型的，注释丰富的自我参与的数据集，这是其首创。我们的方法胜过了一系列现有的方法。我们显示参与可以很好地独立于场景外观和相机佩戴者的身份来检测。

##### URL
[https://arxiv.org/abs/1604.00906](https://arxiv.org/abs/1604.00906)

##### PDF
[https://arxiv.org/pdf/1604.00906](https://arxiv.org/pdf/1604.00906)

