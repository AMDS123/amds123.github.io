---
layout: post
title: "Learning to Grasp from a Single Demonstration"
date: 2018-06-09 15:30:36
categories: arXiv_CV
tags: arXiv_CV CNN
author: Pieter Van Molle, Tim Verbelen, Elias De Coninck, Cedric De Boom, Pieter Simoens, Bart Dhoedt
mathjax: true
---

* content
{:toc}

##### Abstract
Learning-based approaches for robotic grasping using visual sensors typically require collecting a large size dataset, either manually labeled or by many trial and errors of a robotic manipulator in the real or simulated world. We propose a simpler learning-from-demonstration approach that is able to detect the object to grasp from merely a single demonstration using a convolutional neural network we call GraspNet. In order to increase robustness and decrease the training time even further, we leverage data from previous demonstrations to quickly fine-tune a GrapNet for each new demonstration. We present some preliminary results on a grasping experiment with the Franka Panda cobot for which we can train a GraspNet with only hundreds of train iterations.

##### Abstract (translated by Google)
使用视觉传感器进行机器人抓取的基于学习的方法通常需要收集大型数据集，无论是手动标记还是通过实际或模拟世界中的机器人操纵器的许多试验和错误。我们提出了一个更简单的学习演示方法，它能够检测到的对象只需使用我们称之为GraspNet的卷积神经网络进行的单个演示即可掌握。为了进一步提高鲁棒性并缩短培训时间，我们利用先前演示的数据为每个新演示快速微调GrapNet。我们介绍了一些关于Franka Panda cobot的抓取实验的初步结果，我们可以通过数百次的训练迭代训练GraspNet。

##### URL
[http://arxiv.org/abs/1806.03486](http://arxiv.org/abs/1806.03486)

##### PDF
[http://arxiv.org/pdf/1806.03486](http://arxiv.org/pdf/1806.03486)

