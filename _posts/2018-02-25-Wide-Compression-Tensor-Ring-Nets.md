---
layout: post
title: "Wide Compression: Tensor Ring Nets"
date: 2018-02-25 18:09:04
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Classification Deep_Learning
author: Wenqi Wang, Yifan Sun, Brian Eriksson, Wenlin Wang, Vaneet Aggarwal
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep neural networks. Our results show that our TR-Nets approach {is able to compress LeNet-5 by $11\times$ without losing accuracy}, and can compress the state-of-the-art Wide ResNet by $243\times$ with only 2.3\% degradation in {Cifar10 image classification}. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.

##### Abstract (translated by Google)
深度神经网络已经在各种实际应用中展现了最先进的性能。为了获得性能提升，这些网络已经变得越来越大，包含数百万甚至数十亿个参数以及超过一千个层。权衡是这些大架构需要大量的内存，存储和计算，从而限制了它们的可用性。受最近张量环分解的启发，我们引入张量环网络（TR-Nets），它显着压缩完全连接的层和深度神经网络的卷积层。我们的结果显示，我们的TR-Nets方法{能够将LeNet-5压缩$ 11 / times $而不会损失准确性}，并且可以将最先进的Wide ResNet压缩$ 243 \ times $，只需2.3％ {Cifar10图像分类}降解。总体而言，这种压缩方案在科学计算和深度学习方面表现出前景，特别是对于智能手机，可穿戴设备和物联网设备等新兴资源受限设备。

##### URL
[http://arxiv.org/abs/1802.09052](http://arxiv.org/abs/1802.09052)

##### PDF
[http://arxiv.org/pdf/1802.09052](http://arxiv.org/pdf/1802.09052)

