---
layout: post
title: "Training Group Orthogonal Neural Networks with Privileged Information"
date: 2017-08-18 05:57:25
categories: arXiv_CV
tags: arXiv_CV Knowledge Segmentation CNN Image_Classification Classification
author: Yunpeng Chen, Xiaojie Jin, Jiashi Feng, Shuicheng Yan
mathjax: true
---

* content
{:toc}

##### Abstract
Learning rich and diverse representations is critical for the performance of deep convolutional neural networks (CNNs). In this paper, we consider how to use privileged information to promote inherent diversity of a single CNN model such that the model can learn better representations and offer stronger generalization ability. To this end, we propose a novel group orthogonal convolutional neural network (GoCNN) that learns untangled representations within each layer by exploiting provided privileged information and enhances representation diversity effectively. We take image classification as an example where image segmentation annotations are used as privileged information during the training process. Experiments on two benchmark datasets -- ImageNet and PASCAL VOC -- clearly demonstrate the strong generalization ability of our proposed GoCNN model. On the ImageNet dataset, GoCNN improves the performance of state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses privileged information of 10% of the training images, confirming effectiveness of GoCNN on utilizing available privileged knowledge to train better CNNs.

##### Abstract (translated by Google)
学习丰富多样的表示对深度卷积神经网络（CNN）的性能至关重要。在本文中，我们考虑如何使用特权信息来促进单个CNN模型的内在多样性，使得该模型能够学习更好的表示并提供更强的泛化能力。为此，我们提出了一种新的群组正交卷积神经网络（GoCNN），通过利用提供的特权信息，有效地提高代表性的多样性，从而在每一层内学习不同的表示。我们以图像分类为例，在训练过程中将图像分割注释用作特权信息。对两个基准数据集（ImageNet和PASCAL VOC）的实验清楚地表明了我们提出的GoCNN模型的强泛化能力。在ImageNet数据集中，GoCNN以绝对值1.2％提高了最先进的ResNet-152模型的性能，而仅使用10％训练图像的特权信息，证实了GoCNN利用可用特权知识的有效性培训更好的CNN。

##### URL
[https://arxiv.org/abs/1701.06772](https://arxiv.org/abs/1701.06772)

##### PDF
[https://arxiv.org/pdf/1701.06772](https://arxiv.org/pdf/1701.06772)

