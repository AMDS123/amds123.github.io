---
layout: post
title: "Tensor Product Generation Networks for Deep NLP Modeling"
date: 2017-12-16 12:01:09
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption RNN Deep_Learning
author: Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, Dapeng Wu
mathjax: true
---

* content
{:toc}

##### Abstract
We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture --- the Tensor Product Generation Network (TPGN) --- is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.

##### Abstract (translated by Google)
基于张量产品表示（TPR）的通用技术，在分布式神经网络中对符号结构进行编码和处理，提出了一种基于自然语言处理（NLP）的深度网络设计新方法。提出了一种网络结构 - 张量产品生成网络（TPGN），该网络结构原则上能够执行TPR计算，但是使用无约束深度学习来设计其内部表示。在用于图像标题生成的模型中实例化时，当在COCO数据集上评估时，TPGN优于LSTM基线。支持TPR的结构能够解释内部表示和操作，这被证明包含相当大的语法内容。我们的字幕生成模型可以被解释为生成语法类别的序列，并从编码为分布式表示的计划中按照它们的类别来检索单词。

##### URL
[http://arxiv.org/abs/1709.09118](http://arxiv.org/abs/1709.09118)

##### PDF
[http://arxiv.org/pdf/1709.09118](http://arxiv.org/pdf/1709.09118)

