---
layout: post
title: "Towards Estimating the Upper Bound of Visual-Speech Recognition: The Visual Lip-Reading Feasibility Database"
date: 2017-04-26 09:19:26
categories: arXiv_CV
tags: arXiv_CV Speech_Recognition Recognition
author: Adriana Fernandez-Lopez, Oriol Martinez, Federico M. Sukno
mathjax: true
---

* content
{:toc}

##### Abstract
Speech is the most used communication method between humans and it involves the perception of auditory and visual channels. Automatic speech recognition focuses on interpreting the audio signals, although the video can provide information that is complementary to the audio. Exploiting the visual information, however, has proven challenging. On one hand, researchers have reported that the mapping between phonemes and visemes (visual units) is one-to-many because there are phonemes which are visually similar and indistinguishable between them. On the other hand, it is known that some people are very good lip-readers (e.g: deaf people). We study the limit of visual only speech recognition in controlled conditions. With this goal, we designed a new database in which the speakers are aware of being read and aim to facilitate lip-reading. In the literature, there are discrepancies on whether hearing-impaired people are better lip-readers than normal-hearing people. Then, we analyze if there are differences between the lip-reading abilities of 9 hearing-impaired and 15 normal-hearing people. Finally, human abilities are compared with the performance of a visual automatic speech recognition system. In our tests, hearing-impaired participants outperformed the normal-hearing participants but without reaching statistical significance. Human observers were able to decode 44% of the spoken message. In contrast, the visual only automatic system achieved 20% of word recognition rate. However, if we repeat the comparison in terms of phonemes both obtained very similar recognition rates, just above 50%. This suggests that the gap between human lip-reading and automatic speech-reading might be more related to the use of context than to the ability to interpret mouth appearance.

##### Abstract (translated by Google)
语音是人类使用最多的通信方式，它涉及到听觉和视觉通道的感知。自动语音识别重点在于解释音频信号，尽管视频可以提供与音频互补的信息。然而，利用视觉信息已被证明是具有挑战性的。一方面，研究人员报告说，音素和视觉（视觉单位）之间的映射是一对多的，因为音素在视觉上是相似的，并且在它们之间不可区分。另一方面，众所周知，有些人是非常好的口头读者（如聋人）。我们研究了在可控条件下视觉语言识别的极限。为了达到这个目的，我们设计了一个新的数据库，使发言者知道正在阅读的内容，并且旨在促进口头阅读。在文献中，听力障碍人士是否比听力正常的人更喜欢口头禅。然后，分析9名听力障碍者和15名正常听力者唇读能力是否存在差异。最后，将人的能力与视觉自动语音识别系统的性能进行比较。在我们的测试中，听力受损的参与者表现优于正常听力参与者，但没有达到统计学意义。人类观察者能够解码44％的口头信息。相比之下，视觉自动系统达到了20％的文字识别率。但是，如果我们重复音素比较，则两者的识别率非常相似，只有50％以上。这表明，人类口唇和自动读音之间的差距可能更多地与情境的使用相关，而不是解读口腔外观的能力。

##### URL
[https://arxiv.org/abs/1704.08028](https://arxiv.org/abs/1704.08028)

##### PDF
[https://arxiv.org/pdf/1704.08028](https://arxiv.org/pdf/1704.08028)

