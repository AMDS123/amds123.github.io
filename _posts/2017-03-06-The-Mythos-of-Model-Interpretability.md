---
layout: post
title: "The Mythos of Model Interpretability"
date: 2017-03-06 08:51:10
categories: arXiv_CV
tags: arXiv_CV
author: Zachary C. Lipton
mathjax: true
---

* content
{:toc}

##### Abstract
Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.

##### Abstract (translated by Google)
监督机器学习模型具有显着的预测能力。但是，你能相信你的模型吗？它会在部署中工作吗？还有什么能告诉你这个世界？我们希望模型不仅好，而且可以解释。然而解释的任务似乎没有明确说明。论文提供了不同的，有时不可重复的解释动机，并提供了模型可解释的属性的无数概念。尽管含糊不清，但许多论文都以公理的方式宣扬了解释性，没有进一步的解释。在本文中，我们试图提炼可解释性的话语。首先，我们考察对可解释性有兴趣的动机，发现它们是多样的，偶尔也是不一致的。然后，我们讲述模型的属性和技术思想赋予可解释性，确定对人的透明度和事后解释作为竞争的概念。在整个过程中，我们讨论不同概念的可行性和可取性，并质疑线性模型是可解释的，而深层神经网络则不是。

##### URL
[https://arxiv.org/abs/1606.03490](https://arxiv.org/abs/1606.03490)

##### PDF
[https://arxiv.org/pdf/1606.03490](https://arxiv.org/pdf/1606.03490)

