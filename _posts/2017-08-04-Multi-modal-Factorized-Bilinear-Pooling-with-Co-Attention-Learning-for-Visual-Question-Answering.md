---
layout: post
title: "Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering"
date: 2017-08-04 12:17:49
categories: arXiv_CV
tags: arXiv_CV VQA
author: Zhou Yu, Jun Yu, Jianping Fan, Dacheng Tao
mathjax: true
---

* content
{:toc}

##### Abstract
Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a fine-grained manner and questions and to fuse these multi-modal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multi-modal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For fine-grained image and question representation, we develop a co-attention mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a unified model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-the-art performance on the real-world VQA dataset. Code available at this https URL

##### Abstract (translated by Google)
视觉问答（VQA）具有挑战性，因为它需要同时理解图像的视觉内容和问题的文本内容。用于以细粒度的方式表达图像和问题的方法以及融合这些多模态特征的方法在性能中起关键作用。已经证明基于双线性池的模型优于传统的VQA线性模型，但是它们的高维表示和高计算复杂度可能严重限制了其在实践中的适用性。对于多模态特征融合，在这里我们开发了一种多模式分比双线性（MFB）池方法，以有效地组合多模态特征，与其他双线性池方法相比，VQA具有更好的性能。对于细粒度的图像和问题表示，我们使用端到端的深度网络架构开发了一个共同关注机制，共同学习图像和问题的关注。将所提出的MFB方法与在新网络架构中的共同学习相结合，为VQA提供了一个统一的模型。我们的实验结果表明，具有共同关注模型的单个MFB在实际的VQA数据集上实现了新的最新的性能。此https网址提供的代码

##### URL
[https://arxiv.org/abs/1708.01471](https://arxiv.org/abs/1708.01471)

##### PDF
[https://arxiv.org/pdf/1708.01471](https://arxiv.org/pdf/1708.01471)

