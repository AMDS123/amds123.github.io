---
layout: post
title: "Scalable Image Retrieval by Sparse Product Quantization"
date: 2016-03-15 09:53:32
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval Sparse
author: Qingqun Ning, Jianke Zhu, Zhiyuan Zhong, Steven C.H. Hoi, Chun Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional feature indexing and retrieval is the crux of large-scale image retrieval. A recent promising technique is Product Quantization, which attempts to index high-dimensional image features by decomposing the feature space into a Cartesian product of low dimensional subspaces and quantizing each of them separately. Despite the promising results reported, their quantization approach follows the typical hard assignment of traditional quantization methods, which may result in large quantization errors and thus inferior search performance. Unlike the existing approaches, in this paper, we propose a novel approach called Sparse Product Quantization (SPQ) to encoding the high-dimensional feature vectors into sparse representation. We optimize the sparse representations of the feature vectors by minimizing their quantization errors, making the resulting representation is essentially close to the original data in practice. Experiments show that the proposed SPQ technique is not only able to compress data, but also an effective encoding technique. We obtain state-of-the-art results for ANN search on four public image datasets and the promising results of content-based image retrieval further validate the efficacy of our proposed method.

##### Abstract (translated by Google)
高维特征索引和检索的快速近似最近邻（ANN）搜索技术是大规模图像检索的关键。产品量化是一种最近很有前途的技术，它试图通过将特征空间分解成低维子空间的笛卡儿积并分别量化它们中的每一个来量化高维图像特征。尽管报道了有希望的结果，但是它们的量化方法遵循传统量化方法的典型硬分配，这可能导致较大的量化误差并因此导致较差的搜索性能。与现有的方法不同，在本文中，我们提出了一种称为稀疏产品量化（Sparse Product Quantization，SPQ）的新方法来将高维特征向量编码为稀疏表示。我们通过最小化它们的量化误差来优化特征向量的稀疏表示，使得得到的表示在实践中基本上接近于原始数据。实验表明，所提出的SPQ技术不仅能够压缩数据，而且是一种有效的编码技术。我们在四个公共图像数据集上获得最新的神经网络搜索结果，基于内容的图像检索有希望的结果进一步验证了我们提出的方法的有效性。

##### URL
[https://arxiv.org/abs/1603.04614](https://arxiv.org/abs/1603.04614)

##### PDF
[https://arxiv.org/pdf/1603.04614](https://arxiv.org/pdf/1603.04614)

