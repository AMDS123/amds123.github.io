---
layout: post
title: "An Introduction to Deep Visual Explanation"
date: 2017-11-26 22:54:18
categories: arXiv_CV
tags: arXiv_CV Image_Classification Inference Classification Deep_Learning
author: Housam Khalifa Bashier Babiker, Randy Goebel
mathjax: true
---

* content
{:toc}

##### Abstract
The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call "deep visual explanation" (DVE). "Deep," because it is the development and performance of deep neural network models that we want to understand. "Visual," because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and "Explanation," because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.

##### Abstract (translated by Google)
深度学习对复杂的监督学习问题的实际影响已经非常显着，几乎每一个人工智能问题或者至少其中的一部分都已经被重写为深度学习问题。应用程序的吸引力是显着的，但是这种吸引力正在越来越受到一些人称之为可解释性挑战的挑战，或者更普遍地说是更传统的可调试性挑战：如果深度学习过程的结果产生意想不到的结果（例如，低于预期一个分类器），那么在理论或工具方面几乎没有什么可用来帮助调查这种意外行为的潜在原因，特别是当这种行为可能影响人们的生活时。我们描述了一个初步框架来帮助解决这个问题，我们称之为“深度视觉解释”（DVE）。 “深”，因为它是我们想要了解的深层神经网络模型的发展和表现。 “视觉”，因为我们相信，通过适当的可视化技术和“解释”提供了对复杂多维模型的最迅速的了解，因为从插入印刷陈述到解释性假说的诱人推理的插图中，我们相信理解深度学习的关键依赖于对学习深度模型的表现行为的假设的识别和暴露。在我们的初步框架的阐述中，我们使用相对简单的图像分类实例和在深层建模场景的初始配置中的各种选择。通过仔细但不复杂的仪器，我们使用可视化揭示了深层模型的分类结果，并且还展示了一个解释性的潜在应用的初步结果。

##### URL
[https://arxiv.org/abs/1711.09482](https://arxiv.org/abs/1711.09482)

##### PDF
[https://arxiv.org/pdf/1711.09482](https://arxiv.org/pdf/1711.09482)

