---
layout: post
title: "A Powerful Generative Model Using Random Weights for the Deep Image Representation"
date: 2016-06-16 06:53:55
categories: arXiv_CV
tags: arXiv_CV Image_Caption Knowledge CNN Relation
author: Kun He, Yan Wang, John Hopcroft
mathjax: true
---

* content
{:toc}

##### Abstract
To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.

##### Abstract (translated by Google)
由于培训，深度可视化的成功程度如何？我们可以使用未经训练的随机权重网络来进行深度可视化吗？为了解决这个问题，我们使用未经训练的随机加权卷积神经网络为三个流行的深度可视化任务探索新的强大的生成模型。首先，我们对特征空间中的表示进行反转，并从白噪声输入重建图像。重构质量在统计上高于相同架构的训练良好的网络上应用的相同方法。接下来，我们使用缩放的多层表示的相关性来合成纹理，并且我们的结果几乎与原始自然纹理和基于训练网络的合成纹理不可区分。第三，通过以各种艺术作品的风格重现一幅图像的内容，我们创造出具有高度感性质量的艺术形象，与Gatys等人的先前工作极具竞争力。在预训练网络上。就我们所知，这是使用未经训练的深度神经网络的图像表示的第一次演示。我们的工作提供了一个新的和迷人的工具来研究深层网络体系结构的表示，并揭示了对深度可视化的新认识。

##### URL
[https://arxiv.org/abs/1606.04801](https://arxiv.org/abs/1606.04801)

##### PDF
[https://arxiv.org/pdf/1606.04801](https://arxiv.org/pdf/1606.04801)

