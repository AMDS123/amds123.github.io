---
layout: post
title: "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context"
date: 2017-06-25 20:00:54
categories: arXiv_CL
tags: arXiv_CL Embedding Represenation_Learning
author: Shyam Upadhyay, Kai-Wei Chang, Matt Taddy, Adam Kalai, James Zou
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art mono-lingual model trained on five times more training data.

##### Abstract (translated by Google)
单词嵌入代表了一个单词作为一个向量空间中的一个点，已经成为无处不在的几个NLP任务。最近的一系列工作使用双语（两种语言）语料库，通过利用交叉语言信号来帮助识别身份，从而为每个词的意义学习不同的媒介。 （a）使用多种语言（即，两种以上的语言）语料库来显着改善对于双语信息所实现的意义嵌入，并且（b）使用多语言贝叶斯非参数算法来改善多义词嵌入，以数据驱动的方式使用原则性方法来学习每个词的可变数量的感觉。我们是能够有效利用多语言语料库进行多义表征学习的第一种方法。实验表明，多语言培训显着提高了单语和双语培训的绩效，让我们结合不同的平行语料库来利用多语言背景。多语种训练产生了与五倍于训练数据的现有技术单语模型相当的性能。

##### URL
[https://arxiv.org/abs/1706.08160](https://arxiv.org/abs/1706.08160)

##### PDF
[https://arxiv.org/pdf/1706.08160](https://arxiv.org/pdf/1706.08160)

