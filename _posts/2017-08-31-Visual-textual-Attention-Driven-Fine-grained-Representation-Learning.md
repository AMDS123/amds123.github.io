---
layout: post
title: "Visual-textual Attention Driven Fine-grained Representation Learning"
date: 2017-08-31 12:41:55
categories: arXiv_CV
tags: arXiv_CV Adversarial Object_Detection Knowledge Attention GAN Image_Classification Represenation_Learning Classification Detection
author: Xiangteng He, Yuxin Peng
mathjax: true
---

* content
{:toc}

##### Abstract
Fine-grained image classification is to recognize hundreds of subcategories belonging to the same basic-level category, which is a highly challenging task due to the quite subtle visual distinctions among similar subcategories. Most existing methods generally learn part detectors to discover discriminative regions for better performance. However, not all localized parts are beneficial and indispensable for classification, and the setting for number of part detectors relies heavily on prior knowledge as well as experimental results. As is known to all, when we describe the object of an image into text via natural language, we only focus on the pivotal characteristics, and rarely pay attention to common characteristics as well as the background areas. This is an involuntary transfer from human visual attention to textual attention, which leads to the fact that textual attention tells us how many and which parts are discriminative and significant. So textual attention of natural language descriptions could help us to discover visual attention in image. Inspired by this, we propose a visual-textual attention driven fine-grained representation learning (VTA) approach, and its main contributions are: (1) Fine-grained visual-textual pattern mining devotes to discovering discriminative visual-textual pairwise information for boosting classification through jointly modeling vision and text with generative adversarial networks (GANs), which automatically and adaptively discovers discriminative parts. (2) Visual-textual representation learning jointly combine visual and textual information, which preserves the intra-modality and inter-modality information to generate complementary fine-grained representation, and further improve classification performance. Experiments on the two widely-used datasets demonstrate the effectiveness of our VTA approach, which achieves the best classification accuracy.

##### Abstract (translated by Google)
细粒度的图像分类是识别属于同一基本类别的数百个子类别，这是一个非常具有挑战性的任务，由于类似的子类别之间的相当微妙的视觉差异。大多数现有的方法通常学习部分检测器来发现区分区域以获得更好的性能然而，并非所有的局部化部件对于分类都是有益的和不可缺少的，部件检测器的数量设置很大程度上依赖于先验知识以及实验结果。众所周知，当我们通过自然语言将图像的对象描述为文本时，我们只关注关键特征，很少关注共同特征以及背景区域。这是从人类视觉注意力到文本注意力的非自愿转移，这导致文本注意力告诉我们有多少和哪些部分是有区别的和重要的。因此，自然语言描述的文本注意可以帮助我们发现图像中的视觉注意力。受其启发，我们提出一种视觉文本注意力驱动的细粒度表征学习（VTA）方法，其主要贡献有：（1）细粒度视觉 - 文本模式挖掘致力于发现区分视觉文本配对信息通过联合建模视觉和文本与生成对抗网络（GAN）分类，自动和自适应地发现歧视性的部分。 （2）视觉文本表示学习将视觉和文本信息联合起来，保留了模式内和模态间的信息，产生互补的细粒度表示，进一步提高了分类性能。对两个广泛使用的数据集进行实验证明了我们的VTA方法的有效性，从而达到了最佳的分类精度。

##### URL
[https://arxiv.org/abs/1709.00340](https://arxiv.org/abs/1709.00340)

##### PDF
[https://arxiv.org/pdf/1709.00340](https://arxiv.org/pdf/1709.00340)

