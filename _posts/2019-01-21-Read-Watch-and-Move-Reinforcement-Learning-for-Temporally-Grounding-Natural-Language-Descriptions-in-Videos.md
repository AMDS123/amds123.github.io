---
layout: post
title: "Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos"
date: 2019-01-21 09:00:11
categories: arXiv_CV
tags: arXiv_CV Reinforcement_Learning Caption
author: Dongliang He, Xiang Zhao, Jizhou Huang, Fu Li, Xiao Liu, Shilei Wen
mathjax: true
---

* content
{:toc}

##### Abstract
The task of video grounding, which temporally localizes a natural language description in a video, plays an important role in understanding videos. Existing studies have adopted strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a pre-segmented video, which inevitably suffer from exhaustively enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy. Specifically, we propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training. Our proposed framework achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1901.06829](http://arxiv.org/abs/1901.06829)

##### PDF
[http://arxiv.org/pdf/1901.06829](http://arxiv.org/pdf/1901.06829)

