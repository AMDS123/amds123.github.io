---
layout: post
title: "TVQA: Localized, Compositional Video Question Answering"
date: 2018-09-05 19:14:11
categories: arXiv_AI
tags: arXiv_AI QA VQA
author: Jie Lei, Licheng Yu, Mohit Bansal, Tamara L. Berg
mathjax: true
---

* content
{:toc}

##### Abstract
Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at <a href="http://tvqa.cs.unc.edu.">this http URL</a>

##### Abstract (translated by Google)
近年来，人们越来越关注基于图像的问答（QA）任务。但是，由于数据限制，基于视频的QA的工作要少得多。在本文中，我们提出了基于6个热门电视节目的大规模视频质量保证数据集TVQA。 TVQA由来自21,793个剪辑的152,545个QA对组成，跨越460小时的视频。问题的设计本质上是构图，要求系统共同定位剪辑中的相关时刻，理解基于字幕的对话，并识别相关的视觉概念。我们提供了这个新数据集的分析以及几个基线和用于TVQA任务的多流端到端可训练神经网络框架。该数据集可在<a href="http://tvqa.cs.unc.edu.">此http URL </a>上公开获取

##### URL
[http://arxiv.org/abs/1809.01696](http://arxiv.org/abs/1809.01696)

##### PDF
[http://arxiv.org/pdf/1809.01696](http://arxiv.org/pdf/1809.01696)

