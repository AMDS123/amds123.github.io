---
layout: post
title: "Hybrid Model For Word Prediction Using Naive Bayes and Latent Information"
date: 2018-03-02 18:34:08
categories: arXiv_CL
tags: arXiv_CL Attention Deep_Learning Prediction
author: Henrique X. Goulart, Mauro D. L. Tosi, Daniel Soares Gon&#xe7;alves, Rodrigo F. Maia, Guilherme A. Wachs-Lopes
mathjax: true
---

* content
{:toc}

##### Abstract
Historically, the Natural Language Processing area has been given too much attention by many researchers. One of the main motivation beyond this interest is related to the word prediction problem, which states that given a set words in a sentence, one can recommend the next word. In literature, this problem is solved by methods based on syntactic or semantic analysis. Solely, each of these analysis cannot achieve practical results for end-user applications. For instance, the Latent Semantic Analysis can handle semantic features of text, but cannot suggest words considering syntactical rules. On the other hand, there are models that treat both methods together and achieve state-of-the-art results, e.g. Deep Learning. These models can demand high computational effort, which can make the model infeasible for certain types of applications. With the advance of the technology and mathematical models, it is possible to develop faster systems with more accuracy. This work proposes a hybrid word suggestion model, based on Naive Bayes and Latent Semantic Analysis, considering neighbouring words around unfilled gaps. Results show that this model could achieve 44.2% of accuracy in the MSR Sentence Completion Challenge.

##### Abstract (translated by Google)
历史上，自然语言处理领域受到了许多研究人员的过分关注。超出这种兴趣的主要动机之一与单词预测问题有关，该问题指出，给定一个句子中的一个单词，可以推荐下一个单词。在文献中，这个问题通过基于句法或语义分析的方法来解决。单单，这些分析都不能为最终用户应用取得实际结果。例如，潜在语义分析可以处理文本的语义特征，但不能根据语法规则提出单词。另一方面，有一些模型将两种方法结合在一起并获得最新的结果，例如，深度学习。这些模型可能需要很高的计算工作量，这可能使模型对某些类型的应用程序不可行。随着技术和数学模型的进步，有可能开发出更高精度的更快速的系统。这项工作提出了一种基于朴素贝叶斯和潜在语义分析的混合词建议模型，考虑了未填充空白周围的相邻词语。结果表明，该模型在MSR句子完成挑战中可以达到44.2％的准确率。

##### URL
[http://arxiv.org/abs/1803.00985](http://arxiv.org/abs/1803.00985)

##### PDF
[http://arxiv.org/pdf/1803.00985](http://arxiv.org/pdf/1803.00985)

