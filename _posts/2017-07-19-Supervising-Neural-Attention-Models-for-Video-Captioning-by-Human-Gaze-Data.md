---
layout: post
title: "Supervising Neural Attention Models for Video Captioning by Human Gaze Data"
date: 2017-07-19 11:44:36
categories: arXiv_CV
tags: arXiv_CV Video_Caption Attention Tracking Caption Prediction
author: Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, Gunhee Kim
mathjax: true
---

* content
{:toc}

##### Abstract
The attention mechanisms in deep neural networks are inspired by human's attention that sequentially focuses on the most relevant parts of the information over time to generate prediction output. The attention parameters in those models are implicitly trained in an end-to-end manner, yet there have been few trials to explicitly incorporate human gaze tracking to supervise the attention models. In this paper, we investigate whether attention models can benefit from explicit human gaze labels, especially for the task of video captioning. We collect a new dataset called VAS, consisting of movie clips, and corresponding multiple descriptive sentences along with human gaze tracking data. We propose a video captioning model named Gaze Encoding Attention Network (GEAN) that can leverage gaze tracking information to provide the spatial and temporal attention for sentence generation. Through evaluation of language similarity metrics and human assessment via Amazon mechanical Turk, we demonstrate that spatial attentions guided by human gaze data indeed improve the performance of multiple captioning methods. Moreover, we show that the proposed approach achieves the state-of-the-art performance for both gaze prediction and video captioning not only in our VAS dataset but also in standard datasets (e.g. LSMDC and Hollywood2).

##### Abstract (translated by Google)
深度神经网络中的注意机制受到人类注意力的启发，其随着时间的推移依次关注信息的最相关部分以生成预测输出。这些模型中的注意力参数是以端到端的方式进行隐式训练的，但很少有试验明确将人类注视跟踪用于监督注意力模型。在本文中，我们研究了注意力模型是否可以从明确的人类凝视标签中受益，特别是对于视频字幕的任务。我们收集了一个名为VAS的新数据集，其中包括影片剪辑，以及相应的多个描述性句子以及人类凝视跟踪数据。我们提出了一种名为Gaze编码注意网络（GEAN）的视频字幕模型，它可以利用注视跟踪信息来提供句子生成的空间和时间关注。通过亚马逊机械土耳其人对语言相似性度量和人工评估的评估，我们证明了人类注视数据引导的空间注意力确实提高了多种字幕方法的性能。此外，我们表明，所提出的方法不仅在我们的VAS数据集中而且在标准数据集（例如LSMDC和Hollywood2）中实现了凝视预测和视频字幕的最先进性能。

##### URL
[https://arxiv.org/abs/1707.06029](https://arxiv.org/abs/1707.06029)

##### PDF
[https://arxiv.org/pdf/1707.06029](https://arxiv.org/pdf/1707.06029)

