---
layout: post
title: "A Comparative Study for the Weighted Nuclear Norm Minimization and Nuclear Norm Minimization"
date: 2017-05-17 01:39:00
categories: arXiv_CV
tags: arXiv_CV Sparse Quantitative
author: Zhiyuan Zha, Xinggan Zhang, Yu Wu, Qiong Wang, Lan Tang
mathjax: true
---

* content
{:toc}

##### Abstract
Nuclear norm minimization (NNM) tends to over-shrink the rank components and treats the different rank components equally, thus limits its capability and flexibility. Recent studies have shown that the weighted nuclear norm minimization (WNNM) is expected to be more accurate than NNM. However, it still lacks a plausible mathematical explanation why WNNM is more accurate than NNM. This paper analyzes the WNNM and NNM from the perspective of the group sparse representation (GSR). In particular, an adaptive dictionary for each group is designed to connect the rank minimization and GSR models. Then, we prove that the rank minimization model is equivalent to GSR model. Based on that conclusion, we show mathematically that WNNM is more accurate than NNM. To make the proposed model tractable and robust, the alternative direction multiplier method (ADMM) framework is developed to solve the proposed model. We exploit the proposed scheme to three low level vision tasks, including image deblurring, image inpainting and image compressive sensing (CS) recovery. Experimental results demonstrate that the proposed scheme outperforms many state-of-the-art methods in terms of both quantitative measures and visual perception quality.

##### Abstract (translated by Google)
核范数最小化（NNM）倾向于过分缩小等级分量并且平等地处理不同的等级分量，因此限制了其能力和灵活性。最近的研究表明，加权核范数最小化（WNNM）预计比NNM更准确。然而，为什么WNNM比NNM更准确，它仍然缺乏合理的数学解释。本文从群体稀疏表示（GSR）的角度分析了WNNM和NNM。具体而言，每个组的自适应字典被设计为连接秩最小化和GSR模型。然后，证明秩最小化模型等价于GSR模型。基于这个结论，我们从数学上表明WNNM比NNM更准确。为了使所提出的模型易于处理和鲁棒性，开发了替代方向乘子法（ADMM）框架来解决所提出的模型。我们利用所提出的方案来进行三个低级视觉任务，包括图像去模糊，图像修复和图像压缩感知（CS）恢复。实验结果表明，所提出的方案在定量测量和视觉感知质量方面均优于许多最先进的方法。

##### URL
[https://arxiv.org/abs/1608.04517](https://arxiv.org/abs/1608.04517)

##### PDF
[https://arxiv.org/pdf/1608.04517](https://arxiv.org/pdf/1608.04517)

