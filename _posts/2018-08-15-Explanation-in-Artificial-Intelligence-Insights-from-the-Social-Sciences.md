---
layout: post
title: "Explanation in Artificial Intelligence: Insights from the Social Sciences"
date: 2018-08-15 00:50:00
categories: arXiv_AI
tags: arXiv_AI Review
author: Tim Miller
mathjax: true
---

* content
{:toc}

##### Abstract
There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.

##### Abstract (translated by Google)
随着研究人员和从业人员试图使他们的算法更容易理解，最近在可解释的人工智能领域再次出现了复苏。这项研究的大部分内容都集中在明确地向人类观察者解释决策或行动，并且说看人类如何相互解释可以作为人工智能解释的有用起点不应该引起争议。然而，可以公平地说，可解释的人工智能中的大部分工作仅使用研究人员对什么构成“好”解释的直觉。关于人们如何定义，生成，选择，评估和呈现解释，哲学，心理学和认知科学中存在大量有价值的研究机构，这些研究认为人们对解释过程采用某些认知偏见和社会期望。本文认为，可解释的人工智能领域应建立在这一现有研究的基础上，并回顾研究这些主题的哲学，认知心理学/科学和社会心理学相关论文。它提出了一些重要的发现，并讨论了如何将这些发现与可解释的人工智能相结合。

##### URL
[http://arxiv.org/abs/1706.07269](http://arxiv.org/abs/1706.07269)

##### PDF
[http://arxiv.org/pdf/1706.07269](http://arxiv.org/pdf/1706.07269)

