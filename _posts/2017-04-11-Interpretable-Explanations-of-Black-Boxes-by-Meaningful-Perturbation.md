---
layout: post
title: "Interpretable Explanations of Black Boxes by Meaningful Perturbation"
date: 2017-04-11 14:15:20
categories: arXiv_CV
tags: arXiv_CV Salient Prediction
author: Ruth Fong, Andrea Vedaldi
mathjax: true
---

* content
{:toc}

##### Abstract
As machine learning algorithms are increasingly applied to high impact yet high risk tasks, e.g. problems in health, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we introduce a paradigm that learns the minimally salient part of an image by directly editing it and learning from the corresponding changes to its output. Unlike previous works, our method is model-agnostic and testable because it is grounded in replicable image perturbations.

##### Abstract (translated by Google)
随着机器学习算法越来越多地应用于高影响但高风险的任务，例如，在健康问题上，研究人员可以解释这些算法如何达到他们的预测是至关重要的。近年来，已经开发出许多图像显着性方法来总结高度复杂的神经网络在图像中“看”的位置以作为其预测的证据。然而，这些技术受到启发性和建筑限制的限制。在本文中，我们做出了两个主要的贡献：首先，我们提出了一个通用框架来学习任何黑箱算法的各种解释。其次，我们引入一个范例，通过直接编辑图像，并从相应的输出变化中学习，学习图像的最小显着部分。与以前的作品不同，我们的方法是模型不可知和可测试的，因为它基于可复制的图像干扰。

##### URL
[https://arxiv.org/abs/1704.03296](https://arxiv.org/abs/1704.03296)

##### PDF
[https://arxiv.org/pdf/1704.03296](https://arxiv.org/pdf/1704.03296)

