---
layout: post
title: "LSTM Neural Reordering Feature for Statistical Machine Translation"
date: 2016-06-16 10:01:49
categories: arXiv_CL
tags: arXiv_CL NMT RNN Language_Model Prediction
author: Yiming Cui, Shijin Wang, Jianfeng Li
mathjax: true
---

* content
{:toc}

##### Abstract
Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.

##### Abstract (translated by Google)
人工神经网络是强大的模型，已被广泛应用于机器翻译的许多方面，如语言建模和翻译建模。尽管在这些领域已经取得了显着的进步，但重排序问题在统计机器翻译中仍然是一个挑战。在本文中，我们提出了一种新的神经重新排序模型，直接模拟单词对和对齐。通过利用LSTM递归神经网络，可以学习更长的上下文来重新排序预测。 NIST OpenMT12阿拉伯语 - 英语和汉英 - 英语1000最佳重新调整任务的实验结果表明，我们的LSTM神经重新排序功能是强大的，并实现了对各种基准系统的重大改进。

##### URL
[https://arxiv.org/abs/1512.00177](https://arxiv.org/abs/1512.00177)

##### PDF
[https://arxiv.org/pdf/1512.00177](https://arxiv.org/pdf/1512.00177)

