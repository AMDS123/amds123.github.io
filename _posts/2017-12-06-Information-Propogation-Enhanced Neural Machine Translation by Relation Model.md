---
layout: post
title: 'Information-Propogation-Enhanced Neural Machine Translation by Relation Model'
date: 2017-12-06 02:49:44
categories: arXiv_CL
tags: arXiv_CL
author: Wen Zhang, Jiawei Hu, Yang Feng, Qun Liu
---

* content
{:toc}

##### Abstract
Even though sequence-to-sequence neural machine translation (NMT) model have achieved state-of-art performance in the recent fewer years, but it is widely concerned that the recurrent neural network (RNN) units are very hard to capture the long-distance state information, which means RNN can hardly find the feature with long term dependency as the sequence becomes longer. Similarly, convolutional neural network (CNN) is introduced into NMT for speeding recently, however, CNN focus on capturing the local feature of the sequence; To relieve this issue, we incorporate a relation network into the standard encoder-decoder framework to enhance information-propogation in neural network, ensuring that the information of the source sentence can flow into the decoder adequately. Experiments show that proposed framework outperforms the statistical MT model and the state-of-art NMT model significantly on two data sets with different scales.

##### Abstract (translated by Google)
尽管序列 - 序列神经机器翻译（NMT）模型在最近的几年中取得了最新的性能，但是人们普遍认为递归神经网络（RNN）单元很难捕获长期的数据，距离状态信息，这意味着随着序列变长，RNN很难找到长期依赖的特征。同样，卷积神经网络（CNN）最近也被引入到NMT中，但CNN侧重于捕获序列的局部特征;为了解决这个问题，我们将关系网络纳入到标准的编解码器框架中，以加强神经网络中的信息传播，保证源句子的信息能够充分流入解码器。实验表明，所提出的框架在两个不同尺度的数据集上均显着优于统计的MT模型和最先进的NMT模型。

##### URL
[https://arxiv.org/abs/1709.01766](https://arxiv.org/abs/1709.01766)

