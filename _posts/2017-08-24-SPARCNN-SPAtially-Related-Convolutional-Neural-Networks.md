---
layout: post
title: "SPARCNN: SPAtially Related Convolutional Neural Networks"
date: 2017-08-24 18:31:03
categories: arXiv_CV
tags: arXiv_CV Object_Detection Knowledge CNN Classification Detection Relation
author: JT Turner, Kalyan Moy Gupta, David Aha
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to accurately detect and classify objects at varying pixel sizes in cluttered scenes is crucial to many Navy applications. However, detection performance of existing state-of the-art approaches such as convolutional neural networks (CNNs) degrade and suffer when applied to such cluttered and multi-object detection tasks. We conjecture that spatial relationships between objects in an image could be exploited to significantly improve detection accuracy, an approach that had not yet been considered by any existing techniques (to the best of our knowledge) at the time the research was conducted. We introduce a detection and classification technique called Spatially Related Detection with Convolutional Neural Networks (SPARCNN) that learns and exploits a probabilistic representation of inter-object spatial configurations within images from training sets for more effective region proposals to use with state-of-the-art CNNs. Our empirical evaluation of SPARCNN on the VOC 2007 dataset shows that it increases classification accuracy by 8% when compared to a region proposal technique that does not exploit spatial relations. More importantly, we obtained a higher performance boost of 18.8% when task difficulty in the test set is increased by including highly obscured objects and increased image clutter.

##### Abstract (translated by Google)
在杂乱的场景中精确检测和分类不同像素尺寸的物体的能力，对于许多海军的应用至关重要。然而，现有技术如卷积神经网络（CNN）的现有技术的检测性能在应用于这种杂乱的多目标检测任务时降级并受损。我们推测图像中物体之间的空间关系可以被利用来显着提高检测的准确性，这是在研究进行的时候，任何现有技术尚未考虑的方法（据我们所知）。我们引入了一种称为空间相关检测和卷积神经网络（SPARCNN）的检测和分类技术，学习和利用来自训练集的图像内的对象间空间配置的概率表示，以用于更有效的区域提议，艺术CNNs。我们对VOC 2007数据集上SPARCNN的实证评估显示，与不利用空间关系的地区建议技术相比，它将分类准确度提高了8％。更重要的是，当测试集中的任务难度增加时，由于包含高度模糊的对象和增加的图像杂波，我们获得了更高的性能提升18.8％。

##### URL
[https://arxiv.org/abs/1708.07522](https://arxiv.org/abs/1708.07522)

##### PDF
[https://arxiv.org/pdf/1708.07522](https://arxiv.org/pdf/1708.07522)

