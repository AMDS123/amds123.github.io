---
layout: post
title: "Abstractive Summarization Using Attentive Neural Techniques"
date: 2018-10-20 18:24:48
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Jacob Krantz, Jugal Kalita
mathjax: true
---

* content
{:toc}

##### Abstract
In a world of proliferating data, the ability to rapidly summarize text is growing in importance. Automatic summarization of text can be thought of as a sequence to sequence problem. Another area of natural language processing that solves a sequence to sequence problem is machine translation, which is rapidly evolving due to the development of attention-based encoder-decoder networks. This work applies these modern techniques to abstractive summarization. We perform analysis on various attention mechanisms for summarization with the goal of developing an approach and architecture aimed at improving the state of the art. In particular, we modify and optimize a translation model with self-attention for generating abstractive sentence summaries. The effectiveness of this base model along with attention variants is compared and analyzed in the context of standardized evaluation sets and test metrics. However, we show that these metrics are limited in their ability to effectively score abstractive summaries, and propose a new approach based on the intuition that an abstractive model requires an abstractive evaluation.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.08838](http://arxiv.org/abs/1810.08838)

##### PDF
[http://arxiv.org/pdf/1810.08838](http://arxiv.org/pdf/1810.08838)

