---
layout: post
title: "Analyzing the Affect of a Group of People Using Multi-modal Framework"
date: 2016-10-13 21:43:39
categories: arXiv_CV
tags: arXiv_CV Face Recognition
author: Xiaohua Huang, Abhinav Dhall, Xin Liu, Guoying Zhao, Jingang Shi, Roland Goecke, Matti Pietikainen
mathjax: true
---

* content
{:toc}

##### Abstract
Millions of images on the web enable us to explore images from social events such as a family party, thus it is of interest to understand and model the affect exhibited by a group of people in images. But analysis of the affect expressed by multiple people is challenging due to varied indoor and outdoor settings, and interactions taking place between various numbers of people. A few existing works on Group-level Emotion Recognition (GER) have investigated on face-level information. Due to the challenging environments, face may not provide enough information to GER. Relatively few studies have investigated multi-modal GER. Therefore, we propose a novel multi-modal approach based on a new feature description for understanding emotional state of a group of people in an image. In this paper, we firstly exploit three kinds of rich information containing face, upperbody and scene in a group-level image. Furthermore, in order to integrate multiple person's information in a group-level image, we propose an information aggregation method to generate three features for face, upperbody and scene, respectively. We fuse face, upperbody and scene information for robustness of GER against the challenging environments. Intensive experiments are performed on two challenging group-level emotion databases to investigate the role of face, upperbody and scene as well as multi-modal framework. Experimental results demonstrate that our framework achieves very promising performance for GER.

##### Abstract (translated by Google)
网络上数以百万计的图像使我们能够从家庭聚会等社交活动中探索图像，因此了解和模拟一群人在图像中展示的情感是有趣的。但是，分析由多个人表达的影响是具有挑战性的，因为不同的室内和室外环境以及不同数量的人之间发生的相互作用。目前关于群体层次情绪识别（GER）的一些研究已经对人脸信息进行了调查。由于具有挑战性的环境，脸部可能无法提供足够的信息给GER。相对较少的研究调查了多模GER。因此，我们提出了一种基于新特征描述的多模态方法来理解图像中一组人的情绪状态。在本文中，我们首先利用三种丰富的包含脸部，上身和场景的信息在一个组的图像。此外，为了将多个人的信息集成到一个组的图像中，我们提出了一种信息聚合的方法，分别为人脸，上身和场景生成三个特征。我们将GER的面部，上身和场景信息融合在一起，以应对具有挑战性的环境。对两个具有挑战性的群体情感数据库进行强化实验，以调查面部，上身和场景以及多模态框架的作用。实验结果表明，我们的框架为GER取得了非常有希望的性能。

##### URL
[https://arxiv.org/abs/1610.03640](https://arxiv.org/abs/1610.03640)

##### PDF
[https://arxiv.org/pdf/1610.03640](https://arxiv.org/pdf/1610.03640)

