---
layout: post
title: "Semi-Supervised Model Training for Unbounded Conversational Speech Recognition"
date: 2017-05-26 21:10:15
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition Language_Model Recognition
author: Shane Walker, Morten Pedersen, Iroro Orife, Jason Flaks
mathjax: true
---

* content
{:toc}

##### Abstract
For conversational large-vocabulary continuous speech recognition (LVCSR) tasks, up to about two thousand hours of audio is commonly used to train state of the art models. Collection of labeled conversational audio however, is prohibitively expensive, laborious and error-prone. Furthermore, academic corpora like Fisher English (2004) or Switchboard (1992) are inadequate to train models with sufficient accuracy in the unbounded space of conversational speech. These corpora are also timeworn due to dated acoustic telephony features and the rapid advancement of colloquial vocabulary and idiomatic speech over the last decades. Utilizing the colossal scale of our unlabeled telephony dataset, we propose a technique to construct a modern, high quality conversational speech training corpus on the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and language model training. We describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test corpus of 7K manually transcribed utterances. We show relative word error rate (WER) reductions of {35%, 19%} on {agent, caller} utterances over our seed model and 5% absolute WER improvements over IBM Watson STT on this conversational speech task.

##### Abstract (translated by Google)
对于会话式大词汇量连续语音识别（LVCSR）任务，通常使用长达约2000小时的音频来训练最先进的模型。但是，标记会话音频的收集过于昂贵，费力且容易出错。此外，诸如Fisher English（2004）或Switchboard（1992）这样的学术语料库不足以在对话语言的无限空间中以足够的准确度训练模型。由于过时的声学电话功能以及过去几十年口语词汇和惯用语言的快速发展，这些语料库也经历了磨练。利用我们未标记的电话数据集的庞大规模，我们提出了一种技术来构建现代的，高质量的对话式语音训练语料库，语音和语言模型训练的语音数量为数以亿计（或数万小时）。我们描述数据收集，选择和培训，评估我们更新的语音识别系统的结果在7K手动转录的话语的测试语料库。我们在种子模型上显示了{35％，19％}的{代理，主叫}言语的相对误码率（WER）减少，以及在这个对话式语音任务上相对于IBM Watson STT的5％绝对WER改进。

##### URL
[https://arxiv.org/abs/1705.09724](https://arxiv.org/abs/1705.09724)

##### PDF
[https://arxiv.org/pdf/1705.09724](https://arxiv.org/pdf/1705.09724)

