---
layout: post
title: "Dynamic Entity Representations in Neural Language Models"
date: 2017-08-02 14:49:03
categories: arXiv_CL
tags: arXiv_CL Tracking Language_Model Prediction
author: Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, Noah A. Smith
mathjax: true
---

* content
{:toc}

##### Abstract
Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.

##### Abstract (translated by Google)
理解长文档需要跟踪实体如何引入和随着时间的推移而发展。我们提出了一种新的语言模型，EntityNLM，它可以明确地为实体建模，动态地更新它们的表示，并在上下文中生成它们的提及。我们的模式是生成和灵活的;它可以在上下文中建模任意数量的实体，同时生成任意长度的实体提及。另外，它可以用于几个不同的任务，如语言建模，共同参与解决和实体预测。所有这些任务的实验结果表明，我们的模型始终胜过强大的基线和之前的工作。

##### URL
[https://arxiv.org/abs/1708.00781](https://arxiv.org/abs/1708.00781)

##### PDF
[https://arxiv.org/pdf/1708.00781](https://arxiv.org/pdf/1708.00781)

