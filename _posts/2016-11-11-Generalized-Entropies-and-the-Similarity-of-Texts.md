---
layout: post
title: "Generalized Entropies and the Similarity of Texts"
date: 2016-11-11 06:36:53
categories: arXiv_CL
tags: arXiv_CL
author: Eduardo G. Altmann, Laercio Dias, Martin Gerlach
mathjax: true
---

* content
{:toc}

##### Abstract
We show how generalized Gibbs-Shannon entropies can provide new insights on the statistical properties of texts. The universal distribution of word frequencies (Zipf's law) implies that the generalized entropies, computed at the word level, are dominated by words in a specific range of frequencies. Here we show that this is the case not only for the generalized entropies but also for the generalized (Jensen-Shannon) divergences, used to compute the similarity between different texts. This finding allows us to identify the contribution of specific words (and word frequencies) for the different generalized entropies and also to estimate the size of the databases needed to obtain a reliable estimation of the divergences. We test our results in large databases of books (from the Google n-gram database) and scientific papers (indexed by Web of Science).

##### Abstract (translated by Google)
我们展示了广义Gibbs-Shannon熵如何为文本的统计特性提供新的见解。词频的普遍分布（Zipf定律）意味着在词水平上计算的广义熵由特定频率范围内的词支配。这里我们表明这不仅是广义熵的情况，也是广义（Jensen-Shannon）分歧的情况，用于计算不同文本之间的相似性。这一发现使我们能够识别不同广义熵的特定词汇（和词频）的贡献，并估计获得差异的可靠估计所需的数据库的大小。我们在大型数据库（Google n-gram数据库）和科学论文（由Web of Science索引）中测试我们的结果。

##### URL
[https://arxiv.org/abs/1611.03596](https://arxiv.org/abs/1611.03596)

##### PDF
[https://arxiv.org/pdf/1611.03596](https://arxiv.org/pdf/1611.03596)

