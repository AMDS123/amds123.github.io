---
layout: post
title: "Learning Concept Taxonomies from Multi-modal Data"
date: 2016-06-29 19:52:53
categories: arXiv_CL
tags: arXiv_CL
author: Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan, Eric P. Xing
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.

##### Abstract (translated by Google)
我们研究从文本和视觉数据自动构建上位分类法的问题。先前的分类归纳工作通常忽略日益突出的视觉数据，这些数据编码重要的感知语义。相反，我们通过联合利用文本和图像来提出分类归纳的概率模型。为了避免手工制作特征工程，我们设计了基于图像和文字的分布式表示的端到端特征。该模型在现有的一些本体论的基础上进行有区别地训练，并且能够从头开始构建完整的分类标准，用于收集具有相关图像的不可见概念标签项目。我们在WordNet层次结构上评估我们的模型和功能，其中我们的系统比以前的方法有很大的差距。

##### URL
[https://arxiv.org/abs/1606.09239](https://arxiv.org/abs/1606.09239)

##### PDF
[https://arxiv.org/pdf/1606.09239](https://arxiv.org/pdf/1606.09239)

