---
layout: post
title: "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering"
date: 2017-02-05 17:51:19
categories: arXiv_CV
tags: arXiv_CV Object_Detection Summarization CNN Language_Model Prediction Quantitative Detection Recommendation
author: Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, Christopher Pal
mathjax: true
---

* content
{:toc}

##### Abstract
While deep convolutional neural networks frequently approach or exceed human-level performance at benchmark tasks involving static images, extending this success to moving images is not straightforward. Having models which can learn to understand video is of interest for many applications, including content recommendation, prediction, summarization, event/object detection and understanding human visual perception, but many domains lack sufficient data to explore and perfect video models. In order to address the need for a simple, quantitative benchmark for developing and understanding video, we present MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. In addition to presenting statistics and a description of the dataset, we perform a detailed analysis of 5 different models' predictions, and compare these with human performance. We investigate the relative importance of language, static (2D) visual features, and moving (3D) visual features; the effects of increasing dataset size, the number of frames sampled; and of vocabulary size. We illustrate that: this task is not solvable by a language model alone; our model combining 2D and 3D visual information indeed provides the best result; all models perform significantly worse than human-level. We provide human evaluations for responses given by different models and find that accuracy on the MovieFIB evaluation corresponds well with human judgement. We suggest avenues for improving video models, and hope that the proposed dataset can be useful for measuring and encouraging progress in this very interesting field.

##### Abstract (translated by Google)
虽然深卷积神经网络在涉及静态图像的基准任务中经常接近或超过人类级别的性能，但是将这种成功扩展到移动图像并不是直截了当的。对于许多应用，包括内容推荐，预测，总结，事件/对象检测和理解人类视觉感知，具有能够学习理解视频的模型是很感兴趣的，但是许多领域缺乏足够的数据来探索和完善视频模型。为了解决开发和理解视频所需的简单，定量基准的需求，我们根据视觉障碍者的描述性视频注释，提供了一个包含超过30万个例子的填空题答案数据集MovieFIB。除了提供数据集的统计数据和描述之外，我们还对5种不同模型的预测进行了详细的分析，并将其与人的表现进行比较。我们调查语言，静态（2D）视觉特征和移动（3D）视觉特征的相对重要性;增加的数据集大小的影响，采样的帧数;和词汇大小。我们说明：这个任务不能单靠语言模型来解决，我们的模型结合了二维和三维视觉信息确实提供了最好的结果;所有的模型都比人类的水平差得多。我们针对不同模型给出的反应提供了人类评估，并发现MovieFIB评估的准确性与人为判断相符。我们建议改进视频模型的途径，并希望提出的数据集可以用于衡量和鼓励这个非常有趣的领域的进展。

##### URL
[https://arxiv.org/abs/1611.07810](https://arxiv.org/abs/1611.07810)

##### PDF
[https://arxiv.org/pdf/1611.07810](https://arxiv.org/pdf/1611.07810)

