---
layout: post
title: "A Cost-Sensitive Visual Question-Answer Framework for Mining a Deep And-OR Object Semantics from Web Images"
date: 2017-08-13 14:11:15
categories: arXiv_CV
tags: arXiv_CV QA VQA
author: Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a cost-sensitive Question-Answering (QA) framework for learning a nine-layer And-Or graph (AoG) from web images, which explicitly represents object categories, poses, parts, and detailed structures within the parts in a compositional hierarchy. The QA framework is designed to minimize an overall risk, which trades off the loss and query costs. The loss is defined for nodes in all layers of the AoG, including the generative loss (measuring the likelihood for the images) and the discriminative loss (measuring the fitness to human answers). The cost comprises both human labor of answering questions and the computational cost of model learning. The cost-sensitive QA framework iteratively selects different storylines of questions to update different nodes in the AoG. Experiments showed that our method required much less human supervision (e.g., labeling parts on 3--10 training objects for each category) and achieved better performance than baseline methods.

##### Abstract (translated by Google)
本文提出了一个成本敏感的问答（QA）框架，用于从网络图像中学习九层的和图（AoG），它明确地表示对象类别，姿势，部分和组成部分内部的详细结构层次结构。质量保证框架旨在最大限度地降低整体风险，从而减少损失和查询成本。 AoG各层节点的损失定义包括生成损失（测量图像的可能性）和判别性损失（测量对人类答案的适应度）。成本包括回答问题的人工和模型学习的计算成本。成本敏感的QA框架迭代地选择不同的问题故事情节来更新AoG中的不同节点。实验表明，我们的方法所需的人力监督要少得多（例如，为每个类别标记3-10个训练对象的部分），并且取得比基线方法更好的性能。

##### URL
[https://arxiv.org/abs/1708.03911](https://arxiv.org/abs/1708.03911)

##### PDF
[https://arxiv.org/pdf/1708.03911](https://arxiv.org/pdf/1708.03911)

