---
layout: post
title: "Analyzing Hypersensitive AI: Instability in Corporate-Scale Machine Learning"
date: 2018-07-17 14:02:14
categories: arXiv_AI
tags: arXiv_AI Language_Model Recommendation
author: Michaela Regneri, Malte Hoffmann, Jurij Kost, Niklas Pietsch, Timo Schulz, Sabine Stamm
mathjax: true
---

* content
{:toc}

##### Abstract
Predictive geometric models deliver excellent results for many Machine Learning use cases. Despite their undoubted performance, neural predictive algorithms can show unexpected degrees of instability and variance, particularly when applied to large datasets. We present an approach to measure changes in geometric models with respect to both output consistency and topological stability. Considering the example of a recommender system using word2vec, we analyze the influence of single data points, approximation methods and parameter settings. Our findings can help to stabilize models where needed and to detect differences in informational value of data points on a large scale.

##### Abstract (translated by Google)
预测几何模型为许多机器学习用例提供了出色的结果。尽管它们具有无可置疑的性能，但神经预测算法可以显示出意想不到的不稳定性和方差，特别是在应用于大型数据集时。我们提出了一种在输出一致性和拓扑稳定性方面测量几何模型变化的方法。考虑到使用word2vec的推荐系统的例子，我们分析了单个数据点，近似方法和参数设置的影响。我们的研究结果有助于在需要时稳定模型，并大规模检测数据点的信息价值差异。

##### URL
[http://arxiv.org/abs/1807.07404](http://arxiv.org/abs/1807.07404)

##### PDF
[http://arxiv.org/pdf/1807.07404](http://arxiv.org/pdf/1807.07404)

