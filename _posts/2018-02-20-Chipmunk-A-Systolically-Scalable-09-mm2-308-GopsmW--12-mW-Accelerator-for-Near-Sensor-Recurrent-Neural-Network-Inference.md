---
layout: post
title: "Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW Accelerator for Near-Sensor Recurrent Neural Network Inference"
date: 2018-02-20 21:43:55
categories: arXiv_SD
tags: arXiv_SD Face Speech_Recognition Inference RNN Recognition
author: Francesco Conti, Lukas Cavigelli, Gianna Paulin, Igor Susmelj, Luca Benini
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) are state-of-the-art in voice awareness/understanding and speech recognition. On-device computation of RNNs on low-power mobile and wearable devices would be key to applications such as zero-latency voice-based human-machine interfaces. Here we present Chipmunk, a small (&lt;1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC 65 nm technology capable to operate at a measured peak efficiency up to 3.08 Gop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring in huge memory transfer overhead, multiple Chipmunk engines can cooperate to form a single systolic array. In this way, the Chipmunk architecture in a 75 tiles configuration can achieve real-time phoneme extraction on a demanding RNN topology proposed by Graves et al., consuming less than 13 mW of average power.

##### Abstract (translated by Google)
递归神经网络（RNN）是语音意识/理解和语音识别的最新技术。低功耗移动设备和可穿戴设备上的RNN的在设备上计算对于零延迟基于语音的人机界面等应用而言至关重要。在这里，我们将介绍Chipmunk，一种用于UMC 65纳米技术的长短期记忆RNN的小型（<1 mm $ {^ 2 $）硬件加速器，能够以最高达3.08 Gop / s / mW的测量峰值效率工作峰值功率1.24 mW。为了实现大的RNN模型而不会产生巨大的内存传输开销，多个Chipmunk引擎可以合作形成一个单一的收缩阵列。通过这种方式，75瓦片配置中的Chipmunk架构可以在Graves等人提出的要求严格的RNN拓扑上实现实时音素提取，平均功耗小于13 mW。

##### URL
[http://arxiv.org/abs/1711.05734](http://arxiv.org/abs/1711.05734)

##### PDF
[http://arxiv.org/pdf/1711.05734](http://arxiv.org/pdf/1711.05734)

