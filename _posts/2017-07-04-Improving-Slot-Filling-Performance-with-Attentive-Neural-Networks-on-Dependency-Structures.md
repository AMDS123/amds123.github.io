---
layout: post
title: "Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures"
date: 2017-07-04 17:18:50
categories: arXiv_CL
tags: arXiv_CL Knowledge Relation_Extraction Attention Relation
author: Lifu Huang, Avirup Sil, Heng Ji, Radu Florian
mathjax: true
---

* content
{:toc}

##### Abstract
Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person:cities\_of\_residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies: (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; (2). Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction (16\% absolute F-score gain) and slot filling validation for each individual system (up to 8.5\% absolute F-score gain).

##### Abstract (translated by Google)
时隙填充（SF）旨在从大量源文档中为给定实体提取某些类型的属性（或时隙，比如person：cities \ _of \ _residence）的值。在本文中，我们提出了一个有效的DN的体系结构，并采用了以下新策略：（1）。将规则化的依赖关系图而不是原始语句作为DNN的输入，以压缩查询和候选填充符之间的广泛上下文; （2）。结合两个关注机制：从查询和候选填充者中获得的本地关注，以及从外部知识库中学习的全局关注，引导模型更好地选择指示性上下文以确定时隙类型。实验表明，这个框架在两个关系抽取（16％绝对F-分数增益）和每个单独系统的插槽填充验证（高达8.5％绝对F-分数增益）方面优于现有技术。

##### URL
[https://arxiv.org/abs/1707.01075](https://arxiv.org/abs/1707.01075)

##### PDF
[https://arxiv.org/pdf/1707.01075](https://arxiv.org/pdf/1707.01075)

