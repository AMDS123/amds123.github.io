---
layout: post
title: "Mean Field Multi-Agent Reinforcement Learning"
date: 2018-02-15 09:07:57
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, Jun Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of user interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution. Experiments on resource allocation, Ising model estimation, and battle game tasks verify the learning effectiveness of our mean field approaches in handling many-agent interactions in population.

##### Abstract (translated by Google)
现有的多智能体强化学习方法通​​常限于少数智能体。当代理人数量大幅增加时，由于维度的诅咒和用户交互的指数增长，学习变得棘手。在本文中，我们提出了平均场强化学习，其中代理人口中的相互作用通过单个代理与总体人口或邻近代理的平均效应之间的相互作用来近似;两个实体之间的相互作用是相互加强的：学习个体代理人的最优政策取决于人口动态，而人口动态则根据个体政策的集体模式而改变。我们开发实用的平均场Q学习和平均场Actor-Critic算法并分析解的收敛性。在资源分配，Ising模型估计和战斗游戏任务上的实验验证了我们的平均场方法在处理群体中的许多智能体相互作用时的学习有效性。

##### URL
[http://arxiv.org/abs/1802.05438](http://arxiv.org/abs/1802.05438)

##### PDF
[http://arxiv.org/pdf/1802.05438](http://arxiv.org/pdf/1802.05438)

