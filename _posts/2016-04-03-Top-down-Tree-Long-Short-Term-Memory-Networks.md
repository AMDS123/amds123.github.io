---
layout: post
title: "Top-down Tree Long Short-Term Memory Networks"
date: 2016-04-03 23:30:17
categories: arXiv_CV
tags: arXiv_CV RNN Relation Memory_Networks
author: Xingxing Zhang, Liang Lu, Mirella Lapata
mathjax: true
---

* content
{:toc}

##### Abstract
Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance.

##### Abstract (translated by Google)
长期短期记忆（LSTM）网络是一种具有更复杂计算单元的递归神经网络，已经成功应用于各种序列建模任务。在本文中，我们开发了树长短期记忆（TreeLSTM），一种基于LSTM的神经网络模型，它被设计为预测树而不是线性序列。 TreeLSTM通过估计其依赖树的生成概率来定义句子的概率。在每个时间步骤，基于所生成的子树的表示来生成节点。我们通过明确地表示左侧和右侧依赖关系之间的相关性来进一步增强TreeLSTM的建模能力。将我们的模型应用于MSR句子完成挑战达到超出当前技术水平的结果。我们还报告依赖性分析重新排名的结果，以达到竞争的表现。

##### URL
[https://arxiv.org/abs/1511.00060](https://arxiv.org/abs/1511.00060)

##### PDF
[https://arxiv.org/pdf/1511.00060](https://arxiv.org/pdf/1511.00060)

