---
layout: post
title: "Learning to Learn Image Classifiers with Visual Analogy"
date: 2019-04-16 12:11:30
categories: arXiv_CV
tags: arXiv_CV GAN Embedding Classification
author: Linjun Zhou, Peng Cui, Shiqiang Yang, Wenwu Zhu, Qi Tian
mathjax: true
---

* content
{:toc}

##### Abstract
Humans are far better learners who can learn a new concept very fast with only a few samples compared with machines. The plausible mystery making the difference is two fundamental learning mechanisms: learning to learn and learning by analogy. In this paper, we attempt to investigate a new human-like learning method by organically combining these two mechanisms. In particular, we study how to generalize the classification parameters from previously learned concepts to a new concept. we first propose a novel Visual Analogy Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional embedding space and a linear mapping function from the embedding space to classification parameters for base classes. We then propose an out-of-sample embedding method to learn the embedding of a new class represented by a few samples through its visual analogy with base classes and derive the classification parameters for the new class. We conduct extensive experiments on ImageNet dataset and the results show that our method could consistently and significantly outperform state-of-the-art baselines.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1710.06177](http://arxiv.org/abs/1710.06177)

##### PDF
[http://arxiv.org/pdf/1710.06177](http://arxiv.org/pdf/1710.06177)

