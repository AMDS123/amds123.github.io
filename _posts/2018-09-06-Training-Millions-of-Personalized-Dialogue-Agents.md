---
layout: post
title: "Training Millions of Personalized Dialogue Agents"
date: 2018-09-06 13:36:40
categories: arXiv_CL
tags: arXiv_CL
author: Pierre-Emmanuel Mazar&#xe9;, Samuel Humeau, Martin Raison, Antoine Bordes
mathjax: true
---

* content
{:toc}

##### Abstract
Current dialogue systems are not very engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.

##### Abstract (translated by Google)
当前的对话系统对用户来说并不是很吸引人，特别是在端到端的训练中，而不依赖于主动重新编写脚本策略。张等人。 （2018）表明，当对文本角色进行调整时，端到端对话模型的参与度会提高，从而为模型提供一些个性化的背景故事。但是，Zhang等人使用的数据集。 （2018）是合成的并且尺寸有限，因为它包含大约1k个不同的角色。在本文中，我们介绍了一个新的数据集，提供了500万个人物角色和7亿个基于人物角色的对话。我们的实验表明，在这种规模下，使用角色的培训仍然可以提高端到端系统的性能。此外，我们通过对Zhang等人的数据进行微调，表明其他任务可以从我们的数据集的广泛覆盖中受益。 （2018）并取得最先进的成果。

##### URL
[http://arxiv.org/abs/1809.01984](http://arxiv.org/abs/1809.01984)

##### PDF
[http://arxiv.org/pdf/1809.01984](http://arxiv.org/pdf/1809.01984)

