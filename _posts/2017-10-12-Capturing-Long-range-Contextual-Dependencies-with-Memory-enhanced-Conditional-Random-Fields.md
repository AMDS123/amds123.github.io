---
layout: post
title: "Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields"
date: 2017-10-12 03:31:10
categories: arXiv_CL
tags: arXiv_CL Inference RNN Memory_Networks
author: Fei Liu, Timothy Baldwin, Trevor Cohn
mathjax: true
---

* content
{:toc}

##### Abstract
Despite successful applications across a broad range of NLP tasks, conditional random fields ("CRFs"), in particular the linear-chain variant, are only able to model local features. While this has important benefits in terms of inference tractability, it limits the ability of the model to capture long-range dependencies between items. Attempts to extend CRFs to capture long-range dependencies have largely come at the cost of computational complexity and approximate inference. In this work, we propose an extension to CRFs by integrating external memory, taking inspiration from memory networks, thereby allowing CRFs to incorporate information far beyond neighbouring steps. Experiments across two tasks show substantial improvements over strong CRF and LSTM baselines.

##### Abstract (translated by Google)
尽管在广泛的NLP任务中成功应用，但是条件随机字段（“CRF”），特别是线性链变体，仅能够模拟局部特征。虽然这在推理易处理性方面具有重要的好处，但它限制了模型捕获项目之间长期依赖关系的能力。尝试扩展CRF来捕获远程依赖主要是以计算复杂度和近似推理为代价的。在这项工作中，我们通过集成外部存储器，从存储器网络中获得灵感，提出了对CRFs的扩展，从而使得CRF能够将信息合并到远远超出相邻步骤的地方。跨两个任务的实验显示，相对于强大的CRF和LSTM基线，实质性的改进。

##### URL
[https://arxiv.org/abs/1709.03637](https://arxiv.org/abs/1709.03637)

##### PDF
[https://arxiv.org/pdf/1709.03637](https://arxiv.org/pdf/1709.03637)

