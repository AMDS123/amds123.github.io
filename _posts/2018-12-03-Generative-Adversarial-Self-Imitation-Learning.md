---
layout: post
title: "Generative Adversarial Self-Imitation Learning"
date: 2018-12-03 18:21:18
categories: arXiv_AI
tags: arXiv_AI Adversarial Sparse Reinforcement_Learning Optimization
author: Yijie Guo, Junhyuk Oh, Satinder Singh, Honglak Lee
mathjax: true
---

* content
{:toc}

##### Abstract
This paper explores a simple regularizer for reinforcement learning by proposing Generative Adversarial Self-Imitation Learning (GASIL), which encourages the agent to imitate past good trajectories via generative adversarial imitation learning framework. Instead of directly maximizing rewards, GASIL focuses on reproducing past good trajectories, which can potentially make long-term credit assignment easier when rewards are sparse and delayed. GASIL can be easily combined with any policy gradient objective by using GASIL as a learned shaped reward function. Our experimental results show that GASIL improves the performance of proximal policy optimization on 2D Point Mass and MuJoCo environments with delayed reward and stochastic dynamics.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.00950](http://arxiv.org/abs/1812.00950)

##### PDF
[http://arxiv.org/pdf/1812.00950](http://arxiv.org/pdf/1812.00950)

