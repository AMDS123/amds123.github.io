---
layout: post
title: "Syntax-Aware Language Modeling with Recurrent Neural Networks"
date: 2018-03-02 14:47:24
categories: arXiv_CL
tags: arXiv_CL Face RNN Language_Model
author: Duncan Blythe, Alan Akbik, Roland Vollgraf
mathjax: true
---

* content
{:toc}

##### Abstract
Neural language models (LMs) are typically trained using only lexical features, such as surface forms of words. In this paper, we argue this deprives the LM of crucial syntactic signals that can be detected at high confidence using existing parsers. We present a simple but highly effective approach for training neural LMs using both lexical and syntactic information, and a novel approach for applying such LMs to unparsed text using sequential Monte Carlo sampling. In experiments on a range of corpora and corpus sizes, we show our approach consistently outperforms standard lexical LMs in character-level language modeling; on the other hand, for word-level models the models are on a par with standard language models. These results indicate potential for expanding LMs beyond lexical surface features to higher-level NLP features for character-level models.

##### Abstract (translated by Google)
通常仅使用词汇特征（例如表面形式的词）训练神经语言模型（LM）。在本文中，我们认为这剥夺了使用现有解析器可以高置信度检测到的关键句法信号的LM。我们提出了一种简单但非常有效的方法来训练使用词法和句法信息的神经LM，以及使用顺序蒙特卡罗采样将这些LM应用于未分析文本的新方法。在一系列语料库和语料库大小的实验中，我们展示了我们的方法在字符级语言建模中始终优于标准词法LM;另一方面，对于字级模型，模型与标准语言模型相当。这些结果表明，除了词汇表面特征之外，将LM扩展到字符级模型的更高级NLP特征的潜力。

##### URL
[https://arxiv.org/abs/1803.03665](https://arxiv.org/abs/1803.03665)

##### PDF
[https://arxiv.org/pdf/1803.03665](https://arxiv.org/pdf/1803.03665)

