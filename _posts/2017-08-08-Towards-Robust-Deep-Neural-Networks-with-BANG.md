---
layout: post
title: "Towards Robust Deep Neural Networks with BANG"
date: 2017-08-08 21:57:46
categories: arXiv_CV
tags: arXiv_CV Adversarial Classification
author: Andras Rozsa, Manuel Gunther, Terrance E. Boult
mathjax: true
---

* content
{:toc}

##### Abstract
Machine learning models, including state-of-the-art deep neural networks, are vulnerable to small perturbations that cause unexpected classification errors. This unexpected lack of robustness raises fundamental questions about their generalization properties and poses a serious concern for practical deployments. As such perturbations can remain imperceptible - commonly called adversarial examples that demonstrate an inherent inconsistency between vulnerable machine learning models and human perception - some prior work casts this problem as a security issue as well. Despite the significance of the discovered instabilities and ensuing research, their cause is not well understood, and no effective method has been developed to address the problem highlighted by adversarial examples. In this paper, we present a novel theory to explain why this unpleasant phenomenon exists in deep neural networks. Based on that theory, we introduce a simple, efficient and effective training approach, Batch Adjusted Network Gradients (BANG), which significantly improves the robustness of machine learning models. While the BANG technique does not rely on any form of data augmentation or the application of adversarial images for training, the resultant classifiers are more resistant to adversarial perturbations while maintaining or even enhancing classification performance overall.

##### Abstract (translated by Google)
机器学习模型（包括最先进的深度神经网络）容易受到导致意外分类错误的小扰动的影响。这种意想不到的鲁棒性提出了关于其泛化特性的根本性问题，并对实际部署提出了严重的关注。由于这样的扰动可能保持不可察觉 - 通常被称为敌对的例子，表明脆弱的机器学习模型和人类感知之间的内在矛盾 - 一些先前的工作也把这个问题作为一个安全问题。尽管发现的不稳定性和后续研究的重要性，但它们的原因还不是很清楚，也没有找到有效的方法来解决对抗性事例所强调的问题。在本文中，我们提出一个新的理论来解释为什么这个不愉快的现象存在于深度神经网络中。基于这个理论，我们引入了一种简单，高效，有效的训练方法，批量调整网络梯度（BANG），显着提高了机器学习模型的鲁棒性。虽然BANG技术不依赖于任何形式的数据增强或者将对抗图像应用于训练，但是所产生的分类器更能抵抗对抗性扰动，同时保持或者甚至全面提高分类性能。

##### URL
[https://arxiv.org/abs/1612.00138](https://arxiv.org/abs/1612.00138)

##### PDF
[https://arxiv.org/pdf/1612.00138](https://arxiv.org/pdf/1612.00138)

