---
layout: post
title: 'Massive Exploration of Neural Machine Translation Architectures'
date: 2017-12-06 03:09:44
categories: arXiv_CL
tags: arXiv_CL NMT
author: Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le
---

* content
{:toc}

##### Abstract
Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.

##### Abstract (translated by Google)
在过去的几年中，神经机器翻译（NMT）已经显示出显着的进步，现在正在将生产系统部署到最终用户。当前体系结构的一个主要缺点是它们的培训成本很高，通常需要几天到几周的GPU时间才能收敛。像其他神经网络架构一样，这使得穷举的超参数搜索非常昂贵。在这项工作中，我们首先对NMT体系结构超参数进行大规模的分析。我们报告了数百次实验运行的实验结果和方差数，对应于标准WMT英语到德语翻译任务超过250,000个GPU小时。我们的实验为建立和扩展NMT体系结构提供了新的见解和实用建议。作为这一贡献的一部分，我们发布了一个开放源代码的NMT框架，使研究人员能够轻松地尝试新颖的技术并重现最新的结果。

##### URL
[https://arxiv.org/abs/1703.03906](https://arxiv.org/abs/1703.03906)

