---
layout: post
title: "Knowledge Transfer from Weakly Labeled Audio using Convolutional Neural Network for Sound Events and Scenes"
date: 2018-02-12 19:41:44
categories: arXiv_SD
tags: arXiv_SD Knowledge CNN Transfer_Learning Classification Detection Relation
author: Anurag Kumar, Maksim Khadkevich, Christian Fugen
mathjax: true
---

* content
{:toc}

##### Abstract
In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set.

##### Abstract (translated by Google)
在这项工作中，我们提出了从弱标记的网络音频数据有效传输知识的方法。我们首先描述基于卷积神经网络（CNN）的基于弱标记音频数据的声音事件检测和分类框架。我们的模型可以从不同长度的音频中高效地训练;因此，它非常适合转移学习。然后，我们提出了使用这个模型来学习表示的方法，这可以有效地用于解决目标任务。我们研究直推和归纳转移学习任务，显示我们的方法在领域和任务适应方面的有效性。我们表明，使用提出的CNN模型的学习表示一般化足以达到ESC-50声音事件数据集上的人类水平精度，并在该数据集上设置艺术结果状态。我们进一步将它们用于声场分类任务，并再次表明我们提出的方法也适合于此任务。我们还表明，我们的方法也有助于捕获语义和关系。此外，在这个过程中，我们还根据平衡训练集在Audioet数据集上设置了最新的结果。

##### URL
[http://arxiv.org/abs/1711.01369](http://arxiv.org/abs/1711.01369)

##### PDF
[http://arxiv.org/pdf/1711.01369](http://arxiv.org/pdf/1711.01369)

