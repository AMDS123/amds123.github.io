---
layout: post
title: "Learning The Sequential Temporal Information with Recurrent Neural Networks"
date: 2018-07-08 17:57:27
categories: arXiv_CV
tags: arXiv_CV Image_Caption Review Speech_Recognition Tracking Caption Object_Tracking RNN Language_Model Prediction Recognition
author: Pushparaja Murugan
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent Networks are one of the most powerful and promising artificial neural network algorithms to processing the sequential data such as natural languages, sound, time series data. Unlike traditional feed-forward network, Recurrent Network has a inherent feed back loop that allows to store the temporal context information and pass the state of information to the entire sequences of the events. This helps to achieve the state of art performance in many important tasks such as language modeling, stock market prediction, image captioning, speech recognition, machine translation and object tracking etc., However, training the fully connected RNN and managing the gradient flow are the complicated process. Many studies are carried out to address the mentioned limitation. This article is intent to provide the brief details about recurrent neurons, its variances and trips &amp; tricks to train the fully recurrent neural network. This review work is carried out as a part of our IPO studio software module 'Multiple Object Tracking'.

##### Abstract (translated by Google)
循环网络是处理顺序数据（如自然语言，声音，时间序列数据）的最强大和最有前途的人工神经网络算法之一。与传统的前馈网络不同，Recurrent Network具有固有的反馈回路，允许存储时间上下文信息并将信息状态传递给事件的整个序列。这有助于在许多重要任务中实现艺术性能，例如语言建模，股票市场预测，图像字幕，语音识别，机器翻译和对象跟踪等。然而，训练完全连接的RNN和管理梯度流是复杂的过程。进行了许多研究以解决上述限制。本文旨在提供有关复发神经元，其方差和旅行的简要细节。训练完全递归神经网络的技巧。此审查工作是作为我们的IPO工作室软件模块“多目标跟踪”的一部分进行的。

##### URL
[http://arxiv.org/abs/1807.02857](http://arxiv.org/abs/1807.02857)

##### PDF
[http://arxiv.org/pdf/1807.02857](http://arxiv.org/pdf/1807.02857)

