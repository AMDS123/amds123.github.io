---
layout: post
title: "Learning The Sequential Temporal Information with Recurrent Neural Networks"
date: 2018-07-08 17:57:27
categories: arXiv_CV
tags: arXiv_CV Image_Caption Review Speech_Recognition Tracking Caption Object_Tracking RNN Language_Model Prediction Recognition
author: Pushparaja Murugan
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent Networks are one of the most powerful and promising artificial neural network algorithms to processing the sequential data such as natural languages, sound, time series data. Unlike traditional feed-forward network, Recurrent Network has a inherent feed back loop that allows to store the temporal context information and pass the state of information to the entire sequences of the events. This helps to achieve the state of art performance in many important tasks such as language modeling, stock market prediction, image captioning, speech recognition, machine translation and object tracking etc., However, training the fully connected RNN and managing the gradient flow are the complicated process. Many studies are carried out to address the mentioned limitation. This article is intent to provide the brief details about recurrent neurons, its variances and trips & tricks to train the fully recurrent neural network. This review work is carried out as a part of our IPO studio software module 'Multiple Object Tracking'.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1807.02857](https://arxiv.org/abs/1807.02857)

##### PDF
[https://arxiv.org/pdf/1807.02857](https://arxiv.org/pdf/1807.02857)

