---
layout: post
title: "Texts in, meaning out: neural language models in semantic similarity task for Russian"
date: 2015-04-30 12:03:10
categories: arXiv_CL
tags: arXiv_CL Attention Language_Model
author: Andrey Kutuzov, Igor Andreev
mathjax: true
---

* content
{:toc}

##### Abstract
Distributed vector representations for natural language vocabulary get a lot of attention in contemporary computational linguistics. This paper summarizes the experience of applying neural network language models to the task of calculating semantic similarity for Russian. The experiments were performed in the course of Russian Semantic Similarity Evaluation track, where our models took from the 2nd to the 5th position, depending on the task. We introduce the tools and corpora used, comment on the nature of the shared task and describe the achieved results. It was found out that Continuous Skip-gram and Continuous Bag-of-words models, previously successfully applied to English material, can be used for semantic modeling of Russian as well. Moreover, we show that texts in Russian National Corpus (RNC) provide an excellent training material for such models, outperforming other, much larger corpora. It is especially true for semantic relatedness tasks (although stacking models trained on larger corpora on top of RNC models improves performance even more). High-quality semantic vectors learned in such a way can be used in a variety of linguistic tasks and promise an exciting field for further study.

##### Abstract (translated by Google)
自然语言词汇的分布式向量表示在当代计算语言学中引起了很大的关注。本文总结了将神经网络语言模型应用于计算俄语语义相似度任务的经验。实验是在俄语语义相似度评估的过程中进行的，我们的模型从第二到第五的位置取决于任务。我们介绍使用的工具和语料库，评论共享任务的性质，并描述取得的成果。结果发现，以前成功应用于英文材料的连续跳跃和连续袋单词模型也可用于俄语的语义建模。此外，我们显示在俄罗斯国家语料库（RNC）的文本提供了这样的模型，优于其他，更大的语料库优秀的培训材料。对于语义相关性任务尤其如此（尽管在RNC模型之上的较大语料库上训练的堆叠模型提高了性能）。以这种方式学习的高质量语义向量可以用于各种语言任务，并为下一步的研究提供了一个令人兴奋的领域。

##### URL
[https://arxiv.org/abs/1504.08183](https://arxiv.org/abs/1504.08183)

##### PDF
[https://arxiv.org/pdf/1504.08183](https://arxiv.org/pdf/1504.08183)

