---
layout: post
title: "On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis"
date: 2018-08-23 10:05:33
categories: arXiv_CL
tags: arXiv_CL Sentiment Attention Embedding Deep_Learning
author: Jose Camacho-Collados, Mohammad Taher Pilehvar
mathjax: true
---

* content
{:toc}

##### Abstract
Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.

##### Abstract (translated by Google)
文本预处理通常是自然语言处理（NLP）系统的第一步，对其最终性能有潜在影响。尽管文本预处理很重要，但它在深度学习文献中并未受到太多关注。在本文中，我们研究简单文本预处理决策（特别是标记化，词形化，小写和多词分组）对标准神经文本分类器性能的影响。我们对文本分类和情感分析的标准基准进行了广泛的评估。虽然我们的实验表明输入文本的简单标记通常是足够的，但它们也突出了预处理技术的显着程度的可变性。这揭示了注意这个通常被忽视的步骤的重要性，特别是在比较不同的模型时。最后，我们的评估提供了有关培训单词嵌入的最佳预处理实践的见解。

##### URL
[http://arxiv.org/abs/1707.01780](http://arxiv.org/abs/1707.01780)

##### PDF
[http://arxiv.org/pdf/1707.01780](http://arxiv.org/pdf/1707.01780)

