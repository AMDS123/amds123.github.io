---
layout: post
title: "Sequential Attacks on Agents for Long-Term Adversarial Goals"
date: 2018-05-31 14:22:09
categories: arXiv_AI
tags: arXiv_AI Adversarial Reinforcement_Learning Drone
author: Edgar Tretschk, Seong Joon Oh, Mario Fritz
mathjax: true
---

* content
{:toc}

##### Abstract
Reinforcement learning (RL) has advanced greatly in the past few years with the employment of effective deep neural networks (DNNs) on the policy networks. With the great effectiveness came serious vulnerability issues with DNNs that small adversarial perturbations on the input can change the output of the network. Several works have pointed out that learned agents with a DNN policy network can be manipulated against achieving the original task through a sequence of small perturbations on the input states. In this paper, we demonstrate furthermore that it is also possible to impose an arbitrary adversarial reward on the victim policy network through a sequence of attacks. Our method involves the latest adversarial attack technique, Adversarial Transformer Network (ATN), that learns to generate the attack and is easy to integrate into the policy network. As a result of our attack, the victim agent is misguided to optimise for the adversarial reward over time. Our results expose serious security threats for RL applications in safety-critical systems including drones, medical analysis, and self-driving cars.

##### Abstract (translated by Google)
在过去几年中，强化学习（RL）在政策网络上采用了有效的深度神经网络（DNN），并取得了很大的进展。随着DNN的巨大成功带来了严重的漏洞问题，即对输入的小型敌对扰动可能会改变网络的输出。一些作品指出，具有DNN策略网络的学习代理可以通过对输入状态的一系列小扰动来操纵，以实现原始任务。在本文中，我们进一步证明，也可以通过一系列攻击在受害者政策网络上施加任意敌对报酬。我们的方法涉及最新的对抗攻击技术，敌对变压器网络（ATN），它可以学习生成攻击，并且易于集成到策略网络中。由于我们的攻击，受害者代理被误导为随着时间的推移优化对抗奖励。我们的研究结果显示，在包括无人驾驶飞机，医疗分析和自动驾驶汽车在内的安全关键系统中，RL应用存在严重的安全威胁。

##### URL
[http://arxiv.org/abs/1805.12487](http://arxiv.org/abs/1805.12487)

##### PDF
[http://arxiv.org/pdf/1805.12487](http://arxiv.org/pdf/1805.12487)

