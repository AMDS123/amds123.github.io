---
layout: post
title: "Hierarchical Reinforcement Learning with Abductive Planning"
date: 2018-06-28 06:56:19
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning
author: Kazeto Yamamoto, Takashi Onishi, Yoshimasa Tsuruoka
mathjax: true
---

* content
{:toc}

##### Abstract
One of the key challenges in applying reinforcement learning to real-life problems is that the amount of train-and-error required to learn a good policy increases drastically as the task becomes complex. One potential solution to this problem is to combine reinforcement learning with automated symbol planning and utilize prior knowledge on the domain. However, existing methods have limitations in their applicability and expressiveness. In this paper we propose a hierarchical reinforcement learning method based on abductive symbolic planning. The planner can deal with user-defined evaluation functions and is not based on the Herbrand theorem. Therefore it can utilize prior knowledge of the rewards and can work in a domain where the state space is unknown. We demonstrate empirically that our architecture significantly improves learning efficiency with respect to the amount of training examples on the evaluation domain, in which the state space is unknown and there exist multiple goals.

##### Abstract (translated by Google)
将强化学习应用于现实生活问题的一个关键挑战是，随着任务变得复杂，学习良好策略所需的训练错误数量急剧增加。这个问题的一个可能的解决方案是将强化学习与自动符号规划相结合，并利用该领域的先前知识。但是，现有的方法在适用性和表现性方面存在限制。在本文中，我们提出了一种基于诱人符号规划的分层强化学习方法。规划师可以处理用户定义的评估函数，而不是基于Herbrand定理。因此它可以利用先前的奖励知识，并且可以在状态空间未知的领域工作。我们凭经验证明，我们的体系结构在评估领域的训练样例数量方面显着提高了学习效率，其中状态空间未知且存在多个目标。

##### URL
[http://arxiv.org/abs/1806.10792](http://arxiv.org/abs/1806.10792)

##### PDF
[http://arxiv.org/pdf/1806.10792](http://arxiv.org/pdf/1806.10792)

