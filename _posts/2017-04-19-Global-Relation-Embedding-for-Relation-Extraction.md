---
layout: post
title: "Global Relation Embedding for Relation Extraction"
date: 2017-04-19 23:54:46
categories: arXiv_CL
tags: arXiv_CL Knowledge Relation_Extraction Embedding Relation
author: Yu Su, Honglei Liu, Semih Yavuz, Izzeddin Gur, Huan Sun, Xifeng Yan
mathjax: true
---

* content
{:toc}

##### Abstract
Recent studies have shown that embedding textual relations using deep neural networks greatly helps relation extraction. However, many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data. In this work, we generalize textual relation embedding to the distant supervision setting, where much larger-scale but noisy training data is available. We propose leveraging global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus, to embed textual relations. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embeddings can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.

##### Abstract (translated by Google)
最近的研究表明，使用深度神经网络嵌入文本关系极大地有助于关系提取。然而，许多现有的研究依赖于监督学习;训练数据的可用性极大地限制了他们的表现。在这项工作中，我们将文本关系嵌入概括为遥远的监督环境，其中有更大规模但噪音较大的训练数据。我们提出利用全球关系统计，即从整个语料库收集的文本和知识库关系的同现统计，来嵌入文本关系。这种方法变得对由远程监督引入的训练噪音更加鲁棒。在一个流行的关系抽取数据集上，我们表明，学习的文本关系嵌入可以用来扩充现有的关系抽取模型，并显着提高其性能。最显着的是，对于现有最佳模型发现的前1000个关系事实，精度可以从83.9％提高到89.3％。

##### URL
[https://arxiv.org/abs/1704.05958](https://arxiv.org/abs/1704.05958)

##### PDF
[https://arxiv.org/pdf/1704.05958](https://arxiv.org/pdf/1704.05958)

