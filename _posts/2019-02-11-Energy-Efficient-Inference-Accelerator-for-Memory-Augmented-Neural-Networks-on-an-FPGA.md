---
layout: post
title: "Energy-Efficient Inference Accelerator for Memory-Augmented Neural Networks on an FPGA"
date: 2019-02-11 05:00:30
categories: arXiv_CV
tags: arXiv_CV Inference
author: Seongsik Park, Jaehee Jang, Seijoon Kim, Sungroh Yoon
mathjax: true
---

* content
{:toc}

##### Abstract
Memory-augmented neural networks (MANNs) are designed for question-answering tasks. It is difficult to run a MANN effectively on accelerators designed for other neural networks (NNs), in particular on mobile devices, because MANNs require recurrent data paths and various types of operations related to external memory access. We implement an accelerator for MANNs on a field-programmable gate array (FPGA) based on a data flow architecture. Inference times are also reduced by inference thresholding, which is a data-based maximum inner-product search specialized for natural language tasks. Measurements on the bAbI data show that the energy efficiency of the accelerator (FLOPS/kJ) was higher than that of an NVIDIA TITAN V GPU by a factor of about 125, increasing to 140 with inference thresholding

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1805.07978](https://arxiv.org/abs/1805.07978)

##### PDF
[https://arxiv.org/pdf/1805.07978](https://arxiv.org/pdf/1805.07978)

