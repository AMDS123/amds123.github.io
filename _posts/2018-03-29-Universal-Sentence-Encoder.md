---
layout: post
title: "Universal Sentence Encoder"
date: 2018-03-29 17:43:03
categories: arXiv_CL
tags: arXiv_CL Embedding Transfer_Learning Relation
author: Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil
mathjax: true
---

* content
{:toc}

##### Abstract
We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.

##### Abstract (translated by Google)
我们提出了将句子编码成嵌入向量的模型，这些嵌入向量将转移学习专门针对其他NLP任务。这些模型非常高效，可以在不同的传输任务中获得准确的性能。编码模型的两种变体允许在精度和计算资源之间进行权衡。对于这两种变体，我们调查并报告模型复杂性，资源消耗，传输任务训练数据的可用性和任务性能之间的关系。使用通过预训练词嵌入使用词级转移学习的基线进行比较，以及基线不使用任何转移学习。我们发现，使用句子嵌入的转移学习往往比单词转换更好。通过句子嵌入进行转移学习，我们观察到令人惊讶的良好表现，并为转移任务提供最少量的监督训练数据。我们在Word嵌入关联测试（WEAT）上获得令人鼓舞的结果，旨在检测模型偏倚。我们预先训练的句子编码模型可免费下载并在TF Hub上使用。

##### URL
[http://arxiv.org/abs/1803.11175](http://arxiv.org/abs/1803.11175)

##### PDF
[http://arxiv.org/pdf/1803.11175](http://arxiv.org/pdf/1803.11175)

