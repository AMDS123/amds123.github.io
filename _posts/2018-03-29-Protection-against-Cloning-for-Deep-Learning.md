---
layout: post
title: "Protection against Cloning for Deep Learning"
date: 2018-03-29 10:02:09
categories: arXiv_AI
tags: arXiv_AI Adversarial Deep_Learning
author: Richard Kenway
mathjax: true
---

* content
{:toc}

##### Abstract
The susceptibility of deep learning to adversarial attack can be understood in the framework of the Renormalisation Group (RG) and the vulnerability of a specific network may be diagnosed provided the weights in each layer are known. An adversary with access to the inputs and outputs could train a second network to clone these weights and, having identified a weakness, use them to compute the perturbation of the input data which exploits it. However, the RG framework also provides a means to poison the outputs of the network imperceptibly, without affecting their legitimate use, so as to prevent such cloning of its weights and thereby foil the generation of adversarial data.

##### Abstract (translated by Google)
可以在重整化组（RG）的框架内理解深度学习对敌意攻击的敏感性，并且可以在每个层的权重已知的情况下诊断特定网络的脆弱性。可以访问输入和输出的对手可以训练第二个网络来克隆这些权重，并识别出一个弱点，用它们来计算利用它的输入数据的扰动。但是，RG框架还提供了一种手段，在不影响其合法使用的情况下，在不知不觉中对网络输出进行毒化，从而防止对其权重进行克隆，从而阻止产生对抗性数据。

##### URL
[http://arxiv.org/abs/1803.10995](http://arxiv.org/abs/1803.10995)

##### PDF
[http://arxiv.org/pdf/1803.10995](http://arxiv.org/pdf/1803.10995)

