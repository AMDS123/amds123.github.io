---
layout: post
title: "An improvement of the convergence proof of the ADAM-Optimizer"
date: 2018-04-27 16:53:51
categories: arXiv_AI
tags: arXiv_AI Gradient_Descent
author: Sebastian Bock, Josef Goppold, Martin Weiß
mathjax: true
---

* content
{:toc}

##### Abstract
A common way to train neural networks is the Backpropagation. This algorithm includes a gradient descent method, which needs an adaptive step size. In the area of neural networks, the ADAM-Optimizer is one of the most popular adaptive step size methods. It was invented in \cite{Kingma.2015} by Kingma and Ba. The $5865$ citations in only three years shows additionally the importance of the given paper. We discovered that the given convergence proof of the optimizer contains some mistakes, so that the proof will be wrong. In this paper we give an improvement to the convergence proof of the ADAM-Optimizer.

##### Abstract (translated by Google)
训练神经网络的常用方法是Backpropagation。该算法包括一个梯度下降法，它需要一个自适应步长。在神经网络领域，ADAM-Optimizer是最流行的自适应步长方法之一。它是由Kingma和Ba引用的{Kingma.2015}发明的。仅用三年时间就可以获得5865美元的引用额外显示了该论文的重要性。我们发现优化器的给定收敛证明包含一些错误，所以证明是错误的。在本文中，我们改进了ADAM-Optimizer的收敛性证明。

##### URL
[https://arxiv.org/abs/1804.10587](https://arxiv.org/abs/1804.10587)

##### PDF
[https://arxiv.org/pdf/1804.10587](https://arxiv.org/pdf/1804.10587)

