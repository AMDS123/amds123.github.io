---
layout: post
title: "Weakly-Supervised Neural Text Classification"
date: 2018-09-02 02:56:25
categories: arXiv_CL
tags: arXiv_CL Text_Classification Classification
author: Yu Meng, Jiaming Shen, Chao Zhang, Jiawei Han
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks are gaining increasing popularity for the classic text classification task, due to their strong expressive power and less requirement for feature engineering. Despite such attractiveness, neural text classification models suffer from the lack of training data in many real-world applications. Although many semi-supervised and weakly-supervised text classification models exist, they cannot be easily applied to deep neural models and meanwhile support limited supervision types. In this paper, we propose a weakly-supervised method that addresses the lack of training data in neural text classification. Our method consists of two modules: (1) a pseudo-document generator that leverages seed information to generate pseudo-labeled documents for model pre-training, and (2) a self-training module that bootstraps on real unlabeled data for model refinement. Our method has the flexibility to handle different types of weak supervision and can be easily integrated into existing deep neural models for text classification. We have performed extensive experiments on three real-world datasets from different domains. The results demonstrate that our proposed method achieves inspiring performance without requiring excessive training data and outperforms baseline methods significantly.

##### Abstract (translated by Google)
深度神经网络由于其强大的表现力和对特征工程的要求较低而越来越受到经典文本分类任务的欢迎。尽管具有这种吸引力，但神经文本分类模型在许多实际应用中缺乏训练数据。尽管存在许多半监督和弱监督的文本分类模型，但它们不能容易地应用于深度神经模型，同时支持有限的监督类型。在本文中，我们提出了一种弱监督方法，解决了神经文本分类中缺乏训练数据的问题。我们的方法包括两个模块：（1）伪文档生成器，利用种子信息生成用于模型预训练的伪标记文档;以及（2）自我训练模块，用于引导真实的未标记数据以进行模型细化。我们的方法可以灵活地处理不同类型的弱监督，并且可以很容易地集成到现有的深度神经模型中进行文本分类。我们对来自不同领域的三个真实数据集进行了大量实验。结果表明，我们提出的方法可以实现鼓舞人心的性能，而不需要过多的训练数据，并且显着优于基线方法。

##### URL
[http://arxiv.org/abs/1809.01478](http://arxiv.org/abs/1809.01478)

##### PDF
[http://arxiv.org/pdf/1809.01478](http://arxiv.org/pdf/1809.01478)

