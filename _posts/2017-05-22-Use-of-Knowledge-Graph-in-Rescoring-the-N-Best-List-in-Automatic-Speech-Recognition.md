---
layout: post
title: "Use of Knowledge Graph in Rescoring the N-Best List in Automatic Speech Recognition"
date: 2017-05-22 21:53:05
categories: arXiv_CL
tags: arXiv_CL Knowledge_Graph Knowledge Face Speech_Recognition Embedding Language_Model Recognition
author: Ashwini Jaya Kumar, Camilo Morales, Maria-Esther Vidal, Christoph Schmidt, Sören Auer
mathjax: true
---

* content
{:toc}

##### Abstract
With the evolution of neural network based methods, automatic speech recognition (ASR) field has been advanced to a level where building an application with speech interface is a reality. In spite of these advances, building a real-time speech recogniser faces several problems such as low recognition accuracy, domain constraint, and out-of-vocabulary words. The low recognition accuracy problem is addressed by improving the acoustic model, language model, decoder and by rescoring the N-best list at the output of the decoder. We are considering the N-best list rescoring approach to improve the recognition accuracy. Most of the methods in the literature use the grammatical, lexical, syntactic and semantic connection between the words in a recognised sentence as a feature to rescore. In this paper, we have tried to see the semantic relatedness between the words in a sentence to rescore the N-best list. Semantic relatedness is computed using TransE~\cite{bordes2013translating}, a method for low dimensional embedding of a triple in a knowledge graph. The novelty of the paper is the application of semantic web to automatic speech recognition.

##### Abstract (translated by Google)
随着基于神经网络方法的发展，自动语音识别（ASR）领域已经发展到了以语音接口为基础的应用程序的实现。尽管有这些进步，构建一个实时语音识别器面临着几个问题，如识别准确度低，领域约束和词汇超出。通过改进声学模型，语言模型，解码器以及通过在解码器的输出处重新划分N-最佳列表来解决低识别精度问题。我们正在考虑使用N-best list rescoring方法来提高识别的准确性。文献中的大多数方法都使用被认可的句子中的单词之间的语法，词汇，句法和语义连接作为重新定位的特征。在本文中，我们试图看到句子中的单词之间的语义相关性，以重新列出N-best列表。语义相关性是使用TransE〜\ cite {bordes2013translating}来计算的，这是一种用于知识图中三元组的低维嵌入的方法。本文的新颖之处在于语义网在自动语音识别中的应用。

##### URL
[https://arxiv.org/abs/1705.08018](https://arxiv.org/abs/1705.08018)

##### PDF
[https://arxiv.org/pdf/1705.08018](https://arxiv.org/pdf/1705.08018)

