---
layout: post
title: "Task-driven Visual Saliency and Attention-based Visual Question Answering"
date: 2017-02-22 08:19:38
categories: arXiv_CV
tags: arXiv_CV RNN VQA
author: Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang
mathjax: true
---

* content
{:toc}

##### Abstract
Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.

##### Abstract (translated by Google)
自2015年5月以来，视觉问答（VQA）取得了长足的进步，成为将视觉和文本数据统一到系统中的经典问题。许多启发性的VQA作品深入研究了图像和问题编码和融合方法，其中注意力是最有效和最有侵略性的机制。目前基于注意力的方法着重于视觉和文本特征的充分融合，但缺乏对人们关注图像问题的关注。传统的基于注意力的方法在每个空间位置附加一个单一的值，这会损失许多有用的信息。为了弥补这些问题，我们提出了一种通过双向LSTM（BiLSTM）的相互关系来执行重叠区域特征的显着性预选的通用方法，并且使用基于元素的乘法的新的关注方法来捕获更有效的相关信息视觉和文字特征。我们在大规模的COCO-VQA数据集上进行实验，并通过强大的实证结果分析我们模型的有效性。

##### URL
[https://arxiv.org/abs/1702.06700](https://arxiv.org/abs/1702.06700)

##### PDF
[https://arxiv.org/pdf/1702.06700](https://arxiv.org/pdf/1702.06700)

