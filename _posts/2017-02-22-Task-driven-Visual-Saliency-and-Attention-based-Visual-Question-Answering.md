---
layout: post
title: "Task-driven Visual Saliency and Attention-based Visual Question Answering"
date: 2017-02-22 08:19:38
categories: arXiv_CV
tags: arXiv_CV Salient QA Attention RNN Relation VQA
author: Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang
mathjax: true
---

* content
{:toc}

##### Abstract
Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.

##### Abstract (translated by Google)
视觉问答（VQA）自2015年5月以来取得了巨大进步，成为将视觉和文本数据统一到系统中的经典问题。许多有启发性的VQA作品深入研究图像和问题编码和融合方法，其中注意力是最有效和最具侵入性的机制。目前基于注意力的方法侧重于视觉和文本特征的充分融合，但缺乏对人们关注图像问题的关注。传统的基于注意力的方法将单个值附加到每个空间位置处的特征，这损失了许多有用的信息。为了解决这些问题，我们提出了一种通过双向LSTM（BiLSTM）的相互关系对重叠区域特征进行类似显着性预选的一般方法，并使用基于元素的基于乘法的注意方法来捕获更有效的相关信息。视觉和文字功能。我们对大规模COCO-VQA数据集进行了实验，并通过强有力的实证结果分析了我们模型的有效性。

##### URL
[https://arxiv.org/abs/1702.06700](https://arxiv.org/abs/1702.06700)

##### PDF
[https://arxiv.org/pdf/1702.06700](https://arxiv.org/pdf/1702.06700)

