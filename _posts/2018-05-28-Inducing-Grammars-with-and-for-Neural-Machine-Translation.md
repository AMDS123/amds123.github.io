---
layout: post
title: "Inducing Grammars with and for Neural Machine Translation"
date: 2018-05-28 10:19:38
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention NMT
author: Ke Tran, Yonatan Bisk
mathjax: true
---

* content
{:toc}

##### Abstract
Machine translation systems require semantic knowledge and grammatical understanding. Neural machine translation (NMT) systems often assume this information is captured by an attention mechanism and a decoder that ensures fluency. Recent work has shown that incorporating explicit syntax alleviates the burden of modeling both types of knowledge. However, requiring parses is expensive and does not explore the question of what syntax a model needs during translation. To address both of these issues we introduce a model that simultaneously translates while inducing dependency trees. In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance. We show that our dependency trees are 1. language pair dependent and 2. improve translation quality.

##### Abstract (translated by Google)
机器翻译系统需要语义知识和语法理解。神经机器翻译（NMT）系统通常假定这些信息被确保流畅性的注意机制和解码器所捕获。最近的工作表明，结合显式语法减轻了对两类知识建模的负担。但是，要求解析是昂贵的，并且不会探讨模型在翻译过程中需要什么语法的问题。为了解决这两个问题，我们引入了一个模型，该模型在诱导依赖树的同时进行翻译。通过这种方式，我们可以利用结构的好处，同时调查NMT必须引发的语法以最大限度地提高性能。我们显示我们的依赖树是1.语言对依赖和2.提高翻译质量。

##### URL
[http://arxiv.org/abs/1805.10850](http://arxiv.org/abs/1805.10850)

##### PDF
[http://arxiv.org/pdf/1805.10850](http://arxiv.org/pdf/1805.10850)

