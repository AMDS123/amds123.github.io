---
layout: post
title: "Learning to Describe Differences Between Pairs of Similar Images"
date: 2018-08-31 03:15:28
categories: arXiv_CV
tags: arXiv_CV Attention
author: Harsh Jhamtani, Taylor Berg-Kirkpatrick
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we introduce the task of automatically generating text to describe the differences between two similar images. We collect a new dataset by crowd-sourcing difference descriptions for pairs of image frames extracted from video-surveillance footage. Annotators were asked to succinctly describe all the differences in a short paragraph. As a result, our novel dataset provides an opportunity to explore models that align language and vision, and capture visual salience. The dataset may also be a useful benchmark for coherent multi-sentence generation. We perform a firstpass visual analysis that exposes clusters of differing pixels as a proxy for object-level differences. We propose a model that captures visual salience by using a latent variable to align clusters of differing pixels with output sentences. We find that, for both single-sentence generation and as well as multi-sentence generation, the proposed model outperforms the models that use attention alone.

##### Abstract (translated by Google)
在本文中，我们介绍了自动生成文本的任务，以描述两个相似图像之间的差异。我们通过从视频监控录像中提取的成对图像帧的众包差异描述来收集新的数据集。要求注释者简洁地描述短段中的所有差异。因此，我们的新颖数据集提供了一个探索模式，使语言和视觉保持一致，并捕捉视觉显着性的机会。数据集也可以是连贯多句子生成的有用基准。我们执行firstpass可视化分析，将不同像素的聚类公开为对象级差异的代理。我们提出了一种模型，通过使用潜在变量来将不同像素的聚类与输出句子对齐，从而捕获视觉显着性。我们发现，对于单句生成和多句生成，所提出的模型优于仅使用注意力的模型。

##### URL
[http://arxiv.org/abs/1808.10584](http://arxiv.org/abs/1808.10584)

##### PDF
[http://arxiv.org/pdf/1808.10584](http://arxiv.org/pdf/1808.10584)

