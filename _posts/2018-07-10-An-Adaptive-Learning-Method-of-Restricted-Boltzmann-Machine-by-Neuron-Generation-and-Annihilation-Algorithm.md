---
layout: post
title: "An Adaptive Learning Method of Restricted Boltzmann Machine by Neuron Generation and Annihilation Algorithm"
date: 2018-07-10 04:39:18
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Shin Kamada, Takumi Ichimura
mathjax: true
---

* content
{:toc}

##### Abstract
Restricted Boltzmann Machine (RBM) is a generative stochastic energy-based model of artificial neural network for unsupervised learning. Recently, RBM is well known to be a pre-training method of Deep Learning. In addition to visible and hidden neurons, the structure of RBM has a number of parameters such as the weights between neurons and the coefficients for them. Therefore, we may meet some difficulties to determine an optimal network structure to analyze big data. In order to evade the problem, we investigated the variance of parameters to find an optimal structure during learning. For the reason, we should check the variance of parameters to cause the fluctuation for energy function in RBM model. In this paper, we propose the adaptive learning method of RBM that can discover an optimal number of hidden neurons according to the training situation by applying the neuron generation and annihilation algorithm. In this method, a new hidden neuron is generated if the energy function is not still converged and the variance of the parameters is large. Moreover, the inactivated hidden neuron will be annihilated if the neuron does not affect the learning situation. The experimental results for some benchmark data sets were discussed in this paper.

##### Abstract (translated by Google)
受限玻尔兹曼机（RBM）是一种基于随机能量的人工神经网络模型，用于无监督学习。最近，众所周知RBM是深度学习的预训练方法。除了可见和隐藏的神经元之外，RBM的结构还具有许多参数，例如神经元之间的权重和它们的系数。因此，我们可能会遇到一些困难来确定最佳网络结构来分析大数据。为了避免这个问题，我们研究了参数的方差，以便在学习过程中找到最佳结构。因此，我们应该检查参数的方差，以引起RBM模型中能量函数的波动。在本文中，我们提出了RBM的自适应学习方法，通过应用神经元生成和湮灭算法，可以根据训练情况发现最佳数量的隐藏神经元。在该方法中，如果能量函数仍未收敛并且参数的方差很大，则生成新的隐藏神经元。此外，如果神经元不影响学习情况，则灭活的隐藏神经元将被消灭。本文讨论了一些基准数据集的实验结果。

##### URL
[http://arxiv.org/abs/1807.03478](http://arxiv.org/abs/1807.03478)

##### PDF
[http://arxiv.org/pdf/1807.03478](http://arxiv.org/pdf/1807.03478)

