---
layout: post
title: "Adversarial Network Compression"
date: 2018-03-28 17:38:20
categories: arXiv_CV
tags: arXiv_CV Regularization Adversarial Knowledge Attention
author: Vasileios Belagiannis, Azade Farshad, Fabio Galasso
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network compression has recently received much attention due to the computational requirements of modern deep models. In this work, our objective is to transfer knowledge from a deep and accurate model to a smaller one. Our contributions are threefold: (i) we propose an adversarial network compression approach to train the small student network to mimic the large teacher, without the need for labels during training; (ii) we introduce a regularization scheme to prevent a trivially-strong discriminator without reducing the network capacity and (iii) our approach generalizes on different teacher-student models. In an extensive evaluation on five standard datasets, we show that our student has small accuracy drop, achieves better performance than other knowledge transfer approaches and it surpasses the performance of the same network trained with labels. In addition, we demonstrate state-of-the-art results compared to other compression strategies.

##### Abstract (translated by Google)
由于现代深度模型的计算需求，神经网络压缩最近受到了很多关注。在这项工作中，我们的目标是将知识从深度和准确的模型转移到较小的模型。我们的贡献有三个：（i）我们提出一种对抗性网络压缩方法来训练小型学生网络以模拟大型教师，而不需要在培训期间贴上标签; （ii）我们引入一个正则化方案来防止一个强大的鉴别器而不降低网络容量，（iii）我们的方法推广到不同的师生模型。在对五个标准数据集的广泛评估中，我们表明，我们的学生的准确度下降较小，比其他知识传输方法的性能更好，并且超过了使用标签培训的同一网络的性能。另外，与其他压缩策略相比，我们展示了最先进的结果。

##### URL
[https://arxiv.org/abs/1803.10750](https://arxiv.org/abs/1803.10750)

##### PDF
[https://arxiv.org/pdf/1803.10750](https://arxiv.org/pdf/1803.10750)

