---
layout: post
title: "emoji2vec: Learning Emoji Representations from their Description"
date: 2016-11-20 22:43:46
categories: arXiv_CL
tags: arXiv_CL Sentiment Embedding Represenation_Learning Language_Model
author: Ben Eisner, Tim Rocktäschel, Isabelle Augenstein, Matko Bošnjak, Sebastian Riedel
mathjax: true
---

* content
{:toc}

##### Abstract
Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard. The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation.

##### Abstract (translated by Google)
目前许多社交媒体的自然语言处理应用都依赖于表示学习，并利用预先训练的词嵌入。目前存在几种公开可用的预先训练的单词嵌入集合，但是即使在社交媒体中表情符号的使用增加了，但它们包含很少或没有表情符号表示。在本文中，我们发布了emoji2vec，这是针对所有Unicode表情符号的预先训练嵌入，这些表情符号是从Unicode表情符号标准中描述的。由此产生的表情符号嵌入与word2vec一起可用于下游社交自然语言处理应用程序。对于情感分析的下游任务，我们展示了从简短描述中学习的表情符号嵌入优于训练在大量推文集上的跳过模型，同时避免了为了估计表情符号而需要经常出现表情符号的上下文表示。

##### URL
[https://arxiv.org/abs/1609.08359](https://arxiv.org/abs/1609.08359)

##### PDF
[https://arxiv.org/pdf/1609.08359](https://arxiv.org/pdf/1609.08359)

