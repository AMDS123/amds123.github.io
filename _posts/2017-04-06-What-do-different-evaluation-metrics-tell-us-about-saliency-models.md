---
layout: post
title: "What do different evaluation metrics tell us about saliency models?"
date: 2017-04-06 23:46:40
categories: arXiv_CV
tags: arXiv_CV Salient Recommendation
author: Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, Frédo Durand
mathjax: true
---

* content
{:toc}

##### Abstract
How best to evaluate a saliency model's ability to predict where humans look in images is an open research question. The choice of evaluation metric depends on how saliency is defined and how the ground truth is represented. Metrics differ in how they rank saliency models, and this results from how false positives and false negatives are treated, whether viewing biases are accounted for, whether spatial deviations are factored in, and how the saliency maps are pre-processed. In this paper, we provide an analysis of 8 different evaluation metrics and their properties. With the help of systematic experiments and visualizations of metric computations, we add interpretability to saliency scores and more transparency to the evaluation of saliency models. Building off the differences in metric properties and behaviors, we make recommendations for metric selections under specific assumptions and for specific applications.

##### Abstract (translated by Google)
如何最好地评估显着性模型预测人类在图像中的位置的能力是一个开放的研究问题。评估指标的选择取决于显着性如何定义以及如何表达基本事实。度量标准在显着性模型排名方面有所不同，这是由于如何处理假阳性和假阴性，是否考虑了视角偏差，是否考虑了空间偏差以及如何预处理显着性图。在本文中，我们提供了8个不同的评估指标及其属性的分析。在系统实验和度量计算可视化的帮助下，我们增加了显着性分数的可解释性，以及显着性模型评估的更多透明性。建立度量属性和行为的差异，我们提出了在特定的假设和特定的应用下度量选择的建议。

##### URL
[https://arxiv.org/abs/1604.03605](https://arxiv.org/abs/1604.03605)

##### PDF
[https://arxiv.org/pdf/1604.03605](https://arxiv.org/pdf/1604.03605)

