---
layout: post
title: "Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis"
date: 2018-07-11 01:13:13
categories: arXiv_CL
tags: arXiv_CL Sentiment
author: Hai Pham, Thomas Manzini, Paul Pu Liang, Barnabas Poczos
mathjax: true
---

* content
{:toc}

##### Abstract
Multimodal machine learning is a core research area spanning the language, visual and acoustic modalities. The central challenge in multimodal learning involves learning representations that can process and relate information from multiple modalities. In this paper, we propose two methods for unsupervised learning of joint multimodal representations using sequence to sequence (Seq2Seq) methods: a \textit{Seq2Seq Modality Translation Model} and a \textit{Hierarchical Seq2Seq Modality Translation Model}. We also explore multiple different variations on the multimodal inputs and outputs of these seq2seq models. Our experiments on multimodal sentiment analysis using the CMU-MOSI dataset indicate that our methods learn informative multimodal representations that outperform the baselines and achieve improved performance on multimodal sentiment analysis, specifically in the Bimodal case where our model is able to improve F1 Score by twelve points. We also discuss future directions for multimodal Seq2Seq methods.

##### Abstract (translated by Google)
多模式机器学习是跨越语言，视觉和声学模式的核心研究领域。多模式学习的核心挑战涉及学习可以处理和关联来自多种形式的信息的表示。在本文中，我们提出了两种使用序列到序列（Seq2Seq）方法进行联合多模态表示的无监督学习的方法：a \ textit {Seq2Seq模态翻译模型}和\ textit {Hierarchical Seq2Seq模态翻译模型}。我们还探索了这些seq2seq模型的多模式输入和输出的多种不同变化。我们使用CMU-MOSI数据集进行多模态情感分析的实验表明，我们的方法学习的信息多模态表示优于基线，并在多模态情绪分析中实现了改进的性能，特别是在双模情况下，我们的模型能够将F1得分提高12个点。我们还讨论了多模式Seq2Seq方法的未来发展方向。

##### URL
[http://arxiv.org/abs/1807.03915](http://arxiv.org/abs/1807.03915)

##### PDF
[http://arxiv.org/pdf/1807.03915](http://arxiv.org/pdf/1807.03915)

