---
layout: post
title: "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks"
date: 2017-12-18 21:58:03
categories: arXiv_SD
tags: arXiv_SD CNN
author: Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus only on addressing audio information. In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. We also propose a multi-task learning framework for reconstructing audio and visual signals at the output layer. Precisely speaking, the proposed AVDCNN model is structured as an audio-visual encoder-decoder network, in which audio and visual data are first processed using individual CNNs, and then fused into a joint network to generate enhanced speech (the primary task) and reconstructed images (the secondary task) at the output layer. The model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five instrumental criteria. Results show that the AVDCNN model yields a notably superior performance compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process. In addition, the AVDCNN model also outperforms an existing audio-visual SE model, confirming its capability of effectively combining audio and visual information in SE.

##### Abstract (translated by Google)
语音增强（SE）旨在减少语音信号中的噪声。大多数SE技术只专注于处理音频信息。在这项工作中，受到多模式学习的启发，该模式利用了不同形式的数据，以及SE中卷积神经网络（CNNs）的最近成功，我们提出了一个视听深度CNN（AVDCNN）SE模型，该模型包含音频和视频流变成一个统一的网络模型。我们还提出了一个多任务学习框架来重构输出层的音频和视频信号。准确地说，所提出的AVDCNN模型被构建为视听编码器 - 解码器网络，其中音频和视频数据首先使用单独的CNN进行处理，然后融合成联合网络以生成增强的语音（主要任务）和重建图像（次要任务）在输出层。模型以端到端的方式进行训练，通过反向传播共同学习参数。我们使用五个仪器标准评估增强语音。结果表明，AVDCNN模型相比于仅基于音频的CNN SE模型和两种常规SE方法产生显着优越的性能，证实了将视觉信息整合到SE过程中的有效性。另外，AVDCNN模型也优于现有的视听SE模型，证实了其在SE中有效地结合音频和视频信息的能力。

##### URL
[http://arxiv.org/abs/1703.10893](http://arxiv.org/abs/1703.10893)

##### PDF
[http://arxiv.org/pdf/1703.10893](http://arxiv.org/pdf/1703.10893)

