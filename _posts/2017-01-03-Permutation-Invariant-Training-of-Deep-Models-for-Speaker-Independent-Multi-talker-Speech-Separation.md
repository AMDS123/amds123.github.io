---
layout: post
title: "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation"
date: 2017-01-03 19:57:37
categories: arXiv_CL
tags: arXiv_CL Segmentation Deep_Learning
author: Dong Yu, Morten Kolbæk, Zheng-Hua Tan, Jesper Jensen
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a novel deep learning model, which supports permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from most of the prior arts that treat speech separation as a multi-class regression problem and the deep clustering technique that considers it a segmentation (or clustering) problem, our model optimizes for the separation regression error, ignoring the order of mixing sources. This strategy cleverly solves the long-lasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Experiments on the equal-energy mixing setup of a Danish corpus confirms the effectiveness of PIT. We believe improvements built upon PIT can eventually solve the cocktail-party problem and enable real-world adoption of, e.g., automatic meeting transcription and multi-party human-computer interaction, where overlapping speech is common.

##### Abstract (translated by Google)
我们提出了一种新颖的深度学习模型，它支持置换不变式训练（PIT），用于说话者独立的多说话者语音分离，俗称鸡尾酒会问题。与将语音分离视为多类回归问题和将其视为分割（或聚类）问题的深度聚类技术的大多数现有技术不同，我们的模型针对分离回归误差进行优化，忽略混合源的顺序。该策略巧妙地解决了长期的标签置换问题，这阻碍了基于深度学习的语音分离技术的进步。丹麦语语料库等能量混合装置的实验证实了PIT的有效性。我们认为，基于PIT的改进最终可以解决鸡尾酒会问题，并使现实世界采用例如自动会议录音和多方人机交互，其中重叠的讲话是常见的。

##### URL
[https://arxiv.org/abs/1607.00325](https://arxiv.org/abs/1607.00325)

##### PDF
[https://arxiv.org/pdf/1607.00325](https://arxiv.org/pdf/1607.00325)

