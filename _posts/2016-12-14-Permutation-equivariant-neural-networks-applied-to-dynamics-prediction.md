---
layout: post
title: "Permutation-equivariant neural networks applied to dynamics prediction"
date: 2016-12-14 08:31:53
categories: arXiv_CV
tags: arXiv_CV CNN Prediction
author: Nicholas Guttenberg, Nathaniel Virgo, Olaf Witkowski, Hidetoshi Aoki, Ryota Kanai
mathjax: true
---

* content
{:toc}

##### Abstract
The introduction of convolutional layers greatly advanced the performance of neural networks on image tasks due to innately capturing a way of encoding and learning translation-invariant operations, matching one of the underlying symmetries of the image domain. In comparison, there are a number of problems in which there are a number of different inputs which are all 'of the same type' --- multiple particles, multiple agents, multiple stock prices, etc. The corresponding symmetry to this is permutation symmetry, in that the algorithm should not depend on the specific ordering of the input data. We discuss a permutation-invariant neural network layer in analogy to convolutional layers, and show the ability of this architecture to learn to predict the motion of a variable number of interacting hard discs in 2D. In the same way that convolutional layers can generalize to different image sizes, the permutation layer we describe generalizes to different numbers of objects.

##### Abstract (translated by Google)
卷积层的引入极大地提高了神经网络在图像任务上的性能，这是由于天生捕捉了一种编码和学习平移不变操作的方式，匹配了图像域的一个基本对称性。相比之下，存在许多不同的输入都是“同一类型”的多个问题---多个粒子，多个代理商，多个股票价格等等。相应的对称性是置换对称性因为该算法不应该依赖于输入数据的特定排序。我们讨论与卷积层类似的置换不变式神经网络层，并展示了这种架构学习如何预测2D中可变数量的相互作用硬盘的运动的能力。按照卷积层可以推广到不同图像大小的相同方式，我们描述的置换层概括为不同数目的对象。

##### URL
[https://arxiv.org/abs/1612.04530](https://arxiv.org/abs/1612.04530)

##### PDF
[https://arxiv.org/pdf/1612.04530](https://arxiv.org/pdf/1612.04530)

