---
layout: post
title: "Feudal Reinforcement Learning for Dialogue Management in Large Domains"
date: 2018-03-08 18:05:18
categories: arXiv_AI
tags: arXiv_AI Ontology Reinforcement_Learning
author: I&#xf1;igo Casanueva, Pawe&#x142; Budzianowski, Pei-Hao Su, Stefan Ultes, Lina Rojas-Barahona, Bo-Hsiang Tseng, Milica Ga&#x161;i&#x107;
mathjax: true
---

* content
{:toc}

##### Abstract
Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisation. Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality. We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen from the selected subset. The structural information included in the domain ontology is used to abstract the dialogue state space, taking the decisions at each step using different parts of the abstracted state. This, combined with an information sharing mechanism between slots, increases the scalability to large domains. We show that an implementation of this approach, based on Deep-Q Networks, significantly outperforms previous state of the art in several dialogue domains and environments, without the need of any additional reward signal.

##### Abstract (translated by Google)
强化学习（RL）是解决对话策略优化问题的有效方法。然而，传统的RL算法由于维度的诅咒而无法扩展到大的域。我们提出了一种基于Feudal RL的新型对话管理架构，该架构将决策分解为两个步骤;主策略选择原语动作的子集的第一步，以及从所选择的子集中选择原语动作的第二步。包含在领域本体中的结构信息用于抽象对话状态空间，在每个步骤中使用抽象状态的不同部分作出决策。这与插槽之间的信息共享机制相结合，增加了大型域的可扩展性。我们表明，基于Deep-Q网络的这种方法的实施在几个对话领域和环境中显着优于先前的艺术状态，而不需要任何额外的奖励信号。

##### URL
[http://arxiv.org/abs/1803.03232](http://arxiv.org/abs/1803.03232)

##### PDF
[http://arxiv.org/pdf/1803.03232](http://arxiv.org/pdf/1803.03232)

