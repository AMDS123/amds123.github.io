---
layout: post
title: "Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts"
date: 2018-03-29 18:14:19
categories: arXiv_CV
tags: arXiv_CV Knowledge Embedding Prediction Relation
author: Raymond A. Yeh, Jinjun Xiong, Wen-mei W. Hwu, Minh N. Do, Alexander G. Schwing
mathjax: true
---

* content
{:toc}

##### Abstract
Textual grounding is an important but challenging task for human-computer interaction, robotics and knowledge mining. Existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. In this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. Hence, the method is able to consider significantly more proposals and doesn't rely on a successful first stage hypothesizing bounding box proposals. Beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. Lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the Flickr 30k Entities and the ReferItGame dataset by 3.08% and 7.77% respectively.

##### Abstract (translated by Google)
对于人机交互，机器人技术和知识挖掘而言，文本基础是一项重要而又具有挑战性的任务。现有算法通常将任务制定为从基于深度网络的系统获得的一组边界框提案中的选择。在这项工作中，我们证明我们可以将文本基础的问题转化为一个统一的框架，以便对所有可能的边界框进行高效搜索。因此，该方法能够考虑更多的提议并且不依赖于成功的第一阶段假设边界框提议。除此之外，我们证明了我们模型的训练参数可以用作捕获空间图像关系并提供解释性的词嵌入。最后，在提交时，我们的方法比Flickr 30k实体和ReferItGame数据集当前的最新方法分别高出3.08％和7.77％。

##### URL
[https://arxiv.org/abs/1803.11209](https://arxiv.org/abs/1803.11209)

##### PDF
[https://arxiv.org/pdf/1803.11209](https://arxiv.org/pdf/1803.11209)

