---
layout: post
title: "Latent Relation Language Models"
date: 2019-08-21 03:09:16
categories: arXiv_CL
tags: arXiv_CL Knowledge_Graph Knowledge Language_Model Relation
author: Hiroaki Hayashi, Zecong Hu, Chenyan Xiong, Graham Neubig
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose Latent Relation Language Models (LRLMs), a class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations. This model has a number of attractive properties: it not only improves language modeling performance, but is also able to annotate the posterior probability of entity spans for a given text through relations. Experiments demonstrate empirical improvements over both a word-based baseline language model and a previous approach that incorporates knowledge graph information. Qualitative analysis further demonstrates the proposed model's ability to learn to predict appropriate relations in context.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.07690](http://arxiv.org/abs/1908.07690)

##### PDF
[http://arxiv.org/pdf/1908.07690](http://arxiv.org/pdf/1908.07690)

