---
layout: post
title: "Evaluating Visual Conversational Agents via Cooperative Human-AI Games"
date: 2017-08-17 03:27:53
categories: arXiv_CL
tags: arXiv_CL
author: Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh
mathjax: true
---

* content
{:toc}

##### Abstract
As AI continues to advance, human-AI teams are inevitable. However, progress in AI is routinely measured in isolation, without a human in the loop. It is crucial to benchmark progress in AI, not just in isolation, but also in terms of how it translates to helping humans perform certain tasks, i.e., the performance of human-AI teams. In this work, we design a cooperative game - GuessWhich - to measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the AI. The AI, which we call ALICE, is provided an image which is unseen by the human. Following a brief description of the image, the human questions ALICE about this secret image to identify it from a fixed pool of images. We measure performance of the human-ALICE team by the number of guesses it takes the human to correctly identify the secret image after a fixed number of dialog rounds with ALICE. We compare performance of the human-ALICE teams for two versions of ALICE. Our human studies suggest a counterintuitive trend - that while AI literature shows that one version outperforms the other when paired with an AI questioner bot, we find that this improvement in AI-AI performance does not translate to improved human-AI performance. This suggests a mismatch between benchmarking of AI in isolation and in the context of human-AI teams.

##### Abstract (translated by Google)
随着人工智能的不断发展，人工智能团队已经不可避免。然而，AI的进展通常是孤立地进行测量的，没有人在循环中。对AI的进展进行基准测试是至关重要的，不仅仅是孤立地进行，而且还要看它如何转化为帮助人类完成某些任务，即人类AI团队的表现。在这项工作中，我们设计了一个合作博弈 - 猜测它 - 来衡量AI作为一个视觉对话代理人的特定背景下的人工智能团队的表现。猜测是什么涉及人与AI之间的现场互动。我们称之为ALICE的AI被提供了一个人类看不见的图像。在对图像进行简要描述之后，人们对这个秘密图像的ALICE提出疑问，从固定的图像池中识别出来。我们通过在与ALICE进行固定次数的对话之后人类正确识别秘密图像的次数来衡量人类ALICE团队的表现。我们比较了两个ALICE版本的人类ALICE团队的表现。我们的人类研究提出了一个违反直觉的趋势 - 虽然人工智能文献显示，当与人工智能提问机器人配对时，其中一个版本胜过另一个版本，但我们发现AI-AI性能的改善并不能转化为人类AI性能的改善。这表明人工智能在孤立性和人工智能团队背景下的基准不匹配。

##### URL
[https://arxiv.org/abs/1708.05122](https://arxiv.org/abs/1708.05122)

##### PDF
[https://arxiv.org/pdf/1708.05122](https://arxiv.org/pdf/1708.05122)

