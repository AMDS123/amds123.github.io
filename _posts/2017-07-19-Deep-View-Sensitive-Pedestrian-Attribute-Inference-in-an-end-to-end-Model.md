---
layout: post
title: "Deep View-Sensitive Pedestrian Attribute Inference in an end-to-end Model"
date: 2017-07-19 13:49:47
categories: arXiv_CV
tags: arXiv_CV Image_Classification Inference Classification Prediction Relation
author: M. Saquib Sarfraz, Arne Schumann, Yan Wang, Rainer Stiefelhagen
mathjax: true
---

* content
{:toc}

##### Abstract
Pedestrian attribute inference is a demanding problem in visual surveillance that can facilitate person retrieval, search and indexing. To exploit semantic relations between attributes, recent research treats it as a multi-label image classification task. The visual cues hinting at attributes can be strongly localized and inference of person attributes such as hair, backpack, shorts, etc., are highly dependent on the acquired view of the pedestrian. In this paper we assert this dependence in an end-to-end learning framework and show that a view-sensitive attribute inference is able to learn better attribute predictions. Our proposed model jointly predicts the coarse pose (view) of the pedestrian and learns specialized view-specific multi-label attribute predictions. We show in an extensive evaluation on three challenging datasets (PETA, RAP and WIDER) that our proposed end-to-end view-aware attribute prediction model provides competitive performance and improves on the published state-of-the-art on these datasets.

##### Abstract (translated by Google)
行人属性推理是视觉监控中一个非常棘手的问题，可以方便人物检索，搜索和索引。为了挖掘属性之间的语义关系，最近的研究将其视为一个多标签图像分类任务。暗示属性的视觉线索可以被强烈地定位，并且诸如头发，背包，短裤等的人属性的推断高度依赖于所获得的行人的视野。在本文中，我们在端到端的学习框架中声明这种依赖关系，并且表明视图敏感属性推理能够学习更好的属性预测。我们提出的模型联合预测行人的粗略姿态（视图），并学习专门的视图特定的多标签属性预测。我们对三个具有挑战性的数据集（PETA，RAP和WIDER）进行了广泛的评估，我们提出的端到端视图感知属性预测模型提供了竞争性的性能，并改进了这些数据集上公布的最新技术。

##### URL
[https://arxiv.org/abs/1707.06089](https://arxiv.org/abs/1707.06089)

##### PDF
[https://arxiv.org/pdf/1707.06089](https://arxiv.org/pdf/1707.06089)

