---
layout: post
title: "Pay More Attention - Neural Architectures for Question-Answering"
date: 2018-03-25 10:58:42
categories: arXiv_CL
tags: arXiv_CL Attention
author: Zia Hasan, Sebastian Fischer
mathjax: true
---

* content
{:toc}

##### Abstract
Machine comprehension is a representative task of natural language understanding. Typically, we are given context paragraph and the objective is to answer a question that depends on the context. Such a problem requires to model the complex interactions between the context paragraph and the question. Lately, attention mechanisms have been found to be quite successful at these tasks and in particular, attention mechanisms with attention flow from both context-to-question and question-to-context have been proven to be quite useful. In this paper, we study two state-of-the-art attention mechanisms called Bi-Directional Attention Flow (BiDAF) and Dynamic Co-Attention Network (DCN) and propose a hybrid scheme combining these two architectures that gives better overall performance. Moreover, we also suggest a new simpler attention mechanism that we call Double Cross Attention (DCA) that provides better results compared to both BiDAF and Co-Attention mechanisms while providing similar performance as the hybrid scheme. The objective of our paper is to focus particularly on the attention layer and to suggest improvements on that. Our experimental evaluations show that both our proposed models achieve superior results on the Stanford Question Answering Dataset (SQuAD) compared to BiDAF and DCN attention mechanisms.

##### Abstract (translated by Google)
机器理解是自然语言理解的代表性任务。通常情况下，我们给出了上下文段落，目标是回答一个依赖于上下文的问题。这样的问题需要模拟上下文段落和问题之间复杂的相互作用。最近，在这些任务中，关注机制已经被证明是相当成功的，特别是关注流程从问题到问题和问题到问题的关注机制已被证明是非常有用的。在本文中，我们研究了两种称为双向注意流（BiDAF）和动态注意力网（DCN）的最先进的注意机制，并提出了一种混合这两种体系结构的混合方案，以提供更好的整体性能。此外，我们还提出了一种新的更简单的关注机制，我们称之为Double Cross Attention（DCA），与BiDAF和Co-Attention机制相比，提供更好的结果，同时提供与混合方案相似的性能。我们论文的目标是专注于关注层，并提出改进建议。我们的实验评估显示，与BiDAF和DCN注意机制相比，我们提出的模型在斯坦福问题答疑数据集（SQUAD）上都取得了优异的结果。

##### URL
[https://arxiv.org/abs/1803.09230](https://arxiv.org/abs/1803.09230)

##### PDF
[https://arxiv.org/pdf/1803.09230](https://arxiv.org/pdf/1803.09230)

