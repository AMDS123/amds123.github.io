---
layout: post
title: "End-to-End Eye Movement Detection Using Convolutional Neural Networks"
date: 2016-09-08 14:58:15
categories: arXiv_CV
tags: arXiv_CV Object_Detection Segmentation CNN Detection
author: Sabrina Hoppe, Andreas Bulling
mathjax: true
---

* content
{:toc}

##### Abstract
Common computational methods for automated eye movement detection - i.e. the task of detecting different types of eye movement in a continuous stream of gaze data - are limited in that they either involve thresholding on hand-crafted signal features, require individual detectors each only detecting a single movement, or require pre-segmented data. We propose a novel approach for eye movement detection that only involves learning a single detector end-to-end, i.e. directly from the continuous gaze data stream and simultaneously for different eye movements without any manual feature crafting or segmentation. Our method is based on convolutional neural networks (CNN) that recently demonstrated superior performance in a variety of tasks in computer vision, signal processing, and machine learning. We further introduce a novel multi-participant dataset that contains scripted and free-viewing sequences of ground-truth annotated saccades, fixations, and smooth pursuits. We show that our CNN-based method outperforms state-of-the-art baselines by a large margin on this challenging dataset, thereby underlining the significant potential of this approach for holistic, robust, and accurate eye movement protocol analysis.

##### Abstract (translated by Google)
用于自动化眼动检测的常用计算方法 - 即在连续的注视数据流中检测不同类型的眼动的任务受到限制，因为它们要么涉及对手工制造的信号特征的阈值处理，要么各个检测器仅检测单个移动，或需要预先分段的数据。我们提出了一种新颖的眼动检测方法，它只涉及从端到端学习单个检测器，即直接从连续的注视数据流中获取信息，并同时进行不同的眼球运动，而不需要任何人工特征制作或分割。我们的方法基于卷积神经网络（CNN），最近在计算机视觉，信号处理和机器学习等各种任务中表现出优越的性能。我们进一步介绍一个新的多参与者数据集，其中包含脚本和自由观察序列的地面实况注释扫视，注视和顺利追求。我们表明，基于CNN的方法在这个具有挑战性的数据集上大大超过了最先进的基线，从而强调了这种方法在整体，健壮，准确的眼动方案分析中的重大潜力。

##### URL
[https://arxiv.org/abs/1609.02452](https://arxiv.org/abs/1609.02452)

##### PDF
[https://arxiv.org/pdf/1609.02452](https://arxiv.org/pdf/1609.02452)

