---
layout: post
title: "Edge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy"
date: 2018-06-20 16:56:54
categories: arXiv_AI
tags: arXiv_AI Inference Deep_Learning
author: En Li, Zhi Zhou, Xu Chen
mathjax: true
---

* content
{:toc}

##### Abstract
As the backbone technology of machine learning, deep neural networks (DNNs) have have quickly ascended to the spotlight. Running DNNs on resource-constrained mobile devices is, however, by no means trivial, since it incurs high performance and energy overhead. While offloading DNNs to the cloud for execution suffers unpredictable performance, due to the uncontrolled long wide-area network latency. To address these challenges, in this paper, we propose Edgent, a collaborative and on-demand DNN co-inference framework with device-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that adaptively partitions DNN computation between device and edge, in order to leverage hybrid computation resources in proximity for real-time DNN inference. (2) DNN right-sizing that accelerates DNN inference through early-exit at a proper intermediate DNN layer to further reduce the computation latency. The prototype implementation and extensive evaluations based on Raspberry Pi demonstrate Edgent's effectiveness in enabling on-demand low-latency edge intelligence.

##### Abstract (translated by Google)
作为机器学习的骨干技术，深度神经网络（DNNs）已经迅速成为关注焦点。然而，在资源受限的移动设备上运行DNN并不是微不足道的，因为它会导致高性能和能量开销。由于不受控制的长广域网延迟，将DNN卸载到云中以执行时会遭受不可预知的性能损失。为了解决这些挑战，在本文中，我们提出了Edgent，这是一种具有设备边缘协同作用的协作式和按需DNN协同推理框架。 Edgent追求两个设计旋钮：（1）DNN分区，自适应地在设备和边缘之间分割DNN计算，以利用接近的混合计算资源进行实时DNN推断。 （2）通过在合适的中间DNN层提前退出来加速DNN推断的DNN右侧大小，以进一步减少计算延迟。基于Raspberry Pi的原型实施和广泛的评估证明了Edgent在启用按需低延迟边缘智能方面的有效性。

##### URL
[http://arxiv.org/abs/1806.07840](http://arxiv.org/abs/1806.07840)

##### PDF
[http://arxiv.org/pdf/1806.07840](http://arxiv.org/pdf/1806.07840)

