---
layout: post
title: "Learning to Run challenge solutions: Adapting reinforcement learning methods for neuromusculoskeletal environments"
date: 2018-04-02 00:19:31
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Optimization
author: &#x141;ukasz Kidzi&#x144;ski, Sharada Prasanna Mohanty, Carmichael Ong, Zhewei Huang, Shuchang Zhou, Anton Pechenko, Adam Stelmaszczyk, Piotr Jarosik, Mikhail Pavlov, Sergey Kolesnikov, Sergey Plis, Zhibo Chen, Zhizheng Zhang, Jiale Chen, Jun Shi, Zhuobin Zheng, Chun Yuan, Zhihui Lin, Henryk Michalewski, Piotr Mi&#x142;o&#x15b;, B&#x142;a&#x17c;ej Osi&#x144;ski, Andrew Melnik, Malte Schilling, Helge Ritter, Sean Carroll, Jennifer Hicks, Sergey Levine, Marcel Salath&#xe9;, Scott Delp
mathjax: true
---

* content
{:toc}

##### Abstract
In the NIPS 2017 Learning to Run challenge, participants were tasked with building a controller for a musculoskeletal model to make it run as fast as possible through an obstacle course. Top participants were invited to describe their algorithms. In this work, we present eight solutions that used deep reinforcement learning approaches, based on algorithms such as Deep Deterministic Policy Gradient, Proximal Policy Optimization, and Trust Region Policy Optimization. Many solutions use similar relaxations and heuristics, such as reward shaping, frame skipping, discretization of the action space, symmetry, and policy blending. However, each of the eight teams implemented different modifications of the known algorithms.

##### Abstract (translated by Google)
在NIPS 2017学习运行挑战赛中，与会者的任务是建立肌肉骨骼模型的控制器，使其尽可能快地通过障碍课程。邀请顶尖参与者介绍他们的算法。在这项工作中，我们提出了八种使用深度强化学习方法的解决方案，这些方法基于深度确定性策略梯度，近端策略优化和信任域策略优化等算法。许多解决方案使用类似的放松和启发式算法，如奖励整形，跳帧，动作空间的离散化，对称性和策略混合。但是，八个团队中的每一个都对已知算法进行了不同的修改。

##### URL
[http://arxiv.org/abs/1804.00361](http://arxiv.org/abs/1804.00361)

##### PDF
[http://arxiv.org/pdf/1804.00361](http://arxiv.org/pdf/1804.00361)

