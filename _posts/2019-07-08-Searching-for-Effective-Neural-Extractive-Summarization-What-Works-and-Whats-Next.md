---
layout: post
title: "Searching for Effective Neural Extractive Summarization: What Works and What's Next"
date: 2019-07-08 10:17:28
categories: arXiv_CL
tags: arXiv_CL Knowledge Summarization
author: Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
The recent years have seen remarkable success in the use of deep neural networks on text summarization. 
 However, there is no clear understanding of \textit{why} they perform so well, or \textit{how} they might be improved. 
 In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and 
 learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-of-the-art result on CNN/DailyMail by a large margin based on our 
 observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.03491](http://arxiv.org/abs/1907.03491)

##### PDF
[http://arxiv.org/pdf/1907.03491](http://arxiv.org/pdf/1907.03491)

