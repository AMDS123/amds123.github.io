---
layout: post
title: "Learning with Privileged Information for Multi-Label Classification"
date: 2017-03-29 07:17:52
categories: arXiv_CV
tags: arXiv_CV Classification Detection Relation Recognition
author: Shiyu Chen, Shangfei Wang, Tanfang Chen, Xiaoxiao Shi
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a novel approach for learning multi-label classifiers with the help of privileged information. Specifically, we use similarity constraints to capture the relationship between available information and privileged information, and use ranking constraints to capture the dependencies among multiple labels. By integrating similarity constraints and ranking constraints into the learning process of classifiers, the privileged information and the dependencies among multiple labels are exploited to construct better classifiers during training. A maximum margin classifier is adopted, and an efficient learning algorithm of the proposed method is also developed. We evaluate the proposed method on two applications: multiple object recognition from images with the help of implicit information about object importance conveyed by the list of manually annotated image tags; and multiple facial action unit detection from low-resolution images augmented by high-resolution images. Experimental results demonstrate that the proposed method can effectively take full advantage of privileged information and dependencies among multiple labels for better object recognition and better facial action unit detection.

##### Abstract (translated by Google)
在本文中，我们提出了一种新的方法来学习多标签分类器的帮助下的特权信息。具体而言，我们使用相似性约束来捕获可用信息和特权信息之间的关系，并使用排名约束来捕获多个标签之间的依赖关系。通过将相似性约束和排序约束融入到分类器的学习过程中，利用特征信息和多个标签之间的依赖关系构建训练好的分类器。采用最大边缘分类器，并提出了一种有效的学习算法。我们在两个应用中评估所提出的方法：借助于由手动注释的图像标签列表传送的关于对象重要性的隐含信息，从图像中识别多个对象;以及由高分辨率图像增强的低分辨率图像的多个面部动作单元检测。实验结果表明，该方法能够有效地充分利用多个标签间的特权信息和依赖关系，实现更好的对象识别和更好的面部动作单元检测。

##### URL
[https://arxiv.org/abs/1703.09911](https://arxiv.org/abs/1703.09911)

##### PDF
[https://arxiv.org/pdf/1703.09911](https://arxiv.org/pdf/1703.09911)

