---
layout: post
title: "Prior Activation Distribution : A Versatile Representation to Utilize DNN Hidden Units"
date: 2019-07-05 07:55:09
categories: arXiv_CV
tags: arXiv_CV Inference Classification
author: Lakmal Meegahapola, Vengateswaran Subramaniam, Lance Kaplan, Archan Misra
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we introduce the concept of Prior Activation Distribution (PAD) as a versatile and general technique to capture the typical activation patterns of hidden layer units of a Deep Neural Network used for classification tasks. We show that the combined neural activations of such a hidden layer have class-specific distributional properties, and then define multiple statistical measures to compute how far a test sample's activations deviate from such distributions. Using a variety of benchmark datasets (including MNIST, CIFAR10, Fashion-MNIST &amp; notMNIST), we show how such PAD-based measures can be used, independent of any training technique, to (a) derive fine-grained uncertainty estimates for inferences; (b) provide inferencing accuracy competitive with alternatives that require execution of the full pipeline, and (c) reliably isolate out-of-distribution test samples.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.02711](http://arxiv.org/abs/1907.02711)

##### PDF
[http://arxiv.org/pdf/1907.02711](http://arxiv.org/pdf/1907.02711)

