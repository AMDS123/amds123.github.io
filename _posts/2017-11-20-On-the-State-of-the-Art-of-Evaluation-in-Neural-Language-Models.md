---
layout: post
title: "On the State of the Art of Evaluation in Neural Language Models"
date: 2017-11-20 17:57:58
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Gábor Melis, Chris Dyer, Phil Blunsom
mathjax: true
---

* content
{:toc}

##### Abstract
Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.

##### Abstract (translated by Google)
经常性的神经网络体系结构的持续创新为语言建模基准提供了显着的最新成果。然而，这些已经使用不同的代码库和有限的计算资源进行了评估，这些资源代表了不受控制的实验变化的来源。我们重新评估了一些大规模自动黑箱超参数调整的流行体系结构和正则化方法，并得出了一些令人惊讶的结论，即标准LSTM体系结构在适当规范化时，将胜过更新的模型。我们在Penn Treebank和Wikitext-2语料库上建立了一个新的艺术状态，以及Hutter Prize数据集上的强大基线。

##### URL
[https://arxiv.org/abs/1707.05589](https://arxiv.org/abs/1707.05589)

##### PDF
[https://arxiv.org/pdf/1707.05589](https://arxiv.org/pdf/1707.05589)

