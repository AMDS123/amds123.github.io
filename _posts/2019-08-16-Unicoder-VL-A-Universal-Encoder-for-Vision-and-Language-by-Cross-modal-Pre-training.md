---
layout: post
title: "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"
date: 2019-08-16 17:26:56
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption Language_Model Prediction
author: Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, Ming Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM and Unicoder, both visual and linguistic contents are fed into a multi-layer transformer for the cross-modal pre-training, where three pre-trained tasks are employed, including masked language model, masked object label prediction and visual-linguistic matching. The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large amounts of image-caption pairs, we transfer Unicoder-VL to image-text retrieval tasks with just one additional output layer, and achieve state-of-the-art performances on both MSCOCO and Flicker30K.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.06066](https://arxiv.org/abs/1908.06066)

##### PDF
[https://arxiv.org/pdf/1908.06066](https://arxiv.org/pdf/1908.06066)

