---
layout: post
title: "Dependency Grammar Induction with Neural Lexicalization and Big Training Data"
date: 2017-08-02 15:43:30
categories: arXiv_CL
tags: arXiv_CL
author: Wenjuan Han, Yong Jiang, Kewei Tu
mathjax: true
---

* content
{:toc}

##### Abstract
We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence. We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.

##### Abstract (translated by Google)
我们研究了大模型（就词汇化程度而言）和大数据（就训练语料库大小而言）对依赖语法归纳的影响。我们用L-DMV进行了实验，L-DMV是与Valence和L-NDMV的依赖模型的词汇化版本，我们用神经元对Valence进行了神经依赖模型的词汇化扩展。我们发现，L-DMV只受益于很小程度的词汇化和适度规模的训练语料库。 L-NDMV可以受益于大量的训练数据和更大程度的词汇化，尤其是在模型初始化的情况下得到了增强，并且达到了与当前最新水平相竞争的结果。

##### URL
[https://arxiv.org/abs/1708.00801](https://arxiv.org/abs/1708.00801)

##### PDF
[https://arxiv.org/pdf/1708.00801](https://arxiv.org/pdf/1708.00801)

