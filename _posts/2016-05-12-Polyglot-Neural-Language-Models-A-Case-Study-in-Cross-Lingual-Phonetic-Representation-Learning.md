---
layout: post
title: "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning"
date: 2016-05-12 14:37:51
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning Language_Model
author: Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W Black, Lori Levin, Chris Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.

##### Abstract (translated by Google)
我们引入了多语种语言模型，经过训练的递归神经网络模型，用许多不同的语言来预测符号序列，使用符号的共享表示，并调整关于要预测的语言的类型信息。我们将这些应用到电话序列建模的问题---一个通用符号库和跨语言共享特征表示法是一个自然适合的领域。 （i）多边型模型比可比较的单语模型更好地推广到保持数据，（ii）在多语言模型中，多音乐语音特征表示比单独学习的多。

##### URL
[https://arxiv.org/abs/1605.03832](https://arxiv.org/abs/1605.03832)

##### PDF
[https://arxiv.org/pdf/1605.03832](https://arxiv.org/pdf/1605.03832)

