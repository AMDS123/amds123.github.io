---
layout: post
title: "Multimodal Emotion Recognition Using Multimodal Deep Learning"
date: 2016-02-26 07:43:14
categories: arXiv_CV
tags: arXiv_CV Deep_Learning Recognition
author: Wei Liu, Wei-Long Zheng, Bao-Liang Lu
mathjax: true
---

* content
{:toc}

##### Abstract
To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-the-art approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEG-based DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa.

##### Abstract (translated by Google)
为了提高情感模型的性能，降低实际应用中获取生理信号的成本，我们采用多模态深度学习的方法从多个生理信号构建情感模型。对于单峰增强任务，我们表明SEED数据集的最佳识别准确率达到了82.11％，这是通过Deep AutoEncoder（DAE）模型生成的共享表示实现的。对于多模式的促进任务，我们证明了双模式深度自动编码器（BDAE）在SEED和DEAP数据集上分别达到了91.01％和83.25％的平均准确率，这远远优于最先进的方法。对于跨模态学习任务，我们的实验结果表明，通过将基于EEG的DAE生成的共享表示作为训练样本，以眼动DAE生成的共享表示作为测试样本，在SEED数据集上实现了66.34％的平均准确率反之亦然。

##### URL
[https://arxiv.org/abs/1602.08225](https://arxiv.org/abs/1602.08225)

##### PDF
[https://arxiv.org/pdf/1602.08225](https://arxiv.org/pdf/1602.08225)

