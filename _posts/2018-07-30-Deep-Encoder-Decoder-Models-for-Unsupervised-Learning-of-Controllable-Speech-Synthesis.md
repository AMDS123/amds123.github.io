---
layout: post
title: "Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis"
date: 2018-07-30 17:59:28
categories: arXiv_SD
tags: arXiv_SD Inference
author: Gustav Eje Henter, Xin Wang, Junichi Yamagishi
mathjax: true
---

* content
{:toc}

##### Abstract
Generating versatile and appropriate synthetic speech requires control over the output expression separate from the spoken text. Important non-textual speech variation is seldom annotated, in which case output control must be learned in an unsupervised fashion. In this paper, we perform an in-depth study of methods for unsupervised learning of control in statistical speech synthesis. For example, we show that popular unsupervised training heuristics can be interpreted as variational inference in certain autoencoder models. We additionally connect these models to VQ-VAEs, another, recently-proposed class of deep variational autoencoders, which we show can be derived from a very similar mathematical argument. The implications of these new probabilistic interpretations are discussed. We illustrate the utility of the various approaches with an application to emotional speech synthesis, where the unsupervised methods for learning expression control (without access to emotional labels) are found to give results that in many aspects match or surpass the previous best supervised approach.

##### Abstract (translated by Google)
生成通用且适当的合成语音需要控制与语音文本分开的输出表达。重要的非文本语音变化很少被注释，在这种情况下，必须以无人监督的方式学习输出控制。在本文中，我们对统计语音合成中无监督学习控制的方法进行了深入研究。例如，我们表明流行的无监督训练启发式可以解释为某些自动编码器模型中的变分推断。我们还将这些模型连接到VQ-VAE，这是最近提出的另一类深变分自动编码器，我们展示的可以从非常相似的数学论证中得出。讨论了这些新的概率解释的含义。我们通过应用于情绪语音合成来说明各种方法的效用，其中发现用于学习表达控制（无法访问情感标签）的无监督方法给出在许多方面匹配或超过先前最佳监督方法的结果。

##### URL
[http://arxiv.org/abs/1807.11470](http://arxiv.org/abs/1807.11470)

##### PDF
[http://arxiv.org/pdf/1807.11470](http://arxiv.org/pdf/1807.11470)

