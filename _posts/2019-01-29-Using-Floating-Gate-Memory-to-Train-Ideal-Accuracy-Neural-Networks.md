---
layout: post
title: "Using Floating Gate Memory to Train Ideal Accuracy Neural Networks"
date: 2019-01-29 21:41:09
categories: arXiv_CV
tags: arXiv_CV
author: Sapan Agarwal, Diana Garland, John Niroula, Robin B, Jacobs-Gedrim, Alex Hsia, Michael S. Van Heukelom, Elliot Fuller, Bruce Draper, Matthew J. Marinella
mathjax: true
---

* content
{:toc}

##### Abstract
Floating gate SONOS (Silicon-Oxygen-Nitrogen-Oxygen-Silicon) transistors can be used to train neural networks to ideal accuracies that match those of floating point digital weights on the MNIST dataset when using multiple devices to represent a weight or within 1% of ideal accuracy when using a single device. This is enabled by operating devices in the subthreshold regime, where they exhibit symmetric write nonlinearities. A neural training accelerator core based on SONOS with a single device per weight would increase energy efficiency by 120X, operate 2.1X faster and require 5X lower area than an optimized SRAM based ASIC.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1901.10570](https://arxiv.org/abs/1901.10570)

##### PDF
[https://arxiv.org/pdf/1901.10570](https://arxiv.org/pdf/1901.10570)

