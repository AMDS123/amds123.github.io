---
layout: post
title: "Simpler Context-Dependent Logical Forms via Model Projections"
date: 2016-06-16 21:57:11
categories: arXiv_CL
tags: arXiv_CL
author: Reginald Long, Panupong Pasupat, Percy Liang
mathjax: true
---

* content
{:toc}

##### Abstract
We consider the task of learning a context-dependent mapping from utterances to denotations. With only denotations at training time, we must search over a combinatorially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new context-dependent semantic parsing datasets, and develop a new left-to-right parser.

##### Abstract (translated by Google)
我们考虑从话语到指示学习依赖于上下文的映射的任务。只有在培训时间的指示，我们必须搜索一个逻辑形式的组合大空间，这是更大的上下文相关的话语。为了应对这一挑战，我们对整个模型进行了连续的投影，将其运用到逻辑形式的等价类上的更简单的模型上。尽管表现力较差，但我们发现这些简单的模型要快得多，而且可能会有惊人的效果。此外，他们可以用来引导整个模型。最后，我们收集了三个新的依赖于上下文的语义分析数据集，并开发了一个新的从左到右的分析器。

##### URL
[https://arxiv.org/abs/1606.05378](https://arxiv.org/abs/1606.05378)

##### PDF
[https://arxiv.org/pdf/1606.05378](https://arxiv.org/pdf/1606.05378)

