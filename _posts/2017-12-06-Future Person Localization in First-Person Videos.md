---
layout: post
title: 'Future Person Localization in First-Person Videos'
date: 2017-12-06 01:34:45
categories: arXiv_CV
tags: arXiv_CV
author: Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, Yoichi Sato
---

* content
{:toc}

##### Abstract
We present a new task that predicts future locations of people observed in first-person videos. Consider a first-person video stream continuously recorded by a wearable camera. Given a short clip of a person that is extracted from the complete stream, we aim to predict his location in future frames. To facilitate this future person localization ability, we make the following three key observations: a) First-person videos typically involve significant ego-motion which greatly affects the location of the target person in future frames; b) Scale of the target person act as a salient cue to estimate a perspective effect in first-person videos; c) First-person videos often capture people up-close, making it easier to leverage target poses (e.g. where they look) for predicting their future locations. We incorporate these three observations into a prediction framework with a multi-stream convolution-deconvolution architecture. Experimental results reveal our method to be effective on our new dataset as well as on a public social interaction dataset.

##### Abstract (translated by Google)
我们提出了一个新的任务，预测在第一人称视频中观察到的人们的未来位置。考虑由可佩带照相机连续记录的第一人称视频流。给定一个从整个流中提取的人的短片，我们的目标是在未来的帧中预测他的位置。为了促进未来人的定位能力，我们做出以下三个关键的观察：a）第一人称视频通常涉及显着的自我运动，这极大地影响了未来帧中目标人的位置; b）目标人的比例作为一个突出提示来估计第一人称视频中的透视效应; c）第一人称视频经常捕捉到人物靠近，使得更容易利用目标姿势（例如他们看的地方）来预测他们未来的位置。我们将这三个观测数据纳入一个具有多流卷积 - 反卷积架构的预测框架中。实验结果表明，我们的方法是有效的，我们的新数据集以及公共社会互动数据集。

##### URL
[https://arxiv.org/abs/1711.11217](https://arxiv.org/abs/1711.11217)

