---
layout: post
title: "Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges"
date: 2018-03-20 16:44:47
categories: arXiv_AI
tags: arXiv_AI Face Classification Deep_Learning
author: Gabrielle Ras, Pim Haselager, Marcel van Gerven
mathjax: true
---

* content
{:toc}

##### Abstract
Issues regarding explainable AI involve four components: users, laws &amp; regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.

##### Abstract (translated by Google)
有关可解释的AI的问题涉及四个组成部分：用户，法律和政策。法规，解释和算法。这些组件一起提供了一个可以评估解释方法的充分性的背景。本章的目标是弥合专家用户和非专业用户之间的差距。识别出不同类型的用户并揭示他们的关注点，在深度神经网络（DNN）的背景下分析“通用数据保护条例”的相关声明，介绍了现有解释方法分类的分类法，最后介绍了各种类别的解释方法进行分析，以验证用户的担忧是否合理。总体而言，很明显可以对输入对输出影响的各个方面给出（视觉）解释。但是，值得注意的是，对于非专业用户的解释方法或接口缺失，我们推测这些方法/接口应满足哪些标准。最后，值得注意的是，两个重要的问题很难通过解释方法来解决：对数据集中导致偏倚DNN的偏倚的担忧以及对不公平结果的怀疑。

##### URL
[http://arxiv.org/abs/1803.07517](http://arxiv.org/abs/1803.07517)

##### PDF
[http://arxiv.org/pdf/1803.07517](http://arxiv.org/pdf/1803.07517)

