---
layout: post
title: "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning"
date: 2018-02-13 11:18:41
categories: arXiv_AI
tags: arXiv_AI Sparse Reinforcement_Learning
author: Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Chun-Yi Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.

##### Abstract (translated by Google)
有效的探索在强化学习中仍然是一个具有挑战性的研究问题，特别是当环境包含大型状态空间，欺骗性局部最优或稀疏奖励时。为了解决这个问题，我们提出了一种多样性驱动的探索方法，它可以很容易地与离线和在线强化学习算法相结合。我们表明，通过简单地将损失函数添加到距离度量中，所提出的方法显着增强了代理人的探索性行为，从而防止政策被困在当地的最佳状态。我们进一步提出了一种稳定学习过程的自适应缩放方法。我们在Atari 2600上的实验结果表明，我们的方法在平均分数和勘探效率方面胜过了几项任务的基准方法。

##### URL
[http://arxiv.org/abs/1802.04564](http://arxiv.org/abs/1802.04564)

##### PDF
[http://arxiv.org/pdf/1802.04564](http://arxiv.org/pdf/1802.04564)

