---
layout: post
title: "Training Language Models Using Target-Propagation"
date: 2017-02-15 20:56:30
categories: arXiv_SD
tags: arXiv_SD RNN Language_Model
author: Sam Wiseman, Sumit Chopra, Marc'Aurelio Ranzato, Arthur Szlam, Ruoyu Sun, Soumith Chintala, Nicolas Vasilache
mathjax: true
---

* content
{:toc}

##### Abstract
While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.

##### Abstract (translated by Google)
虽然时间截断后向传播（BPTT）是训练递归神经网络（RNN）最常用的方法，但是它遭受固有的顺序（使并行化困难）以及在远处的时间步骤之间截断梯度流。我们调查目标传播（TPROP）风格方法是否可以解决这些缺点。不幸的是，广泛的实验表明，TPROP通常不足BPTT，我们结束对这一现象的分析，并为今后的工作提出建议。

##### URL
[https://arxiv.org/abs/1702.04770](https://arxiv.org/abs/1702.04770)

##### PDF
[https://arxiv.org/pdf/1702.04770](https://arxiv.org/pdf/1702.04770)

