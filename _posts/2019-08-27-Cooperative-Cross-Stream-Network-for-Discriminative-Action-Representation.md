---
layout: post
title: "Cooperative Cross-Stream Network for Discriminative Action Representation"
date: 2019-08-27 11:23:34
categories: arXiv_CV
tags: arXiv_CV Attention Action_Recognition Embedding Relation Recognition
author: Jingran Zhang, Fumin Shen, Xing Xu, Heng Tao Shen
mathjax: true
---

* content
{:toc}

##### Abstract
Spatial and temporal stream model has gained great success in video action recognition. Most existing works pay more attention to designing effective features fusion methods, which train the two-stream model in a separate way. However, it's hard to ensure discriminability and explore complementary information between different streams in existing works. In this work, we propose a novel cooperative cross-stream network that investigates the conjoint information in multiple different modalities. The jointly spatial and temporal stream networks feature extraction is accomplished by an end-to-end learning manner. It extracts this complementary information of different modality from a connection block, which aims at exploring correlations of different stream features. Furthermore, different from the conventional ConvNet that learns the deep separable features with only one cross-entropy loss, our proposed model enhances the discriminative power of the deeply learned features and reduces the undesired modality discrepancy by jointly optimizing a modality ranking constraint and a cross-entropy loss for both homogeneous and heterogeneous modalities. The modality ranking constraint constitutes intra-modality discriminative embedding and inter-modality triplet constraint, and it reduces both the intra-modality and cross-modality feature variations. Experiments on three benchmark datasets demonstrate that by cooperating appearance and motion feature extraction, our method can achieve state-of-the-art or competitive performance compared with existing results.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.10136](http://arxiv.org/abs/1908.10136)

##### PDF
[http://arxiv.org/pdf/1908.10136](http://arxiv.org/pdf/1908.10136)

