---
layout: post
title: "ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism"
date: 2017-11-30 14:50:55
categories: arXiv_CV
tags: arXiv_CV Adversarial Deep_Learning Detection
author: Pierre Stock, Moustapha Cisse
mathjax: true
---

* content
{:toc}

##### Abstract
ConvNets and Imagenet have driven the recent success of deep learning for image classification. However, the marked slowdown in performance improvement, the recent studies on the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases (e.g racial biases) questioned the reliability and the sustained development of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. We experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are underestimated. We show that explanations can mitigate the impact of misclassified adversarial examples from the perspective of the end-user and we introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a promising tool for improving our understanding of ConvNets' predictions and for designing more reliable models

##### Abstract (translated by Google)
ConvNets和Imagenet推动了图像分类深度学习的最近成功。然而，性能改善的明显放缓，最近有关神经网络对敌对案例的鲁棒性缺乏研究以及它们表现出不良偏见（如种族偏见）倾向的研究质疑这些方法的可靠性和持续发展。这项工作从最终用户的角度，通过使用人类学科的研究和解释来调查这些问题。我们通过实验证明，在Imagenet上测量的ConvNets的准确性和鲁棒性被低估了。我们表明，解释可以从最终用户的角度减轻错误分类的敌对性事例的影响，我们引入一种新的工具来揭示模型学到的不良偏见。这些贡献还表明，解释是提高我们对ConvNets预测的理解和设计更可靠模型的有前景的工具

##### URL
[https://arxiv.org/abs/1711.11443](https://arxiv.org/abs/1711.11443)

