---
layout: post
title: "Language to Logical Form with Neural Attention"
date: 2016-06-06 21:06:55
categories: arXiv_SD
tags: arXiv_SD Attention
author: Li Dong, Mirella Lapata
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.

##### Abstract (translated by Google)
语义分析旨在将自然语言映射到机器可解释的含义表示。传统的方法依赖于高质量的词典，手动构建的模板以及特定于域或者表示的语言特征。在本文中，我们提出了一个基于注意力增强编码器 - 解码器模型的一般方法。我们将输入话语编码成矢量表示，并通过调节编码矢量上的输出序列或树来生成其逻辑形式。在四个数据集上的实验结果表明，我们的方法在没有使用手工特征的情况下具有竞争力，并且易于跨领域和意义表示方式进行调整。

##### URL
[https://arxiv.org/abs/1601.01280](https://arxiv.org/abs/1601.01280)

##### PDF
[https://arxiv.org/pdf/1601.01280](https://arxiv.org/pdf/1601.01280)

