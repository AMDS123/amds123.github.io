---
layout: post
title: "Multi-task Learning for Universal Sentence Embeddings: A Thorough Evaluation using Transfer and Auxiliary Tasks"
date: 2018-08-16 18:14:01
categories: arXiv_CL
tags: arXiv_CL Embedding Transfer_Learning Inference RNN Quantitative
author: Wasi Uddin Ahmad, Xueying Bai, Zhechao Huang, Chao Jiang, Nanyun Peng, Kai-Wei Chang
mathjax: true
---

* content
{:toc}

##### Abstract
Learning distributed sentence representations is one of the key challenges in natural language processing. Previous work demonstrated that a recurrent neural network (RNNs) based sentence encoder trained on a large collection of annotated natural language inference data, is efficient in the transfer learning to facilitate other related tasks. In this paper, we show that joint learning of multiple tasks results in better generalizable sentence representations by conducting extensive experiments and analysis comparing the multi-task and single-task learned sentence encoders. The quantitative analysis using auxiliary tasks show that multi-task learning helps to embed better semantic information in the sentence representations compared to single-task learning. In addition, we compare multi-task sentence encoders with contextualized word representations and show that combining both of them can further boost the performance of transfer learning.

##### Abstract (translated by Google)
学习分布式句子表示是自然语言处理中的关键挑战之一。先前的工作证明，基于递归神经网络（RNN）的句子编码器在大量注释的自然语言推断数据上训练，在转移学习中有效地促进其他相关任务。在本文中，我们通过比较多任务和单任务学习句子编码器的广泛实验和分析，表明多个任务的联合学习导致更好的可推广的句子表示。使用辅助任务的定量分析表明，与单任务学习相比，多任务学习有助于在句子表示中嵌入更好的语义信息。此外，我们将多任务句子编码器与情境化词语表示进行比较，并表明将两者结合起来可以进一步提高传递学习的性能。

##### URL
[http://arxiv.org/abs/1804.07911](http://arxiv.org/abs/1804.07911)

##### PDF
[http://arxiv.org/pdf/1804.07911](http://arxiv.org/pdf/1804.07911)

