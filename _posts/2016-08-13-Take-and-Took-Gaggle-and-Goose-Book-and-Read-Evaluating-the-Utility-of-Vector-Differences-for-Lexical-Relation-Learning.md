---
layout: post
title: "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning"
date: 2016-08-13 17:56:01
categories: arXiv_CL
tags: arXiv_CL Embedding Prediction Relation
author: Ekaterina Vylomova, Laura Rimell, Trevor Cohn, Timothy Baldwin
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items.

##### Abstract (translated by Google)
最近有关词语嵌入的研究表明，尽管缺乏明确的监督，但对预先训练好的嵌入进行简单的向量减法在捕捉不同的词汇关系方面出人意料地有效。之前的工作已经使用词类比预测公式和手选关系评估了这个有趣的结果，但是对更广泛的词汇关系类型和不同学习环境的发现的一般性尚未被评估。在本文中，我们在两个学习设置中进行这样的评估：（1）谱聚类来诱导词关系，（2）监督学习将矢量差分类为关系类型。我们发现词嵌入捕获了惊人数量的信息，并且在适当的监督训练下，向量减法一般适用于广泛的关系，包括看不见的词汇项。

##### URL
[https://arxiv.org/abs/1509.01692](https://arxiv.org/abs/1509.01692)

##### PDF
[https://arxiv.org/pdf/1509.01692](https://arxiv.org/pdf/1509.01692)

