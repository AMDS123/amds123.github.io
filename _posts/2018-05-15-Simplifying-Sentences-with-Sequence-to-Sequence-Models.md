---
layout: post
title: "Simplifying Sentences with Sequence to Sequence Models"
date: 2018-05-15 04:49:55
categories: arXiv_CL
tags: arXiv_CL Text_Generation Style_Transfer Embedding
author: Alexander Mathews, Lexing Xie, Xuming He
mathjax: true
---

* content
{:toc}

##### Abstract
We simplify sentences with an attentive neural network sequence to sequence model, dubbed S4. The model includes a novel word-copy mechanism and loss function to exploit linguistic similarities between the original and simplified sentences. It also jointly uses pre-trained and fine-tuned word embeddings to capture the semantics of complex sentences and to mitigate the effects of limited data. When trained and evaluated on pairs of sentences from thousands of news articles, we observe a 8.8 point improvement in BLEU score over a sequence to sequence baseline; however, learning word substitutions remains difficult. Such sequence to sequence models are promising for other text generation tasks such as style transfer.

##### Abstract (translated by Google)
我们用一个细心的神经网络序列来简化句子，以序列模型命名为S4。该模型包括一种新颖的文字复制机制和损失功能，以利用原文和简化句子之间的语言相似性。它还联合使用预先训练和精细调整的词嵌入来捕捉复杂句子的语义并减轻有限数据的影响。当从数千篇新闻文章中对成对的句子进行训练和评估时，我们观察到BLEU评分在序列基线序列上的8.8分改善;然而，学习单词替换仍然很困难。序列模型的这种序列对于样式转换等其他文本生成任务是有前途的。

##### URL
[http://arxiv.org/abs/1805.05557](http://arxiv.org/abs/1805.05557)

##### PDF
[http://arxiv.org/pdf/1805.05557](http://arxiv.org/pdf/1805.05557)

