---
layout: post
title: "Investigating Audio, Visual, and Text Fusion Methods for End-to-End Automatic Personality Prediction"
date: 2018-05-02 10:03:13
categories: arXiv_AI
tags: arXiv_AI CNN Prediction
author: Elham J. Barezi, Onno Kampman, Dario Bertero, Pascale Fung
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a tri-modal architecture to predict Big Five personality trait scores from video clips with different channels for audio, text, and video data. For each channel, stacked Convolutional Neural Networks are employed. The channels are fused both on decision-level and by concatenating their respective fully connected layers. It is shown that a multimodal fusion approach outperforms each single modality channel, with an improvement of 9.4\% over the best individual modality (video). Full backpropagation is also shown to be better than a linear combination of modalities, meaning complex interactions between modalities can be leveraged to build better models. Furthermore, we can see the prediction relevance of each modality for each trait. The described model can be used to increase the emotional intelligence of virtual agents.

##### Abstract (translated by Google)
我们提出了一种三模式架构来预测来自具有不同音频，文本和视频数据频道的视频剪辑的五大人物特质分数。对于每个通道，采用堆叠式卷积神经网络。通道在决策层和连接各自完全连接的层之间融合。结果表明，多模式融合方法胜过每个单一模态通道，比最佳个体模式（视频）提高9.4％。全向反向传播也被证明优于线性模态组合，这意味着可以利用模态之间的复杂交互来构建更好的模型。此外，我们可以看到每种形式对每种特征的预测相关性。所描述的模型可以用来增加虚拟代理的情商。

##### URL
[https://arxiv.org/abs/1805.00705](https://arxiv.org/abs/1805.00705)

##### PDF
[https://arxiv.org/pdf/1805.00705](https://arxiv.org/pdf/1805.00705)

