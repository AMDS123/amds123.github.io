---
layout: post
title: "Recent Advances in Efficient Computation of Deep Convolutional Neural Networks"
date: 2018-02-11 10:22:38
categories: arXiv_CV
tags: arXiv_CV Survey CNN Deep_Learning
author: Jian Cheng, Peisong Wang, Gang Li, Qinghao Hu, Hanqing Lu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks have evolved remarkably over the past few years and they are currently the fundamental tools of many intelligent systems. At the same time, the computational complexity and resource consumption of these networks also continue to increase. This will pose a significant challenge to the deployment of such networks, especially in real-time applications or on resource-limited devices. Thus, network acceleration has become a hot topic within the deep learning community. As for hardware implementation of deep neural networks, a batch of accelerators based on FPGA/ASIC have been proposed in recent years. In this paper, we provide a comprehensive survey of recent advances in network acceleration, compression and accelerator design from both algorithm and hardware points of view. Specifically, we provide a thorough analysis of each of the following topics: network pruning, low-rank approximation, network quantization, teacher-student networks, compact network design and hardware accelerators. Finally, we will introduce and discuss a few possible future directions.

##### Abstract (translated by Google)
在过去的几年中，深度神经网络已经得到了显着的发展，并且它们是目前许多智能系统的基本工具。同时，这些网络的计算复杂度和资源消耗也在不断增加。这将对这些网络的部署构成重大挑战，特别是在实时应用或资源有限的设备上。因此，网络加速已成为深度学习社区的热门话题。对于深度神经网络的硬件实现，近年来提出了一批基于FPGA / ASIC的加速器。在本文中，我们从算法和硬件两个角度对网络加速，压缩和加速器设计的最新进展进行了全面的调查。具体而言，我们对以下每个主题进行了全面分析：网络修剪，低阶逼近，网络量化，师生网络，紧凑型网络设计和硬件加速器。最后，我们将介绍和讨论一些可能的未来方向。

##### URL
[https://arxiv.org/abs/1802.00939](https://arxiv.org/abs/1802.00939)

##### PDF
[https://arxiv.org/pdf/1802.00939](https://arxiv.org/pdf/1802.00939)

