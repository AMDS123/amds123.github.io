---
layout: post
title: "Pragmatic-Pedagogic Value Alignment"
date: 2018-02-05 20:44:09
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning Relation
author: Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry, Thomas L. Griffiths, Anca D. Dragan
mathjax: true
---

* content
{:toc}

##### Abstract
As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.

##### Abstract (translated by Google)
随着智能系统获得自主权和能力，确保其目标与人类用户的目标相匹配变得至关重要。这就是所谓的价值对齐问题。在机器人技术中，价值对齐是协作机器人设计的关键，它可以整合到人的工作流程中，成功推断并适应用户的目标。我们认为，价值对齐的有意义的解决方案必须将多智能体决策理论与丰富的人类认知数学模型相结合，使机器人能够挖掘人的自然协作能力。我们提出了一个基于完善的认知决策模型和心理理论的合作反强化学习（CIRL）动态博弈的解决方案。该解决方案捕捉到一个关键的互惠关系：人类不会孤立地计划自己的行为，而是在教学中从机器人的角度来理解机器人如何从中学习;反过来，机器人可以预见到这一点，并且务实地解释人类的行为。就我们所知，这项工作构成了基于经验验证的认知模型的价值调整的第一次正式分析。

##### URL
[http://arxiv.org/abs/1707.06354](http://arxiv.org/abs/1707.06354)

##### PDF
[http://arxiv.org/pdf/1707.06354](http://arxiv.org/pdf/1707.06354)

