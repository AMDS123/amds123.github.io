---
layout: post
title: "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference"
date: 2017-01-27 05:36:05
categories: arXiv_CL
tags: arXiv_CL Attention Represenation_Learning Inference RNN Gradient_Descent
author: Biswajit Paria, K. M. Annervaz, Ambedkar Dukkipati, Ankush Chatterjee, Sanjay Podder
mathjax: true
---

* content
{:toc}

##### Abstract
In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.

##### Abstract (translated by Google)
在这项工作中，我们使用表示学习的最新进展来提出自然语言推理问题的神经结构。我们的方法是模仿一个人如何做自然语言推理过程给出两个陈述。该模型使用长期短期记忆（LSTM），注意机制和可组合神经网络的变体来执行该任务。我们的模型的每个部分都可以被映射到人类为了执行自然语言推理的整体任务而做的清晰的功能。该模型是端到端可微的，使得随机梯度下降的训练成为可能。在斯坦福自然语言推理（SNLI）数据集中，所提出的模型实现了比文献中所有已发表的模型更好的准确性数字。

##### URL
[https://arxiv.org/abs/1611.04741](https://arxiv.org/abs/1611.04741)

##### PDF
[https://arxiv.org/pdf/1611.04741](https://arxiv.org/pdf/1611.04741)

