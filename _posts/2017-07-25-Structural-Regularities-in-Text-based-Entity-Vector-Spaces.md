---
layout: post
title: "Structural Regularities in Text-based Entity Vector Spaces"
date: 2017-07-25 11:54:19
categories: arXiv_CL
tags: arXiv_CL GAN Language_Model Relation
author: Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas
mathjax: true
---

* content
{:toc}

##### Abstract
Entity retrieval is the task of finding entities such as people or products in response to a query, based solely on the textual documents they are associated with. Recent semantic entity retrieval algorithms represent queries and experts in finite-dimensional vector spaces, where both are constructed from text sequences. We investigate entity vector spaces and the degree to which they capture structural regularities. Such vector spaces are constructed in an unsupervised manner without explicit information about structural aspects. For concreteness, we address these questions for a specific type of entity: experts in the context of expert finding. We discover how clusterings of experts correspond to committees in organizations, the ability of expert representations to encode the co-author graph, and the degree to which they encode academic rank. We compare latent, continuous representations created using methods based on distributional semantics (LSI), topic models (LDA) and neural networks (word2vec, doc2vec, SERT). Vector spaces created using neural methods, such as doc2vec and SERT, systematically perform better at clustering than LSI, LDA and word2vec. When it comes to encoding entity relations, SERT performs best.

##### Abstract (translated by Google)
实体检索是仅仅基于与其相关联的文本文档来查找响应于查询的诸如人员或产品的实体的任务。最近的语义实体检索算法代表有限维矢量空间中的查询和专家，其中两者都是从文本序列构建的。我们调查实体向量空间和它们捕获结构规律的程度。这种向量空间是以无监督的方式构造的，没有关于结构方面的明确信息。具体而言，我们针对特定类型的实体处理这些问题：专家调查中的专家。我们发现专家的组合是如何对应于组织中的委员会的，专家代表对合作者图表进行编码的能力，以及他们对学术等级进行编码的程度。我们比较使用基于分布式语义（LSI），主题模型（LDA）和神经网络（word2vec，doc2vec，SERT）的方法创建的潜在连续表示。使用神经方法创建的向量空间（如doc2vec和SERT）在集群上比LSI，LDA和word2vec系统地更好地执行。当涉及编码实体关系时，SERT执行效果最好。

##### URL
[https://arxiv.org/abs/1707.07930](https://arxiv.org/abs/1707.07930)

##### PDF
[https://arxiv.org/pdf/1707.07930](https://arxiv.org/pdf/1707.07930)

