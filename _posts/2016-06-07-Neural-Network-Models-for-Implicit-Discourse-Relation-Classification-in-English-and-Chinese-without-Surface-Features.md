---
layout: post
title: "Neural Network Models for Implicit Discourse Relation Classification in English and Chinese without Surface Features"
date: 2016-06-07 01:17:00
categories: arXiv_CL
tags: arXiv_CL Face RNN Classification Relation
author: Attapol T. Rutherford, Vera Demberg, Nianwen Xue
mathjax: true
---

* content
{:toc}

##### Abstract
Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Surface features achieve good performance, but they are not readily applicable to other languages without semantic lexicons. Previous neural models require parses, surface features, or a small label set to work well. Here, we propose neural network models that are based on feedforward and long-short term memory architecture without any surface features. To our surprise, our best configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Under various fine-grained label sets and a cross-linguistic setting, our feedforward models perform consistently better or at least just as well as systems that require hand-crafted surface features. Our models present the first neural Chinese discourse parser in the style of Chinese Discourse Treebank, showing that our results hold cross-linguistically.

##### Abstract (translated by Google)
在自然语言文本中推导隐含的话语关系是话语分析中最困难的子任务。表面特征取得了良好的性能，但是它们不适用于没有语义词典的其他语言。以前的神经模型需要解析，表面特征或一个小的标签集合才能正常工作。在这里，我们提出了基于没有任何表面特征的前馈和长短期记忆体系结构的神经网络模型。令我们惊讶的是，尽管进行了彻底的调整，但我们最好的配置前馈体系结构在大多数情况下胜过基于LSTM的模型。在各种细粒度标签集合和跨语言环境下，我们的前馈模型始终能够更好地执行，或者至少与需要手工制作表面特征的系统一样好。我们的模型以汉语话语树库的形式展示了第一个神经汉语话语分析器，表明我们的结果是跨语言的。

##### URL
[https://arxiv.org/abs/1606.01990](https://arxiv.org/abs/1606.01990)

##### PDF
[https://arxiv.org/pdf/1606.01990](https://arxiv.org/pdf/1606.01990)

