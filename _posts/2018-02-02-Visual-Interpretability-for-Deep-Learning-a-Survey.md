---
layout: post
title: "Visual Interpretability for Deep Learning: a Survey"
date: 2018-02-02 09:39:40
categories: arXiv_CV
tags: arXiv_CV Review Survey CNN Deep_Learning
author: Quanshi Zhang, Song-Chun Zhu
mathjax: true
---

* content
{:toc}

##### Abstract
This paper reviews recent studies in emerging directions of understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always an Achilles' heel of deep neural networks. At present, deep neural networks obtain a high discrimination power at the cost of low interpretability of their black-box representations. We believe that the high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. In this paper, we focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends of explainable artificial intelligence.

##### Abstract (translated by Google)
本文回顾了最近的研究在理解神经网络表示和学习神经网络与解释/解开中层表示的新兴方向。尽管深度神经网络在各种任务中表现出优越的性能，但其可解释性始终是深层神经网络的致命弱点。目前，深层神经网络以黑盒表示的可解释性较低的代价获得较高的判别能力。我们认为，高模型的可解释性可以帮助人们打破几个深度学习的瓶颈，例如从很少的注释中学习，在语义层面通过人机交互学习，在语义上调试网络表示。在本文中，我们关注卷积神经网络（CNN），并重新审视CNN表示的可视化，诊断CNN的表示的方法，解开预先训练的CNN表示的方法，学习具有解缠表示的CNN，以及基于模型的可解释性进行中到端的学习。最后，我们讨论可解释的人工智能的趋势。

##### URL
[https://arxiv.org/abs/1802.00614](https://arxiv.org/abs/1802.00614)

##### PDF
[https://arxiv.org/pdf/1802.00614](https://arxiv.org/pdf/1802.00614)

