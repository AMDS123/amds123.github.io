---
layout: post
title: "Exploring Adversarial Examples: Patterns of One-Pixel Attacks"
date: 2018-06-25 12:20:49
categories: arXiv_CV
tags: arXiv_CV Adversarial Pose_Estimation Deep_Learning
author: David K&#xfc;gler, Alexander Distergoft, Arjan Kuijper, Anirban Mukhopadhyay
mathjax: true
---

* content
{:toc}

##### Abstract
Failure cases of black-box deep learning, e.g. adversarial examples, might have severe consequences in healthcare. Yet such failures are mostly studied in the context of real-world images with calibrated attacks. To demystify the adversarial examples, rigorous studies need to be designed. Unfortunately, complexity of the medical images hinders such study design directly from the medical images. We hypothesize that adversarial examples might result from the incorrect mapping of image space to the low dimensional generation manifold by deep networks. To test the hypothesis, we simplify a complex medical problem namely pose estimation of surgical tools into its barest form. An analytical decision boundary and exhaustive search of the one-pixel attack across multiple image dimensions let us localize the regions of frequent successful one-pixel attacks at the image space.

##### Abstract (translated by Google)
黑盒深度学习的失败案例，例如敌对的例子，可能会对医疗保健产生严重后果。然而，这种失败主要是在具有校准攻击的现实世界图像的情况下进行研究的。为了揭开敌对的例子，需要设计严谨的研究。不幸的是，医学图像的复杂性直接阻碍了医学图像的研究设计。我们假设敌对的例子可能是由深层网络将图像空间错误地映射到低维生成流形。为了检验假设，我们简化了一个复杂的医学问题，即将手术工具的姿态估计转换为最简单的形式。通过分析决策边界和遍及多个图像维度的单像素攻击的详尽搜索，我们可以定位图像空间频繁成功的单像素攻击区域。

##### URL
[http://arxiv.org/abs/1806.09410](http://arxiv.org/abs/1806.09410)

##### PDF
[http://arxiv.org/pdf/1806.09410](http://arxiv.org/pdf/1806.09410)

