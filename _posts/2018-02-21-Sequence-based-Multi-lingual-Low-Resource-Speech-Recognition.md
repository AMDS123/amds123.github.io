---
layout: post
title: "Sequence-based Multi-lingual Low Resource Speech Recognition"
date: 2018-02-21 04:09:26
categories: arXiv_CL
tags: arXiv_CL GAN Speech_Recognition Classification Recognition
author: Siddharth Dalmia, Ramon Sanabria, Florian Metze, Alan W. Black
mathjax: true
---

* content
{:toc}

##### Abstract
Techniques for multi-lingual and cross-lingual speech recognition can help in low resource scenarios, to bootstrap systems and enable analysis of new languages and domains. End-to-end approaches, in particular sequence-based techniques, are attractive because of their simplicity and elegance. While it is possible to integrate traditional multi-lingual bottleneck feature extractors as front-ends, we show that end-to-end multi-lingual training of sequence models is effective on context independent models trained using Connectionist Temporal Classification (CTC) loss. We show that our model improves performance on Babel languages by over 6% absolute in terms of word/phoneme error rate when compared to mono-lingual systems built in the same setting for these languages. We also show that the trained model can be adapted cross-lingually to an unseen language using just 25% of the target data. We show that training on multiple languages is important for very low resource cross-lingual target scenarios, but not for multi-lingual testing scenarios. Here, it appears beneficial to include large well prepared datasets.

##### Abstract (translated by Google)
用于多语言和跨语言语音识别的技术可以帮助在低资源情况下引导系统并启用新语言和域的分析。端到端的方法，特别是基于序列的技术，因其简单和优雅而具有吸引力。虽然可以将传统的多语言瓶颈特征提取器作为前端进行集成，但我们表明序列模型的端到端多语言培训对使用连接主义时间分类（CTC）丢失进行训练的上下文无关模型是有效的。我们的模型表明，与单一语言系统相比，这些语言在语言/音素错误率方面的绝对性超过6％，我们的模型提高了巴别语的性能。我们还表明，只需使用目标数据的25％就可以对训练有素的模型进行跨语言调整，以获得一种看不见的语言。我们表明，多语言培训对于非常低的资源跨语言目标场景很重要，但不适用于多语言测试场景。在这里，包含大量准备好的数据集似乎是有益的。

##### URL
[http://arxiv.org/abs/1802.07420](http://arxiv.org/abs/1802.07420)

##### PDF
[http://arxiv.org/pdf/1802.07420](http://arxiv.org/pdf/1802.07420)

