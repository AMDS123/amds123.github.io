---
layout: post
title: "Classify or Select: Neural Architectures for Extractive Document Summarization"
date: 2016-11-14 03:54:10
categories: arXiv_CL
tags: arXiv_CL Summarization RNN Prediction
author: Ramesh Nallapati, Bowen Zhou, Mingbo Ma
mathjax: true
---

* content
{:toc}

##### Abstract
We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.

##### Abstract (translated by Google)
我们提出了两种新颖的，对比的基于回归神经网络（RNN）的文件抽取汇总。基于分类器的体系结构依次接受或拒绝原始文档顺序中的每个句子，作为最终摘要中的成员资格。另一方面，选择器架构可以以任意顺序随意选择一个句子，将摘要拼凑在一起。我们在两种体系结构下的模型共同捕捉到句子的显着性和冗余性的概念。另外，这些模型具有可解释性的优点，因为它们可以通过抽象特征（如信息内容，显着性和冗余性）来分解预测的可视化。我们表明，我们的模型达到或超过两个不同语料库的最先进的监督模型。基于实验证据，我们还推荐了一种架构优于另一种架构的条件。

##### URL
[https://arxiv.org/abs/1611.04244](https://arxiv.org/abs/1611.04244)

##### PDF
[https://arxiv.org/pdf/1611.04244](https://arxiv.org/pdf/1611.04244)

