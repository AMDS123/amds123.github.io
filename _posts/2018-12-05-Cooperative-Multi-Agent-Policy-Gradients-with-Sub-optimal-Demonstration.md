---
layout: post
title: "Cooperative Multi-Agent Policy Gradients with Sub-optimal Demonstration"
date: 2018-12-05 05:47:43
categories: arXiv_AI
tags: arXiv_AI Sparse
author: Peixi Peng, Junliang Xing, Lu Pang
mathjax: true
---

* content
{:toc}

##### Abstract
Many reality tasks such as robot coordination can be naturally modelled as multi-agent cooperative system where the rewards are sparse. This paper focuses on learning decentralized policies for such tasks using sub-optimal demonstration. To learn the multi-agent cooperation effectively and tackle the sub-optimality of demonstration, a self-improving learning method is proposed: On the one hand, the centralized state-action values are initialized by the demonstration and updated by the learned decentralized policy to improve the sub-optimality. On the other hand, the Nash Equilibrium are found by the current state-action value and are used as a guide to learn the policy. The proposed method is evaluated on the combat RTS games which requires a high level of multi-agent cooperation. Extensive experimental results on various combat scenarios demonstrate that the proposed method can learn multi-agent cooperation effectively. It significantly outperforms many state-of-the-art demonstration based approaches.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.01825](http://arxiv.org/abs/1812.01825)

##### PDF
[http://arxiv.org/pdf/1812.01825](http://arxiv.org/pdf/1812.01825)

