---
layout: post
title: "Recurrent Dropout without Memory Loss"
date: 2016-08-05 09:59:25
categories: arXiv_SD
tags: arXiv_SD Regularization RNN
author: Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to \textit{forward} connections of feed-forward architectures or RNNs, we propose to drop neurons directly in \textit{recurrent} connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for Long Short-Term Memory network, the most popular type of RNN cells. Our experiments on NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.

##### Abstract (translated by Google)
本文提出了一种新颖的递归神经网络（RNN）正则化方法。与广泛采用的应用于前馈体系结构或RNN的\ textit {forward}连接的丢弃方法不同，我们建议直接将神经元放在\ textit {经常性}连接中，长期记忆。我们的方法与常规的前馈丢失一样容易实现和应用，我们证明了其对于长期短期记忆网络（最受欢迎的RNN小区类型）的有效性。我们在NLP基准测试中的实验显示，即使与传统的前馈丢失相结合，也能得到一致的改进。

##### URL
[https://arxiv.org/abs/1603.05118](https://arxiv.org/abs/1603.05118)

##### PDF
[https://arxiv.org/pdf/1603.05118](https://arxiv.org/pdf/1603.05118)

