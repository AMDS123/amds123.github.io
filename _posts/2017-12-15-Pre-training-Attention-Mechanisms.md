---
layout: post
title: "Pre-training Attention Mechanisms"
date: 2017-12-15 12:59:22
categories: arXiv_CV
tags: arXiv_CV Salient Attention RNN Classification
author: Jack Lindsey
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks with differentiable attention mechanisms have had success in generative and classification tasks. We show that the classification performance of such models can be enhanced by guiding a randomly initialized model to attend to salient regions of the input in early training iterations. We further show that, if explicit heuristics for guidance are unavailable, a model that is pretrained on an unsupervised reconstruction task can discover good attention policies without supervision. We demonstrate that increased efficiency of the attention mechanism itself contributes to these performance improvements. Based on these insights, we introduce bootstrapped glimpse mimicking, a simple, theoretically task-general method of more effectively training attention models. Our work draws inspiration from and parallels results on human learning of attention.

##### Abstract (translated by Google)
具有可区分的注意机制的递归神经网络在生成和分类任务方面已经取得了成功。我们表明，这种模型的分类性能可以通过引导一个随机初始化模型来加强早期训练迭代中输入的显着区域的输入。我们进一步表明，如果明确的启发式指导是不可用的，那么在无监督的重建任务上预训的模型可以在没有监督的情况下发现好的关注策略。我们证明，提高效率的关注机制本身有助于这些性能的改善。基于这些见解，我们介绍引导式瞥见模仿，一种更加有效地训练注意力模型的简单理论上的任务一般方法。我们的工作从人类的学习注意力中得到灵感和相似的结果。

##### URL
[http://arxiv.org/abs/1712.05652](http://arxiv.org/abs/1712.05652)

##### PDF
[http://arxiv.org/pdf/1712.05652](http://arxiv.org/pdf/1712.05652)

