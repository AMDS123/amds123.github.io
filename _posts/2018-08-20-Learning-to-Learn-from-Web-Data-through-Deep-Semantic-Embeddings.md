---
layout: post
title: "Learning to Learn from Web Data through Deep Semantic Embeddings"
date: 2018-08-20 09:58:23
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval Knowledge Embedding
author: Raul Gomez, Lluis Gomez, Jaume Gibert, Dimosthenis Karatzas
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we propose to learn a multimodal image and text embedding from Web and Social Media data, aiming to leverage the semantic knowledge learnt in the text domain and transfer it to a visual model for semantic image retrieval. We demonstrate that the pipeline can learn from images with associated text without supervision and perform a thourough analysis of five different text embeddings in three different benchmarks. We show that the embeddings learnt with Web and Social Media data have competitive performances over supervised methods in the text based image retrieval task, and we clearly outperform state of the art in the MIRFlickr dataset when training in the target data. Further we demonstrate how semantic multimodal image retrieval can be performed using the learnt embeddings, going beyond classical instance-level retrieval problems. Finally, we present a new dataset, InstaCities1M, composed by Instagram images and their associated texts that can be used for fair comparison of image-text embeddings.

##### Abstract (translated by Google)
在本文中，我们建议从Web和社交媒体数据中学习多模态图像和文本嵌入，旨在利用在文本域中学习的语义知识并将其转换为用于语义图像检索的视觉模型。我们证明了管道可以在没有监督的情况下从具有相关文本的图像中学习，并在三个不同的基准测试中对五种不同的文本嵌入进行了彻底的分析。我们表明，使用Web和社交媒体数据学习的嵌入在基于文本的图像检索任务中具有超过监督方法的竞争性能，并且当在目标数据中进行训练时，我们明显优于MIRFlickr数据集中的现有技术。此外，我们演示了如何使用学习嵌入来执行语义多模态图像检索，超越了传统的实例级检索问题。最后，我们提出了一个新的数据集InstaCities1M，由Instagram图像及其相关文本组成，可用于图像文本嵌入的公平比较。

##### URL
[http://arxiv.org/abs/1808.06368](http://arxiv.org/abs/1808.06368)

##### PDF
[http://arxiv.org/pdf/1808.06368](http://arxiv.org/pdf/1808.06368)

