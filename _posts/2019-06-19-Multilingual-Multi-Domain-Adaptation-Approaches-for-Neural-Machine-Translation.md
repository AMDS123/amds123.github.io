---
layout: post
title: "Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation"
date: 2019-06-19 08:56:02
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Chenhui Chu, Raj Dabre
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose two novel methods for domain adaptation for the attention-only neural machine translation (NMT) model, i.e., the Transformer. Our methods focus on training a single translation model for multiple domains by either learning domain specialized hidden state representations or predictor biases for each domain. We combine our methods with a previously proposed black-box method called mixed fine tuning, which is known to be highly effective for domain adaptation. In addition, we incorporate multilingualism into the domain adaptation framework. Experiments show that multilingual multi-domain adaptation can significantly improve both resource-poor in-domain and resource-rich out-of-domain translations, and the combination of our methods with mixed fine tuning achieves the best performance.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.07978](http://arxiv.org/abs/1906.07978)

##### PDF
[http://arxiv.org/pdf/1906.07978](http://arxiv.org/pdf/1906.07978)

