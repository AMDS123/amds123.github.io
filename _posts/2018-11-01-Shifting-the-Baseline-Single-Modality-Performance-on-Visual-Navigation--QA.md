---
layout: post
title: "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA"
date: 2018-11-01 19:54:58
categories: arXiv_CL
tags: arXiv_CL QA
author: Jesse Thomason, Daniel Gordan, Yonatan Bisk
mathjax: true
---

* content
{:toc}

##### Abstract
Language-and-vision navigation and question answering (QA) are exciting AI tasks situated at the intersection of natural language understanding, computer vision, and robotics. Researchers from all of these fields have begun creating datasets and model architectures for these domains. It is, however, not always clear if strong performance is due to advances in multimodal reasoning or if models are learning to exploit biases and artifacts of the data. We present single modality models and explore the linguistic, visual, and structural biases of these benchmarks. We find that single modality models often outperform published baselines that accompany multimodal task datasets, suggesting a need for change in community best practices moving forward. In light of this, we recommend presenting single modality baselines alongside new multimodal models to provide a fair comparison of information gained over dataset biases when considering multimodal input.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.00613](http://arxiv.org/abs/1811.00613)

##### PDF
[http://arxiv.org/pdf/1811.00613](http://arxiv.org/pdf/1811.00613)

