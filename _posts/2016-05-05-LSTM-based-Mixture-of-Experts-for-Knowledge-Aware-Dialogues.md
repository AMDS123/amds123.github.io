---
layout: post
title: "LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues"
date: 2016-05-05 17:00:44
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention RNN Language_Model Prediction
author: Phong Le, Marc Dymetman, Jean-Michel Renders
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce an LSTM-based method for dynamically integrating several word-prediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP.

##### Abstract (translated by Google)
我们引入了一个基于LSTM的方法来动态集成几个预测专家，从而得到一个条件语言模型，可以很好地同时处理几个子任务。我们用一个对话的应用来说明这个通用的方法，在这个方法中，我们整合了一个善于交谈的神经聊天模型，一个神经问题回答模型，善于从知识库中获取精确的信息，并且展示整合如何结合长处的独立组件。我们希望这个重点突出的贡献将会吸引人们注意在NLP中使用这种混合专家的好处。

##### URL
[https://arxiv.org/abs/1605.01652](https://arxiv.org/abs/1605.01652)

##### PDF
[https://arxiv.org/pdf/1605.01652](https://arxiv.org/pdf/1605.01652)

