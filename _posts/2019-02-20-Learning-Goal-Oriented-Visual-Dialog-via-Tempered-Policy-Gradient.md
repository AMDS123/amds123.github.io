---
layout: post
title: "Learning Goal-Oriented Visual Dialog via Tempered Policy Gradient"
date: 2019-02-20 10:22:01
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Rui Zhao, Volker Tresp
mathjax: true
---

* content
{:toc}

##### Abstract
Learning goal-oriented dialogues by means of deep reinforcement learning has recently become a popular research topic. However, commonly used policy-based dialogue agents often end up focusing on simple utterances and suboptimal policies. To mitigate this problem, we propose a class of novel temperature-based extensions for policy gradient methods, which are referred to as Tempered Policy Gradients (TPGs). On a recent AI-testbed, i.e., the GuessWhat?! game, we achieve significant improvements with two innovations. The first one is an extension of the state-of-the-art solutions with Seq2Seq and Memory Network structures that leads to an improvement of 7%. The second one is the application of our newly developed TPG methods, which improves the performance additionally by around 5% and, even more importantly, helps produce more convincing utterances.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1807.00737](http://arxiv.org/abs/1807.00737)

##### PDF
[http://arxiv.org/pdf/1807.00737](http://arxiv.org/pdf/1807.00737)

