---
layout: post
title: "Black-box Adversarial Attacks with Limited Queries and Information"
date: 2018-06-07 18:17:09
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin
mathjax: true
---

* content
{:toc}

##### Abstract
Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.

##### Abstract (translated by Google)
当前的基于神经网络的分类器即使在黑盒子设置中也容易受到对手攻击，其中攻击者只能对该模型进行查询访问。实际上，真实世界系统的威胁模型通常比典型的黑盒模型更具限制性，在黑盒模型中，攻击者可以在任意多个选定的输入上观察网络的全部输出。我们定义了三个逼真的威胁模型，可以更准确地描述许多真实世界的分类器：查询限制设置，部分信息设置和仅标签设置。我们在这些更严格的威胁模型下开发新的攻击来欺骗分类器，因为以前的方法不切实际或无效。我们证明我们的方法在我们提出的威胁模型下对ImageNet分类器是有效的。我们还展示了针对商业分类器的针对性黑盒攻击，克服了有限查询访问，部分信息和其他实用问题以打破Google Cloud Vision API的挑战。

##### URL
[http://arxiv.org/abs/1804.08598](http://arxiv.org/abs/1804.08598)

##### PDF
[http://arxiv.org/pdf/1804.08598](http://arxiv.org/pdf/1804.08598)

