---
layout: post
title: "iVQA: Inverse Visual Question Answering"
date: 2018-03-16 06:54:11
categories: arXiv_CV
tags: arXiv_CV QA Attention Inference VQA
author: Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun
mathjax: true
---

* content
{:toc}

##### Abstract
We propose the inverse problem of Visual question answering (iVQA), and explore its suitability as a benchmark for visuo-linguistic understanding. The iVQA task is to generate a question that corresponds to a given image and answer pair. Since the answers are less informative than the questions, and the questions have less learnable bias, an iVQA model needs to better understand the image to be successful than a VQA model. We pose question generation as a multi-modal dynamic inference process and propose an iVQA model that can gradually adjust its focus of attention guided by both a partially generated question and the answer. For evaluation, apart from existing linguistic metrics, we propose a new ranking metric. This metric compares the ground truth question's rank among a list of distractors, which allows the drawbacks of different algorithms and sources of error to be studied. Experimental results show that our model can generate diverse, grammatically correct and content correlated questions that match the given answer.

##### Abstract (translated by Google)
我们提出了视觉问答（iVQA）的逆问题，并探讨其作为视觉语言理解基准的适用性。 iVQA任务是生成与给定图像和答案对相对应的问题。由于答案的信息量少于问题，而且问题的可学习偏差较小，因此iVQA模型需要比VQA模型更好地理解图像是否成功。我们将问题生成作为一种多模式动态推理过程，并提出一种iVQA模型，该模型可以逐步调整其注意力的焦点，并由部分生成的问题和答案引导。对于评估，除了现有的语言指标外，我们还提出了新的排名指标。该指标将基础事实问题的排名与干扰因素列表进行比较，从而可以研究不同算法和错误来源的缺点。实验结果表明，我们的模型可以生成与给定答案匹配的多样，语法正确和内容相关的问题。

##### URL
[https://arxiv.org/abs/1710.03370](https://arxiv.org/abs/1710.03370)

##### PDF
[https://arxiv.org/pdf/1710.03370](https://arxiv.org/pdf/1710.03370)

