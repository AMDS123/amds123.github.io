---
layout: post
title: 'Tree-to-Sequence Attentional Neural Machine Translation'
date: 2017-12-06 09:36:23
categories: arXiv_CL
tags: arXiv_CL NMT
author: Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka
---

* content
{:toc}

##### Abstract
Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.

##### Abstract (translated by Google)
现有的神经机器翻译（NMT）模型大多集中于顺序数据的转换，不直接使用句法信息。我们提出了一个新的端到端句法NMT模型，用源端短语结构扩展了序列到序列模型。我们的模型有一个注意机制，使解码器能够生成一个翻译的单词，同时轻松地将它与短语以及源句子的单词对齐。 WAT'15 English-to-Japanese数据集上的实验结果表明，我们提出的模型在性能上明显优于序列注意NMT模型，并且与最先进的树 - 串SMT系统相比较。

##### URL
[https://arxiv.org/abs/1603.06075](https://arxiv.org/abs/1603.06075)

