---
layout: post
title: "Probabilistic Sparse Subspace Clustering Using Delayed Association"
date: 2018-08-28 23:03:55
categories: arXiv_CV
tags: arXiv_CV Sparse Optimization Recognition
author: Maryam Jaberi, Marianna Pensky, Hassan Foroosh
mathjax: true
---

* content
{:toc}

##### Abstract
Discovering and clustering subspaces in high-dimensional data is a fundamental problem of machine learning with a wide range of applications in data mining, computer vision, and pattern recognition. Earlier methods divided the problem into two separate stages of finding the similarity matrix and finding clusters. Similar to some recent works, we integrate these two steps using a joint optimization approach. We make the following contributions: (i) we estimate the reliability of the cluster assignment for each point before assigning a point to a subspace. We group the data points into two groups of "certain" and "uncertain", with the assignment of latter group delayed until their subspace association certainty improves. (ii) We demonstrate that delayed association is better suited for clustering subspaces that have ambiguities, i.e. when subspaces intersect or data are contaminated with outliers/noise. (iii) We demonstrate experimentally that such delayed probabilistic association leads to a more accurate self-representation and final clusters. The proposed method has higher accuracy both for points that exclusively lie in one subspace, and those that are on the intersection of subspaces. (iv) We show that delayed association leads to huge reduction of computational cost, since it allows for incremental spectral clustering.

##### Abstract (translated by Google)
在高维数据中发现和聚类子空间是机器学习的基本问题，在数据挖掘，计算机视觉和模式识别方面具有广泛的应用。早期的方法将问题分为两个独立的阶段：寻找相似性矩阵和寻找聚类。与最近的一些作品类似，我们使用联合优化方法整合这两个步骤。我们做出以下贡献：（i）我们在将点分配给子空间之前估计每个点的簇分配的可靠性。我们将数据点分为两组“确定”和“不确定”，后一组的分配被延迟，直到它们的子空间关联确定性得到改善。 （ii）我们证明延迟关联更适合于聚类具有模糊性的子空间，即当子空间相交或数据被异常值/噪声污染时。 （iii）我们通过实验证明，这种延迟的概率关联导致更准确的自我表现和最终聚类。所提出的方法对于专门位于一个子空间中的点和位于子空间交叉点上的点具有更高的准确度。 （iv）我们表明延迟关联导致计算成本的大幅降低，因为它允许增量谱聚类。

##### URL
[http://arxiv.org/abs/1808.09574](http://arxiv.org/abs/1808.09574)

##### PDF
[http://arxiv.org/pdf/1808.09574](http://arxiv.org/pdf/1808.09574)

