---
layout: post
title: "Learning End-to-End Goal-Oriented Dialog with Multiple Answers"
date: 2018-08-24 19:24:58
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Janarthanan Rajendran, Jatin Ganhotra, Satinder Singh, Lazaros Polymenakos
mathjax: true
---

* content
{:toc}

##### Abstract
In a dialog, there can be multiple valid next utterances at any point. The present end-to-end neural methods for dialog do not take this into account. They learn with the assumption that at any time there is only one correct next utterance. In this work, we focus on this problem in the goal-oriented dialog setting where there are different paths to reach a goal. We propose a new method, that uses a combination of supervised learning and reinforcement learning approaches to address this issue. We also propose a new and more effective testbed, permuted-bAbI dialog tasks, by introducing multiple valid next utterances to the original-bAbI dialog tasks, which allows evaluation of goal-oriented dialog systems in a more realistic setting. We show that there is a significant drop in performance of existing end-to-end neural methods from 81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on permuted-bAbI dialog tasks. We also show that our proposed method improves the performance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog tasks.

##### Abstract (translated by Google)
在对话框中，任何时候都可以有多个有效的下一个话语。目前用于对话的端到端神经方法不考虑这一点。他们学习的假设是，在任何时候只有一个正确的下一个话语。在这项工作中，我们专注于面向目标的对话框设置中的这个问题，其中有不同的路径来达到目标​​。我们提出了一种新方法，它使用监督学习和强化学习方法的组合来解决这个问题。我们还提出了一个新的，更有效的testbed，permuted-bAbI对话任务，通过在原始-bAbI对话任务中引入多个有效的下一个话语，这允许在更真实的设置中评估面向目标的对话系统。我们表明，现有端到端神经方法的性能显着下降，从原始-bAbI对话任务的每对话精度81.5％到置换-bAbI对话任务的30.3％。我们还表明，我们提出的方法提高了性能，并且在置换-bAbI对话任务中实现了每对话精度47.3％。

##### URL
[http://arxiv.org/abs/1808.09996](http://arxiv.org/abs/1808.09996)

##### PDF
[http://arxiv.org/pdf/1808.09996](http://arxiv.org/pdf/1808.09996)

