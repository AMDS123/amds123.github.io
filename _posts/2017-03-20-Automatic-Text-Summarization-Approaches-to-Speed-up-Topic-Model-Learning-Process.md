---
layout: post
title: "Automatic Text Summarization Approaches to Speed up Topic Model Learning Process"
date: 2017-03-20 08:19:43
categories: arXiv_SD
tags: arXiv_SD Summarization
author: Mohamed Morchid, Juan-Manuel Torres-Moreno, Richard Dufour, Javier Ramírez-Rodríguez, Georges Linarès
mathjax: true
---

* content
{:toc}

##### Abstract
The number of documents available into Internet moves each day up. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents. In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages. Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents (more than 60\% in general). This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or latin languages.

##### Abstract (translated by Google)
互联网上可用的文件数量每天都在增加。出于这个原因，有效和明确地处理这些信息成为公司和科学家的一个主要关注点。通过主题表示来表示文本文档的方法被广泛用于信息检索（IR）以处理诸如维基百科文章的大数据。在大量数据收集中使用主题模型的主要困难之一与模型估计所需的物质资源（CPU时间和内存）有关。为了处理这个问题，我们建议从汇总文档中建立主题空间。在本文中，我们提出了一个关于大数据背景下的主题空间表示的研究。主题空间表示行为分析在不同的语言。实验表明，从文本摘要估计的主题空间与完整文档估计的主题空间是相关的。这种方法的真正优势在于处理时间的增加：我们发现使用汇总文档（通常超过60％）可以大大缩短处理时间。本研究最后指出了文件的主题表征之间的差异取决于目标语言，如英语或拉丁语言。

##### URL
[https://arxiv.org/abs/1703.06630](https://arxiv.org/abs/1703.06630)

##### PDF
[https://arxiv.org/pdf/1703.06630](https://arxiv.org/pdf/1703.06630)

