---
layout: post
title: "Expressing Visual Relationships via Language"
date: 2019-06-18 17:01:21
categories: arXiv_CV
tags: arXiv_CV Image_Caption Attention Caption Relation
author: Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, Mohit Bansal
mathjax: true
---

* content
{:toc}

##### Abstract
Describing images with text is a fundamental problem in vision-language research. Current studies in this domain mostly focus on single image captioning. However, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. This important problem has not been explored mostly due to lack of datasets and effective models. To push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. We also extend the model with dynamic relational attention, which calculates visual alignment while decoding. Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. Experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.07689](http://arxiv.org/abs/1906.07689)

##### PDF
[http://arxiv.org/pdf/1906.07689](http://arxiv.org/pdf/1906.07689)

