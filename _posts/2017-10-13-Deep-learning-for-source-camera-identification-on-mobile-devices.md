---
layout: post
title: "Deep learning for source camera identification on mobile devices"
date: 2017-10-13 17:03:52
categories: arXiv_CV
tags: arXiv_CV CNN Deep_Learning Recognition
author: David Freire-Obregón, Fabio Narducci, Silvio Barra, Modesto Castrillón-Santana
mathjax: true
---

* content
{:toc}

##### Abstract
In the present paper, we propose a source camera identification method for mobile devices based on deep learning. Recently, convolutional neural networks (CNNs) have shown a remarkable performance on several tasks such as image recognition, video analysis or natural language processing. A CNN consists on a set of layers where each layer is composed by a set of high pass filters which are applied all over the input image. This convolution process provides the unique ability to extract features automatically from data and to learn from those features. Our proposal describes a CNN architecture which is able to infer the noise pattern of mobile camera sensors (also known as camera fingerprint) with the aim at detecting and identifying not only the mobile device used to capture an image (with a 98\% of accuracy), but also from which embedded camera the image was captured. More specifically, we provide an extensive analysis on the proposed architecture considering different configurations. The experiment has been carried out using the images captured from different mobile devices cameras (MICHE-I Dataset was used) and the obtained results have proved the robustness of the proposed method.

##### Abstract (translated by Google)
在本文中，我们提出了一种基于深度学习的移动设备源摄像头识别方法。最近，卷积神经网络（CNN）在诸如图像识别，视频分析或自然语言处理等几项任务中表现出了卓越的性能。 CNN由一组层组成，每层由一组高通滤波器组成，这些高通滤波器被应用到整个输入图像上。这种卷积过程提供了独特的功能，可以从数据中自动提取特征，并从这些特征中学习。我们的建议描述了一种能够推断出移动照相机传感器（也称为照相机指纹）的噪声模式的CNN架构，其目的在于不仅检测和识别用于捕获图像的移动设备（具有98％的准确度），而且还从哪个嵌入式相机捕获图像。更具体地说，我们对考虑不同配置的建议架构进行了广泛的分析。使用从不同的移动设备摄像机（使用MICHE-I数据集）捕获的图像进行实验，并且所获得的结果证明了所提出的方法的鲁棒性。

##### URL
[https://arxiv.org/abs/1710.01257](https://arxiv.org/abs/1710.01257)

##### PDF
[https://arxiv.org/pdf/1710.01257](https://arxiv.org/pdf/1710.01257)

