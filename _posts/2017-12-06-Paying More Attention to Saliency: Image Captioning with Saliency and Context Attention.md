---
layout: post
title: 'Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention'
date: 2017-12-06 02:40:01
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption CNN RNN
author: Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, Rita Cucchiara
---

* content
{:toc}

##### Abstract
Image captioning has been recently gaining a lot of attention thanks to the impressive achievements shown by deep captioning architectures, which combine Convolutional Neural Networks to extract image representations, and Recurrent Neural Networks to generate the corresponding captions. At the same time, a significant research effort has been dedicated to the development of saliency prediction models, which can predict human eye fixations. Even though saliency information could be useful to condition an image captioning architecture, by providing an indication of what is salient and what is not, research is still struggling to incorporate these two techniques. In this work, we propose an image captioning approach in which a generative recurrent neural network can focus on different parts of the input image during the generation of the caption, by exploiting the conditioning given by a saliency prediction model on which parts of the image are salient and which are contextual. We show, through extensive quantitative and qualitative experiments on large scale datasets, that our model achieves superior performances with respect to captioning baselines with and without saliency, and to different state of the art approaches combining saliency and captioning.

##### Abstract (translated by Google)
由于深度字幕体系结构（将卷积神经网络提取图像表示以及使用递归神经网络生成相应字幕）结合在一起显示出令人印象深刻的成就，因此图像字幕最近得到了很多关注。与此同时，一项重大的研究工作一直致力于显着预测模型的发展，这可以预测人眼的注视。即使显着性信息可能有助于调整图像字幕体系结构，通过提供什么是显着的和什么不显示的指示，研究仍然在努力合并这两种技术。在这项工作中，我们提出了一个图像字幕的方法，其中一个生成循环的神经网络可以集中在输入图像的不同部分在标题的生成，通过利用显着性预测模型给出的条件，图像的部分是突出和哪些是上下文的。我们通过对大规模数据集进行广泛的定量和定性实验，显示出我们的模型在字幕显示基线有无显着性方面取得了优异的表现，并且显示了结合显着性和字幕的不同状态的艺术方法。

##### URL
[https://arxiv.org/abs/1706.08474](https://arxiv.org/abs/1706.08474)

