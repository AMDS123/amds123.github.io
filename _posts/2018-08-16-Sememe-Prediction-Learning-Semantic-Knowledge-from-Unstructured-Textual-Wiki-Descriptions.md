---
layout: post
title: "Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions"
date: 2018-08-16 12:13:16
categories: arXiv_CL
tags: arXiv_CL Knowledge Prediction
author: Wei Li, Xuancheng Ren, Damai Dai, Yunfang Wu, Houfeng Wang, Xu Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Huge numbers of new words emerge every day, leading to a great need for representing them with semantic meaning that is understandable to NLP systems. Sememes are defined as the minimum semantic units of human languages, the combination of which can represent the meaning of a word. Manual construction of sememe based knowledge bases is time-consuming and labor-intensive. Fortunately, communities are devoted to composing the descriptions of words in the wiki websites. In this paper, we explore to automatically predict lexical sememes based on the descriptions of the words in the wiki websites. We view this problem as a weakly ordered multi-label task and propose a Label Distributed seq2seq model (LD-seq2seq) with a novel soft loss function to solve the problem. In the experiments, we take a real-world sememe knowledge base HowNet and the corresponding descriptions of the words in Baidu Wiki for training and evaluation. The results show that our LD-seq2seq model not only beats all the baselines significantly on the test set, but also outperforms amateur human annotators in a random subset of the test set.

##### Abstract (translated by Google)
每天都会出现大量新词，因此非常需要用NLP系统可理解的语义来表示它们。 Sememes被定义为人类语言的最小语义单位，其组合可以表示单词的含义。手工构建基于sememe的知识库是耗时且劳动密集的。幸运的是，社区致力于在wiki网站上撰写单词的描述。在本文中，我们探索基于wiki网站中的单词描述自动预测词汇sememes。我们将此问题视为弱有序的多标签任务，并提出具有新颖软丢失函数的Label Distributed seq2seq模型（LD-seq2seq）来解决该问题。在实验中，我们采用现实世界的知识库知网和百度Wiki中相应的词语描述进行培训和评估。结果表明，我们的LD-seq2seq模型不仅在测试集上显着地击败了所有基线，而且在测试集的随机子集中也优于业余人类注释器。

##### URL
[http://arxiv.org/abs/1808.05437](http://arxiv.org/abs/1808.05437)

##### PDF
[http://arxiv.org/pdf/1808.05437](http://arxiv.org/pdf/1808.05437)

