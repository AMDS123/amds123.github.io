---
layout: post
title: "Efficient Decentralized Deep Learning by Dynamic Model Averaging"
date: 2018-07-09 15:01:51
categories: arXiv_AI
tags: arXiv_AI Image_Classification Classification Deep_Learning Recognition
author: Michael Kamp, Linara Adilova, Joachim Sicking, Fabian H&#xfc;ger, Peter Schlicht, Tim Wirtz, Stefan Wrobel
mathjax: true
---

* content
{:toc}

##### Abstract
We propose an efficient protocol for decentralized training of deep neural networks from distributed data sources. The proposed protocol allows to handle different phases of model training equally well and to quickly adapt to concept drifts. This leads to a reduction of communication by an order of magnitude compared to periodically communicating state-of-the-art approaches. Moreover, we derive a communication bound that scales well with the hardness of the serialized learning problem. The reduction in communication comes at almost no cost, as the predictive performance remains virtually unchanged. Indeed, the proposed protocol retains loss bounds of periodically averaging schemes. An extensive empirical evaluation validates major improvement of the trade-off between model performance and communication which could be beneficial for numerous decentralized learning applications, such as autonomous driving, or voice recognition and image classification on mobile phones.

##### Abstract (translated by Google)
我们提出了一种有效的协议，用于从分布式数据源分散训练深度神经网络。所提议的协议允许同样良好地处理模型训练的不同阶段并快速适应概念漂移。与周期性地传达现有技术方法相比，这导致通信减少了一个数量级。此外，我们得出了一个与序列化学习问题的硬度很好地扩展的通信界限。通信的减少几乎没有成本，因为预测性能基本保持不变。实际上，所提出的协议保留了周期性平均方案的损失范围。广泛的实证评估验证了模型性能和通信之间权衡的重大改进，这可能有益于许多分散的学习应用，例如自动驾驶，或语音识别和移动电话上的图像分类。

##### URL
[http://arxiv.org/abs/1807.03210](http://arxiv.org/abs/1807.03210)

##### PDF
[http://arxiv.org/pdf/1807.03210](http://arxiv.org/pdf/1807.03210)

