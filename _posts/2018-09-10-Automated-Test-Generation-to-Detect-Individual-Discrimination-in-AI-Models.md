---
layout: post
title: "Automated Test Generation to Detect Individual Discrimination in AI Models"
date: 2018-09-10 12:11:21
categories: arXiv_AI
tags: arXiv_AI
author: Aniya Agarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, Diptikalyan Saha
mathjax: true
---

* content
{:toc}

##### Abstract
Dependability on AI models is of utmost importance to ensure full acceptance of the AI systems. One of the key aspects of the dependable AI system is to ensure that all its decisions are fair and not biased towards any individual. In this paper, we address the problem of detecting whether a model has an individual discrimination. Such a discrimination exists when two individuals who differ only in the values of their protected attributes (such as, gender/race) while the values of their non-protected ones are exactly the same, get different decisions. Measuring individual discrimination requires an exhaustive testing, which is infeasible for a non-trivial system. In this paper, we present an automated technique to generate test inputs, which is geared towards finding individual discrimination. Our technique combines the well-known technique called symbolic execution along with the local explainability for generation of effective test cases. Our experimental results clearly demonstrate that our technique produces 3.72 times more successful test cases than the existing state-of-the-art across all our chosen benchmarks.

##### Abstract (translated by Google)
AI模型的可靠性对于确保完全接受AI系统至关重要。可靠的人工智能系统的一个关键方面是确保其所有决策都是公平的，而不是偏向任何个人。在本文中，我们解决了检测模型是否具有个体歧视的问题。当两个人的受保护属性（如性别/种族）的价值不同而其未受保护的人的价值完全相同时，就会产生不同的决定。衡量个人歧视需要进行详尽的测试，这对于非平凡的系统是不可行的。在本文中，我们提出了一种生成测试输入的自动化技术，旨在发现个体歧视。我们的技术结合了众所周知的称为符号执行的技术以及用于生成有效测试用例的局部可解释性。我们的实验结果清楚地表明，我们的技术在所有选定基准测试中的成功测试用例比现有技术水平高出3.72倍。

##### URL
[http://arxiv.org/abs/1809.03260](http://arxiv.org/abs/1809.03260)

##### PDF
[http://arxiv.org/pdf/1809.03260](http://arxiv.org/pdf/1809.03260)

