---
layout: post
title: "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference"
date: 2018-08-12 05:42:26
categories: arXiv_AI
tags: arXiv_AI Salient Attention Inference RNN Deep_Learning
author: Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.

##### Abstract (translated by Google)
深度学习模型在自然语言推理（NLI）任务中取得了显着的成功。虽然这些模型被广泛探索，但它们很难解释，而且通常不清楚它们实际上是如何工作的以及为什么工作的。在本文中，我们通过一个流行的NLI神经模型的案例研究，朝着解释这种基于深度学习的模型迈出了一步。特别是，我们建议通过可视化注意力和LSTM门控信号的显着性来解释NLI模型的中间层。我们提供了几个示例，我们的方法能够揭示有趣的见解并确定有助于模型决策的关键信息。

##### URL
[http://arxiv.org/abs/1808.03894](http://arxiv.org/abs/1808.03894)

##### PDF
[http://arxiv.org/pdf/1808.03894](http://arxiv.org/pdf/1808.03894)

