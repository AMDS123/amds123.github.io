---
layout: post
title: "Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images"
date: 2016-04-14 15:05:28
categories: arXiv_CV
tags: arXiv_CV Image_Caption CNN
author: Aravindh Mahendran, Andrea Vedaldi
mathjax: true
---

* content
{:toc}

##### Abstract
Image representations, from SIFT and bag of visual words to Convolutional Neural Networks (CNNs) are a crucial component of almost all computer vision systems. However, our understanding of them remains limited. In this paper we study several landmark representations, both shallow and deep, by a number of complementary visualization techniques. These visualizations are based on the concept of "natural pre-image", namely a natural-looking image whose representation has some notable property. We study in particular three such visualizations: inversion, in which the aim is to reconstruct an image from its representation, activation maximization, in which we search for patterns that maximally stimulate a representation component, and caricaturization, in which the visual patterns that a representation detects in an image are exaggerated. We pose these as a regularized energy-minimization framework and demonstrate its generality and effectiveness. In particular, we show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.

##### Abstract (translated by Google)
从SIFT和视觉词汇袋到卷积神经网络（CNN）的图像表示是几乎所有计算机视觉系统的重要组成部分。但是，我们对它们的理解仍然有限。在本文中，我们通过一些互补的可视化技术来研究几个具有里程碑意义的表示，包括浅层和深层。这些可视化是基于“自然的前映像”的概念，即一个自然的图像，其表示具有一些显着的性质。我们特别研究三种这样的可视化：反转，其目的是从其表示重建图像，激活最大化，其中我们寻找模式，最大限度地刺激表示组件，和caricaturization，其中的视觉模式，表示图像中的检测被夸大了。我们将这些作为一个正规化的能量最小化框架，并展示其一般性和有效性。特别是，我们表明，这种方法可以反转表示，如HOG更准确地比最近的替代品，同时也适用于CNNs。在我们的研究结果中，我们展示了CNN中的几个图层保留了关于图像的照片上准确的信息，具有不同程度的几何和光度不变性。

##### URL
[https://arxiv.org/abs/1512.02017](https://arxiv.org/abs/1512.02017)

##### PDF
[https://arxiv.org/pdf/1512.02017](https://arxiv.org/pdf/1512.02017)

