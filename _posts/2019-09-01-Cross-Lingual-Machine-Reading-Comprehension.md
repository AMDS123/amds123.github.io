---
layout: post
title: "Cross-Lingual Machine Reading Comprehension"
date: 2019-09-01 09:14:52
categories: arXiv_CL
tags: arXiv_CL Knowledge Relation
author: Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu
mathjax: true
---

* content
{:toc}

##### Abstract
Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data. In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task, which is straightforward to adopt. However, to accurately align the answer into another language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in a bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available: https://github.com/ymcui/Cross-Lingual-MRC

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1909.00361](http://arxiv.org/abs/1909.00361)

##### PDF
[http://arxiv.org/pdf/1909.00361](http://arxiv.org/pdf/1909.00361)

