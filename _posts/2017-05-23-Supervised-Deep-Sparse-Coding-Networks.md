---
layout: post
title: "Supervised Deep Sparse Coding Networks"
date: 2017-05-23 01:31:04
categories: arXiv_CV
tags: arXiv_CV Regularization Sparse Optimization Classification
author: Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we describe the deep sparse coding network (SCN), a novel deep network that encodes intermediate representations with nonnegative sparse coding. The SCN is built upon a number of cascading bottleneck modules, where each module consists of two sparse coding layers with relatively wide and slim dictionaries that are specialized to produce high dimensional discriminative features and low dimensional representations for clustering, respectively. During training, both the dictionaries and regularization parameters are optimized with an end-to-end supervised learning algorithm based on multilevel optimization. Effectiveness of an SCN with seven bottleneck modules is verified on several popular benchmark datasets. Remarkably, with few parameters to learn, our SCN achieves 5.81% and 19.93% classification error rate on CIFAR-10 and CIFAR-100, respectively.

##### Abstract (translated by Google)
在本文中，我们描述了深度稀疏编码网络（SCN），一种新的深度网络，用非负稀疏编码编码中间表示。 SCN建立在许多级联瓶颈模块之上，其中每个模块由两个稀疏编码层组成，这两个稀疏编码层具有相对较宽和较薄的字典，这些字典分别专门用于产生高维判别特征和低维表示以用于聚类。在训练过程中，词典和正则化参数均采用基于多级优化的端到端监督学习算法进行优化。在几个流行的基准数据集上验证了具有七个瓶颈模块的SCN的有效性。值得注意的是，在参数少的情况下，我们的SCN在CIFAR-10和CIFAR-100上分别达到了5.81％和19.93％的分类错误率。

##### URL
[https://arxiv.org/abs/1701.08349](https://arxiv.org/abs/1701.08349)

##### PDF
[https://arxiv.org/pdf/1701.08349](https://arxiv.org/pdf/1701.08349)

