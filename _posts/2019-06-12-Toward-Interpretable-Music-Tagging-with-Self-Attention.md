---
layout: post
title: "Toward Interpretable Music Tagging with Self-Attention"
date: 2019-06-12 07:08:01
categories: arXiv_SD
tags: arXiv_SD Sparse Attention CNN RNN Relation
author: Minz Won, Sanghyuk Chun, Xavier Serra
mathjax: true
---

* content
{:toc}

##### Abstract
Self-attention is an attention mechanism that learns a representation by relating different positions in the sequence. The transformer, which is a sequence model solely based on self-attention, and its variants achieved state-of-the-art results in many natural language processing tasks. Since music composes its semantics based on the relations between components in sparse positions, adopting the self-attention mechanism to solve music information retrieval (MIR) problems can be beneficial. Hence, we propose a self-attention based deep sequence model for music tagging. The proposed architecture consists of shallow convolutional layers followed by stacked Transformer encoders. Compared to conventional approaches using fully convolutional or recurrent neural networks, our model is more interpretable while reporting competitive results. We validate the performance of our model with the MagnaTagATune and the Million Song Dataset. In addition, we demonstrate the interpretability of the proposed architecture with a heat map visualization.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.04972](http://arxiv.org/abs/1906.04972)

##### PDF
[http://arxiv.org/pdf/1906.04972](http://arxiv.org/pdf/1906.04972)

