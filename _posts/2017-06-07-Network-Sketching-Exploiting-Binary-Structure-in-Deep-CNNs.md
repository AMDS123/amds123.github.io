---
layout: post
title: "Network Sketching: Exploiting Binary Structure in Deep CNNs"
date: 2017-06-07 01:53:44
categories: arXiv_CV
tags: arXiv_CV CNN Inference Classification
author: Yiwen Guo, Anbang Yao, Hao Zhao, Yurong Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural networks (CNNs) with deep architectures have substantially advanced the state-of-the-art in computer vision tasks. However, deep networks are typically resource-intensive and thus difficult to be deployed on mobile devices. Recently, CNNs with binary weights have shown compelling efficiency to the community, whereas the accuracy of such models is usually unsatisfactory in practice. In this paper, we introduce network sketching as a novel technique of pursuing binary-weight CNNs, targeting at more faithful inference and better trade-off for practical applications. Our basic idea is to exploit binary structure directly in pre-trained filter banks and produce binary-weight models via tensor expansion. The whole process can be treated as a coarse-to-fine model approximation, akin to the pencil drawing steps of outlining and shading. To further speedup the generated models, namely the sketches, we also propose an associative implementation of binary tensor convolutions. Experimental results demonstrate that a proper sketch of AlexNet (or ResNet) outperforms the existing binary-weight models by large margins on the ImageNet large scale classification task, while the committed memory for network parameters only exceeds a little.

##### Abstract (translated by Google)
具有深层架构的卷积神经网络（CNN）大大提高了计算机视觉任务的最新水平。但是，深度网络通常是资源密集型的，因此很难在移动设备上部署。最近，有二元权重的有线电视网络向社区表现出令人信服的效率，而这种模式的准确性实际上通常不令人满意。在本文中，我们将网络草图引入作为一种追求二进制权重的CNNs的新技术，针对更加忠实的推论和更好的折衷的实际应用。我们的基本思想是在预先训练好的滤波器组中直接利用二进制结构，并通过张量扩展生成二进制权重模型。整个过程可以被看作粗糙到精细的模型逼近，类似于轮廓和阴影的铅笔绘制步骤。为了进一步加速生成的模型，即草图，我们还提出了二元张量卷积的关联实现。实验结果表明，在ImageNet大规模分类任务中，AlexNet（或ResNet）的适当的草图比现有的二进制权重模型大得多，而网络参数提交的内存只有一点点。

##### URL
[https://arxiv.org/abs/1706.02021](https://arxiv.org/abs/1706.02021)

##### PDF
[https://arxiv.org/pdf/1706.02021](https://arxiv.org/pdf/1706.02021)

