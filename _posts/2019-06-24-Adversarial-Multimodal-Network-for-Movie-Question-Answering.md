---
layout: post
title: "Adversarial Multimodal Network for Movie Question Answering"
date: 2019-06-24 10:44:48
categories: arXiv_CV
tags: arXiv_CV Adversarial QA Attention GAN Relation VQA
author: Zhaoquan Yuan, Siyuan Sun, Lixin Duan, Xiao Wu, Changsheng Xu
mathjax: true
---

* content
{:toc}

##### Abstract
Visual question answering by using information from multiple modalities has attracted more and more attention in recent years. However, it is a very challenging task, as the visual content and natural language have quite different statistical properties. In this work, we present a method called Adversarial Multimodal Network (AMN) to better understand video stories for question answering. In AMN, as inspired by generative adversarial networks, we propose to learn multimodal feature representations by finding a more coherent subspace for video clips and the corresponding texts (e.g., subtitles and questions). Moreover, we introduce a self-attention mechanism to enforce the so-called consistency constraints in order to preserve the self-correlation of visual cues of the original video clips in the learned multimodal representations. Extensive experiments on the MovieQA dataset show the effectiveness of our proposed AMN over other published state-of-the-art methods.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.09844](http://arxiv.org/abs/1906.09844)

##### PDF
[http://arxiv.org/pdf/1906.09844](http://arxiv.org/pdf/1906.09844)

