---
layout: post
title: "Deep Temporal Linear Encoding Networks"
date: 2016-11-21 08:27:31
categories: arXiv_CV
tags: arXiv_CV Video_Classification Classification
author: Ali Diba, Vivek Sharma, Luc Van Gool
mathjax: true
---

* content
{:toc}

##### Abstract
The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space; (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification; and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.

##### Abstract (translated by Google)
对于人类行为的表示，整个视频的特征的CNN编码很少得到解决。相反，有线电视新闻网的工作侧重于融合空间和时间网络的方法，但这些方法通常局限于处理较短的序列。我们提出了一种称为时间线性编码（TLE）的新视频表示形式，并嵌入在CNN内部作为一个新层，捕捉整个视频中的外观和运动。它通过端到端的学习将这些汇总的信息编码成一个强大的视频特征表示。 TLE的优点是：（a）将整个视频编码成紧凑的特征表示，学习语义和区分性特征空间; （b）适用于2D和3D CNN等各种网络的视频分类; （c）以更具表现力的方式建模特征交互，而不会丢失信息。我们在两个具有挑战性的人类行动数据集上进行实验：HMDB51和UCF101。实验表明，TLE优于目前最先进的方法在这两个数据集上。

##### URL
[https://arxiv.org/abs/1611.06678](https://arxiv.org/abs/1611.06678)

##### PDF
[https://arxiv.org/pdf/1611.06678](https://arxiv.org/pdf/1611.06678)

