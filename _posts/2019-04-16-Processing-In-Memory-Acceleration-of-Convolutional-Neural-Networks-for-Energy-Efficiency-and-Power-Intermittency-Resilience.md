---
layout: post
title: "Processing-In-Memory Acceleration of Convolutional Neural Networks for Energy-Efficiency, and Power-Intermittency Resilience"
date: 2019-04-16 16:22:16
categories: arXiv_CV
tags: arXiv_CV CNN Inference
author: Arman Roohi, Shaahin Angizi, Deliang Fan, Ronald F DeMara
mathjax: true
---

* content
{:toc}

##### Abstract
Herein, a bit-wise Convolutional Neural Network (CNN) in-memory accelerator is implemented using Spin-Orbit Torque Magnetic Random Access Memory (SOT-MRAM) computational sub-arrays. It utilizes a novel AND-Accumulation method capable of significantly-reduced energy consumption within convolutional layers and performs various low bit-width CNN inference operations entirely within MRAM. Power-intermittence resiliency is also enhanced by retaining the partial state information needed to maintain computational forward-progress, which is advantageous for battery-less IoT nodes. Simulation results indicate $\sim$5.4$\times$ higher energy-efficiency and 9$\times$ speedup over ReRAM-based acceleration, or roughly $\sim$9.7$\times$ higher energy-efficiency and 13.5$\times$ speedup over recent CMOS-only approaches, while maintaining inference accuracy comparable to baseline designs.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1904.07864](https://arxiv.org/abs/1904.07864)

##### PDF
[https://arxiv.org/pdf/1904.07864](https://arxiv.org/pdf/1904.07864)

