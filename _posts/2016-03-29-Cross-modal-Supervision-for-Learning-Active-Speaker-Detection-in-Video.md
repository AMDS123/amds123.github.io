---
layout: post
title: "Cross-modal Supervision for Learning Active Speaker Detection in Video"
date: 2016-03-29 19:47:46
categories: arXiv_CV
tags: arXiv_CV Knowledge Weakly_Supervised Detection
author: Punarjay Chakravarty, Tinne Tuytelaars
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we show how to use audio to supervise the learning of active speaker detection in video. Voice Activity Detection (VAD) guides the learning of the vision-based classifier in a weakly supervised manner. The classifier uses spatio-temporal features to encode upper body motion - facial expressions and gesticulations associated with speaking. We further improve a generic model for active speaker detection by learning person specific models. Finally, we demonstrate the online adaptation of generic models learnt on one dataset, to previously unseen people in a new dataset, again using audio (VAD) for weak supervision. The use of temporal continuity overcomes the lack of clean training data. We are the first to present an active speaker detection system that learns on one audio-visual dataset and automatically adapts to speakers in a new dataset. This work can be seen as an example of how the availability of multi-modal data allows us to learn a model without the need for supervision, by transferring knowledge from one modality to another.

##### Abstract (translated by Google)
在本文中，我们展示了如何使用音频来监督视频中主动说话人检测的学习。语音活动检测（VAD）以弱监督的方式指导基于视觉的分类器的学习。分类器使用时空特征来编码上身运动 - 与说话相关的面部表情和姿势。我们通过学习特定人员模型来进一步改进主动说话人检测的通用模型。最后，我们展示了在一个数据集上学习的通用模型的在线适应，以前在未知的人在一个新的数据集，再次使用音频（VAD）弱监督。时间连续性的使用克服了缺乏干净的训练数据。我们是第一个提供主动说话人检测系统，学习一个视听数据集并自动适应新的数据集中的发言人。这项工作可以被看作是一个例子，说明多模态数据的可用性如何通过将知识从一种模式转移到另一种模式而使我们无需监督地学习模型。

##### URL
[https://arxiv.org/abs/1603.08907](https://arxiv.org/abs/1603.08907)

##### PDF
[https://arxiv.org/pdf/1603.08907](https://arxiv.org/pdf/1603.08907)

