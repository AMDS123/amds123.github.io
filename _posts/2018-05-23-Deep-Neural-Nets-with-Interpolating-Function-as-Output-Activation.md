---
layout: post
title: "Deep Neural Nets with Interpolating Function as Output Activation"
date: 2018-05-23 17:38:08
categories: arXiv_CV
tags: arXiv_CV
author: Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley J. Osher
mathjax: true
---

* content
{:toc}

##### Abstract
We replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First, it is better applicable to the case with insufficient training data. Second, it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch, and code will be made publicly available.

##### Abstract (translated by Google)
我们用一个新颖的插值函数替换深层神经网络的输出层，通常是softmax函数。我们为这种新架构提出了端到端的培训和测试算法。与具有softmax函数作为输出激活的经典神经网络相比，具有插值函数作为输出激活的替代项结合了深度和流形学习的优点。新框架显示出以下主要优势：首先，它适用于训练数据不足的情况。其次，它显着提高了各种网络的泛化精度。该算法在PyTorch中实现，代码将公开。

##### URL
[http://arxiv.org/abs/1802.00168](http://arxiv.org/abs/1802.00168)

##### PDF
[http://arxiv.org/pdf/1802.00168](http://arxiv.org/pdf/1802.00168)

