---
layout: post
title: "Convergence of Online Mirror Descent Algorithms"
date: 2018-02-18 09:36:09
categories: arXiv_AI
tags: arXiv_AI
author: Yunwen Lei, Ding-Xuan Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we consider online mirror descent (OMD) algorithms, a class of scalable online learning algorithms exploiting data geometric structures through mirror maps. Necessary and sufficient conditions are presented in terms of the step size sequence $\{\eta_t\}_{t}$ for the convergence of an OMD algorithm with respect to the expected Bregman distance induced by the mirror map. The condition is $\lim_{t\to\infty}\eta_t=0, \sum_{t=1}^{\infty}\eta_t=\infty$ in the case of positive variances. It is reduced to $\sum_{t=1}^{\infty}\eta_t=\infty$ in the case of zero variances for which the linear convergence may be achieved by taking a constant step size sequence. A sufficient condition on the almost sure convergence is also given. We establish tight error bounds under mild conditions on the mirror map, the loss function, and the regularizer. Our results are achieved by some novel analysis on the one-step progress of the OMD algorithm using smoothness and strong convexity of the mirror map and the loss function.

##### Abstract (translated by Google)
在本文中，我们考虑在线镜像下降（OMD）算法，这是一类通过镜像映射利用数据几何结构的可扩展在线学习算法。对于OMD算法相对于由镜像映射引起的预期Bregman距离的收敛，步长大小序列$ \ {eta_t \} _ {t} $表示必要和充分的条件。在正差异的情况下，条件是$ \ lim_ {t \ to \ infty} \ eta_t = 0，\ sum_ {t = 1} ^ {\ infty} \ eta_t = \ infty $。在零偏差的情况下，它可以简化为$ \ sum_ {t = 1} ^ {\ infty} \ eta_t = \ infty $，其中线性收敛可以通过采用恒定步长序列来实现。还给出了几乎确定收敛的充分条件。我们在镜像映射，损失函数和正规化器的温和条件下建立严格的误差界限。我们的结果是通过一些新颖的分析来实现的，使用平滑性和镜像映射的强凸性和损失函数的OMD算法的一步进展。

##### URL
[http://arxiv.org/abs/1802.06357](http://arxiv.org/abs/1802.06357)

##### PDF
[http://arxiv.org/pdf/1802.06357](http://arxiv.org/pdf/1802.06357)

