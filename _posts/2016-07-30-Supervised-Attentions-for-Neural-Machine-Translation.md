---
layout: post
title: "Supervised Attentions for Neural Machine Translation"
date: 2016-07-30 12:39:19
categories: arXiv_CL
tags: arXiv_CL Attention
author: Haitao Mi, Zhiguo Wang, Abe Ittycheriah
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the "true" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.

##### Abstract (translated by Google)
在本文中，我们利用训练句对的对齐来提高神经机器翻译的注意力或对准精度。我们只需计算机器关注点与“真实”路线之间的距离，并将训练过程中的成本降至最低。我们的大规模汉英机器翻译实验表明，我们的模型在大词汇量的神经机器翻译系统上显着提高了翻译和对齐质量，甚至超越了传统的基于语法的传统系统。

##### URL
[https://arxiv.org/abs/1608.00112](https://arxiv.org/abs/1608.00112)

##### PDF
[https://arxiv.org/pdf/1608.00112](https://arxiv.org/pdf/1608.00112)

