---
layout: post
title: "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems"
date: 2017-07-16 16:43:08
categories: arXiv_SD
tags: arXiv_SD Attention Relation
author: Chongyang Tao, Lili Mou, Dongyan Zhao, Rui Yan
mathjax: true
---

* content
{:toc}

##### Abstract
Open-domain human-computer conversation has been attracting increasing attention over the past few years. However, there does not exist a standard automatic evaluation metric for open-domain dialog systems; researchers usually resort to human annotation for model evaluation, which is time- and labor-intensive. In this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine, which evaluates a reply by taking into consideration both a groundtruth reply and a query (previous user-issued utterance). Our metric is learnable, but its training does not require labels of human satisfaction. Hence, RUBER is flexible and extensible to different datasets and languages. Experiments on both retrieval and generative dialog systems show that RUBER has a high correlation with human annotation.

##### Abstract (translated by Google)
过去几年来，开放式的人机对话已经引起越来越多的关注。但是，开放域对话系统不存在标准的自动评估指标。研究人员通常会使用人工标注进行模型评估，这是时间和劳动密集型的。在本文中，我们提出了RUBER，一个参考度量和Unreferenced度量混合评估例程，它通过考虑一个groundtruth答复和一个查询（先前用户发出的话语）来评估答复。我们的指标是可以学习的，但是它的训练不需要人类满意的标签。因此，RUBER是灵活的，可扩展到不同的数据集和语言。在检索和生成对话系统上的实验表明，RUBER与人类的注释有很高的相关性。

##### URL
[https://arxiv.org/abs/1701.03079](https://arxiv.org/abs/1701.03079)

##### PDF
[https://arxiv.org/pdf/1701.03079](https://arxiv.org/pdf/1701.03079)

