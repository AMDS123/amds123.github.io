---
layout: post
title: "About Pyramid Structure in Convolutional Neural Networks"
date: 2016-08-14 06:03:09
categories: arXiv_CV
tags: arXiv_CV Attention CNN
author: Ihsan Ullah, Alfredo Petrosino
mathjax: true
---

* content
{:toc}

##### Abstract
Deep convolutional neural networks (CNN) brought revolution without any doubt to various challenging tasks, mainly in computer vision. However, their model designing still requires attention to reduce number of learnable parameters, with no meaningful reduction in performance. In this paper we investigate to what extend CNN may take advantage of pyramid structure typical of biological neurons. A generalized statement over convolutional layers from input till fully connected layer is introduced that helps further in understanding and designing a successful deep network. It reduces ambiguity, number of parameters, and their size on disk without degrading overall accuracy. Performance are shown on state-of-the-art models for MNIST, Cifar-10, Cifar-100, and ImageNet-12 datasets. Despite more than 80% reduction in parameters for Caffe_LENET, challenging results are obtained. Further, despite 10-20% reduction in training data along with 10-40% reduction in parameters for AlexNet model and its variations, competitive results are achieved when compared to similar well-engineered deeper architectures.

##### Abstract (translated by Google)
深度卷积神经网络（CNN）毫无疑问地带来了革命性的任务，主要是在计算机视觉领域。然而，他们的模型设计仍然需要注意减少可学习参数的数量，而没有有意义的性能下降。在本文中，我们研究了CNN可以利用生物神经元典型的金字塔结构的优势。介绍了卷积层从输入层到完全连通层的一般表述，有助于进一步理解和设计一个成功的深层网络。它可以减少模糊度，参数数量和磁盘大小，而不会降低整体的准确性。性能显示在MNIST，Cifar-10，Cifar-100和ImageNet-12数据集的最先进的模型上。尽管Caffe_LENET的参数降低了80％以上，但仍然存在具有挑战性的结果。此外，尽管培训数据减少了10-20％，并且AlexNet模型及其变体的参数减少了10-40％，但与类似的精心设计的更深层体系结构相比，实现了竞争结果。

##### URL
[https://arxiv.org/abs/1608.04064](https://arxiv.org/abs/1608.04064)

##### PDF
[https://arxiv.org/pdf/1608.04064](https://arxiv.org/pdf/1608.04064)

