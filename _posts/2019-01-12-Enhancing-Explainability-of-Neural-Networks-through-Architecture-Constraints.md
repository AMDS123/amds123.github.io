---
layout: post
title: "Enhancing Explainability of Neural Networks through Architecture Constraints"
date: 2019-01-12 10:17:36
categories: arXiv_CV
tags: arXiv_CV Sparse Prediction Gradient_Descent
author: Zebin Yang, Aijun Zhang, Agus Sudjianto
mathjax: true
---

* content
{:toc}

##### Abstract
Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. The neural networks are known to possess good prediction performance, but lack of sufficient model explainability. In this paper, we propose to enhance the explainability of neural networks through the following architecture constraints: a) sparse additive subnetworks; b) orthogonal projection pursuit; and c) smooth function approximation. It leads to a sparse, orthogonal and smooth explainable neural network (SOSxNN). The multiple parameters in the SOSxNN model are simultaneously estimated by a modified mini-batch gradient descent algorithm based on the backpropagation technique for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. The hyperparameters controlling the sparse and smooth constraints are optimized by the grid search. Through simulation studies, we compare the SOSxNN method to several benchmark methods including least absolute shrinkage and selection operator, support vector machine, random forest, and multi-layer perceptron. It is shown that proposed model keeps the flexibility of pursuing prediction accuracy while attaining the improved interpretability, which can be therefore used as a promising surrogate model for complex model approximation. Finally, the real data example from the Lending Club is employed as a showcase of the SOSxNN application.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1901.03838](https://arxiv.org/abs/1901.03838)

##### PDF
[https://arxiv.org/pdf/1901.03838](https://arxiv.org/pdf/1901.03838)

