---
layout: post
title: "Deep Reinforcement Learning for Mention-Ranking Coreference Models"
date: 2016-10-31 20:30:15
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Kevin Clark, Christopher D. Manning
mathjax: true
---

* content
{:toc}

##### Abstract
Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.

##### Abstract (translated by Google)
核心参与解决方案系统通常用启发式丢失功能进行训练，这需要仔细调整。在本文中，我们改为使用强化学习来直接优化神经提及排序模型，用于共同评估度量。我们尝试两种方法：REINFORCE策略梯度算法和奖励重新调整的最大保证金目标。我们发现后者更有效，导致CoNLL 2012共享任务中英文部分的现有技术水平显着提高。

##### URL
[https://arxiv.org/abs/1609.08667](https://arxiv.org/abs/1609.08667)

##### PDF
[https://arxiv.org/pdf/1609.08667](https://arxiv.org/pdf/1609.08667)

