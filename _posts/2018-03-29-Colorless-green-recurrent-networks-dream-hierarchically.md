---
layout: post
title: "Colorless green recurrent networks dream hierarchically"
date: 2018-03-29 16:27:36
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model Prediction
author: Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, Marco Baroni
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues ("The colorless green ideas I ate with the chair sleep furiously"), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.

##### Abstract (translated by Google)
递归神经网络（RNN）在各种语言处理任务中取得了令人印象深刻的结果，表明它们可以诱发语言的非平凡属性。我们在此调查RNN学习跟踪抽象层次句法结构的程度。我们测试是否用四种语言（意大利语，英语，希伯来语，俄语）的通用语言建模目标训练的RNN可以预测各种建筑中的长途号码协议。我们在评估中加入了无意义的句子，在这些句子中RNN不能依赖语义或词汇线索（“我用椅子睡觉时吃的无色绿色想法”），对于意大利语，我们将模型表现与人类直觉进行比较。我们的语言模型训练的RNN对长距离协议做出可靠的预测，并且不会落后于人类的表现。因此，我们支持RNN不仅仅是浅层提取器的假设，而且它们也获得更深的语法能力。

##### URL
[http://arxiv.org/abs/1803.11138](http://arxiv.org/abs/1803.11138)

##### PDF
[http://arxiv.org/pdf/1803.11138](http://arxiv.org/pdf/1803.11138)

