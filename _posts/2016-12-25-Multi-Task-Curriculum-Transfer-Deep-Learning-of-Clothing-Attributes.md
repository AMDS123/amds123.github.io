---
layout: post
title: "Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes"
date: 2016-12-25 23:43:22
categories: arXiv_CV
tags: arXiv_CV Transfer_Learning Deep_Learning
author: Qi Dong, Shaogang Gong, Xiatian Zhu
mathjax: true
---

* content
{:toc}

##### Abstract
Recognising detailed clothing characteristics (fine-grained attributes) in unconstrained images of people in-the-wild is a challenging task for computer vision, especially when there is only limited training data from the wild whilst most data available for model learning are captured in well-controlled environments using fashion models (well lit, no background clutter, frontal view, high-resolution). In this work, we develop a deep learning framework capable of model transfer learning from well-controlled shop clothing images collected from web retailers to in-the-wild images from the street. Specifically, we formulate a novel Multi-Task Curriculum Transfer (MTCT) deep learning method to explore multiple sources of different types of web annotations with multi-labelled fine-grained attributes. Our multi-task loss function is designed to extract more discriminative representations in training by jointly learning all attributes, and our curriculum strategy exploits the staged easy-to-complex transfer learning motivated by cognitive studies. We demonstrate the advantages of the MTCT model over the state-of-the-art methods on the X-Domain benchmark, a large scale clothing attribute dataset. Moreover, we show that the MTCT model has a notable advantage over contemporary models when the training data size is small.

##### Abstract (translated by Google)
在无约束的野外人员图像中识别详细的服装特征（细粒度属性）对于计算机视觉来说是一项具有挑战性的任务，特别是当野外的训练数据有限时，大部分可用于模型学习的数据都被捕获使用时尚模型（光线充足，没有背景混乱，正面视图，高分辨率）的受控环境。在这项工作中，我们开发了一个深度的学习框架，能够从网络零售商收集的良好控制的店铺服装图像到街上的野外图像进行模型转移学习。具体而言，我们制定了一种新的多任务课程转移（MTCT）深度学习方法来探索具有多标记细粒度属性的不同类型的网络注释的多种来源。我们的多任务损失函数旨在通过联合学习所有属性来提取更多的歧视性表征，而我们的课程策略则利用了由认知研究激发的阶段性的，易于复杂的转移学习。我们展示了MTCT模型相对于大型服装属性数据集X-Domain基准测试中最先进的方法的优势。此外，我们表明，当训练数据量很小时，MTCT模型比当代模型有显着的优势。

##### URL
[https://arxiv.org/abs/1610.03670](https://arxiv.org/abs/1610.03670)

##### PDF
[https://arxiv.org/pdf/1610.03670](https://arxiv.org/pdf/1610.03670)

