---
layout: post
title: "Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation"
date: 2019-08-17 08:09:33
categories: arXiv_CL
tags: arXiv_CL NMT
author: Tianyu He, Xu Tan, Tao Qin
mathjax: true
---

* content
{:toc}

##### Abstract
Neural machine translation (NMT) typically adopts the encoder-decoder framework. A good understanding of the characteristics and functionalities of the encoder and decoder can help to explain the pros and cons of the framework, and design better models for NMT. In this work, we conduct an empirical study on the encoder and the decoder in NMT, taking Transformer as an example. We find that 1) the decoder handles an easier task than the encoder in NMT, 2) the decoder is more sensitive to the input noise than the encoder, and 3) the preceding words/tokens in the decoder provide strong conditional information, which accounts for the two observations above. We hope those observations can shed light on the characteristics of the encoder and decoder and inspire future research on NMT.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.06259](http://arxiv.org/abs/1908.06259)

##### PDF
[http://arxiv.org/pdf/1908.06259](http://arxiv.org/pdf/1908.06259)

