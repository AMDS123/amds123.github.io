---
layout: post
title: "Broad Context Language Modeling as Reading Comprehension"
date: 2017-02-16 21:33:30
categories: arXiv_CL
tags: arXiv_CL Knowledge Language_Model Prediction
author: Zewei Chu, Hai Wang, Kevin Gimpel, David McAllester
mathjax: true
---

* content
{:toc}

##### Abstract
Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.

##### Abstract (translated by Google)
文本理解的进展是由测试特定功能的大型数据集驱动的，如最近的阅读理解数据集（Hermann等，2015）。我们把重点放在LAMBADA数据集上（Paperno等，2016），这个词预测任务要求比直接句子更广泛的语境。我们将LAMBADA视为一个阅读理解问题，并应用基于神经网络的理解模型。虽然这些模型被限制从背景中选择一个单词，但他们将LAMBADA的艺术水平从7.3％提高到了49％。我们分析了100个实例，发现神经网络阅读器在涉及基于对话或话语线索从上下文中选择名字的情况下表现良好，但是在需要合作解决或外部知识时挣扎。

##### URL
[https://arxiv.org/abs/1610.08431](https://arxiv.org/abs/1610.08431)

##### PDF
[https://arxiv.org/pdf/1610.08431](https://arxiv.org/pdf/1610.08431)

