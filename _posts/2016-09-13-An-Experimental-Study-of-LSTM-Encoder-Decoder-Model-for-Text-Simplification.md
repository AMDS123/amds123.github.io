---
layout: post
title: "An Experimental Study of LSTM Encoder-Decoder Model for Text Simplification"
date: 2016-09-13 03:02:32
categories: arXiv_CL
tags: arXiv_CL RNN
author: Tong Wang, Ping Chen, Kevin Amaral, Jipeng Qiang
mathjax: true
---

* content
{:toc}

##### Abstract
Text simplification (TS) aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning. Current automatic TS techniques are limited to either lexical-level applications or manually defining a large amount of rules. Since deep neural networks are powerful models that have achieved excellent performance over many difficult tasks, in this paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder model for sentence level TS, which makes minimal assumptions about word sequence. We conduct preliminary experiments to find that the model is able to learn operation rules such as reversing, sorting and replacing from sequence pairs, which shows that the model may potentially discover and apply rules such as modifying sentence structure, substituting words, and removing words for TS.

##### Abstract (translated by Google)
文本简化（TS）旨在减少文本的词汇和结构复杂性，同时保留语义。目前的自动TS技术限于词法层面的应用程序或手动定义大量的规则。由于深度神经网络是在许多困难的任务中取得了优异性能的强大模型，本文提出在句子级别TS中使用长期短期记忆（LSTM）编码器 - 解码器模型，对于单词序列做出最小的假设。我们进行了初步的实验，发现该模型能够学习如序列对的翻转，排序，替换等操作规则，说明该模型可能会发现和应用规则，如修改句子结构，替换词语， TS。

##### URL
[https://arxiv.org/abs/1609.03663](https://arxiv.org/abs/1609.03663)

##### PDF
[https://arxiv.org/pdf/1609.03663](https://arxiv.org/pdf/1609.03663)

