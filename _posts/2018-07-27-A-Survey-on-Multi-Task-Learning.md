---
layout: post
title: "A Survey on Multi-Task Learning"
date: 2018-07-27 03:17:17
categories: arXiv_AI
tags: arXiv_AI Review Reinforcement_Learning Survey Relation
author: Yu Zhang, Qiang Yang
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.

##### Abstract (translated by Google)
多任务学习（MTL）是机器学习的学习范例，其目的是利用多个相关任务中包含的有用信息来帮助提高所有任务的泛化性能。在本文中，我们给出了MTL的调查。首先，我们将不同的MTL算法分为几类，包括特征学习方法，低秩方法，任务聚类方法，任务关系学习方法和分解方法，然后讨论每种方法的特征。为了进一步提高学习任务的表现，MTL可以与其他学习范例相结合，包括半监督学习，主动学习，无监督学习，强化学习，多视图学习和图形模型。当任务数量很大或数据维度很高时，批量MTL模型很难处理这种情况，并且审查在线，并行和分布式MTL模型以及降维和特征散列以揭示其计算和存储优势。许多真实世界的应用程序使用MTL来提高其性能，我们审查代表性的工作。最后，我们提出理论分析并讨论MTL的未来几个方向。

##### URL
[http://arxiv.org/abs/1707.08114](http://arxiv.org/abs/1707.08114)

##### PDF
[http://arxiv.org/pdf/1707.08114](http://arxiv.org/pdf/1707.08114)

