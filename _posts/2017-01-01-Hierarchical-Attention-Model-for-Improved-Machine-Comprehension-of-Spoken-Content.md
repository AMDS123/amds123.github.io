---
layout: post
title: "Hierarchical Attention Model for Improved Machine Comprehension of Spoken Content"
date: 2017-01-01 12:17:13
categories: arXiv_CL
tags: arXiv_CL Attention RNN Relation
author: Wei Fang, Jui-Yang Hsu, Hung-yi Lee, Lin-Shan Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained.

##### Abstract (translated by Google)
多媒体或口语内容呈现比纯文本内容更有吸引力的信息，但前者更难以在屏幕上显示并由用户选择。因此，访问前者的大量集合比后者对人类来说要困难得多和耗时。因此开发能够自动理解口头内容的机器以及总结人类浏览的关键信息是非常有吸引力的。在这个努力中，最近提出了一个机器理解口语内容的新任务。最初的目标被定义为托福听力理解考试，这是一项针对母语不是英语的英语学习者的具有挑战性的学术英语考试。基于注意的多跳递归神经网络（AMRNN）体系结构也被提出用于这个任务，它只考虑语音话语中的顺序关系。在本文中，我们提出了一个新的分层注意模型（HAM），它构造了多跳注意机制而不是树状结构，而不是序列表示。获得了针对ASR错误的改善的理解性能。

##### URL
[https://arxiv.org/abs/1608.07775](https://arxiv.org/abs/1608.07775)

##### PDF
[https://arxiv.org/pdf/1608.07775](https://arxiv.org/pdf/1608.07775)

