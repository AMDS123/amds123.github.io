---
layout: post
title: "DYAN: A Dynamical Atoms Network for Video Prediction"
date: 2018-03-20 00:14:23
categories: arXiv_CV
tags: arXiv_CV Adversarial Represenation_Learning RNN Prediction
author: Wenqian Liu, Abhishek Sharma, Octavia Camps, Mario Sznaier
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to anticipate the future is essential when making real time critical decisions, provides valuable information to understand dynamic natural scenes, and can help unsupervised video representation learning. State-of-art video prediction is based on LSTM recursive networks and/or generative adversarial network learning. These are complex architectures that need to learn large numbers of parameters, are potentially hard to train, slow to run, and may produce blurry predictions. In this paper, we introduce DYAN, a novel network with very few parameters and easy to train, which produces accurate, high quality frame predictions, significantly faster than previous approaches. DYAN owes its good qualities to its encoder and decoder, which are designed following concepts from systems identification theory and exploit the dynamics-based invariants of the data. Extensive experiments using several standard video datasets show that DYAN is superior generating frames and that it generalizes well across domains.

##### Abstract (translated by Google)
在做出实时关键决策时，预测未来的能力至关重要，为了解动态自然场景提供有价值的信息，并可帮助无监督的视频表示学习。最先进的视频预测基于LSTM递归网络和/或生成敌对网络学习。这些是复杂的架构，需要学习大量参数，可能难以训练，运行速度慢，并且可能产生模糊的预测。在本文中，我们介绍了DYAN，这是一种参数很少且易于训练的新型网络，能够产生准确，高质量的帧预测，比以前的方法快得多。 DYAN的优点在于它的编码器和解码器，它们遵循系统识别理论的概念设计并利用基于动态的数据不变量。使用多个标准视频数据集进行的大量实验表明，DYAN具有出众的生成框架，并且能够跨域进行良好的泛化。

##### URL
[http://arxiv.org/abs/1803.07201](http://arxiv.org/abs/1803.07201)

##### PDF
[http://arxiv.org/pdf/1803.07201](http://arxiv.org/pdf/1803.07201)

