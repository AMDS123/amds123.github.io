---
layout: post
title: "Data Distillation: Towards Omni-Supervised Learning"
date: 2017-12-12 18:55:57
categories: arXiv_CV
tags: arXiv_CV Object_Detection Prediction Detection Recognition
author: Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.

##### Abstract (translated by Google)
我们调查全​​监督学习，一个半监督学习的特殊制度，学习者利用所有可用的标记数据加上互联网规模的未标记数据源。全方位监督学习在现有标记数据集上的表现更低，提供了超越最先进的完全监督方法的潜力。为了利用全方位监督设置，我们提出了数据精馏，一种使用单一模型集合来自多个未标记数据变换的预测的方法，以自动生成新的训练注释。我们认为视觉识别模型最近已经足够准确，现在可以将关于自我训练的经典观点应用于挑战现实世界的数据。我们的实验结果表明，在人类关键点检测和一般对象检测的情况下，用数据蒸馏进行训练的最新模型超过了单独使用来自COCO数据集的标记数据的性能。

##### URL
[https://arxiv.org/abs/1712.04440](https://arxiv.org/abs/1712.04440)

##### PDF
[https://arxiv.org/pdf/1712.04440](https://arxiv.org/pdf/1712.04440)

