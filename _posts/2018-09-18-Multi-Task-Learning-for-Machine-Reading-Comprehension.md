---
layout: post
title: "Multi-Task Learning for Machine Reading Comprehension"
date: 2018-09-18 23:44:03
categories: arXiv_CL
tags: arXiv_CL QA Embedding Language_Model
author: Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, Jianfeng Gao
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a multi-task learning framework to jointly train a Machine Reading Comprehension (MRC) model on multiple datasets across different domains. Key to the proposed method is to learn robust and general contextual representations with the help of out-domain data in a multi-task framework. Empirical study shows that the proposed approach is orthogonal to the existing pre-trained representation models, such as word embedding and language models. Experiments on the Stanford Question Answering Dataset (SQuAD), the Microsoft MAchine Reading COmprehension Dataset (MS MARCO), NewsQA and other datasets show that our multi-task learning approach achieves significant improvement over state-of-the-art models in most MRC tasks.

##### Abstract (translated by Google)
我们提出了一个多任务学习框架，用于在不同领域的多个数据集上联合训练机器阅读理解（MRC）模型。所提出的方法的关键是在多任务框架中借助于域外数据来学习鲁棒的和一般的上下文表示。实证研究表明，所提出的方法与现有的预训练表示模型正交，例如单词嵌入和语言模型。斯坦福问答数据集（SQuAD），Microsoft机器阅读理解数据集（MS MARCO），NewsQA和其他数据集的实验表明，我们的多任务学习方法在大多数MRC任务中实现了对最先进模型的显着改进。

##### URL
[http://arxiv.org/abs/1809.06963](http://arxiv.org/abs/1809.06963)

##### PDF
[http://arxiv.org/pdf/1809.06963](http://arxiv.org/pdf/1809.06963)

