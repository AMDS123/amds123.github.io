---
layout: post
title: "Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for Isolated Gesture Recognition"
date: 2016-11-28 08:16:27
categories: arXiv_CV
tags: arXiv_CV Salient CNN Quantitative Recognition
author: Jiali Duan, Shuai Zhou, Jun Wan, Xiaoyuan Guo, Stan Z. Li
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, the popularity of depth-sensors such as Kinect has made depth videos easily available while its advantages have not been fully exploited. This paper investigates, for gesture recognition, to explore the spatial and temporal information complementarily embedded in RGB and depth sequences. We propose a convolutional twostream consensus voting network (2SCVN) which explicitly models both the short-term and long-term structure of the RGB sequences. To alleviate distractions from background, a 3d depth-saliency ConvNet stream (3DDSN) is aggregated in parallel to identify subtle motion characteristics. These two components in an unified framework significantly improve the recognition accuracy. On the challenging Chalearn IsoGD benchmark, our proposed method outperforms the first place on the leader-board by a large margin (10.29%) while also achieving the best result on RGBD-HuDaAct dataset (96.74%). Both quantitative experiments and qualitative analysis shows the effectiveness of our proposed framework and codes will be released to facilitate future research.

##### Abstract (translated by Google)
最近，Kinect等深度传感器的普及使得深度视频更容易获得，而其优势尚未被充分利用。本文研究了手势识别，以探索在RGB和深度序列中互补嵌入的空间和时间信息。我们提出了一个卷积双向流共识投票网络（2SCVN），它明确地模拟了RGB序列的短期和长期结构。为了减轻背景干扰，3D深度显着ConvNet流（3DDSN）被并行聚合以识别微妙的运动特征。这两个组件在一个统一的框架中显着提高了识别的准确性。在具有挑战性的Chalearn IsoGD基准测试中，我们提出的方法在排行榜上大幅领先10.29％，同时在RGBD-HuDaAct数据集上取得最好成绩（96.74％）。定量实验和定性分析都显示我们提出的框架和代码的有效性将被发布以便于未来的研究。

##### URL
[https://arxiv.org/abs/1611.06689](https://arxiv.org/abs/1611.06689)

##### PDF
[https://arxiv.org/pdf/1611.06689](https://arxiv.org/pdf/1611.06689)

