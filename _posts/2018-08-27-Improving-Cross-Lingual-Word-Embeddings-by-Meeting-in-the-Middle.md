---
layout: post
title: "Improving Cross-Lingual Word Embeddings by Meeting in the Middle"
date: 2018-08-27 10:54:37
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Yerai Doval, Jose Camacho-Collados, Luis Espinosa-Anke, Steven Schockaert
mathjax: true
---

* content
{:toc}

##### Abstract
Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.

##### Abstract (translated by Google)
跨语言词汇嵌入在多语言NLP中变得越来越重要。最近，已经表明，通过线性变换对齐两个不相交的单语向量空间，使用不超过一个小的双语词典作为监督，可以有效地学习这些嵌入。在这项工作中，我们建议在初始对齐步骤之后应用一个额外的转换，它将跨语言同义词移到它们之间的中间点。通过应用这种转换，我们的目标是获得更好的矢量空间的跨语言整合。此外，也许令人惊讶的是，单语空间也通过这种转变而得到改善。这与原始对齐形成对比，原始对齐通常被学习以保持单语空间的结构。我们的实验证实，由此产生的跨语言嵌入在单语和跨语言评估任务中都优于最先进的模型。

##### URL
[http://arxiv.org/abs/1808.08780](http://arxiv.org/abs/1808.08780)

##### PDF
[http://arxiv.org/pdf/1808.08780](http://arxiv.org/pdf/1808.08780)

