---
layout: post
title: 'Can Active Memory Replace Attention?'
date: 2017-12-06 08:12:21
categories: arXiv_CV
tags: arXiv_CV Image_Caption Speech_Recognition Caption Deep_Learning Recognition
author: Łukasz Kaiser, Samy Bengio
---

* content
{:toc}

##### Abstract
Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.

##### Abstract (translated by Google)
近年来，在深度学习模型中成功使用了几种将神经网络的注意力集中在输入或记忆的选定部分上的机制。注意力提高了图像分类，图像字幕，语音识别，生成模型和学习算法任务，但它可能对神经机器翻译影响最大。最近，类似的改进已经通过使用替代的机制而获得，这些机制不集中在存储器的单个部分上，而是以统一的方式并行操作所有部分。这种我们称之为主动记忆的机制，在算法任务，图像处理和生成建模方面都得到了更多的关注。但是到目前为止，大多数自然语言处理任务，特别是机器翻译，主动内存还没有得到重视。本文分析了这个缺点，提出了一个与神经机器翻译现有注意模型相匹配的主动记忆扩展模型，并将其推广到更长的句子。我们调查这个模型，并解释为什么以前的主动记忆模型没有成功。最后，我们讨论当主动记忆带来最多的好处，并且关注点可能是一个更好的选择。

##### URL
[https://arxiv.org/abs/1610.08613](https://arxiv.org/abs/1610.08613)

