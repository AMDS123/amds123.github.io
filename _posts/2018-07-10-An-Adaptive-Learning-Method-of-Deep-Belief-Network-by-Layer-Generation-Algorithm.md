---
layout: post
title: "An Adaptive Learning Method of Deep Belief Network by Layer Generation Algorithm"
date: 2018-07-10 05:55:26
categories: arXiv_CV
tags: arXiv_CV Sparse Knowledge
author: Shin Kamada, Takumi Ichimura
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Belief Network (DBN) has a deep architecture that represents multiple features of input patterns hierarchically with the pre-trained Restricted Boltzmann Machines (RBM). A traditional RBM or DBN model cannot change its network structure during the learning phase. Our proposed adaptive learning method can discover the optimal number of hidden neurons and weights and/or layers according to the input space. The model is an important method to take account of the computational cost and the model stability. The regularities to hold the sparse structure of network is considerable problem, since the extraction of explicit knowledge from the trained network should be required. In our previous research, we have developed the hybrid method of adaptive structural learning method of RBM and Learning Forgetting method to the trained RBM. In this paper, we propose the adaptive learning method of DBN that can determine the optimal number of layers during the learning. We evaluated our proposed model on some benchmark data sets.

##### Abstract (translated by Google)
Deep Belief Network（DBN）具有深层体系结构，使用预先训练的Restricted Boltzmann Machines（RBM）分层次地表示输入模式的多个特征。传统的RBM或DBN模型在学习阶段不能改变其网络结构。我们提出的自适应学习方法可以根据输入空间发现隐藏神经元和权重和/或层的最佳数量。该模型是考虑计算成本和模型稳定性的重要方法。保持稀疏网络结构的规律性是相当大的问题，因为应该需要从训练的网络中提取显式知识。在我们之前的研究中，我们开发了RBM自适应结构学习方法和学习遗忘方法的混合方法到训练的RBM。在本文中，我们提出了DBN的自适应学习方法，可以确定学习过程中的最佳层数。我们在一些基准数据集上评估了我们提出的模型。

##### URL
[http://arxiv.org/abs/1807.03486](http://arxiv.org/abs/1807.03486)

##### PDF
[http://arxiv.org/pdf/1807.03486](http://arxiv.org/pdf/1807.03486)

