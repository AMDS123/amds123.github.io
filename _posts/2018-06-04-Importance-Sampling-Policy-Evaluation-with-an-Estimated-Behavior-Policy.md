---
layout: post
title: "Importance Sampling Policy Evaluation with an Estimated Behavior Policy"
date: 2018-06-04 19:47:24
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Josiah Hanna, Scott Niekum, Peter Stone
mathjax: true
---

* content
{:toc}

##### Abstract
In reinforcement learning, off-policy evaluation is the task of using data generated by one policy to determine the expected return of a second policy. Importance sampling is a standard technique for off-policy evaluation, allowing off-policy data to be used as if it were on-policy. When the policy that generated the off-policy data is unknown, the ordinary importance sampling estimator cannot be applied. In this paper, we study a family of regression importance sampling (RIS) methods that apply importance sampling by first estimating the behavior policy. We find that these estimators give strong empirical performance---surprisingly often outperforming importance sampling with the true behavior policy in both discrete and continuous domains. Our results emphasize the importance of estimating the behavior policy using only the data that will also be used for the importance sampling estimate.

##### Abstract (translated by Google)
在强化学习中，非策略评估是使用一种策略生成的数据来确定第二种策略的预期回报的任务。重要性抽样是用于非政策评估的标准技术，允许使用非政策数据，就好像它是在政策上一样。当产生关闭策略数据的策略是未知的时，普通重要性抽样估计器不能被应用。在本文中，我们研究了一组回归重要抽样（RIS）方法，通过首先估计行为策略来应用重要性抽样。我们发现这些估计量给出了强大的经验性表现 - 令人惊讶的是，在离散和连续域中，通常都比真正的行为策略更重要。我们的结果强调了仅使用也将用于重要性抽样估计的数据来估计行为策略的重要性。

##### URL
[http://arxiv.org/abs/1806.01347](http://arxiv.org/abs/1806.01347)

##### PDF
[http://arxiv.org/pdf/1806.01347](http://arxiv.org/pdf/1806.01347)

