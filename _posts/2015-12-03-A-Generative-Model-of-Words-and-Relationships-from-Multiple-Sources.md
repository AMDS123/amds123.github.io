---
layout: post
title: "A Generative Model of Words and Relationships from Multiple Sources"
date: 2015-12-03 17:08:28
categories: arXiv_CL
tags: arXiv_CL Knowledge Ontology Embedding Language_Model Prediction Relation
author: Stephanie L. Hyland, Theofanis Karaletsos, Gunnar Rätsch
mathjax: true
---

* content
{:toc}

##### Abstract
Neural language models are a powerful tool to embed words into semantic vector spaces. However, learning such models generally relies on the availability of abundant and diverse training examples. In highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus, or the limited range of expression in average use. Such domains may encode prior knowledge about entities in a knowledge base or ontology. We propose a generative model which integrates evidence from diverse data sources, enabling the sharing of semantic information. We achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words, which we model as affine transformations on the embedding space. We demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels. We further demonstrate the usefulness of learning from different data sources with overlapping vocabularies.

##### Abstract (translated by Google)
神经语言模型是将单词嵌入语义向量空间的有力工具。但是，学习这样的模型通常依赖于丰富多样的训练样例的可用性。在高度专业化的领域，由于难以获得大的语料库，或者在平均使用中表达的范围有限，这个要求可能不能满足。这样的领域可以对关于知识库或本体中的实体的先前知识进行编码。我们提出了一个集成来自不同数据源的证据的生成模型，实现了语义信息的共享。我们通过将分布式语义的共现概念概括为包含实体或词之间的其他关系来实现这一点，我们将这些关系建模为嵌入空间上的仿射变换。我们通过超越最近的链接预测任务模型来证明这种方法的有效性，并证明了它能够从部分或完全未观察到的数据训练标签中获利。我们进一步证明了从不同数据源学习重叠词汇的有用性。

##### URL
[https://arxiv.org/abs/1510.00259](https://arxiv.org/abs/1510.00259)

##### PDF
[https://arxiv.org/pdf/1510.00259](https://arxiv.org/pdf/1510.00259)

