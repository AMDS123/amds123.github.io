---
layout: post
title: "Self-Supervised Generation of Spatial Audio for 360 Video"
date: 2018-09-07 17:25:59
categories: arXiv_SD
tags: arXiv_SD
author: Pedro Morgado, Nuno Vasconcelos, Timothy Langlois, Oliver Wang
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce an approach to convert mono audio recorded by a 360 video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360 video viewing, but spatial audio microphones are still rare in current 360 video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360 video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360 videos uploaded with spatial audio. During training, ground-truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360 video and a mono audio track.

##### Abstract (translated by Google)
我们介绍了一种将360摄像机录制的单声道音频转换为空间音频的方法，该方法表示整个观看球体上的声音分布。空间音频是沉浸式360视频观看的重要组成部分，但空间音频麦克风在当前的360视频制作中仍然很少见。我们的系统由端到端的可训练神经网络组成，这些神经网络将各个声源分开，并将它们定位在观察球上，以音频和360视频帧的多模态分析为条件。我们介绍了几个数据集，其中一个是我们自己拍摄的，另一个是从YouTube收集的，包括360个带空间音频的视频。在训练期间，地面实况空间音频用作自我监督，混合向下单声道轨道形成我们网络的输入。使用我们的方法，我们表明可以仅基于360视频和单声道音轨来推断声源的空间位置。

##### URL
[https://arxiv.org/abs/1809.02587](https://arxiv.org/abs/1809.02587)

##### PDF
[https://arxiv.org/pdf/1809.02587](https://arxiv.org/pdf/1809.02587)

