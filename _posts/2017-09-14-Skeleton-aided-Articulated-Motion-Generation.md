---
layout: post
title: "Skeleton-aided Articulated Motion Generation"
date: 2017-09-14 09:06:02
categories: arXiv_CV
tags: arXiv_CV GAN Embedding
author: Yichao Yan, Jingwei Xu, Bingbing Ni, Xiaokang Yang
mathjax: true
---

* content
{:toc}

##### Abstract
This work make the first attempt to generate articulated human motion sequence from a single image. On the one hand, we utilize paired inputs including human skeleton information as motion embedding and a single human image as appearance reference, to generate novel motion frames, based on the conditional GAN infrastructure. On the other hand, a triplet loss is employed to pursue appearance-smoothness between consecutive frames. As the proposed framework is capable of jointly exploiting the image appearance space and articulated/kinematic motion space, it generates realistic articulated motion sequence, in contrast to most previous video generation methods which yield blurred motion effects. We test our model on two human action datasets including KTH and Human3.6M, and the proposed framework generates very promising results on both datasets.

##### Abstract (translated by Google)
这项工作首次尝试从单个图像生成明确的人体运动序列。一方面，我们利用包括人体骨架信息在内的成对输入作为运动嵌入和单个人体图像作为外观参考，以基于条件GAN基础设施来生成新的运动帧。另一方面，为了追求连续帧之间的外观平滑性，采用了三重点丢失（triplet loss）。由于所提出的框架能够共同利用图像外观空间和关节/运动空间，所以它产生了真实的关节运动序列，与大多数先前的视频生成方法产生模糊的运动效果相反。我们在包括KTH和Human3.6M在内的两个人类行为数据集上测试了我们的模型，并且所提出的框架在两个数据集上都产生了非常有希望的结果。

##### URL
[https://arxiv.org/abs/1707.01058](https://arxiv.org/abs/1707.01058)

##### PDF
[https://arxiv.org/pdf/1707.01058](https://arxiv.org/pdf/1707.01058)

