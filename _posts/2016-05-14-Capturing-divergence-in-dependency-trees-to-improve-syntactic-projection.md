---
layout: post
title: "Capturing divergence in dependency trees to improve syntactic projection"
date: 2016-05-14 22:11:07
categories: arXiv_CL
tags: arXiv_CL Knowledge Detection
author: Ryan Georgi, Fei Xia, William D. Lewis
mathjax: true
---

* content
{:toc}

##### Abstract
Obtaining syntactic parses is a crucial part of many NLP pipelines. However, most of the world's languages do not have large amounts of syntactically annotated corpora available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora consisting of resource-poor and resource-rich language pairs, taking advantage of a parser for the resource-rich language and word alignment between the languages to project the parses onto the data for the resource-poor language. These projection methods can suffer, however, when the two languages are divergent. In this paper, we investigate the possibility of using small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. These patterns can then be used to improve structural projection algorithms, allowing for better performing NLP tools for resource-poor languages, in particular those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that common patterns of divergence can be identified automatically without prior knowledge of a given language pair, and the patterns can be used to improve performance of projection algorithms.

##### Abstract (translated by Google)
获得句法分析是许多NLP管线的重要组成部分。但是，世界上大多数的语言都没有大量可用于构建解析器的语法注释语料库。句法投影技术试图通过使用由资源贫乏和资源丰富的语言对组成的平行语料库来解决这个问题，利用资源丰富的语言的解析器和语言之间的词语对齐来将解析投影到资源贫乏的语言。但是，这两种语言的发散方式可能会有所不同。在本文中，我们调查使用小，平行，注释语料库自动检测两种语言之间的发散结构模式的可能性。然后可以使用这些模式来改进结构投影算法，从而为资源匮乏的语言提供更好的执行NLP工具，特别是那些可能没有传统的完全监督方法所需的大量注释数据的工具。虽然这个检测过程并不详尽，但是我们证明了常见的发散模式可以在没有给定语言对的先验知识的情况下被自动识别，并且这些模式可以用来改善投影算法的性能。

##### URL
[https://arxiv.org/abs/1605.04475](https://arxiv.org/abs/1605.04475)

##### PDF
[https://arxiv.org/pdf/1605.04475](https://arxiv.org/pdf/1605.04475)

