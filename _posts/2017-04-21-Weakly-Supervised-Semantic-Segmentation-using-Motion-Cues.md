---
layout: post
title: "Weakly-Supervised Semantic Segmentation using Motion Cues"
date: 2017-04-21 08:16:06
categories: arXiv_CV
tags: arXiv_CV Segmentation CNN Semantic_Segmentation
author: Pavel Tokmakov, Karteek Alahari, Cordelia Schmid
mathjax: true
---

* content
{:toc}

##### Abstract
Fully convolutional neural networks (FCNNs) trained on a large number of images with strong pixel-level annotations have become the new state of the art for the semantic segmentation task. While there have been recent attempts to learn FCNNs from image-level weak annotations, they need additional constraints, such as the size of an object, to obtain reasonable performance. To address this issue, we present motion-CNN (M-CNN), a novel FCNN framework which incorporates motion cues and is learned from video-level weak annotations. Our learning scheme to train the network uses motion segments as soft constraints, thereby handling noisy motion information. When trained on weakly-annotated videos, our method outperforms the state-of-the-art EM-Adapt approach on the PASCAL VOC 2012 image segmentation benchmark. We also demonstrate that the performance of M-CNN learned with 150 weak video annotations is on par with state-of-the-art weakly-supervised methods trained with thousands of images. Finally, M-CNN substantially outperforms recent approaches in a related task of video co-localization on the YouTube-Objects dataset.

##### Abstract (translated by Google)
完全卷积神经网络（FCNNs）在大量的具有强像素级注释的图像上训练已经成为语义分割任务的新技术。虽然最近有尝试从图像级弱注释中学习FCNN，但是为了获得合理的性能，还需要额外的约束，比如对象的大小。为了解决这个问题，我们提出了运动CNN（M-CNN），一种结合运动线索并从视频级弱注释中学习的新型FCNN框架。我们的训练网络的学习方案使用运动片段作为软约束，从而处理噪声运动信息。在对弱注释视频进行训练时，我们的方法在PASCAL VOC 2012图像分割基准测试中胜过了最先进的EM-Adapt方法。我们还证明，使用150个弱视频注释学习的M-CNN的性能与用数千个图像训练的最先进的弱监督方法相当。最后，M-CNN在YouTube-对象数据集上的视频共同定位的相关任务中大大优于近期的方法。

##### URL
[https://arxiv.org/abs/1603.07188](https://arxiv.org/abs/1603.07188)

##### PDF
[https://arxiv.org/pdf/1603.07188](https://arxiv.org/pdf/1603.07188)

