---
layout: post
title: "HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment"
date: 2017-05-10 15:07:53
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning Prediction Relation
author: Ivan Vulić, Daniela Gerz, Douwe Kiela, Felix Hill, Anna Korhonen
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce HyperLex - a dataset and evaluation resource that quantifies the extent of of the semantic category membership, that is, type-of relation also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. Cognitive psychology research has established that typicality and category/class membership are computed in human semantic memory as a gradual rather than binary relation. Nevertheless, most NLP research, and existing large-scale invetories of concept category membership (WordNet, DBPedia, etc.) treat category membership and LE as binary. To address this, we asked hundreds of native English speakers to indicate typicality and strength of category membership between a diverse range of concept pairs on a crowdsourcing platform. Our results confirm that category membership and LE are indeed more gradual than binary. We then compare these human judgements with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art LE, distributional and representation learning models, and substantial differences between the models themselves. We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded LE systems.

##### Abstract (translated by Google)
我们引入HyperLex  - 一个数据集和评估资源，量化语义范畴成员的范围，也就是类型关系，也就是2,616个概念对之间的下位词 - 上位词或词汇蕴涵（LE）关系。认知心理学研究已经确定，典型性和类别/类成员在人类语义记忆中被计算为逐渐而不是二元关系。尽管如此，大多数自然语言处理研究，以及现有的概念类成员（WordNet，DBPedia等）的大规模联合处理都将类成员和LE视为二进制。为了解决这个问题，我们要求数以百计的母语为英语的人士在众包平台上指出各种概念对之间类别成员的典型性和实力。我们的结果证实，类别成员和LE确实比二进制更为渐进。然后，我们将这些人的判断与自动系统的预测进行比较，这揭示了人类的表现和最先进的LE，分布和表征学习模型之间的巨大差距，以及这些模型本身之间的巨大差异。我们讨论改进语义模型的途径来克服这种差异，并指出改进的LE系统的未来应用领域。

##### URL
[https://arxiv.org/abs/1608.02117](https://arxiv.org/abs/1608.02117)

##### PDF
[https://arxiv.org/pdf/1608.02117](https://arxiv.org/pdf/1608.02117)

