---
layout: post
title: "OBOE: Collaborative Filtering for AutoML Initialization"
date: 2018-08-09 16:56:04
categories: arXiv_AI
tags: arXiv_AI Optimization
author: Chengrun Yang, Yuji Akimoto, Dae Won Kim, Madeleine Udell
mathjax: true
---

* content
{:toc}

##### Abstract
Algorithm selection and hyperparameter tuning remain two of the most challenging tasks in machine learning. The number of machine learning applications is growing much faster than the number of machine learning experts, hence we see an increasing demand for efficient automation of learning processes. Here, we introduce OBOE, an algorithm for time-constrained model selection and hyperparameter tuning. Taking advantage of similarity between datasets, OBOE finds promising algorithm and hyperparameter configurations through collaborative filtering. Our system explores these models under time constraints, so that rapid initializations can be provided to warm-start more fine-grained optimization methods. One novel aspect of our approach is a new heuristic for active learning in time-constrained matrix completion based on optimal experiment design. Our experiments demonstrate that OBOE delivers state-of-the-art performance faster than competing approaches on a test bed of supervised learning problems.

##### Abstract (translated by Google)
算法选择和超参数调整仍然是机器学习中最具挑战性的两项任务。机器学习应用程序的数量增长速度远远超过机器学习专家的数量，因此我们看到对学习过程的高效自动化的需求不断增加。在这里，我们介绍了OBOE，一种用于时间约束模型选择和超参数调整的算法。利用数据集之间的相似性，OBOE通过协同过滤发现了有前途的算法和超参数配置。我们的系统在时间限制下探索这些模型，因此可以提供快速初始化以热启动更细粒度的优化方法。我们的方法的一个新颖方面是基于最优实验设计的时间约束矩阵完成中的主动学习的新启发式。我们的实验表明，OBOE在监督学习问题的测试台上比竞争方法更快地提供最先进的性能。

##### URL
[http://arxiv.org/abs/1808.03233](http://arxiv.org/abs/1808.03233)

##### PDF
[http://arxiv.org/pdf/1808.03233](http://arxiv.org/pdf/1808.03233)

