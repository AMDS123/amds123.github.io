---
layout: post
title: "Instance-Level Salient Object Segmentation"
date: 2017-04-12 03:05:27
categories: arXiv_CV
tags: arXiv_CV Salient Segmentation CNN Optimization Detection
author: Guanbin Li, Yuan Xie, Liang Lin, Yizhou Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Image saliency detection has recently witnessed rapid progress due to deep convolutional neural networks. However, none of the existing methods is able to identify object instances in the detected salient regions. In this paper, we present a salient instance segmentation method that produces a saliency mask with distinct object instance labels for an input image. Our method consists of three steps, estimating saliency map, detecting salient object contours and identifying salient object instances. For the first two steps, we propose a multiscale saliency refinement network, which generates high-quality salient region masks and salient object contours. Once integrated with multiscale combinatorial grouping and a MAP-based subset optimization framework, our method can generate very promising salient object instance segmentation results. To promote further research and evaluation of salient instance segmentation, we also construct a new database of 1000 images and their pixelwise salient instance annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks for salient region detection as well as on our new dataset for salient instance segmentation.

##### Abstract (translated by Google)
由于深度卷积神经网络，图像显着性检测近来已经取得了迅速的进展。然而，现有的方法都不能识别检测到的显着区域中的对象实例。在本文中，我们提出了一个突出的实例分割方法，为输入图像生成一个具有不同对象实例标签的显着模板。我们的方法包括三个步骤，估计显着性图，检测显着的对象轮廓和识别显着的对象实例。对于前两步，我们提出了一个多尺度显着细化网络，它产生了高质量的显着区域掩模和显着的物体轮廓。一旦与多尺度组合分组和一个基于MAP的子集优化框架相结合，我们的方法可以生成非常有前途的显着对象实例分割结果。为了促进对显着实例分割的进一步研究和评估，我们还构建了1000个图像的新数据库及其像素显着实例注释。实验结果表明，我们提出的方法能够在显着区域检测的所有公共基准以及用于显着实例分割的新数据集上实现最先进的性能。

##### URL
[https://arxiv.org/abs/1704.03604](https://arxiv.org/abs/1704.03604)

##### PDF
[https://arxiv.org/pdf/1704.03604](https://arxiv.org/pdf/1704.03604)

