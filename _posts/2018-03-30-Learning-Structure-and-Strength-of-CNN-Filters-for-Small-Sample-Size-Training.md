---
layout: post
title: "Learning Structure and Strength of CNN Filters for Small Sample Size Training"
date: 2018-03-30 10:34:33
categories: arXiv_CV
tags: arXiv_CV Face CNN Recognition Face_Recognition
author: Rohit Keshari, Mayank Vatsa, Richa Singh, Afzel Noore
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional Neural Networks have provided state-of-the-art results in several computer vision problems. However, due to a large number of parameters in CNNs, they require a large number of training samples which is a limiting factor for small sample size problems. To address this limitation, we propose SSF-CNN which focuses on learning the structure and strength of filters. The structure of the filter is initialized using a dictionary-based filter learning algorithm and the strength of the filter is learned using the small sample training data. The architecture provides the flexibility of training with both small and large training databases and yields good accuracies even with small size training data. The effectiveness of the algorithm is first demonstrated on MNIST, CIFAR10, and NORB databases, with a varying number of training samples. The results show that SSF-CNN significantly reduces the number of parameters required for training while providing high accuracies the test databases. On small sample size problems such as newborn face recognition and Omniglot, it yields state-of-the-art results. Specifically, on the IIITD Newborn Face Database, the results demonstrate improvement in rank-1 identification accuracy by at least 10%.

##### Abstract (translated by Google)
卷积神经网络在几种计算机视觉问题中提供了最新的结果。然而，由于CNN中有大量的参数，它们需要大量的训练样本，这是小样本量问题的限制因素。为了解决这个限制，我们提出了SSF-CNN，其重点在于学习滤波器的结构和强度。使用基于字典的过滤器学习算法来初始化过滤器的结构，并且使用小样本训练数据来学习过滤器的强度。该架构为小型和大型培训数据库提供了培训的灵活性，即使小规模培训数据也能产生良好的精度。该算法的有效性首先在MNIST，CIFAR10和NORB数据库中展示，并且具有不同数量的训练样本。结果表明，SSF-CNN显着减少了训练所需的参数数量，同时提供了高精度的测试数据库。对于小样本大小的问题，如新生儿脸部识别和Omniglot，它会产生最新的结果。具体而言，在IIITD新生儿脸部数据库中，结果显示等级1识别准确度提高至少10％。

##### URL
[https://arxiv.org/abs/1803.11405](https://arxiv.org/abs/1803.11405)

##### PDF
[https://arxiv.org/pdf/1803.11405](https://arxiv.org/pdf/1803.11405)

