---
layout: post
title: "What the Vec? Towards Probabilistically Grounded Embeddings"
date: 2019-05-26 14:38:29
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model Relation
author: Carl Allen, Ivana Bala&#x17e;evi&#x107;, Timothy Hospedales
mathjax: true
---

* content
{:toc}

##### Abstract
Word2Vec (W2V) and Glove are popular word embedding algorithms that perform well on a variety of natural language processing tasks. The algorithms are fast, efficient and their embeddings widely used. Moreover, the W2V algorithm has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and the relative simplicity of their common architecture, what the embedding parameters of W2V and Glove learn and why that it useful in downstream tasks largely remains a mystery. We show that different interactions of PMI vectors encode semantic properties that can be captured in low dimensional word embeddings by suitable projection, theoretically explaining why the embeddings of W2V and Glove work, and, in turn, revealing an interesting mathematical interconnection between the semantic relationships of relatedness, similarity, paraphrase and analogy.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1805.12164](http://arxiv.org/abs/1805.12164)

##### PDF
[http://arxiv.org/pdf/1805.12164](http://arxiv.org/pdf/1805.12164)

