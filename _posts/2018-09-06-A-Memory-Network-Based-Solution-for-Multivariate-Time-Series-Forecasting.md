---
layout: post
title: "A Memory-Network Based Solution for Multivariate Time-Series Forecasting"
date: 2018-09-06 17:29:10
categories: arXiv_CV
tags: arXiv_CV Attention RNN Deep_Learning
author: Yen-Yu Chang, Fan-Yun Sun, Yueh-Hua Wu, Shou-De Lin
mathjax: true
---

* content
{:toc}

##### Abstract
Multivariate time series forecasting is extensively studied throughout the years with ubiquitous applications in areas such as finance, traffic, environment, etc. Still, concerns have been raised on traditional methods for incapable of modeling complex patterns or dependencies lying in real word data. To address such concerns, various deep learning models, mainly Recurrent Neural Network (RNN) based methods, are proposed. Nevertheless, capturing extremely long-term patterns while effectively incorporating information from other variables remains a challenge for time-series forecasting. Furthermore, lack-of-explainability remains one serious drawback for deep neural network models. Inspired by Memory Network proposed for solving the question-answering task, we propose a deep learning based model named Memory Time-series network (MTNet) for time series forecasting. MTNet consists of a large memory component, three separate encoders, and an autoregressive component to train jointly. Additionally, the attention mechanism designed enable MTNet to be highly interpretable. We can easily tell which part of the historic data is referenced the most.

##### Abstract (translated by Google)
多年期的多变量时间序列预测在金融，交通，环境等领域普遍存在应用，但仍然存在对传统方法的关注，这些方法无法对真实单词数据中的复杂模式或依赖关系进行建模。为了解决这些问题，提出了各种深度学习模型，主要是基于递归神经网络（RNN）的方法。尽管如此，在有效整合来自其他变量的信息的同时捕获极长期模式仍然是时间序列预测的挑战。此外，缺乏可解释性仍然是深度神经网络模型的一个严重缺陷。受建议解决问答任务的Memory Network的启发，我们提出了一种基于深度学习的模型，名为Memory Time-series network（MTNet），用于时间序列预测。 MTNet由一个大型存储器组件，三个独立的编码器和一个共同训练的自回归组件组成。此外，设计的注意机制使MTNet具有高度可解释性。我们可以很容易地分辨出哪些历史数据被引用最多。

##### URL
[https://arxiv.org/abs/1809.02105](https://arxiv.org/abs/1809.02105)

##### PDF
[https://arxiv.org/pdf/1809.02105](https://arxiv.org/pdf/1809.02105)

