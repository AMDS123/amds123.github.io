---
layout: post
title: "What do we need to build explainable AI systems for the medical domain?"
date: 2017-12-28 16:46:05
categories: arXiv_AI
tags: arXiv_AI Knowledge Speech_Recognition Reinforcement_Learning Deep_Learning Recommendation Recognition
author: Andreas Holzinger, Chris Biemann, Constantinos S. Pattichis, Douglas B. Kell
mathjax: true
---

* content
{:toc}

##### Abstract
Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.

##### Abstract (translated by Google)
通常人工智能（AI）和机器学习（ML）在许多不同的应用领域具体地展示了令人印象深刻的实际成功，例如，在自动驾驶，语音识别或推荐系统中。在超大型数据集或强化学习方法方面训练的深度学习方法甚至超过了人类在视觉任务方面的表现，特别是在玩Atari游戏或掌握Go游戏时。即使在医疗领域也有显着的成效。这些模型的核心问题是它们被视为黑箱模型，即使我们理解了基本的数学原理，它们也缺少明确的陈述性知识表示，因此难以产生基本的解释结构。这就要求系统能够使决策透明化，可理解和可解释。我们的方法的巨大动力是法律和隐私方面的不断增长。新的“欧洲通用数据保护条例”于2018年5月25日生效，将使黑箱办法难以用于商业。这并不意味着禁止自动学习的方法，也不意味着有义务随时解释所有的事情，但是，必须有可能根据需要使结果可以重新追溯。在本文中，我们概述了我们在解释性AI相对较新的领域中的一些研究课题，重点是在医学领域的应用，这是一个非常特殊的领域。这是由于医疗专业人员主要工作在分布式，异构和复杂的数据源。在本文中，我们集中在三个来源：图像，组合数据和文本。我们认为，对可解释人工智能的研究通常有助于促进AI / ML在医学领域的实施，特别有助于促进透明度和信任度。

##### URL
[http://arxiv.org/abs/1712.09923](http://arxiv.org/abs/1712.09923)

##### PDF
[http://arxiv.org/pdf/1712.09923](http://arxiv.org/pdf/1712.09923)

