---
layout: post
title: "Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning"
date: 2017-09-14 16:38:23
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Classification Deep_Learning
author: Briland Hitaj, Giuseppe Ateniese, Fernando Perez-Cruz
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Learning has recently become hugely popular in machine learning, providing significant improvements in classification accuracy in the presence of highly-structured and large databases. Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level DP applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).

##### Abstract (translated by Google)
深度学习最近在机器学习中变得非常流行，在高度结构化和大型数据库的存在下，分类准确性显着提高。研究人员还考虑了深度学习的隐私影响。典型的模型是以集中的方式进行训练，所有的数据都是由相同的训练算法处理的。如果数据是用户私人数据（包括习惯，个人照片，地理位置，兴趣等等）的集合，中央服务器将访问可能被错误处理的敏感信息。为了解决这个问题，最近已经提出了协作式深度学习模型，其中各方在本地训练其深度学习结构，并且仅仅共享参数的一个子集以试图保持它们各自的训练集私有化。通过差分隐私（DP），参数也可以被模糊处理，以使信息提取更具挑战性，正如Shokri和Shmatikov在CCS'15所提出的。不幸的是，我们发现任何隐私保护协作深度学习都容易受到我们在本文中设计的强大攻击的影响。特别是，我们表明，分布式，联邦式或分散式的深度学习方法从根本上被打破，并不能保护诚实参与者的训练集。我们开发的攻击利用了学习过程的实时性，允许攻击者训练一个生成对手网络（GAN），生成目标训练集的原型样本，这个训练集是私有的（由GAN生成的样本是意图来自与训练数据相同的分配）。有趣的是，我们表明，应用于模型的共享参数的记录级别DP是无效的（即记录级别的DP不是为了解决我们的攻击）。

##### URL
[https://arxiv.org/abs/1702.07464](https://arxiv.org/abs/1702.07464)

##### PDF
[https://arxiv.org/pdf/1702.07464](https://arxiv.org/pdf/1702.07464)

