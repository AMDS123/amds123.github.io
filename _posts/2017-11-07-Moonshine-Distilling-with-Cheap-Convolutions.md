---
layout: post
title: "Moonshine: Distilling with Cheap Convolutions"
date: 2017-11-07 17:21:06
categories: arXiv_CV
tags: arXiv_CV Attention CNN
author: Elliot J. Crowley, Gavin Gray, Amos Storkey
mathjax: true
---

* content
{:toc}

##### Abstract
Model distillation compresses a trained machine learning model, such as a neural network, into a smaller alternative such that it could be easily deployed in a resource limited setting. Unfortunately, this requires engineering two architectures: a student architecture smaller than the first teacher architecture but trained to emulate it. In this paper, we present a distillation strategy that produces a student architecture that is a simple transformation of the teacher architecture. Recent model distillation methods allow us to preserve most of the performance of the trained model after replacing convolutional blocks with a cheap alternative. In addition, distillation by attention transfer provides student network performance that is better than training that student architecture directly on data.

##### Abstract (translated by Google)
模型蒸馏将经过训练的机器学习模型（例如神经网络）压缩成更小的替代方案，使得它可以容易地部署在资源有限的环境中。不幸的是，这需要设计两种架构：一种比第一种教师架构小的学生架构，但经过训练仿效它。在这篇文章中，我们提出了一个精炼的策略，产生一个学生的架构，这是一个简单的教师架构转型。最近的模型蒸馏方法允许我们在用廉价的替代方案替换卷积块之后保留训练过的模型的大部分性能。另外，通过注意力转移来提升学生的网络表现比直接在数据上训练学生的体系结构要好。

##### URL
[https://arxiv.org/abs/1711.02613](https://arxiv.org/abs/1711.02613)

##### PDF
[https://arxiv.org/pdf/1711.02613](https://arxiv.org/pdf/1711.02613)

