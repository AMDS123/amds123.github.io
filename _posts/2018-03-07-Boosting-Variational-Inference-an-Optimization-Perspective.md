---
layout: post
title: "Boosting Variational Inference: an Optimization Perspective"
date: 2018-03-07 13:04:35
categories: arXiv_AI
tags: arXiv_AI Optimization Inference
author: Francesco Locatello, Rajiv Khanna, Joydeep Ghosh, Gunnar R&#xe4;tsch
mathjax: true
---

* content
{:toc}

##### Abstract
Variational inference is a popular technique to approximate a possibly intractable Bayesian posterior with a more tractable one. Recently, boosting variational inference has been proposed as a new paradigm to approximate the posterior by a mixture of densities by greedily adding components to the mixture. However, as is the case with many other variational inference algorithms, its theoretical properties have not been studied. In the present work, we study the convergence properties of this approach from a modern optimization viewpoint by establishing connections to the classic Frank-Wolfe algorithm. Our analyses yields novel theoretical insights regarding the sufficient conditions for convergence, explicit rates, and algorithmic simplifications. Since a lot of focus in previous works for variational inference has been on tractability, our work is especially important as a much needed attempt to bridge the gap between probabilistic models and their corresponding theoretical properties.

##### Abstract (translated by Google)
变分推理是一种流行的技术，用于更容易处理可能难以控制的贝叶斯后验。近来，推动变分推理已被提出作为一种新的范例，通过在混合物中贪婪地添加组分来混合密度来近似后验。然而，与许多其他变分推理算法一样，其理论性质尚未研究。在目前的工作中，我们通过建立与经典Frank-Wolfe算法的连接，从现代优化的角度研究这种方法的收敛性质。我们的分析对于收敛的充分条件，显式速率和算法简化提供了新颖的理论见解。由于以前关于变分推理的大量工作集中在易处理性方面，因此我们的工作尤其重要，因为它是弥合概率模型与其相应理论性质之间差距的非常必要的尝试。

##### URL
[http://arxiv.org/abs/1708.01733](http://arxiv.org/abs/1708.01733)

##### PDF
[http://arxiv.org/pdf/1708.01733](http://arxiv.org/pdf/1708.01733)

