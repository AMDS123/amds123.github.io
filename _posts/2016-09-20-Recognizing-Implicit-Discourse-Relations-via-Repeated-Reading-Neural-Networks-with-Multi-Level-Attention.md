---
layout: post
title: "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention"
date: 2016-09-20 22:59:19
categories: arXiv_CL
tags: arXiv_CL Attention Relation
author: Yang Liu, Sujian Li
mathjax: true
---

* content
{:toc}

##### Abstract
Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.

##### Abstract (translated by Google)
认识隐含的话语关系是自然语言处理领域中具有挑战性但又重要的任务。对于这样一个复杂的文本处理任务，与以往的研究不同，我们认为有必要反复阅读论证，并动态地利用有效识别话语关系的有效特征。为了模仿重复阅读策略，我们提出了多层次关注神经网络（NNMA），结合注意机制和外部记忆，逐渐将注意力集中在有助于判断话语关系的特定词上。在PDTB数据集上的实验表明，我们提出的方法达到了最新的结果。关注权重的可视化也说明了我们的模型观察每个层次上的论点并逐渐定位重要的话语的进展。

##### URL
[https://arxiv.org/abs/1609.06380](https://arxiv.org/abs/1609.06380)

##### PDF
[https://arxiv.org/pdf/1609.06380](https://arxiv.org/pdf/1609.06380)

