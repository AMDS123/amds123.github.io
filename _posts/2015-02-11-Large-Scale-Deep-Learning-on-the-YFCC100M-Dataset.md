---
layout: post
title: "Large-Scale Deep Learning on the YFCC100M Dataset"
date: 2015-02-11 19:24:36
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Karl Ni, Roger Pearce, Kofi Boakye, Brian Van Essen, Damian Borth, Barry Chen, Eric Wang
mathjax: true
---

* content
{:toc}

##### Abstract
We present a work-in-progress snapshot of learning with a 15 billion parameter deep learning network on HPC architectures applied to the largest publicly available natural image and video dataset released to-date. Recent advancements in unsupervised deep neural networks suggest that scaling up such networks in both model and training dataset size can yield significant improvements in the learning of concepts at the highest layers. We train our three-layer deep neural network on the Yahoo! Flickr Creative Commons 100M dataset. The dataset comprises approximately 99.2 million images and 800,000 user-created videos from Yahoo's Flickr image and video sharing platform. Training of our network takes eight days on 98 GPU nodes at the High Performance Computing Center at Lawrence Livermore National Laboratory. Encouraging preliminary results and future research directions are presented and discussed.

##### Abstract (translated by Google)
我们提供了一个在HPC架构上应用于迄今为止公布的最大的公开可用自然图像和视频数据集的150亿参数深度学习网络的学习快照。最近在无监督的深度神经网络中的进步表明，在模型和训练数据集大小上扩大这种网络可以在最高层的概念学习上产生显着的改进。我们在Yahoo!上训练我们的三层深层神经网络。 Flickr Creative Commons 100M数据集。该数据集包含约9920万个图像和80万个来自雅虎Flickr图像和视频共享平台的用户创建的视频。在劳伦斯利弗莫尔国家实验室的高性能计算中心，我们的网络培训需要八天时间处理98个GPU节点。提出和讨论鼓励初步结果和未来的研究方向。

##### URL
[https://arxiv.org/abs/1502.03409](https://arxiv.org/abs/1502.03409)

##### PDF
[https://arxiv.org/pdf/1502.03409](https://arxiv.org/pdf/1502.03409)

