---
layout: post
title: "A Generic Online Parallel Learning Framework for Large Margin Models"
date: 2017-03-02 13:52:47
categories: arXiv_SD
tags: arXiv_SD Gradient_Descent
author: Shuming Ma, Xu Sun
mathjax: true
---

* content
{:toc}

##### Abstract
To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy.

##### Abstract (translated by Google)
为了加快培训过程，许多现有的系统使用并行技术来进行在线学习算法。然而，大多数研究主要集中在随机梯度下降（SGD）而不是其他算法。我们提出了一个大型边缘模型的通用在线并行学习框架，并分析了我们的框架，包括MIRA和结构化感知器等流行的大型边缘算法。我们的框架是无锁的，易于在现有系统上实现。实验表明，使用我们的框架的系统可以通过增加运行线程而获得接近线性的速度，并且不会降低准确性。

##### URL
[https://arxiv.org/abs/1703.00786](https://arxiv.org/abs/1703.00786)

##### PDF
[https://arxiv.org/pdf/1703.00786](https://arxiv.org/pdf/1703.00786)

