---
layout: post
title: "Addressing the Data Sparsity Issue in Neural AMR Parsing"
date: 2017-02-16 17:09:12
categories: arXiv_SD
tags: arXiv_SD Attention
author: Xiaochang Peng, Chuan Wang, Daniel Gildea, Nianwen Xue
mathjax: true
---

* content
{:toc}

##### Abstract
Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.

##### Abstract (translated by Google)
神经注意模型在不同的NLP任务中取得了巨大的成功。但是，由于数据稀疏问题，他们还没有在AMR解析任务上履行诺言。在本文中，我们描述了AMR解析的序列 - 序列模型，并提出了解决数据稀疏性问题的不同方法。我们展示了我们的方法在基线神经注意模型上取得了显着的改进，而且我们的结果也与不使用额外语言资源的最新系统竞争。

##### URL
[https://arxiv.org/abs/1702.05053](https://arxiv.org/abs/1702.05053)

##### PDF
[https://arxiv.org/pdf/1702.05053](https://arxiv.org/pdf/1702.05053)

