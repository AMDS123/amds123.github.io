---
layout: post
title: 'Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images And Text'
date: 2017-12-06 03:36:48
categories: arXiv_CV
tags: arXiv_CV Caption Deep_Learning
author: Ayush Jaiswal, Ekraam Sabir, Wael AbdAlmageed, Premkumar Natarajan
---

* content
{:toc}

##### Abstract
Real world multimedia data is often composed of multiple modalities such as an image or a video with associated text (e.g. captions, user comments, etc.) and metadata. Such multimodal data packages are prone to manipulations, where a subset of these modalities can be altered to misrepresent or repurpose data packages, with possible malicious intent. It is, therefore, important to develop methods to assess or verify the integrity of these multimedia packages. Using computer vision and natural language processing methods to directly compare the image (or video) and the associated caption to verify the integrity of a media package is only possible for a limited set of objects and scenes. In this paper, we present a novel deep learning-based approach for assessing the semantic integrity of multimedia packages containing images and captions, using a reference set of multimedia packages. We construct a joint embedding of images and captions with deep multimodal representation learning on the reference dataset in a framework that also provides image-caption consistency scores (ICCSs). The integrity of query media packages is assessed as the inlierness of the query ICCSs with respect to the reference dataset. We present the MultimodAl Information Manipulation dataset (MAIM), a new dataset of media packages from Flickr, which we make available to the research community. We use both the newly created dataset as well as Flickr30K and MS COCO datasets to quantitatively evaluate our proposed approach. The reference dataset does not contain unmanipulated versions of tampered query packages. Our method is able to achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO, respectively, for detecting semantically incoherent media packages.

##### Abstract (translated by Google)
真实世界的多媒体数据通常由诸如具有相关文本（例如字幕，用户评论等）和元数据的图像或视频的多种形式组成。这样的多模式数据包容易被操纵，其中这些模态的子集可以被改变，以便以可能的恶意目的来歪曲或重新设计数据包。因此，开发评估或验证这些多媒体包的完整性的方法是非常重要的。使用计算机视觉和自然语言处理方法直接比较图像（或视频）和相关联的标题，以验证媒体包的完整性只能用于有限的一组对象和场景。在本文中，我们提出了一种新颖的基于深度学习的方法来评估包含图像和标题的多媒体包的语义完整性，使用多媒体包的参考集。我们构建了图像和字幕的联合嵌入，在提供图像字幕一致性评分（ICCS）的框架中，在参考数据集上进行深度多模式表示学习。查询媒体包的完整性被评估为查询ICCS关于参考数据集的内在性。我们提出了多模式信息处理数据集（MAIM），这是一个来自Flickr的媒体包的新数据集，我们可以提供给研究团体。我们使用新创建的数据集以及Flickr30K和MS COCO数据集来定量评估我们提出的方法。参考数据集不包含篡改查询包的未处理版本。我们的方法能够在MAIM，Flickr30K和MS COCO上分别获得0.75，0.89和0.94的F1分数，用于检测语义不连贯的媒体包。

##### URL
[https://arxiv.org/abs/1707.01606](https://arxiv.org/abs/1707.01606)

