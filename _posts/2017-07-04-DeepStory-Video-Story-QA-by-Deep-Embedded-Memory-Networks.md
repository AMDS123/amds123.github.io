---
layout: post
title: "DeepStory: Video Story QA by Deep Embedded Memory Networks"
date: 2017-07-04 07:42:05
categories: arXiv_CV
tags: arXiv_CV QA Attention Embedding RNN Memory_Networks
author: Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, Byoung-Tak Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Question-answering (QA) on video contents is a significant challenge for achieving human-level intelligence as it involves both vision and language in real-world settings. Here we demonstrate the possibility of an AI agent performing video story QA by learning from a large amount of cartoon videos. We develop a video-story learning model, i.e. Deep Embedded Memory Networks (DEMN), to reconstruct stories from a joint scene-dialogue video stream using a latent embedding space of observed data. The video stories are stored in a long-term memory component. For a given question, an LSTM-based attention model uses the long-term memory to recall the best question-story-answer triplet by focusing on specific words containing key information. We trained the DEMN on a novel QA dataset of children's cartoon video series, Pororo. The dataset contains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained sentences for scene description, and 8,913 story-related QA pairs. Our experimental results show that the DEMN outperforms other QA models. This is mainly due to 1) the reconstruction of video stories in a scene-dialogue combined form that utilize the latent embedding and 2) attention. DEMN also achieved state-of-the-art results on the MovieQA benchmark.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1707.00836](https://arxiv.org/abs/1707.00836)

##### PDF
[https://arxiv.org/pdf/1707.00836](https://arxiv.org/pdf/1707.00836)

