---
layout: post
title: "Self-Attention-Based Message-Relevant Response Generation for Neural Conversation Model"
date: 2018-05-23 07:14:21
categories: arXiv_CL
tags: arXiv_CL Attention
author: Jonggu Kim, Doyeon Kong, Jong-Hyeok Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Using a sequence-to-sequence framework, many neural conversation models for chit-chat succeed in naturalness of the response. Nevertheless, the neural conversation models tend to give generic responses which are not specific to given messages, and it still remains as a challenge. To alleviate the tendency, we propose a method to promote message-relevant and diverse responses for neural conversation model by using self-attention, which is time-efficient as well as effective. Furthermore, we present an investigation of why and how effective self-attention is in deep comparison with the standard dialogue generation. The experiment results show that the proposed method improves the standard dialogue generation in various evaluation metrics.

##### Abstract (translated by Google)
使用序列到序列框架，许多用于聊天的神经对话模型成功地反应了自然。尽管如此，神经对话模型倾向于给出不特定于给定消息的通用响应，并且它仍然是一个挑战。为了缓解这一趋势，我们提出了一种通过使用自我注意来促进与神经对话模型的消息相关和多样化反应的方法，该方法既省时又高效。此外，我们还介绍了为什么有效的自我关注与标准对话世代进行深入比较的原因。实验结果表明，该方法改进了各种评估指标的标准对话生成。

##### URL
[http://arxiv.org/abs/1805.08983](http://arxiv.org/abs/1805.08983)

##### PDF
[http://arxiv.org/pdf/1805.08983](http://arxiv.org/pdf/1805.08983)

