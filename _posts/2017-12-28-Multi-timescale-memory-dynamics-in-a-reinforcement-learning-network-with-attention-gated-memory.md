---
layout: post
title: "Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory"
date: 2017-12-28 21:26:43
categories: arXiv_CV
tags: arXiv_CV Attention Reinforcement_Learning Relation
author: Marco Martinolli, Wulfram Gerstner, Aditya Gilra
mathjax: true
---

* content
{:toc}

##### Abstract
Learning and memory are intertwined in our brain and their relationship is at the core of several recent neural network models. In particular, the Attention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning network with an emphasis on biological plausibility of memory dynamics and learning. We find that the AuGMEnT network does not solve some hierarchical tasks, where higher-level stimuli have to be maintained over a long time, while lower-level stimuli need to be remembered and forgotten over a shorter timescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky or short-timescale and non-leaky or long-timescale units in memory, that allow to exchange lower-level information while maintaining higher-level one, thus solving both hierarchical and distractor tasks.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1712.10062](https://arxiv.org/abs/1712.10062)

##### PDF
[https://arxiv.org/pdf/1712.10062](https://arxiv.org/pdf/1712.10062)

