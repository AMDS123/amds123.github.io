---
layout: post
title: "Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory"
date: 2017-12-28 21:26:43
categories: arXiv_CV
tags: arXiv_CV Attention Reinforcement_Learning Relation
author: Marco Martinolli, Wulfram Gerstner, Aditya Gilra
mathjax: true
---

* content
{:toc}

##### Abstract
Learning and memory are intertwined in our brain and their relationship is at the core of several recent neural network models. In particular, the Attention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning network with an emphasis on biological plausibility of memory dynamics and learning. We find that the AuGMEnT network does not solve some hierarchical tasks, where higher-level stimuli have to be maintained over a long time, while lower-level stimuli need to be remembered and forgotten over a shorter timescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky or short-timescale and non-leaky or long-timescale units in memory, that allow to exchange lower-level information while maintaining higher-level one, thus solving both hierarchical and distractor tasks.

##### Abstract (translated by Google)
学习和记忆交织在我们的大脑中，他们的关系是最近几个神经网络模型的核心。特别是注意控制的记忆标记模型（AuGMEnT）是一个强调学习网络，强调记忆动态和学习的生物合理性。我们发现，AuGMEnT网络不能解决一些分层次的任务，需要长时间维持更高层次的刺激，而需要在较短时间内记住和忘记较低层次的刺激。为了克服这个限制，我们引入了混合型AuGMEnT，它具有泄漏或短时间尺度，内存中没有泄漏或长时间尺度的单元，可以在保持较高级别的信息的同时交换较低级别的信息，从而解决分层和分散任务。

##### URL
[https://arxiv.org/abs/1712.10062](https://arxiv.org/abs/1712.10062)

##### PDF
[https://arxiv.org/pdf/1712.10062](https://arxiv.org/pdf/1712.10062)

