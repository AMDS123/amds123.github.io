---
layout: post
title: "Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering"
date: 2017-04-12 05:53:56
categories: arXiv_CV
tags: arXiv_CV QA VQA
author: Vahid Kazemi, Ali Elqursh
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a new baseline for visual question answering task. Given an image and a question in natural language, our model produces accurate answers according to the content of the image. Our model, while being architecturally simple and relatively small in terms of trainable parameters, sets a new state of the art on both unbalanced and balanced VQA benchmark. On VQA 1.0 open ended challenge, our model achieves 64.6% accuracy on the test-standard set without using additional data, an improvement of 0.4% over state of the art, and on newly released VQA 2.0, our model scores 59.7% on validation set outperforming best previously reported results by 0.5%. The results presented in this paper are especially interesting because very similar models have been tried before but significantly lower performance were reported. In light of the new results we hope to see more meaningful research on visual question answering in the future.

##### Abstract (translated by Google)
本文提出了视觉问题解答任务的新基准。给定自然语言的图像和问题，我们的模型根据图像的内容产生准确的答案。我们的模型虽然体系结构简单，在可训练参数方面相对较小，但却为不平衡和平衡的VQA基准设定了新的技术水平。在VQA 1.0开放式挑战中，我们的模型在测试标准集上的准确率达到了64.6％，而不使用额外的数据，比现有技术水平提高了0.4％，在新发布的VQA 2.0上，我们的模型在验证集上得分为59.7％超过以前最好的报告结果0.5％。本文提出的结果是特别有趣的，因为以前已经尝试过非常相似的模型，但报告的性能明显降低。鉴于新的结果，我们希望将来能够看到更多有意义的视觉问题的研究。

##### URL
[https://arxiv.org/abs/1704.03162](https://arxiv.org/abs/1704.03162)

##### PDF
[https://arxiv.org/pdf/1704.03162](https://arxiv.org/pdf/1704.03162)

