---
layout: post
title: "Improving Question Answering by Commonsense-Based Pre-Training"
date: 2018-09-05 13:04:56
categories: arXiv_CL
tags: arXiv_CL Knowledge Relation
author: Wanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin
mathjax: true
---

* content
{:toc}

##### Abstract
Although neural network approaches achieve remarkable success on a variety of NLP tasks, many of them struggle to answer questions that require commonsense knowledge. We believe the main reason is the lack of commonsense connections between concepts. To remedy this, we provide a simple and effective method that leverages external commonsense knowledge base such as ConceptNet. We pre-train direct and indirect relational functions between concepts, and show that these pre-trained functions could be easily added to existing neural network models. Results show that incorporating commonsense-based function improves the state-of-the-art on two question answering tasks that require commonsense reasoning. Further analysis shows that our system discovers and leverages useful evidences from an external commonsense knowledge base, which is missing in existing neural network models and help derive the correct answer.

##### Abstract (translated by Google)
尽管神经网络方法在各种NLP任务上取得了显着的成功，但是他们中的许多人很难回答需要常识知识的问题。我们认为主要原因是概念之间缺乏常识联系。为了解决这个问题，我们提供了一种简单有效的方法，可以利用外部常识知识库，如ConceptNet。我们预先训练概念之间的直接和间接关系函数，并表明这些预先训练的函数可以很容易地添加到现有的神经网络模型中。结果表明，结合基于常识的功能改进了需要常识推理的两个问题回答任务的最新技术。进一步的分析表明，我们的系统发现并利用来自外部常识知识库的有用证据，这在现有神经网络模型中缺失并有助于得出正确的答案。

##### URL
[http://arxiv.org/abs/1809.03568](http://arxiv.org/abs/1809.03568)

##### PDF
[http://arxiv.org/pdf/1809.03568](http://arxiv.org/pdf/1809.03568)

