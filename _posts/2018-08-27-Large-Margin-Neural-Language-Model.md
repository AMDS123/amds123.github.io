---
layout: post
title: "Large Margin Neural Language Model"
date: 2018-08-27 18:31:33
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition Language_Model Recognition
author: Jiaji Huang, Yi Li, Wei Ping, Liang Huang
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a large margin criterion for training neural language models. Conventionally, neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However, we demonstrate that PPL may not be the best metric to optimize in some tasks, and further propose a large margin formulation. The proposed method aims to enlarge the margin between the "good" and "bad" sentences in a task-specific sense. It is trained end-to-end and can be widely applied to tasks that involve re-scoring of generated text. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction for speech recognition and 1.0 BLEU increase for machine translation.

##### Abstract (translated by Google)
我们提出了一个用于训练神经语言模型的大边界标准。通常，通过最小化语法句子上的困惑（PPL）来训练神经语言模型。但是，我们证明PPL可能不是在某些任务中优化的最佳指标，并进一步提出了大幅度的公式。所提出的方法旨在在任务特定意义上扩大“好”和“坏”句子之间的界限。它是端到端训练的，可以广泛应用于涉及重新评分生成文本的任务。与最小PPL训练相比，我们的方法在语音识别方面减少了1.1 WER，在机器翻译方面增加了1.0 BLEU。

##### URL
[http://arxiv.org/abs/1808.08987](http://arxiv.org/abs/1808.08987)

##### PDF
[http://arxiv.org/pdf/1808.08987](http://arxiv.org/pdf/1808.08987)

