---
layout: post
title: "Visual to Sound: Generating Natural Sound for Videos in the Wild"
date: 2017-12-04 22:24:29
categories: arXiv_CV
tags: arXiv_CV
author: Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara L. Berg
---

* content
{:toc}

##### Abstract
As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Such capabilities could help enable applications in virtual reality (generating sound for virtual scenes automatically) or provide additional accessibility to images or videos for people with visual impairments. As a first step in this direction, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs.

##### Abstract (translated by Google)
视觉，听觉，味觉，嗅觉和触觉这五种传统人类感官中的两种，视觉和声音是人类理解世界的基本来源。通常在自然事件中相互关联，这两种模式相结合，共同影响人类的感知。在本文中，我们提出了给定视觉输入的声音任务。这些功能有助于在虚拟现实中启用应用程序（自动为虚拟场景生成声音），或为视觉障碍人士提供对图像或视频的附加访问。作为这个方向的第一步，我们应用基于学习的方法来生成输入视频帧的原始波形采样。我们在包含各种声音（如环境声音和来自人/动物的声音）的视频数据集上评估我们的模型。我们的实验表明，生成的声音是相当现实的，并与视觉输入具有良好的时间同步。

##### URL
[http://arxiv.org/abs/1712.01393](http://arxiv.org/abs/1712.01393)

