---
layout: post
title: "Learning Safe Policies with Expert Guidance"
date: 2018-05-21 22:40:07
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning Optimization
author: Jessie Huang, Fa Wu, Doina Precup, Yang Cai
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the "follow-the-perturbed-leader" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.

##### Abstract (translated by Google)
当奖励功能可能难以明确时，我们提出了确保强化学习代理人安全行为的框架。为了做到这一点，我们依靠专家政策中的示范性存在，并且我们提供了一个理论框架，以便代理人根据其现有知识优化奖励空间。我们提出了两种方法来解决最终的优化问题：一种精确的基于椭球体的方法和一种根据“跟踪扰动 - 领导者”算法的方法。我们的实验证明了我们的算法在离散和连续问题中的行为。训练有素的代理人在模仿其他州专家的行为的同时，安全地避免了具有潜在负面影响的状态。

##### URL
[https://arxiv.org/abs/1805.08313](https://arxiv.org/abs/1805.08313)

##### PDF
[https://arxiv.org/pdf/1805.08313](https://arxiv.org/pdf/1805.08313)

