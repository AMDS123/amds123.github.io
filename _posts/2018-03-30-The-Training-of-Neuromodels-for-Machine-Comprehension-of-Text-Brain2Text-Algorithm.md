---
layout: post
title: "The Training of Neuromodels for Machine Comprehension of Text. Brain2Text Algorithm"
date: 2018-03-30 08:32:42
categories: arXiv_CL
tags: arXiv_CL Relation
author: A.Artemov, A. Sergeev, A. Khasenevich, A. Yuzhakov, M. Chugunov
mathjax: true
---

* content
{:toc}

##### Abstract
Nowadays, the Internet represents a vast informational space, growing exponentially and the problem of search for relevant data becomes essential as never before. The algorithm proposed in the article allows to perform natural language queries on content of the document and get comprehensive meaningful answers. The problem is partially solved for English as SQuAD contains enough data to learn on, but there is no such dataset in Russian, so the methods used by scientists now are not applicable to Russian. Brain2 framework allows to cope with the problem - it stands out for its ability to be applied on small datasets and does not require impressive computing power. The algorithm is illustrated on Sberbank of Russia Strategy's text and assumes the use of a neuromodel consisting of 65 mln synapses. The trained model is able to construct word-by-word answers to questions based on a given text. The existing limitations are its current inability to identify synonyms, pronoun relations and allegories. Nevertheless, the results of conducted experiments showed high capacity and generalisation ability of the suggested approach.

##### Abstract (translated by Google)
如今，互联网代表了一个巨大的信息空间，呈指数级增长，搜索相关数据的问题变得前所未有的重要。本文提出的算法允许对文档内容执行自然语言查询并获得全面的有意义的答案。由于SQUAD包含足够的数据可供学习，但由于俄语没有这样的数据集，所以英语部分解决了这个问题，因此科学家所使用的方法现在不适用于俄语。 Brain2框架可以解决这个问题 - 它能够应用于小型数据集并且不需要令人印象深刻的计算能力。该算法在俄罗斯战略的Sberbank文本中进行了说明，并假设使用由6500万个突触构成的神经模型。训练有素的模型能够基于给定的文本逐个词地回答问题。现有的限制是它目前无法识别同义词，代词关系和寓言。尽管如此，所进行的实验的结果显示了所建议方法的高容量和泛化能力。

##### URL
[http://arxiv.org/abs/1804.00551](http://arxiv.org/abs/1804.00551)

##### PDF
[http://arxiv.org/pdf/1804.00551](http://arxiv.org/pdf/1804.00551)

