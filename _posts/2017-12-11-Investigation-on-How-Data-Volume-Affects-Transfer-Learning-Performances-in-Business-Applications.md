---
layout: post
title: "Investigation on How Data Volume Affects Transfer Learning Performances in Business Applications"
date: 2017-12-11 20:30:44
categories: arXiv_CV
tags: arXiv_CV Knowledge Transfer_Learning
author: Michael Bernico, Yuntao Li, Dingchao Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Transfer Learning helps to build a system to recognize and apply knowledge and experience learned in previous tasks (source task) to new tasks or new domains (target task), which share some commonality. The two important factors that impact the performance of transfer learning models are: (a) the size of the target dataset and (b) the similarity in distribution between source and target domains. Thus far there has been little investigation into just how important these factors are. In this paper, we investigated the impact of target dataset size and source/target domain similarity on model performance through a series of experiments. We found that more data is always beneficial, and that model performance improved linearly with the log of data size, until we were out of data. As source/target domains differ, more data is required and fine tuning will render better performance than feature extraction. When source/target domains are similar and data size is small, fine tuning and feature extraction renders equivalent performance. We hope that our study inspires further work in transfer learning, which continues to be a very important technique for developing practical machine learning applications in business domains.

##### Abstract (translated by Google)
转移学习有助于建立一个系统，将以前的任务（源任务）中学到的知识和经验识别和应用到新的任务或新的领域（目标任务），这些任务有共同之处。影响转移学习模型性能的两个重要因素是：（a）目标数据集的大小和（b）源域和目标域分布的相似性。迄今为止，对这些因素是多么重要的调查很少。本文通过一系列的实验研究了目标数据集大小与源/目标域相似度对模型性能的影响。我们发现更多的数据总是有益的，并且模型性能随着数据大小的对数线性地改善，直到我们没有数据。由于源/目标域不同，所以需要更多的数据，精细调整将比特征提取提供更好的性能。当源/目标域相似且数据量较小时，微调和特征提取会产生相同的性能。我们希望我们的研究能够激发进一步的转移学习工作，这在开发业务领域的实际机器学习应用中仍然是非常重要的技术。

##### URL
[http://arxiv.org/abs/1712.04008](http://arxiv.org/abs/1712.04008)

##### PDF
[http://arxiv.org/pdf/1712.04008](http://arxiv.org/pdf/1712.04008)

