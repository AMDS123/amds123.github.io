---
layout: post
title: "Explanation in Artificial Intelligence: Insights from the Social Sciences"
date: 2018-05-24 02:43:30
categories: arXiv_AI
tags: arXiv_AI Review
author: Tim Miller
mathjax: true
---

* content
{:toc}

##### Abstract
There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.

##### Abstract (translated by Google)
随着研究人员和从业人员试图使他们的算法更易于理解，最近人们已经在可解释的人工智能领域重新出现了复苏。这些研究大部分都集中在向人类观察者明确解释决策或行动上，并且说人类如何相互解释可以作为解释人工智能的有用起点，这应该是有争议的。然而，可以公平地说，大多数解释性人工智能方面的工作只使用研究人员对构成“良好”解释的直觉。对于人们如何定义，产生，选择，评估和呈现解释，哲学，心理学和认知科学存在着巨大而有价值的研究机构，认为人们对解释过程采用某些认知偏见和社会期望。本文认为，可解释的人工智能领域应该建立在现有的研究基础之上，并且从研究这些主题的哲学，认知心理学/科学和社会心理学的相关论文进行评论。它提出了一些重要的发现，并讨论了这些可以用于解释性人工智能的方法。

##### URL
[http://arxiv.org/abs/1706.07269](http://arxiv.org/abs/1706.07269)

##### PDF
[http://arxiv.org/pdf/1706.07269](http://arxiv.org/pdf/1706.07269)

