---
layout: post
title: "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks"
date: 2015-11-19 19:19:58
categories: arXiv_CV
tags: arXiv_CV CNN
author: Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, Dhruv Batra
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional Neural Networks have achieved state-of-the-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss, achieving significantly higher "oracle" accuracies than classical ensembles.

##### Abstract (translated by Google)
卷积神经网络已经在广泛的任务上取得了最先进的性能。大多数的基准测试都是由这些强大的学习者的合奏团队领导的，但是合奏通常被视为一个事后过程，通过平均独立训练的模型来实现，这些模型包括由装袋或随机初始化引起的模型变化。在本文中，我们严格把集合作为一个一流的问题来明确地解决这个问题：建立一个集合的最佳策略是什么？我们首先比较大量的合成策略，然后提出和评估新的策略，比如参数共享（通过一个叫做TreeNets的新的模型家族），以及集合意识和多样性鼓励的损失下的训练。我们证明TreeNets可以提高整体性能，并且可以在统一损失下端到端地训练多样化的合奏，实现比经典合奏更高的“预言”精度。

##### URL
[https://arxiv.org/abs/1511.06314](https://arxiv.org/abs/1511.06314)

##### PDF
[https://arxiv.org/pdf/1511.06314](https://arxiv.org/pdf/1511.06314)

