---
layout: post
title: "Understanding Regularization in Batch Normalization"
date: 2018-09-04 09:01:10
categories: arXiv_CV
tags: arXiv_CV Regularization Optimization
author: Ping Luo, Xinjiang Wang, Wenqi Shao, Zhanglin Peng
mathjax: true
---

* content
{:toc}

##### Abstract
Batch Normalization (BN) makes output of hidden neuron had zero mean and unit variance, improving convergence and generalization when training neural networks. This work understands these phenomena theoretically. We analyze BN by using a building block of neural networks, which consists of a weight layer, a BN layer, and a nonlinear activation function. This simple network helps us understand the characteristics of BN, where the results are generalized to deep models in numerical studies. We explore BN in three aspects. First, by viewing BN as a stochastic process, an analytical form of regularization inherited in BN is derived. Second, the optimization dynamic with this regularization shows that BN enables training converged with large maximum and effective learning rates. Third, BN's generalization with regularization is explored by using random matrix theory and statistical mechanics. Both simulations and experiments support our analyses.

##### Abstract (translated by Google)
批量归一化（BN）使得隐藏神经元的输出具有零均值和单位方差，在训练神经网络时改善收敛和泛化。这项工作从理论上理解这些现象。我们通过使用神经网络的构建块来分析BN，神经网络由权重层，BN层和非线性激活函数组成。这个简单的网络有助于我们理解BN的特征，其中结果被推广到数值研究中的深层模型。我们从三个方面探索BN。首先，通过将BN视为随机过程，导出了在BN中继承的正则化的分析形式。其次，这种正则化的优化动态表明，BN使训练能够以最大和最大的有效学习率收敛。第三，利用随机矩阵理论和统计力学探讨了BN对正则化的推广。模拟和实验都支持我们的分析。

##### URL
[http://arxiv.org/abs/1809.00846](http://arxiv.org/abs/1809.00846)

##### PDF
[http://arxiv.org/pdf/1809.00846](http://arxiv.org/pdf/1809.00846)

