---
layout: post
title: "Fourier Policy Gradients"
date: 2018-02-19 22:28:56
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Matthew Fellows, Kamil Ciosek, Shimon Whiteson
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a new way of deriving policy gradient updates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal approximation property. The choice of policy can be almost arbitrary, including mixtures or hybrid continuous-discrete probability distributions. Moreover, we derive a general family of sample-based estimators for stochastic policy gradients, which unifies existing results on sample-based approximation. We believe that this technique has the potential to shape the next generation of policy gradient approaches, powered by analytical results.

##### Abstract (translated by Google)
我们提出了一种派生政策梯度更新以强化学习的新方法。我们基于傅里叶分析的技术，将预期策略梯度产生的积分重新调整为卷积并将其转化为乘积。获得的分析解决方案使我们能够在广泛的环境中捕获EPG的低变化优势。对于评论家，我们将三角函数和径向基函数，两个具有通用逼近性质的函数族进行处理。政策的选择几乎是任意的，包括混合或混合连续离散概率分布。此外，我们推导了随机策略梯度的基于样本的估计器的一般家族，它统一了基于样本近似的现有结果。我们相信这种技术有可能塑造下一代政策梯度方法，并由分析结果提供支持。

##### URL
[http://arxiv.org/abs/1802.06891](http://arxiv.org/abs/1802.06891)

##### PDF
[http://arxiv.org/pdf/1802.06891](http://arxiv.org/pdf/1802.06891)

