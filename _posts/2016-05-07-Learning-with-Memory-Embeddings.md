---
layout: post
title: "Learning with Memory Embeddings"
date: 2016-05-07 09:06:15
categories: arXiv_CL
tags: arXiv_CL Knowledge_Graph Knowledge Embedding Represenation_Learning Relation
author: Volker Tresp, Cristóbal Esteban, Yinchong Yang, Stephan Baier, Denis Krompaß
mathjax: true
---

* content
{:toc}

##### Abstract
Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models.

##### Abstract (translated by Google)
嵌入式学习，也称为表示学习，已被证明能够模拟大规模的语义知识图。一个关键的概念是知识图到张量表示的映射，张量表示由模型使用广义实体的潜在表示来预测。潜变量模型非常适合处理典型知识图的高维和稀疏性。在最近的出版物中，嵌入模型被扩展到还考虑时间演变，时间模式和子符号表示。在本文中，我们将嵌入模型映射到各种认知记忆功能，特别是语义和概念记忆，情景记忆，感觉记忆，短期记忆和工作中记忆。我们讨论学习，查询回答，从感官输入到语义解码的路径，以及情景记忆和语义记忆之间的关系。我们介绍了一些关于人类记忆的假设，这些假设可以从开发的数学模型中得出。

##### URL
[https://arxiv.org/abs/1511.07972](https://arxiv.org/abs/1511.07972)

##### PDF
[https://arxiv.org/pdf/1511.07972](https://arxiv.org/pdf/1511.07972)

