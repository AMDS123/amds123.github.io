---
layout: post
title: "Conditional Generation and Snapshot Learning in Neural Dialogue Systems"
date: 2016-06-10 14:56:19
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, Steve Young
mathjax: true
---

* content
{:toc}

##### Abstract
Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.

##### Abstract (translated by Google)
最近，许多基于LSTM的条件语言模型（LM）已经应用于各种语言生成任务。在这项工作中，我们研究了不同的模型体系结构和不同的方法来表示和汇总源信息在一个端到端的神经对话系统框架。还提出了一种称为快照学习的方法，以通过将伴随交叉熵目标函数应用于调节向量来促进从受监督的顺序信号学习。实验和分析结果首先表明，调节向量和LM之间发生竞争，不同的体系结构提供了两者之间的不同折衷。其次，调节矢量的判别能力和透明度是提供模型可解释性和更好性能的关键。第三，快照学习导致不依赖于哪种架构而进行一致的性能改进。

##### URL
[https://arxiv.org/abs/1606.03352](https://arxiv.org/abs/1606.03352)

##### PDF
[https://arxiv.org/pdf/1606.03352](https://arxiv.org/pdf/1606.03352)

