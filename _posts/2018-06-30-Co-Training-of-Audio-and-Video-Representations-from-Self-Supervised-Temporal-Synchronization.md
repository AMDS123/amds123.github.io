---
layout: post
title: "Co-Training of Audio and Video Representations from Self-Supervised Temporal Synchronization"
date: 2018-06-30 21:50:21
categories: arXiv_CV
tags: arXiv_CV Action_Recognition Classification Relation Recognition
author: Bruno Korbar, Du Tran, Lorenzo Torresani
mathjax: true
---

* content
{:toc}

##### Abstract
There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective features for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +16.7% in action recognition accuracy on UCF101 and a boost of +13.0% on HMDB51.

##### Abstract (translated by Google)
视频的视觉和听觉元素之间存在自然关联。在这项工作中，我们利用此连接来学习自我监督时间同步的音频和视频分析的一般和有效功能。我们证明校准的课程学习方案，仔细选择反面例子和使用对比度损失是从优化的模型中获得强大的多感官表示以识别音频 - 视频对的时间同步的关键因素。在没有进一步微调的情况下，所产生的音频功能可以达到性能优于现有音频分类基准（DCASE2014和ESC-50）的最新技术水平。同时，我们的可视子网提供了非常有效的初始化，以提高基于视频的动作识别模型的准确性：与从头开始学习相比，我们的自我监督预训练在UCF101上的动作识别准确度中获得了+ 16.7％的显着增益并且在HMDB51上提升了+ 13.0％。

##### URL
[http://arxiv.org/abs/1807.00230](http://arxiv.org/abs/1807.00230)

##### PDF
[http://arxiv.org/pdf/1807.00230](http://arxiv.org/pdf/1807.00230)

