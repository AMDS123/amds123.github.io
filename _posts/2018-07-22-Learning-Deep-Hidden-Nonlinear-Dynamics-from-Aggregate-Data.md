---
layout: post
title: "Learning Deep Hidden Nonlinear Dynamics from Aggregate Data"
date: 2018-07-22 05:59:41
categories: arXiv_AI
tags: arXiv_AI
author: Yisen Wang, Bo Dai, Lingkai Kong, Xingjun Ma, Sarah Monazam Erfani, James Bailey, Shu-Tao Xia, Le Song, Hongyuan Zha
mathjax: true
---

* content
{:toc}

##### Abstract
Learning nonlinear dynamics from diffusion data is a challenging problem since the individuals observed may be different at different time points, generally following an aggregate behaviour. Existing work cannot handle the tasks well since they model such dynamics either directly on observations or enforce the availability of complete longitudinal individual-level trajectories. However, in most of the practical applications, these requirements are unrealistic: the evolving dynamics may be too complex to be modeled directly on observations, and individual-level trajectories may not be available due to technical limitations, experimental costs and/or privacy issues. To address these challenges, we formulate a model of diffusion dynamics as the {\em hidden stochastic process} via the introduction of hidden variables for flexibility, and learn the hidden dynamics directly on {\em aggregate observations} without any requirement for individual-level trajectories. We propose a dynamic generative model with Wasserstein distance for LEarninG dEep hidden Nonlinear Dynamics (LEGEND) and prove its theoretical guarantees as well. Experiments on a range of synthetic and real-world datasets illustrate that LEGEND has very strong performance compared to state-of-the-art baselines.

##### Abstract (translated by Google)
从扩散数据学习非线性动力学是一个具有挑战性的问题，因为观察到的个体在不同的时间点可能是不同的，通常遵循聚合行为。现有的工作不能很好地处理任务，因为他们直接在观察上模拟这种动态或强制实现完整的纵向个体水平轨迹的可用性。然而，在大多数实际应用中，这些要求是不现实的：演变的动态可能过于复杂而不能直接在观察上建模，并且由于技术限制，实验成本和/或隐私问题，个别级别的轨迹可能不可用。为了应对这些挑战，我们通过引入隐藏变量来灵活地制定扩散动力学模型作为{\ em隐藏随机过程}，并直接在{\ em聚合观察}上学习隐藏动力学，而不需要任何个体层次轨迹。我们提出了一个具有Wasserstein距离的动态生成模型，用于LEarninG dEep隐藏非线性动力学（LEGEND）并证明其理论保证。在一系列合成和真实数据集上的实验表明，与最先进的基线相比，LEGEND具有非常强大的性能。

##### URL
[http://arxiv.org/abs/1807.08237](http://arxiv.org/abs/1807.08237)

##### PDF
[http://arxiv.org/pdf/1807.08237](http://arxiv.org/pdf/1807.08237)

