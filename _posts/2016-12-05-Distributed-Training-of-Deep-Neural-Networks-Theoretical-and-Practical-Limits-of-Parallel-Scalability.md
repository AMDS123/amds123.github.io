---
layout: post
title: "Distributed Training of Deep Neural Networks: Theoretical and Practical Limits of Parallel Scalability"
date: 2016-12-05 08:19:11
categories: arXiv_CV
tags: arXiv_CV Gradient_Descent
author: Janis Keuper, Franz-Josef Pfreundt
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a theoretical analysis and practical evaluation of the main bottlenecks towards a scalable distributed solution for the training of Deep Neuronal Networks (DNNs). The presented results show, that the current state of the art approach, using data-parallelized Stochastic Gradient Descent (SGD), is quickly turning into a vastly communication bound problem. In addition, we present simple but fixed theoretic constraints, preventing effective scaling of DNN training beyond only a few dozen nodes. This leads to poor scalability of DNN training in most practical scenarios.

##### Abstract (translated by Google)
本文对深度神经元网络（DNNs）训练的可扩展分布式解决方案的主要瓶颈进行了理论分析和实际评估。所呈现的结果表明，使用数据并行化的随机梯度下降（SGD）的当前最先进的方法正迅速变成一个大量的通信限制问题。此外，我们提出了简单但固定的理论约束，防止DNN训练超过几十个节点的有效缩放。这导致在大多数实际情况下DNN训练的可扩展性差。

##### URL
[https://arxiv.org/abs/1609.06870](https://arxiv.org/abs/1609.06870)

##### PDF
[https://arxiv.org/pdf/1609.06870](https://arxiv.org/pdf/1609.06870)

