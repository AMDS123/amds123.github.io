---
layout: post
title: "Distilling Word Embeddings: An Encoding Approach"
date: 2016-07-24 16:22:09
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding
author: Lili Mou, Ran Jia, Yan Xu, Ge Li, Lu Zhang, Zhi Jin
mathjax: true
---

* content
{:toc}

##### Abstract
Distilling knowledge from a well-trained cumbersome network to a small one has recently become a new research topic, as lightweight neural networks with high performance are particularly in need in various resource-restricted systems. This paper addresses the problem of distilling word embeddings for NLP tasks. We propose an encoding approach to distill task-specific knowledge from a set of high-dimensional embeddings, which can reduce model complexity by a large margin as well as retain high accuracy, showing a good compromise between efficiency and performance. Experiments in two tasks reveal the phenomenon that distilling knowledge from cumbersome embeddings is better than directly training neural networks with small embeddings.

##### Abstract (translated by Google)
从训练有素的繁琐网络向小型网络提炼知识近来成为一个新的研究课题，因为在各种资源受限的系统中，特别需要高性能的轻量级神经网络。本文讨论了为NLP任务提炼词嵌入的问题。我们提出了一种编码方法，从一组高维嵌入中提取特定任务的知识，这可以大大降低模型的复杂性，同时保持高精度，显示效率和性能之间的良好折中。在两个任务中的实验揭示了从繁琐的嵌入中提取知识的现象比用小嵌入直接训练神经网络好的现象。

##### URL
[https://arxiv.org/abs/1506.04488](https://arxiv.org/abs/1506.04488)

##### PDF
[https://arxiv.org/pdf/1506.04488](https://arxiv.org/pdf/1506.04488)

