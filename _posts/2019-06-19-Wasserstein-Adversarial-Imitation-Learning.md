---
layout: post
title: "Wasserstein Adversarial Imitation Learning"
date: 2019-06-19 14:20:58
categories: arXiv_AI
tags: arXiv_AI Regularization Adversarial Reinforcement_Learning
author: Huang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, Thai Hong Linh
mathjax: true
---

* content
{:toc}

##### Abstract
Imitation Learning describes the problem of recovering an expert policy from demonstrations. While inverse reinforcement learning approaches are known to be very sample-efficient in terms of expert demonstrations, they usually require problem-dependent reward functions or a (task-)specific reward-function regularization. In this paper, we show a natural connection between inverse reinforcement learning approaches and Optimal Transport, that enables more general reward functions with desirable properties (e.g., smoothness). Based on our observation, we propose a novel approach called Wasserstein Adversarial Imitation Learning. Our approach considers the Kantorovich potentials as a reward function and further leverages regularized optimal transport to enable large-scale applications. In several robotic experiments, our approach outperforms the baselines in terms of average cumulative rewards and shows a significant improvement in sample-efficiency, by requiring just one expert demonstration.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.08113](http://arxiv.org/abs/1906.08113)

##### PDF
[http://arxiv.org/pdf/1906.08113](http://arxiv.org/pdf/1906.08113)

