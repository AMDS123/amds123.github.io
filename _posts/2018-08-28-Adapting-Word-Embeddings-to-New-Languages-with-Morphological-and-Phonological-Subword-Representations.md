---
layout: post
title: "Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations"
date: 2018-08-28 19:11:20
categories: arXiv_CL
tags: arXiv_CL Embedding Transfer_Learning Recognition
author: Aditi Chaudhary, Chunting Zhou, Lori Levin, Graham Neubig, David R. Mortensen, Jaime G. Carbonell
mathjax: true
---

* content
{:toc}

##### Abstract
Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.

##### Abstract (translated by Google)
自然语言处理（NLP）中的大量工作一直用于资源丰富的语言，这使得对资源较少，资源较少的语言的概括具有挑战性。我们提出了两种方法，通过使用语言动机的子词单元：音素，语素和字素来调整连续的单词表示，从而改进对低资源语言的泛化。我们的方法既不需要并行语料库也不需要双语词典，并且相对于依赖这些资源的先前方法，其性能显着提高。我们证明了我们的方法对名称实体识别方法的有效性，包括维吾尔语，土耳其语，孟加拉语和印地语，其中维吾尔语和孟加拉语是低资源语言，并且还进行机器翻译实验。通过转移学习开发子词为维吾尔语提供+15.2 NER F1，为孟加拉语提供+9.7 F1。我们还显示了单语设置的改进，我们实现了（平均）+3 F1和（平均）+1.35 BLEU。

##### URL
[http://arxiv.org/abs/1808.09500](http://arxiv.org/abs/1808.09500)

##### PDF
[http://arxiv.org/pdf/1808.09500](http://arxiv.org/pdf/1808.09500)

