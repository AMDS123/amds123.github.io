---
layout: post
title: "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine"
date: 2017-06-11 11:51:06
categories: arXiv_CL
tags: arXiv_CL QA Deep_Learning
author: Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, Kyunghyun Cho
mathjax: true
---

* content
{:toc}

##### Abstract
We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.

##### Abstract (translated by Google)
我们公开发布了一个名为SearchQA的新型大型数据集，用于机器理解或回答问题。与最近发布的数据集（如DeepMind CNN / DailyMail和SQuAD）不同，所提出的SearchQA是为了反映一个全面的问答系统而设计的。也就是说，我们不是从现有的文章开始，产生一个问题 - 答案对，而是从一个现存的问题 - 答案对开始，从J爬取！存档，并用Google检索的文本片段进行扩充。按照这种方法，我们构建了SearchQA，它包含超过140k个问答对，平均每对有49.6个片段。 SearchQA的每个问题 - 答案上下文元组都附带了额外的元数据，例如片段的URL，我们相信这些元数据将成为未来研究的宝贵资源。我们在SearchQA上进行人体评估，并测试两种基准方法，一种简单的词选择和另一种基于深度学习的方法。我们表明，人类和机器表演之间有一个有意义的差距。这表明提出的数据集可以作为回答问题的基准。

##### URL
[https://arxiv.org/abs/1704.05179](https://arxiv.org/abs/1704.05179)

##### PDF
[https://arxiv.org/pdf/1704.05179](https://arxiv.org/pdf/1704.05179)

