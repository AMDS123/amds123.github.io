---
layout: post
title: "Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models"
date: 2017-11-17 06:10:03
categories: arXiv_CV
tags: arXiv_CV Embedding
author: Jiuxiang Gu, Jianfei Cai, Shafiq Joty, Li Niu, Gang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.

##### Abstract (translated by Google)
文本视觉跨模式检索一直是计算机视觉和自然语言处理社区的热门研究课题。学习适当的多模式数据表示对于跨模式检索性能至关重要。与现有的将图像文本对作为单一特征向量嵌入共同表征空间的图像文本检索方法不同，我们建议将生成过程纳入到跨模式特征嵌入中，通过这种方法我们不仅可以学习全局抽象特征还有当地的接地功能。大量的实验表明，我们的框架可以很好地匹配复杂内容的图像和句子，并在MSCOCO数据集上实现最先进的跨模态检索结果。

##### URL
[https://arxiv.org/abs/1711.06420](https://arxiv.org/abs/1711.06420)

##### PDF
[https://arxiv.org/pdf/1711.06420](https://arxiv.org/pdf/1711.06420)

