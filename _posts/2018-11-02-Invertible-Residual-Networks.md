---
layout: post
title: "Invertible Residual Networks"
date: 2018-11-02 17:17:55
categories: arXiv_AI
tags: arXiv_AI Regularization
author: Jens Behrmann, David Duvenaud, J&#xf6;rn-Henrik Jacobsen
mathjax: true
---

* content
{:toc}

##### Abstract
Reversible deep networks provide useful theoretical guarantees and have proven to be a powerful class of functions in many applications. Usually, they rely on analytical inverses using dimension splitting, fundamentally constraining their structure compared to common architectures. Based on recent links between ordinary differential equations and deep networks, we provide a sufficient condition when standard ResNets are invertible. This condition allows unconstrained architectures for residual blocks, while only requiring an adaption to their regularization scheme. We numerically compute their inverse, which has O(1) memory cost and computational cost of 5-20 forward passes. Finally, we show that invertible ResNets perform on par with standard ResNets on classifying MNIST and CIFAR10 images.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.00995](http://arxiv.org/abs/1811.00995)

##### PDF
[http://arxiv.org/pdf/1811.00995](http://arxiv.org/pdf/1811.00995)

