---
layout: post
title: "Robust exponential memory in Hopfield networks"
date: 2015-06-05 23:14:55
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Christopher Hillar, Ngoc M. Tran
mathjax: true
---

* content
{:toc}

##### Abstract
The Hopfield recurrent neural network is a classical auto-associative model of memory, in which collections of symmetrically-coupled McCulloch-Pitts neurons interact to perform emergent computation. Although previous researchers have explored the potential of this network to solve combinatorial optimization problems and store memories as attractors of its deterministic dynamics, a basic open problem is to design a family of Hopfield networks with a number of noise-tolerant memories that grows exponentially with neural population size. Here, we discover such networks by minimizing probability flow, a recently proposed objective for estimating parameters in discrete maximum entropy models. By descending the gradient of the convex probability flow, our networks adapt synaptic weights to achieve robust exponential storage, even when presented with vanishingly small numbers of training patterns. In addition to providing a new set of error-correcting codes that achieve Shannon's channel capacity bound, these networks also efficiently solve a variant of the hidden clique problem in computer science, opening new avenues for real-world applications of computational models originating from biology.

##### Abstract (translated by Google)
Hopfield递归神经网络是一种经典的记忆自相关模型，其中对称耦合的McCulloch-Pitts神经元集合进行紧急计算。虽然以前的研究人员已经探索了这个网络的潜力，以解决组合优化问题，并作为其确定性动力学的吸引力储存记忆，一个基本的开放问题是设计一个Hopfield网络家族与一些噪声容忍记忆与神经元呈指数增长人口规模。在这里，我们通过最小化概率流来发现这样的网络，最近提出的用于估计离散最大熵模型中的参数的目标。通过降低凸概率流的梯度，我们的网络适应突触权重来实现鲁棒的指数存储，即使在呈现少量的训练模式时也是如此。这些网络除了提供一套能够实现Shannon信道容量限制的新的纠错码外，还能有效地解决计算机科学中隐藏的系统问题的一个变种，为真实世界的生物学应用计算模型开辟了新的途径。

##### URL
[https://arxiv.org/abs/1411.4625](https://arxiv.org/abs/1411.4625)

##### PDF
[https://arxiv.org/pdf/1411.4625](https://arxiv.org/pdf/1411.4625)

