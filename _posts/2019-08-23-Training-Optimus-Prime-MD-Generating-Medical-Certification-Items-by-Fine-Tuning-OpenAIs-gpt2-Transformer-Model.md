---
layout: post
title: "Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI's gpt2 Transformer Model"
date: 2019-08-23 00:58:21
categories: arXiv_AI
tags: arXiv_AI Language_Model
author: Matthias von Davier
mathjax: true
---

* content
{:toc}

##### Abstract
This article describes new results of an application using transformer-based language models to automated item generation, an area of ongoing interest in the domain of certification testing as well as in educational measurement and psychological testing. OpenAI's gpt2 pre-trained 345M parameter language model was retrained using the public domain text mining set of PubMed articles and subsequently used to generate item stems (case vignettes) as well as distractor proposals for multiple-choice items. This case study shows promise and produces draft text that can be used by human item writers as input for authoring. Future experiments with more recent transformer models (such as Grover, TransformerXL) using existing item pools is expected to further improve results and facilitate the development of assessment materials.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.08594](http://arxiv.org/abs/1908.08594)

##### PDF
[http://arxiv.org/pdf/1908.08594](http://arxiv.org/pdf/1908.08594)

