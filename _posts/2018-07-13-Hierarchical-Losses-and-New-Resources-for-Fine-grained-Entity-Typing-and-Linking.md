---
layout: post
title: "Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking"
date: 2018-07-13 15:15:41
categories: arXiv_CL
tags: arXiv_CL Knowledge Ontology Prediction
author: Shikhar Murty*, Patrick Verga*, Luke Vilnis, Irena Radovanovic, Andrew McCallum
mathjax: true
---

* content
{:toc}

##### Abstract
Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.

##### Abstract (translated by Google)
从原始文本提取到实体和细粒度类型的知识库通常被预测为一组实体和类型标签的预测，忽略了策划本体中包含的类型和实体的丰富层次结构。先前尝试结合分层结构已经产生了很少的好处并且仅限于浅层本体。本文介绍了使用实数和复数双线性映射来整合分层信息的新方法，在实体链接和细粒度实体类型中实现了平坦预测的实质性改进，并为端到端模型实现了最新的最新结果在基准FIGER数据集上。我们还提出了两个包含广泛和深层次的人类注释数据集，我们将向社区发布以鼓励进一步研究这个方向：MedMentions，PubMed摘要的集合，其中246k提及已经映射到大规模UMLS本体;和TypeNet，它将Freebase类型与WordNet层次结构对齐，以获得近2k个实体类型。在对所有三个数据集的实验中，我们从层次感知培训中获得了实质性收益。

##### URL
[http://arxiv.org/abs/1807.05127](http://arxiv.org/abs/1807.05127)

##### PDF
[http://arxiv.org/pdf/1807.05127](http://arxiv.org/pdf/1807.05127)

