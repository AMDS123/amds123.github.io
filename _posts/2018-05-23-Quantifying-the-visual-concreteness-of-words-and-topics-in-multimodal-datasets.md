---
layout: post
title: "Quantifying the visual concreteness of words and topics in multimodal datasets"
date: 2018-05-23 19:15:45
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption Relation Recommendation
author: Jack Hessel, David Mimno, Lillian Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.

##### Abstract (translated by Google)
多模式机器学习算法旨在学习视觉 - 文本对应。以前的工作表明，具有具体视觉表现形式的概念可能比具有抽象概念的概念更容易学习。我们给出了一种算法，用于自动计算多模态数据集中单词和主题的视觉具体性。我们将该方法应用于四种设置，从图像标题到从历史书籍中删除的图像/文本。除了能够探索多模态数据集中的概念之外，我们的具体分数还预测了机器学习算法学习文本/视觉关系的能力。我们发现1）具体概念确实更容易学习; 2）我们考虑的大量算法都有类似的失败案例; 3）具体与性能之间的确切正关系因数据集而异。最后，我们建议使用具体性分数来促进未来的多式联运研究。

##### URL
[https://arxiv.org/abs/1804.06786](https://arxiv.org/abs/1804.06786)

##### PDF
[https://arxiv.org/pdf/1804.06786](https://arxiv.org/pdf/1804.06786)

