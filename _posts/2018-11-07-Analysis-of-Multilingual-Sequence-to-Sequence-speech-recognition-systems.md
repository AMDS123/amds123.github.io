---
layout: post
title: "Analysis of Multilingual Sequence-to-Sequence speech recognition systems"
date: 2018-11-07 09:59:24
categories: arXiv_CL
tags: arXiv_CL Attention Speech_Recognition Transfer_Learning Recognition
author: Martin Karafi&#xe1;t, Murali Karthick Baskar, Shinji Watanabe, Takaaki Hori, Matthew Wiesner, Jan &quot;Honza&#x27;&#x27; &#x10c;ernock&#xfd;
mathjax: true
---

* content
{:toc}

##### Abstract
This paper investigates the applications of various multilingual approaches developed in conventional hidden Markov model (HMM) systems to sequence-to-sequence (seq2seq) automatic speech recognition (ASR). On a set composed of Babel data, we first show the effectiveness of multi-lingual training with stacked bottle-neck (SBN) features. Then we explore various architectures and training strategies of multi-lingual seq2seq models based on CTC-attention networks including combinations of output layer, CTC and/or attention component re-training. We also investigate the effectiveness of language-transfer learning in a very low resource scenario when the target language is not included in the original multi-lingual training data. Interestingly, we found multilingual features superior to multilingual models, and this finding suggests that we can efficiently combine the benefits of the HMM system with the seq2seq system through these multilingual feature techniques.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.03451](http://arxiv.org/abs/1811.03451)

##### PDF
[http://arxiv.org/pdf/1811.03451](http://arxiv.org/pdf/1811.03451)

