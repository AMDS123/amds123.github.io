---
layout: post
title: "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
date: 2018-02-01 18:20:05
categories: arXiv_AI
tags: arXiv_AI Adversarial Optimization
author: Anish Athalye, Nicholas Carlini, David Wagner
mathjax: true
---

* content
{:toc}

##### Abstract
We identify obfuscated gradients as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat optimization-based attacks, we find defenses relying on this effect can be circumvented. 
 For each of the three types of obfuscated gradients we discover, we describe indicators of defenses exhibiting this effect and develop attack techniques to overcome it. In a case study, examining all defenses accepted to ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients. Using our new attack techniques, we successfully circumvent all 7 of them.

##### Abstract (translated by Google)
我们将混淆的渐变识别为一种现象，导致对抗对抗案例的防御方面的虚假安全感。虽然引起模糊渐变的防御似乎可以打败基于优化的攻击，但是我们发现依靠这种效果的防御措施可以被规避。
 对于我们发现的三种混淆梯度类型中的每一种，我们都会描述展示这种效果的防御指标并开发攻击技术来克服它。在一个案例研究中，考察了ICLR 2018接受的所有防御措施，我们发现混淆梯度是常见现象，其中8个防御中的7个依靠混淆梯度。使用我们新的攻击技术，我们成功地规避了所有7个。

##### URL
[http://arxiv.org/abs/1802.00420](http://arxiv.org/abs/1802.00420)

##### PDF
[http://arxiv.org/pdf/1802.00420](http://arxiv.org/pdf/1802.00420)

