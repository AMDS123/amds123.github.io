---
layout: post
title: "Augmenting Bottleneck Features of Deep Neural Network Employing Motor State for Speech Recognition at Humanoid Robots"
date: 2018-08-27 06:41:47
categories: arXiv_AI
tags: arXiv_AI Speech_Recognition Recognition
author: Moa Lee, Joon Hyuk Chang
mathjax: true
---

* content
{:toc}

##### Abstract
As for the humanoid robots, the internal noise, which is generated by motors, fans and mechanical components when the robot is moving or shaking its body, severely degrades the performance of the speech recognition accuracy. In this paper, a novel speech recognition system robust to ego-noise for humanoid robots is proposed, in which on/off state of the motor is employed as auxiliary information for finding the relevant input features. For this, we consider the bottleneck features, which have been successfully applied to deep neural network (DNN) based automatic speech recognition (ASR) system. When learning the bottleneck features to catch, we first exploit the motor on/off state data as supplementary information in addition to the acoustic features as the input of the first deep neural network (DNN) for preliminary acoustic modeling. Then, the second DNN for primary acoustic modeling employs both the bottleneck features tossed from the first DNN and the acoustics features. When the proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, the experimental results show that achieve obvious improvement (11% relative) is achieved by our algorithm over the conventional systems.

##### Abstract (translated by Google)
对于类人机器人，当机器人移动或摇动其身体时由电动机，风扇和机械部件产生的内部噪声严重降低了语音识别精度的性能。本文提出了一种新颖的人形机器人自我噪声语音识别系统，其中电机的开/关状态被用作寻找相关输入特征的辅助信息。为此，我们考虑了瓶颈特征，这些特征已成功应用于基于深度神经网络（DNN）的自动语音识别（ASR）系统。在学习要捕获的瓶颈特征时，我们首先利用电机开/关状态数据作为补充信息，以及声学特征作为用于初步声学建模的第一深度神经网络（DNN）的输入。然后，用于初级声学建模的第二DNN采用从第一DNN抛出的瓶颈特征和声学特征。当根据TIMIT数据库的音素错误率（PER）评估所提出的方法时，实验结果表明，我们的算法比传统系统实现了明显的改进（相对11％）。

##### URL
[http://arxiv.org/abs/1808.08702](http://arxiv.org/abs/1808.08702)

##### PDF
[http://arxiv.org/pdf/1808.08702](http://arxiv.org/pdf/1808.08702)

