---
layout: post
title: "Egocentric Video Description based on Temporally-Linked Sequences"
date: 2017-11-09 09:55:39
categories: arXiv_CV
tags: arXiv_CV Attention
author: Marc Bolaños, Álvaro Peris, Francisco Casacuberta, Sergi Soler, Petia Radeva
mathjax: true
---

* content
{:toc}

##### Abstract
Egocentric vision consists in acquiring images along the day from a first person point-of-view using wearable cameras. The automatic analysis of this information allows to discover daily patterns for improving the quality of life of the user. A natural topic that arises in egocentric vision is storytelling, that is, how to understand and tell the story relying behind the pictures. In this paper, we tackle storytelling as an egocentric sequences description problem. We propose a novel methodology that exploits information from temporally neighboring events, matching precisely the nature of egocentric sequences. Furthermore, we present a new method for multimodal data fusion consisting on a multi-input attention recurrent network. We also publish the first dataset for egocentric image sequences description, consisting of 1,339 events with 3,991 descriptions, from 55 days acquired by 11 people. Furthermore, we prove that our proposal outperforms classical attentional encoder-decoder methods for video description.

##### Abstract (translated by Google)
以自我为中心的愿景包括使用可穿戴相机从第一人称视角获取图像。这种信息的自动分析允许发现日常模式以改善用户的生活质量。以自我为中心的视觉出现的一个自然的话题是讲故事，也就是如何理解和讲述依赖于图片背后的故事。在本文中，我们把讲故事作为一个以自我为中心的序列描述问题。我们提出了一种新的方法，利用来自时间上相邻事件的信息，精确地匹配自我中心序列的性质。此外，我们提出了一个多输入关注循环网络的多模态数据融合的新方法。我们还发表了第一个以自我为中心的图像序列描述数据集，由11个人获得的55天内的1,339个事件和3,991个描述组成。此外，我们证明，我们的建议比经典注意编码器解码器方法的视频描述。

##### URL
[https://arxiv.org/abs/1704.02163](https://arxiv.org/abs/1704.02163)

##### PDF
[https://arxiv.org/pdf/1704.02163](https://arxiv.org/pdf/1704.02163)

