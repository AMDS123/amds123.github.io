---
layout: post
title: "Dual CNN Models for Unsupervised Monocular Depth Estimation"
date: 2018-05-07 11:08:56
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Vamshi Krishna Repala, Shiv Ram Dubey
mathjax: true
---

* content
{:toc}

##### Abstract
A lot of progress has been made to solve the depth estimation problem in stereo vision. Though, a very satisfactory performance is observed by utilizing the deep learning in supervised manner for depth estimation. This approach needs huge amount of ground truth training data as well as depth maps which is very laborious to prepare and many times it is not available in real scenario. Thus, the unsupervised depth estimation is the recent trend by utilizing the binocular stereo images to get rid of depth map ground truth. In unsupervised depth computation, the disparity images are generated by training the CNN with an image reconstruction loss based on the epipolar geometry constraints. The effective way of using CNN as well as investigating the better losses for the said problem needs to be addressed. In this paper, a dual CNN based model is presented for unsupervised depth estimation with 6 losses (DNM6) with individual CNN for each view to generate the corresponding disparity map. The proposed dual CNN model is also extended with 12 losses (DNM12) by utilizing the cross disparities. The presented DNM6 and DNM12 models are experimented over KITTI driving and Cityscapes urban database and compared with the recent state-of-the-art result of unsupervised depth estimation.

##### Abstract (translated by Google)
已经取得了很多进展来解决立体视觉中的深度估计问题。虽然通过利用监督深度学习的深度估计来观察到非常令人满意的表现。这种方法需要大量的地面实况训练数据以及深度图，这些图非常费力地准备，并且很多时候在实际情况下不可用。因此，无监督深度估计是利用双目立体图像摆脱深度图地面真实的最新趋势。在无监督深度计算中，通过基于极线几何约束以图像重构损失对CNN进行训练来生成视差图像。需要解决使用CNN的有效方法以及调查该问题的更好的损失。在本文中，提出了一种基于双重CNN的模型，用于无监督深度估计，每个视图具有6个损失（DNM6）和单个CNN，以生成相应的视差图。所提出的双CNN模型也通过利用交叉差异扩大了12个损失（DNM12）。所提出的DNM6和DNM12模型在KITTI驾驶和Cityscapes城市数据库上进行了试验，并与最近最先进的无监督深度估计结果进行了比较。

##### URL
[http://arxiv.org/abs/1804.06324](http://arxiv.org/abs/1804.06324)

##### PDF
[http://arxiv.org/pdf/1804.06324](http://arxiv.org/pdf/1804.06324)

