---
layout: post
title: "KGCleaner : Identifying and Correcting Errors Produced by Information Extraction Systems"
date: 2018-08-14 17:55:13
categories: arXiv_CL
tags: arXiv_CL Knowledge Classification Relation
author: Ankur Padia, Frank Ferraro, Tim Finin
mathjax: true
---

* content
{:toc}

##### Abstract
KGCleaner is a framework to \emph{identify} and \emph{correct} errors in data produced and delivered by an information extraction system. These tasks have been understudied and KGCleaner is the first to address both. We introduce a multi-task model that jointly learns to predict if an extracted relation is credible and repair it if not. We evaluate our approach and other models as instance of our framework on two collections: a Wikidata corpus of nearly 700K facts and 5M fact-relevant sentences and a collection of 30K facts from the 2015 TAC Knowledge Base Population task. For credibility classification, parameter efficient simple shallow neural network can achieve an absolute performance gain of 30 $F_1$ points on Wikidata and comparable performance on TAC. For the repair task, significant performance (at more than twice) gain can be obtained depending on the nature of the dataset and the models.

##### Abstract (translated by Google)
KGCleaner是一个框架，用于在信息提取系统生成和交付的数据中\ emph {identify}和\ emph {correct}错误。这些任务一直未得到充分考虑，KGCleaner是第一个解决这两个问题的人。我们引入了一个多任务模型，它共同学习预测提取的关系是否可信，如果没有，则进行修复。我们在两个集合中评估我们的方法和其他模型作为我们框架的实例：具有近700K事实和5M事实相关句子的维基数据语料库以及来自2015 TAC知识库人口任务的30K事实的集合。对于可信度分类，参数有效的简单浅层神经网络可以在维基数据上获得30 $ F_1 $点的绝对性能增益，并在TAC上获得相当的性能。对于修复任务，可以获得显着性能（超过两倍）的增益，具体取决于数据集和模型的性质。

##### URL
[http://arxiv.org/abs/1808.04816](http://arxiv.org/abs/1808.04816)

##### PDF
[http://arxiv.org/pdf/1808.04816](http://arxiv.org/pdf/1808.04816)

