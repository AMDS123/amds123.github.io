---
layout: post
title: "Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation"
date: 2017-07-17 12:09:08
categories: arXiv_CL
tags: arXiv_CL Attention
author: Baosong Yang, Derek F. Wong, Tong Xiao, Lidia S. Chao, Jingbo Zhu
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.

##### Abstract (translated by Google)
本文提出了一个分层注意神经网络翻译模型，其重点是通过使用基于双向树的编码器来覆盖本地和全局语义信息，从而增强源端分层表示。为了最大化目标词的预测可能性，使用注意机制的加权变体来平衡词汇和短语向量之间的注意力信息。使用基于树的罕见字编码，将所提出的模型扩展到子字级以减轻词外（OOV）问题。实证结果表明，该模型在英汉翻译任务中显着优于基于序列注意和基于树的神经翻译模型。

##### URL
[https://arxiv.org/abs/1707.05114](https://arxiv.org/abs/1707.05114)

##### PDF
[https://arxiv.org/pdf/1707.05114](https://arxiv.org/pdf/1707.05114)

