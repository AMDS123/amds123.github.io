---
layout: post
title: "Modeling the Formation of Social Conventions in Multi-Agent Populations"
date: 2018-02-16 20:22:41
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Ismael T. Freire, Clement Moulin-Frier, Marti Sanchez-Fibla, Xerxes D. Arsiwalla, Paul Verschure
mathjax: true
---

* content
{:toc}

##### Abstract
In order to understand the formation of social conventions we need to know the specific role of control and learning in multi-agent systems. To advance in this direction, we propose, within the framework of the Distributed Adaptive Control (DAC) theory, a novel Control-based Reinforcement Learning architecture (CRL) that can account for the acquisition of social conventions in multi-agent populations that are solving a benchmark social decision-making problem. Our new CRL architecture, as a concrete realization of DAC multi-agent theory, implements a low-level sensorimotor control loop handling the agent's reactive behaviors (pre-wired reflexes), along with a layer based on model-free reinforcement learning that maximizes long-term reward. We apply CRL in a multi-agent game-theoretic task in which coordination must be achieved in order to find an optimal solution. We show that our CRL architecture is able to both find optimal solutions in discrete and continuous time and reproduce human experimental data on standard game-theoretic metrics such as efficiency in acquiring rewards, fairness in reward distribution and stability of convention formation.

##### Abstract (translated by Google)
为了理解社会习俗的形成，我们需要知道控制和学习在多智能体系统中的具体作用。为了推进这个方向，我们在分布式自适应控制（DAC）理论的框架内提出了一种新型的基于控制的强化学习架构（CRL），该架构可以解释获取多智能体群体中的社会约定，一个基准的社会决策问题。作为DAC多智能体理论的具体实现，我们的新CRL架构实现了一个处理智能体反应行为（预接线反射）的低级感应电机控制回路，以及一个基于无模型强化学习的层，期限奖励。我们将CRL应用于多代理游戏理论任务中，在该任务中必须实现协调才能找到最佳解决方案。我们证明，我们的CRL架构能够在离散和连续时间中找到最优解，并且在标准博弈论度量标准（例如获取奖励的效率，奖励分配的公平性以及公约形成的稳定性）上重现人类实验数据。

##### URL
[http://arxiv.org/abs/1802.06108](http://arxiv.org/abs/1802.06108)

##### PDF
[http://arxiv.org/pdf/1802.06108](http://arxiv.org/pdf/1802.06108)

