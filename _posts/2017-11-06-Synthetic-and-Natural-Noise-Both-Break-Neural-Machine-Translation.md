---
layout: post
title: "Synthetic and Natural Noise Both Break Neural Machine Translation"
date: 2017-11-06 20:59:58
categories: arXiv_CL
tags: arXiv_CL CNN NMT
author: Yonatan Belinkov, Yonatan Bisk
---

* content
{:toc}

##### Abstract
Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.

##### Abstract (translated by Google)
基于字符的神经机器翻译（NMT）模型缓解了词汇量不足的问题，学习形态学，使我们更接近完全的端到端翻译系统。不幸的是，他们也很脆弱，容易出现噪音数据。在本文中，我们用合成的和自然的噪音源来对抗NMT模型。我们发现，最先进的模型甚至不能翻译出人类没有理解的适度嘈杂的文本。我们探索两种方法来提高模型的鲁棒性：结构不变字表示和强大的噪声文本训练。我们发现基于字符卷积神经网络的模型能够同时学习对多种噪声具有鲁棒性的表示。

##### URL
[https://arxiv.org/abs/1711.02173](https://arxiv.org/abs/1711.02173)

