---
layout: post
title: "Dual Recurrent Attention Units for Visual Question Answering"
date: 2018-02-01 09:35:33
categories: arXiv_CV
tags: arXiv_CV QA Attention Embedding Relation VQA
author: Ahmed Osman, Wojciech Samek
mathjax: true
---

* content
{:toc}

##### Abstract
We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-of-the-art ensemble model. We also experiment with replacing attention mechanisms in other state-of-the-art models with our implementation and show increased accuracy. In both cases, our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the VQA dataset.

##### Abstract (translated by Google)
我们提出了一种VQA架构，它利用循环层产生视觉和文本注意力。所提出的周期性注意单元的记忆特征提供了丰富的视觉和文本特征的联合嵌入，并使模型能够推理图像的几个部分和问题之间的关系。我们的单一模型优于VQA 1.0数据集的第一名获胜者，在当前最先进的集合模型的边缘内执行。我们还尝试用我们的实现替换其他最先进模型中的注意机制，并显示出更高的准确性。在这两种情况下，我们的周期性注意机制可以提高在VQA数据集上需要顺序或关系推理的任务的性能。

##### URL
[https://arxiv.org/abs/1802.00209](https://arxiv.org/abs/1802.00209)

##### PDF
[https://arxiv.org/pdf/1802.00209](https://arxiv.org/pdf/1802.00209)

