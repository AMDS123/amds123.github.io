---
layout: post
title: "GeThR-Net: A Generalized Temporally Hybrid Recurrent Neural Network for Multimodal Information Fusion"
date: 2016-09-17 04:18:02
categories: arXiv_CV
tags: arXiv_CV RNN Classification Prediction
author: Ankit Gandhi, Arjun Sharma, Arijit Biswas, Om Deshmukh
mathjax: true
---

* content
{:toc}

##### Abstract
Data generated from real world events are usually temporal and contain multimodal information such as audio, visual, depth, sensor etc. which are required to be intelligently combined for classification tasks. In this paper, we propose a novel generalized deep neural network architecture where temporal streams from multiple modalities are combined. There are total M+1 (M is the number of modalities) components in the proposed network. The first component is a novel temporally hybrid Recurrent Neural Network (RNN) that exploits the complimentary nature of the multimodal temporal information by allowing the network to learn both modality specific temporal dynamics as well as the dynamics in a multimodal feature space. M additional components are added to the network which extract discriminative but non-temporal cues from each modality. Finally, the predictions from all of these components are linearly combined using a set of automatically learned weights. We perform exhaustive experiments on three different datasets spanning four modalities. The proposed network is relatively 3.5%, 5.7% and 2% better than the best performing temporal multimodal baseline for UCF-101, CCV and Multimodal Gesture datasets respectively.

##### Abstract (translated by Google)
从现实世界事件中产生的数据通常是暂时的，并且包含诸如音频，视觉，深度，传感器等的多模式信息，这些信息需要智能地结合用于分类任务。在本文中，我们提出了一种新的广义深度神经网络架构，其中来自多个模态的时间流被组合。在所提出的网络中总共有M + 1个（M是模态的数量）分量。第一个组件是一个新颖的时间混合递归神经网络（RNN），通过允许网络学习模态特定的时间动态以及多模态特征空间中的动态，利用多模态时间信息的互补特性。将M个额外的组件添加到网络中，从每个模式中提取有区别但非时间的线索。最后，使用一组自动学习的权重将来自所有这些组件的预测线性组合。我们在三个不同的数据集上进行详尽的实验，包括四种模式。与UCF-101，CCV和Multimodal Gesture数据集的最佳表现时间多模态基线相比，所提出的网络分别为3.5％，5.7％和2％。

##### URL
[https://arxiv.org/abs/1609.05281](https://arxiv.org/abs/1609.05281)

##### PDF
[https://arxiv.org/pdf/1609.05281](https://arxiv.org/pdf/1609.05281)

