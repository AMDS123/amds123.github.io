---
layout: post
title: "Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generation"
date: 2018-03-20 08:44:47
categories: arXiv_CL
tags: arXiv_CL Attention Summarization Embedding
author: Shuming Ma, Xu Sun, Wei Li, Sujian Li, Wenjie Li, Xuancheng Ren
mathjax: true
---

* content
{:toc}

##### Abstract
Most recent approaches use the sequence-to-sequence model for paraphrase generation. The existing sequence-to-sequence model tends to memorize the words and the patterns in the training dataset instead of learning the meaning of the words. Therefore, the generated sentences are often grammatically correct but semantically improper. In this work, we introduce a novel model based on the encoder-decoder framework, called Word Embedding Attention Network (WEAN). Our proposed model generates the words by querying distributed word representations (i.e. neural word embeddings), hoping to capturing the meaning of the according words. Following previous work, we evaluate our model on two paraphrase-oriented tasks, namely text simplification and short text abstractive summarization. Experimental results show that our model outperforms the sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two English text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a Chinese summarization dataset. Moreover, our model achieves state-of-the-art performances on these three benchmark datasets.

##### Abstract (translated by Google)
最近的方法使用序列 - 序列模型来进行释义。现有的序列 - 序列模型倾向于记忆训练数据集中的单词和模式，而不是学习单词的含义。因此，生成的句子通常在语法上是正确的，但语义上不合适。在这项工作中，我们引入了一个基于编码器 - 解码器框架的新模型，称为Word嵌入注意网络（WEAN）。我们提出的模型通过查询分布式词语表示（即神经词嵌入）来生成单词，希望捕获相关单词的含义。继之前的工作，我们评估我们的模型在两个基于释义的任务，即文本简化和短文本抽象总结。实验结果表明，我们的模型在两个英文文本简化数据集上的BLEU得分为6.3和5.5，并且在中文摘要数据集上的ROUGE-2 F1得分为5.7，优于序列到基因的基线。此外，我们的模型在这三个基准数据集上实现了最先进的性能。

##### URL
[http://arxiv.org/abs/1803.01465](http://arxiv.org/abs/1803.01465)

##### PDF
[http://arxiv.org/pdf/1803.01465](http://arxiv.org/pdf/1803.01465)

