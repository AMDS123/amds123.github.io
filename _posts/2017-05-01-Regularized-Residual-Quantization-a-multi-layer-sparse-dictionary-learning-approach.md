---
layout: post
title: "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach"
date: 2017-05-01 13:59:04
categories: arXiv_CV
tags: arXiv_CV Regularization Super_Resolution Sparse
author: Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov
mathjax: true
---

* content
{:toc}

##### Abstract
The Residual Quantization (RQ) framework is revisited where the quantization distortion is being successively reduced in multi-layers. Inspired by the reverse-water-filling paradigm in rate-distortion theory, an efficient regularization on the variances of the codewords is introduced which allows to extend the RQ for very large numbers of layers and also for high dimensional data, without getting over-trained. The proposed Regularized Residual Quantization (RRQ) results in multi-layer dictionaries which are additionally sparse, thanks to the soft-thresholding nature of the regularization when applied to variance-decaying data which can arise from de-correlating transformations applied to correlated data. Furthermore, we also propose a general-purpose pre-processing for natural images which makes them suitable for such quantization. The RRQ framework is first tested on synthetic variance-decaying data to show its efficiency in quantization of high-dimensional data. Next, we use the RRQ in super-resolution of a database of facial images where it is shown that low-resolution facial images from the test set quantized with codebooks trained on high-resolution images from the training set show relevant high-frequency content when reconstructed with those codebooks.

##### Abstract (translated by Google)
剩余量化（RQ）框架在量化失真在多层中连续减小的情况下被重新讨论。受率失真理论中的逆水填充范例的启发，引入了码字方差的有效正则化，其允许将RQ扩展到非常大量的层以及高维数据，而不会受到过度训练。由于正则化的软阈值性质适用于由于应用于相关数据的去相关变换而产生的方差衰减数据，所提出的正则化剩余量化（RRQ）导致多层词典额外稀疏。此外，我们还提出了一个通用的自然图像预处理，使它们适合这种量化。 RRQ框架首先在合成方差衰减数据上进行测试，以显示其在高维数据量化中的效率。接下来，我们使用RRQ对面部图像的数据库进行超分辨率，其中示出了来自测试集的低分辨率面部图像与在来自训练集的高分辨率图像上训练的码本一起量化的时候显示相关的高频内容用这些码书重建。

##### URL
[https://arxiv.org/abs/1705.00522](https://arxiv.org/abs/1705.00522)

##### PDF
[https://arxiv.org/pdf/1705.00522](https://arxiv.org/pdf/1705.00522)

