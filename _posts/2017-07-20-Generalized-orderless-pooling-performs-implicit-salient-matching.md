---
layout: post
title: "Generalized orderless pooling performs implicit salient matching"
date: 2017-07-20 13:59:36
categories: arXiv_CV
tags: arXiv_CV Salient Prediction Recognition
author: Marcel Simon, Yang Gao, Trevor Darrell, Joachim Denzler, Erik Rodner
mathjax: true
---

* content
{:toc}

##### Abstract
Most recent CNN architectures use average pooling as a final feature encoding step. In the field of fine-grained recognition, however, recent global representations like bilinear pooling offer improved performance. In this paper, we generalize average and bilinear pooling to "alpha-pooling", allowing for learning the pooling strategy during training. In addition, we present a novel way to visualize decisions made by these approaches. We identify parts of training images having the highest influence on the prediction of a given test image. It allows for justifying decisions to users and also for analyzing the influence of semantic parts. For example, we can show that the higher capacity VGG16 model focuses much more on the bird's head than, e.g., the lower-capacity VGG-M model when recognizing fine-grained bird categories. Both contributions allow us to analyze the difference when moving between average and bilinear pooling. In addition, experiments show that our generalized approach can outperform both across a variety of standard datasets.

##### Abstract (translated by Google)
最近的CNN架构使用平均池作为最终的特征编码步骤。然而，在细粒度识别领域，最近的全球代表如双线性汇集提供了改进的性能。在本文中，我们将平均和双线性汇总概括为“alpha-pooling”，允许在训练期间学习汇总策略。此外，我们提出了一种新颖的方式来可视化这些方法做出的决定。我们识别对给定测试图像的预测具有最高影响的训练图像的部分。它允许为用户辩护，也可以分析语义部分的影响。例如，我们可以证明，当识别细粒度的鸟类时，较高容量的VGG16模型更多地关注鸟的头部，而不是更低容量的VGG-M模型。这两个贡献使我们能够分析在平均和双线性汇总之间移动的差异。另外，实验表明，我们的广义方法可以在各种标准数据集上都胜过两者。

##### URL
[https://arxiv.org/abs/1705.00487](https://arxiv.org/abs/1705.00487)

##### PDF
[https://arxiv.org/pdf/1705.00487](https://arxiv.org/pdf/1705.00487)

