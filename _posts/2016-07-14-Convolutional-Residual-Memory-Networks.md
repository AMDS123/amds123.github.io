---
layout: post
title: "Convolutional Residual Memory Networks"
date: 2016-07-14 18:40:24
categories: arXiv_CV
tags: arXiv_CV Knowledge Face CNN Memory_Networks Recognition
author: Joel Moniz, Christopher Pal
mathjax: true
---

* content
{:toc}

##### Abstract
Very deep convolutional neural networks (CNNs) yield state of the art results on a wide variety of visual recognition problems. A number of state of the the art methods for image recognition are based on networks with well over 100 layers and the performance vs. depth trend is moving towards networks in excess of 1000 layers. In such extremely deep architectures the vanishing or exploding gradient problem becomes a key issue. Recent evidence also indicates that convolutional networks could benefit from an interface to explicitly constructed memory mechanisms interacting with a CNN feature processing hierarchy. Correspondingly, we propose and evaluate a memory mechanism enhanced convolutional neural network architecture based on augmenting convolutional residual networks with a long short term memory mechanism. We refer to this as a convolutional residual memory network. To the best of our knowledge this approach can yield state of the art performance on the CIFAR-100 benchmark and compares well with other state of the art techniques on the CIFAR-10 and SVHN benchmarks. This is achieved using networks with more breadth, much less depth and much less overall computation relative to comparable deep ResNets without the memory mechanism. Our experiments and analysis explore the importance of the memory mechanism, network depth, breadth, and predictive performance.

##### Abstract (translated by Google)
非常深的卷积神经网络（CNNs）在各种各样的视觉识别问题上产生了最先进的结果。许多用于图像识别的最先进的方法是基于具有超过100层的网络，并且性能与深度趋势正在向超过1000层的网络发展。在这样极其深厚的体系结构中，消失或爆炸的梯度问题成为关键问题。最近的证据还表明，卷积网络可以从与CNN特征处理层次结构交互的明确构建的存储器机制的接口中受益。相应地，我们提出并评估了一种基于增强卷积残差网络的记忆机制增强型卷积神经网络结构，具有较长的短期记忆机制。我们称之为卷积残余记忆网络。就我们所知，这种方法可以在CIFAR-100基准测试中获得最先进的性能，并与CIFAR-10和SVHN基准测试中的其他先进技术进行比较。这是通过使用更宽广的网络来实现的，相对于没有内存机制的可比较的深度ResNets，深度更少，整体计算更少。我们的实验和分析探索了记忆机制，网络深度，广度和预测性能的重要性。

##### URL
[https://arxiv.org/abs/1606.05262](https://arxiv.org/abs/1606.05262)

##### PDF
[https://arxiv.org/pdf/1606.05262](https://arxiv.org/pdf/1606.05262)

