---
layout: post
title: "Global hard thresholding algorithms for joint sparse image representation and denoising"
date: 2017-05-27 12:40:24
categories: arXiv_CV
tags: arXiv_CV Image_Caption Sparse
author: Reza Borhani, Jeremy Watt, Aggelos Katsaggelos
mathjax: true
---

* content
{:toc}

##### Abstract
Sparse coding of images is traditionally done by cutting them into small patches and representing each patch individually over some dictionary given a pre-determined number of nonzero coefficients to use for each patch. In lack of a way to effectively distribute a total number (or global budget) of nonzero coefficients across all patches, current sparse recovery algorithms distribute the global budget equally across all patches despite the wide range of differences in structural complexity among them. In this work we propose a new framework for joint sparse representation and recovery of all image patches simultaneously. We also present two novel global hard thresholding algorithms, based on the notion of variable splitting, for solving the joint sparse model. Experimentation using both synthetic and real data shows effectiveness of the proposed framework for sparse image representation and denoising tasks. Additionally, time complexity analysis of the proposed algorithms indicate high scalability of both algorithms, making them favorable to use on large megapixel images.

##### Abstract (translated by Google)
图像的稀疏编码传统上通过将其切割成小块并且在给定预定数量的非零系数以用于每个块的情况下分别在一些字典上表示每个块来完成。由于缺乏有效地分配所有补丁中非零系数的总数（或全局预算）的方法，所以当前的稀疏恢复算法在全部补丁中均等地分配全球预算，尽管它们之间的结构复杂度差异很大。在这项工作中，我们提出了一个新的框架，联合稀疏表示和恢复的所有图像补丁同时。我们还提出了两种新的基于可变分裂概念的全局硬阈值算法，用于求解联合稀疏模型​​。使用合成和实际数据的实验显示了所提出的用于稀疏图像表示和去噪任务的框架的有效性。另外，所提出的算法的时间复杂度分析表明两种算法的高度可扩展性，使得它们有利于在大的百万像素图像上使用。

##### URL
[https://arxiv.org/abs/1705.09816](https://arxiv.org/abs/1705.09816)

##### PDF
[https://arxiv.org/pdf/1705.09816](https://arxiv.org/pdf/1705.09816)

