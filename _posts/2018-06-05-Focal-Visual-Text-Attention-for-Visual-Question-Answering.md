---
layout: post
title: "Focal Visual-Text Attention for Visual Question Answering"
date: 2018-06-05 18:08:29
categories: arXiv_CV
tags: arXiv_CV QA Attention VQA
author: Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, Alexander Hauptmann
mathjax: true
---

* content
{:toc}

##### Abstract
Recent insights on language and vision with neural networks have been successfully applied to simple single-image visual question answering. However, to tackle real-life question answering problems on multimedia collections such as personal photos, we have to look at whole collections with sequences of photos or videos. When answering questions from a large collection, a natural problem is to identify snippets to support the answer. In this paper, we describe a novel neural network called Focal Visual-Text Attention network (FVTA) for collective reasoning in visual question answering, where both visual and text sequence information such as images and text metadata are presented. FVTA introduces an end-to-end approach that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question. FVTA can not only answer the questions well but also provides the justifications which the system results are based upon to get the answers. FVTA achieves state-of-the-art performance on the MemexQA dataset and competitive results on the MovieQA dataset.

##### Abstract (translated by Google)
最近对语言和视觉的神经网络见解已成功应用于简单的单图像视觉问答。但是，要解决关于多媒体收藏（如个人照片）的现实问题回答问题，我们必须查看整个系列的照片或视频序列。在回答大量收集的问题时，一个自然的问题是确定片段以支持答案。在本文中，我们描述了一种称为焦点视觉文本注意网络（FVTA）的新型神经网络，用于视觉问题回答中的集体推理，其中呈现视觉和文本序列信息，例如图像和文本元数据。 FVTA引入了一种端到端方法，利用分层流程动态确定在连续数据中重点关注什么媒体和什么时间来回答问题。 FVTA不仅可以很好地回答问题，而且可以提供系统结果所基于的理由来获得答案。 FVTA在MemexQA数据集上实现了最先进的性能，并在MovieQA数据集上实现了竞争结果。

##### URL
[http://arxiv.org/abs/1806.01873](http://arxiv.org/abs/1806.01873)

##### PDF
[http://arxiv.org/pdf/1806.01873](http://arxiv.org/pdf/1806.01873)

