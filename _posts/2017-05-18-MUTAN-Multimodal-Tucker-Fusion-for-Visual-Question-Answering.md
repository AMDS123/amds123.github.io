---
layout: post
title: "MUTAN: Multimodal Tucker Fusion for Visual Question Answering"
date: 2017-05-18 16:23:22
categories: arXiv_CV
tags: arXiv_CV VQA
author: Hedi Ben-younes, Rémi Cadene, Matthieu Cord, Nicolas Thome
mathjax: true
---

* content
{:toc}

##### Abstract
Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how our MUTAN model generalizes some of the latest VQA architectures, providing state-of-the-art results.

##### Abstract (translated by Google)
双线性模型提供了一个吸引人的框架来混合和合并Visual Question Answering（VQA）任务中的信息。他们帮助学习图像中的问题意义和视觉概念之间的高级关联，但是他们遭受了巨大的维度问题。我们引入MUTAN，基于多模张量的Tucker分解来有效地参数化视觉和文本表示之间的双线性相互作用。除了Tucker框架之外，我们设计一个低秩矩阵分解来明确约束交互秩。使用MUTAN，我们控制合并方案的复杂性，同时保持可理解的融合关系。我们展示了我们的MUTAN模型如何推广一些最新的VQA体系结构，提供了最新的结果。

##### URL
[https://arxiv.org/abs/1705.06676](https://arxiv.org/abs/1705.06676)

