---
layout: post
title: "MUTAN: Multimodal Tucker Fusion for Visual Question Answering"
date: 2017-05-18 16:23:22
categories: arXiv_CV
tags: arXiv_CV QA Relation VQA
author: Hedi Ben-younes, Rémi Cadene, Matthieu Cord, Nicolas Thome
mathjax: true
---

* content
{:toc}

##### Abstract
Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how our MUTAN model generalizes some of the latest VQA architectures, providing state-of-the-art results.

##### Abstract (translated by Google)
双线性模型为在视觉问答（VQA）任务中混合和合并信息提供了一个吸引人的框架。它们有助于学习图像中问题意义和视觉概念之间的高级关联，但它们存在巨大的维度问题。我们引入MUTAN，一种基于多峰张量的Tucker分解，有效地参数化视觉和文本表示之间的双线性相互作用。除了Tucker框架之外，我们还设计了一种基于矩阵的低阶分解来明确约束交互等级。使用MUTAN，我们可以控制合并方案的复杂性，同时保持良好的可解释融合关系。我们展示了我们的MUTAN模型如何概括一些最新的VQA架构，提供最先进的结果。

##### URL
[https://arxiv.org/abs/1705.06676](https://arxiv.org/abs/1705.06676)

##### PDF
[https://arxiv.org/pdf/1705.06676](https://arxiv.org/pdf/1705.06676)

