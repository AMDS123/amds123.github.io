---
layout: post
title: "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
date: 2018-05-30 17:55:21
categories: arXiv_RO
tags: arXiv_RO Reinforcement_Learning Optimization
author: Kurtland Chua, Roberto Calandra, Rowan McAllister, Sergey Levine
mathjax: true
---

* content
{:toc}

##### Abstract
Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance, especially those with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g. 25 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).

##### Abstract (translated by Google)
基于模型的强化学习（RL）算法可以实现出色的采样效率，但在渐近性能方面往往落后于最佳无模型算法，特别是那些具有高容量参数函数逼近器（如深度网络）的算法。在本文中，我们研究如何弥合这种差距，通过采用不确定感知动力学模型。我们提出了一种新的算法，称为带有轨迹采样（PETS）的概率集合，它将不确定感知的深层网络动力学模型与基于采样的不确定性传播相结合。我们与最先进的基于模型和无模型深RL算法的比较表明，我们的方法与几种具有挑战性的基准任务的无模型算法的渐近性能相匹配，同时需要的样本数量明显较少（例如25和125倍在半猎豹任务上分别比软演员评论和近端策略优化更少的样本）。

##### URL
[http://arxiv.org/abs/1805.12114](http://arxiv.org/abs/1805.12114)

##### PDF
[http://arxiv.org/pdf/1805.12114](http://arxiv.org/pdf/1805.12114)

