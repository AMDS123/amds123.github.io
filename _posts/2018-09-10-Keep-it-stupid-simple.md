---
layout: post
title: "Keep it stupid simple"
date: 2018-09-10 15:43:57
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Erik J Peterson, Necati Alp M&#xfc;yesser, Timothy Verstynen, Kyle Dunovan
mathjax: true
---

* content
{:toc}

##### Abstract
Deep reinforcement learning can match and exceed human performance, but if even minor changes are introduced to the environment artificial networks often can't adapt. Humans meanwhile are quite adaptable. We hypothesize that this is partly because of how humans use heuristics, and partly because humans can imagine new and more challenging environments to learn from. We've developed a model of hierarchical reinforcement learning that combines both these elements into a stumbler-strategist network. We test transfer performance of this network using Wythoff's game, a gridworld environment with a known optimal strategy. We show that combining imagined play with a heuristic--labeling each position as "good" or "bad"'--both accelerates learning and promotes transfer to novel games, while also improving model interpretability.

##### Abstract (translated by Google)
深层强化学习可以匹配并超越人类的表现，但如果对环境进行微小的改变，人工网络往往无法适应。同时人类具有很强的适应能力。我们假设这部分是因为人类如何使用启发式方法，部分是因为人类可以想象出新的和更具挑战性的环境可供学习。我们开发了一种分层强化学习模型，将这些元素组合成一个笨拙的战略家网络。我们使用Wythoff的游戏测试该网络的传输性能，这是一个具有已知最优策略的网格世界环境。我们展示了将想象游戏与启发式相结合 - 将每个位置标记为“好”或“坏” - 既加速了学习又促进了向新颖游戏的转移，同时也提高了模型的可解释性。

##### URL
[http://arxiv.org/abs/1809.03406](http://arxiv.org/abs/1809.03406)

##### PDF
[http://arxiv.org/pdf/1809.03406](http://arxiv.org/pdf/1809.03406)

