---
layout: post
title: "Interpretable Latent Spaces for Learning from Demonstration"
date: 2018-07-17 17:56:09
categories: arXiv_CV
tags: arXiv_CV
author: Yordan Hristov, Alex Lascarides, Subramanian Ramamoorthy
mathjax: true
---

* content
{:toc}

##### Abstract
Effective human-robot interaction, such as in robot learning from human demonstration, requires the learning agent to be able to ground abstract concepts (such as those contained within instructions) in a corresponding high-dimensional sensory input stream from the world. Models such as deep neural networks, with high capacity through their large parameter spaces, can be used to compress the high-dimensional sensory data to lower dimensional representations. These low-dimensional representations facilitate symbol grounding, but may not guarantee that the representation would be human-interpretable. We propose a method which utilises the grouping of user-defined symbols and their corresponding sensory observations in order to align the learnt compressed latent representation with the semantic notions contained in the abstract labels. We demonstrate this through experiments with both simulated and real-world object data, showing that such alignment can be achieved in a process of physical symbol grounding.

##### Abstract (translated by Google)
有效的人机交互，例如在人类演示的机器人学习中，要求学习代理能够将来自世界的相应高维感觉输入流中的抽象概念（例如指令中包含的概念）接地。诸如深度神经网络之类的模型，通过其大参数空间具有高容量，可用于将高维感觉数据压缩到较低维度表示。这些低维表示有助于符号接地，但可能无法保证表示是人类可解释的。我们提出了一种方法，该方法利用用户定义的符号及其相应的感官观察的分组，以便将学习的压缩潜在表示与抽象标签中包含的语义概念对齐。我们通过对模拟和现实世界对象数据的实验来证明这一点，表明这种对齐可以在物理符号接地的过程中实现。

##### URL
[http://arxiv.org/abs/1807.06583](http://arxiv.org/abs/1807.06583)

##### PDF
[http://arxiv.org/pdf/1807.06583](http://arxiv.org/pdf/1807.06583)

