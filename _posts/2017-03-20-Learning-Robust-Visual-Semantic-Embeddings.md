---
layout: post
title: "Learning Robust Visual-Semantic Embeddings"
date: 2017-03-20 00:28:07
categories: arXiv_SD
tags: arXiv_SD Embedding Represenation_Learning Inference Recognition
author: Yao-Hung Hubert Tsai, Liang-Kang Huang, Ruslan Salakhutdinov
mathjax: true
---

* content
{:toc}

##### Abstract
Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.

##### Abstract (translated by Google)
许多现有的用于学习图像和文本的联合嵌入的方法仅使用来自配对图像及其文本属性的监督信息。利用最近在深度神经网络中无监督学习的成功，我们提出了一个端到端的学习框架，能够跨领域提取更强大的多模态表示。所提出的方法将表示学习模型（即，自动编码器）与跨域学习标准（即，最大平均差异损失）组合在一起以学习用于语义和视觉特征的联合嵌入。引入无监督数据自适应推理的新技术，为标记和未标记数据构建更全面的嵌入。我们评估我们的动物属性和加州理工学院 -  UCSD鸟类200  -  2011数据集的方法，包括零和少量拍摄的图像识别和检索，范围广泛的应用从感应到直流设置。实证上，我们展示了我们的框架在许多所考虑的任务上改进了现有技术的状态。

##### URL
[https://arxiv.org/abs/1703.05908](https://arxiv.org/abs/1703.05908)

##### PDF
[https://arxiv.org/pdf/1703.05908](https://arxiv.org/pdf/1703.05908)

