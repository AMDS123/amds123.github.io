---
layout: post
title: "Make Every Neural Network Better: Generating Neural Network Ensembles by Weight Parameter Resampling"
date: 2018-07-02 18:12:32
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Jiayi Liu, Samarth Tripathi, Unmesh Kurup, Mohak Shah
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks (DNNs) have become increasingly popular in computer vision, natural language processing, and other areas. However, training and fine-tuning a deep learning model is computationally intensive and time-consuming. We propose a new method to improve the performance of nearly every model including pre-trained models. The proposed method uses an ensemble approach where the networks in the ensemble are constructed by reassigning model parameter values based on the probabilistic distribution of these parameters, calculated towards the end of the training process. For pre-trained models, this approach results in an additional training step (usually less than one epoch). We perform a variety of analysis using the MNIST dataset and validate the approach with a number of DNN models using pre-trained models on the ImageNet dataset.

##### Abstract (translated by Google)
深度神经网络（DNN）在计算机视觉，自然语言处理和其他领域越来越受欢迎。然而，深度学习模型的训练和微调是计算密集且耗时的。我们提出了一种新方法来改善几乎所有模型的性能，包括预训练模型。所提出的方法使用集合方法，其中通过基于在训练过程结束时计算的这些参数的概率分布重新分配模型参数值来构建集合中的网络。对于预先训练的模型，这种方法导致额外的训练步骤（通常少于一个纪元）。我们使用MNIST数据集执行各种分析，并使用ImageNet数据集上预先训练的模型使用多个DNN模型验证方法。

##### URL
[https://arxiv.org/abs/1807.00847](https://arxiv.org/abs/1807.00847)

##### PDF
[https://arxiv.org/pdf/1807.00847](https://arxiv.org/pdf/1807.00847)

