---
layout: post
title: "On the Effectiveness of Least Squares Generative Adversarial Networks"
date: 2017-12-18 13:36:09
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN
author: Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley
mathjax: true
---

* content
{:toc}

##### Abstract
Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. We also present a theoretical analysis about the properties of LSGANs and $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. For evaluating the image quality, we train LSGANs on several datasets including LSUN and a cat dataset, and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a new proposed method --- datasets with small variance, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs with gradient penalty succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.

##### Abstract (translated by Google)
无监督学习与生成对抗网络（GAN）已被证明是非常成功的。定期的GANs将鉴别器假设为具有S形交叉熵损失函数的分类器。然而，我们发现这种损失函数可能会导致在学习过程中消失的梯度问题。为了克服这个问题，本文提出了采用最小二乘损失函数的最小二乘生成对抗网络（LSGANs）。我们证明最小化LSGAN的目标函数可以最小化Pearson $ \ chi ^ 2 $ divergence。我们还对LSGANs和$ \ chi ^ 2 $ divergence的性质进行了理论分析。 LSGAN比常规GAN有两个好处。首先，LSGAN能够产生比普通GAN更高质量的图像。其次，LSGAN在学习过程中表现得更加稳定。为了评估图像质量，我们对包括LSUN和cat数据集在内的多个数据集进行LSGAN训练，实验结果表明，LSGAN生成的图像比常规GAN生成的图像质量更好。此外，我们评估两组LSGANs的稳定性。一个是比较LSGANs和常规GANs没有梯度惩罚。我们进行了三个实验，包括高斯混合分布，困难的体系结构和一个新的方法 - 小方差数据集，以说明LSGANs的稳定性。另一个是比较具有梯度惩罚的LSGAN和具有梯度惩罚的WGAN（WGANs-GP）。实验结果表明，具有梯度惩罚的LSGAN成功地训练了WGANs-GP中使用的所有困难的体系结构，包括101层ResNet。

##### URL
[http://arxiv.org/abs/1712.06391](http://arxiv.org/abs/1712.06391)

##### PDF
[http://arxiv.org/pdf/1712.06391](http://arxiv.org/pdf/1712.06391)

