---
layout: post
title: "Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger"
date: 2017-12-23 23:57:55
categories: arXiv_AI
tags: arXiv_AI Adversarial Reinforcement_Learning
author: Vahid Behzadan, Arslan Munir
mathjax: true
---

* content
{:toc}

##### Abstract
Recent developments have established the vulnerability of deep Reinforcement Learning (RL) to policy manipulation attacks via adversarial perturbations. In this paper, we investigate the robustness and resilience of deep RL to training-time and test-time attacks. Through experimental results, we demonstrate that under noncontiguous training-time attacks, Deep Q-Network (DQN) agents can recover and adapt to the adversarial conditions by reactively adjusting the policy. Our results also show that policies learned under adversarial perturbations are more robust to test-time attacks. Furthermore, we compare the performance of $\epsilon$-greedy and parameter-space noise exploration methods in terms of robustness and resilience against adversarial perturbations.

##### Abstract (translated by Google)
最近的发展已经确立了深度强化学习（RL）通过对抗性扰动的政策操纵攻击的脆弱性。在本文中，我们研究深度RL对训练时间和测试时间攻击的稳健性和弹性。通过实验结果，我们证明在非连续训练时间攻击下，深度Q网络（DQN）代理可以通过反应性地调整策略来恢复和适应对抗条件。我们的研究结果还表明，在对抗性扰动下学习的策略对于测试时攻击更为稳健。此外，我们比较了$ \ epsilon $ -greedy和参数空间噪声探测方法在抗对抗性扰动方面的鲁棒性和弹性的性能。

##### URL
[http://arxiv.org/abs/1712.09344](http://arxiv.org/abs/1712.09344)

##### PDF
[http://arxiv.org/pdf/1712.09344](http://arxiv.org/pdf/1712.09344)

