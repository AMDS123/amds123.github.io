---
layout: post
title: "How Well Can Generative Adversarial Networks Learn Densities: A Nonparametric View"
date: 2017-12-21 23:13:27
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Quantitative
author: Tengyuan Liang
mathjax: true
---

* content
{:toc}

##### Abstract
We study in this paper the rate of convergence for learning densities under the Generative Adversarial Networks (GANs) framework, borrowing insights from nonparametric statistics. We introduce an improved GAN estimator that achieves a faster rate, through leveraging the level of smoothness in the target density and the evaluation metric, which in theory remedies the mode collapse problem reported in the literature. A minimax lower bound is constructed to show that when the dimension is large, the exponent in the rate for the new GAN estimator is near optimal. One can view our results as answering in a quantitative way how well GAN learns a wide range of densities with different smoothness properties, under a hierarchy of evaluation metrics. As a byproduct, we also obtain improved bounds for GAN with deeper ReLU discriminator network.

##### Abstract (translated by Google)
在本文中，我们研究了生成对手网络（GANs）框架下学习密度的收敛速度，借鉴了非参数统计的见解。我们引入了一个改进的GAN估计器，通过利用目标密度和评估度量的平滑度水平来实现更快的速率，这在理论上弥补了文献报道的模式崩溃问题。构造一个极大极小下界，以证明当维数较大时，新GAN估计量的指数接近最优。我们可以把我们的结果看作是定量的回答，GAN是如何在评估指标的层次下学习不同光滑性质的各种密度的。作为副产品，我们还通过更深的ReLU鉴别器网络获得改进的GAN边界。

##### URL
[https://arxiv.org/abs/1712.08244](https://arxiv.org/abs/1712.08244)

##### PDF
[https://arxiv.org/pdf/1712.08244](https://arxiv.org/pdf/1712.08244)

