---
layout: post
title: "A Neural Network Approach for Mixing Language Models"
date: 2017-08-23 13:27:16
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Youssef Oualil, Dietrich Klakow
mathjax: true
---

* content
{:toc}

##### Abstract
The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.

##### Abstract (translated by Google)
基于神经网络（NN）的语言模型由于能够学习不同的自然语言特征的新体系结构的出现，正在稳步提高。本文提出了一个新的框架，这表明，可以通过在单个体系结构中结合不同的现有异构模型来实现显着的改进。这是通过1）一个特征层，它分别学习不同的基于神经网络的模型和2）一个混合层，合并所产生的模型特征。在这样做的时候，这个架构受益于每个模型的学习能力，模型参数的数量或训练时间没有显着的增加。在Penn Treebank（PTB）和大文本压缩基准（LTCB）语料库上进行的大量实验显示，与最先进的前馈以及递归的神经网络架构相比，困惑性显着减少。

##### URL
[https://arxiv.org/abs/1708.06989](https://arxiv.org/abs/1708.06989)

##### PDF
[https://arxiv.org/pdf/1708.06989](https://arxiv.org/pdf/1708.06989)

