---
layout: post
title: "NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing"
date: 2018-05-14 18:04:28
categories: arXiv_CL
tags: arXiv_CL Inference
author: Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa, Wenlin Wang, Guoyin Wang, Lawrence Carin, Ricardo Henao
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly back-propagated through the discrete latent variable to optimize the hash function. We also draw connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios.

##### Abstract (translated by Google)
在许多信息检索系统中，语义散列已经成为快速相似搜索的强大范例。虽然相当成功，但以前的技术通常需要两阶段训练，而二进制约束则是临时处理的。在本文中，我们提出了一种用于语义散列的端到端神经架构（NASH），其中二进制散列码被视为伯努利潜变量。提出了一种神经变分推理框架用于训练，其中梯度通过离散潜变量直接反向传播以优化散列函数。我们还提出了所提出的方法和率失真理论之间的联系，这为所提出的框架的有效性提供了理论基础。在三个公共数据集上的实验结果表明，我们的方法在无监督和监督情况下均显着优于几个最先进的模型。

##### URL
[http://arxiv.org/abs/1805.05361](http://arxiv.org/abs/1805.05361)

##### PDF
[http://arxiv.org/pdf/1805.05361](http://arxiv.org/pdf/1805.05361)

