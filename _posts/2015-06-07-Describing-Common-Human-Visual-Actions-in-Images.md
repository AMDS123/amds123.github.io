---
layout: post
title: "Describing Common Human Visual Actions in Images"
date: 2015-06-07 00:33:23
categories: arXiv_CV
tags: arXiv_CV
author: Matteo Ruggero Ronchi, Pietro Perona
mathjax: true
---

* content
{:toc}

##### Abstract
Which common human actions and interactions are recognizable in monocular still images? Which involve objects and/or other people? How many is a person performing at a time? We address these questions by exploring the actions and interactions that are detectable in the images of the MS COCO dataset. We make two main contributions. First, a list of 140 common `visual actions', obtained by analyzing the largest on-line verb lexicon currently available for English (VerbNet) and human sentences used to describe images in MS COCO. Second, a complete set of annotations for those `visual actions', composed of subject-object and associated verb, which we call COCO-a (a for `actions'). COCO-a is larger than existing action datasets in terms of number of actions and instances of these actions, and is unique because it is data-driven, rather than experimenter-biased. Other unique features are that it is exhaustive, and that all subjects and objects are localized. A statistical analysis of the accuracy of our annotations and of each action, interaction and subject-object combination is provided.

##### Abstract (translated by Google)
单眼静止图像中哪些常见的人类行为和相互作用是可识别的？哪些涉及对象和/或其他人？一次有多少人表演？我们通过探索在MS COCO数据集的图像中可检测到的动作和相互作用来解决这些问题。我们做了两个主要的贡献。首先，通过分析当前可用于英语的最大在线动词词典（VerbNet）和用于在MS COCO中描述图像的人类句子而获得的140个常见“视觉动作”的列表。其次，一套完整的“视觉行动”注释，由主体对象和相关动词组成，我们称之为“行动”（COCO-a）。 COCO-a比现有的动作数据集在动作数量和这些动作的实例方面要大，而且是独特的，因为它是数据驱动的，而不是实验者偏好的。其他独特的功能是它是详尽的，所有的主题和对象都是本地化的。提供了我们的注释和每个动作，交互和主客体组合的准确性的统计分析。

##### URL
[https://arxiv.org/abs/1506.02203](https://arxiv.org/abs/1506.02203)

##### PDF
[https://arxiv.org/pdf/1506.02203](https://arxiv.org/pdf/1506.02203)

