---
layout: post
title: "Learning Temporal Transformations From Time-Lapse Videos"
date: 2016-08-27 16:33:48
categories: arXiv_CV
tags: arXiv_CV Prediction Quantitative
author: Yipin Zhou, Tamara L. Berg
mathjax: true
---

* content
{:toc}

##### Abstract
Based on life-long observations of physical, chemical, and biologic phenomena in the natural world, humans can often easily picture in their minds what an object will look like in the future. But, what about computers? In this paper, we learn computational models of object transformations from time-lapse videos. In particular, we explore the use of generative models to create depictions of objects at future times. These models explore several different prediction tasks: generating a future state given a single depiction of an object, generating a future state given two depictions of an object at different times, and generating future states recursively in a recurrent framework. We provide both qualitative and quantitative evaluations of the generated results, and also conduct a human evaluation to compare variations of our models.

##### Abstract (translated by Google)
基于对自然界中的物理，化学和生物现象的终生观察，人类通常可以很容易地在脑海中描绘出未来物体的样子。但是，电脑呢？在本文中，我们学习了时间推移视频对象转换的计算模型。特别是，我们探索使用生成模型在未来创建对象的描述。这些模型探索了几个不同的预测任务：给定一个对象的单一描述，产生一个未来的状态，在不同的时间给予两个对象的描述，产生一个未来的状态，并在一个经常性的框架中递归地产生未来的状态。我们提供定性和定量的评估结果，并进行人体评估，以比较我们的模型的变化。

##### URL
[https://arxiv.org/abs/1608.07724](https://arxiv.org/abs/1608.07724)

##### PDF
[https://arxiv.org/pdf/1608.07724](https://arxiv.org/pdf/1608.07724)

