---
layout: post
title: "The Loss Surface of Residual Networks: Ensembles and the Role of Batch Normalization"
date: 2016-11-08 14:17:13
categories: arXiv_CV
tags: arXiv_CV Face Optimization
author: Etai Littwin, Lior Wolf
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensembles are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network's depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.

##### Abstract (translated by Google)
与传统相同深度的网络相比，深度残留网络在性能上表现出优越性，并且可以在极端深度进行训练。最近已经表明，剩余网络就像相对浅的网络一样。我们证明这些集合是动态的：尽管最初虚拟集合主要是深度低于网络深度的一半，随着训练的进行，它变得越来越深。控制动态集合行为的主要机制是例如通过批量标准化技术引入的缩放。我们解释这种行为，并展示其背后的驱动力。作为我们分析的主要工具，我们使用广义的自旋玻璃模型，我们也用它来研究残留网络优化的关键点数。

##### URL
[https://arxiv.org/abs/1611.02525](https://arxiv.org/abs/1611.02525)

##### PDF
[https://arxiv.org/pdf/1611.02525](https://arxiv.org/pdf/1611.02525)

