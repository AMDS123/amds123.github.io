---
layout: post
title: "Deep Neural Networks Under Stress"
date: 2016-05-23 08:34:50
categories: arXiv_CV
tags: arXiv_CV Image_Caption CNN Transfer_Learning
author: Micael Carvalho, Matthieu Cord, Sandra Avila, Nicolas Thome, Eduardo Valle
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.

##### Abstract (translated by Google)
近年来，在许多数据集中，深度架构已被用于具有最新性能的转移学习。然而，在转移视角下，其特征的属性仍然很大程度上未被研究。在这项工作中，我们对从深度模型中提取的特征向量的弹性进行了广泛的分析，特别关注性能和压缩率之间的折衷。通过对从深度卷积神经网络提取的图像描述引入扰动，我们改变它们的精度和维数，测量它如何影响最终得分。我们发现，与传统方法相比，深度特征对这些干扰更为强大，压缩率达到98.4％，而Pascal VOC 2007仅损失0.88％的原始分数。

##### URL
[https://arxiv.org/abs/1605.03498](https://arxiv.org/abs/1605.03498)

##### PDF
[https://arxiv.org/pdf/1605.03498](https://arxiv.org/pdf/1605.03498)

