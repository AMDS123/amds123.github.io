---
layout: post
title: "Rational Recurrences"
date: 2018-08-28 15:28:25
categories: arXiv_CL
tags: arXiv_CL Text_Classification CNN RNN Classification Language_Model
author: Hao Peng, Roy Schwartz, Sam Thomson, Noah A. Smith
mathjax: true
---

* content
{:toc}

##### Abstract
Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.

##### Abstract (translated by Google)
尽管神经模型在自然语言处理方面取得了巨大的经验成功，但其中许多缺乏伴随经典机器学习方法的强大直觉。最近，已经在卷积神经网络（CNN）和加权有限状态自动机（WFSA）之间建立了连接，从而产生了新的解释和见解。在这项工作中，我们表明一些递归神经网络也与WFSA共享这种联系。我们正式地描述了这种连接，将理性重现定义为循环隐藏状态更新函数，这些函数可以写成有限的WFSA集的正向计算。我们证明了几个最近的神经模型使用了理性复发。我们的分析提供了这些模型的全新视图，并有助于设计从WFSA中汲取灵感的新神经架构。我们提出了一个这样的模型，它比语言建模和文本分类的两个最近基线表现更好。我们的研究结果表明，从像WFSA这样的经典模型转移直觉可以成为设计和理解神经模型的有效方法。

##### URL
[http://arxiv.org/abs/1808.09357](http://arxiv.org/abs/1808.09357)

##### PDF
[http://arxiv.org/pdf/1808.09357](http://arxiv.org/pdf/1808.09357)

