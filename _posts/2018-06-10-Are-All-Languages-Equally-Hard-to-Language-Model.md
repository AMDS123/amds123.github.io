---
layout: post
title: "Are All Languages Equally Hard to Language-Model?"
date: 2018-06-10 23:24:33
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Ryan Cotterell, Sebastian J. Mielke, Jason Eisner, Brian Roark
mathjax: true
---

* content
{:toc}

##### Abstract
For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both $n$-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.

##### Abstract (translated by Google)
对于适用于多种语言的一般建模方法，一个自然的问题是：我们应该如何期望我们的模型能够处理具有不同类型概况的语言？在这项工作中，我们开发了一个公平的跨语言比较语言模型的评估框架，使用翻译文本，以便所有模型都被要求预测大致相同的信息。然后，我们对21种语言进行研究，证明在某些语言中，使用$ n $ -gram和LSTM语言模型都难以预测信息的文本表达。我们显示复杂的折射形态是语言之间性能差异的原因。

##### URL
[http://arxiv.org/abs/1806.03743](http://arxiv.org/abs/1806.03743)

##### PDF
[http://arxiv.org/pdf/1806.03743](http://arxiv.org/pdf/1806.03743)

