---
layout: post
title: "To Know Where We Are: Vision-Based Positioning in Outdoor Environments"
date: 2015-06-19 03:11:33
categories: arXiv_CV
tags: arXiv_CV Attention
author: Kuan-Wen Chen, Chun-Hsin Wang, Xiao Wei, Qiao Liang, Ming-Hsuan Yang, Chu-Song Chen, Yi-Ping Hung
mathjax: true
---

* content
{:toc}

##### Abstract
Augmented reality (AR) displays become more and more popular recently, because of its high intuitiveness for humans and high-quality head-mounted display have rapidly developed. To achieve such displays with augmented information, highly accurate image registration or ego-positioning are required, but little attention have been paid for out-door environments. This paper presents a method for ego-positioning in outdoor environments with low cost monocular cameras. To reduce the computational and memory requirements as well as the communication overheads, we formulate the model compression algorithm as a weighted k-cover problem for better preserving model structures. Specifically for real-world vision-based positioning applications, we consider the issues with large scene change and propose a model update algorithm to tackle these problems. A long- term positioning dataset with more than one month, 106 sessions, and 14,275 images is constructed. Based on both local and up-to-date models constructed in our approach, extensive experimental results show that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can be achieved, which outperforms existing vision-based algorithms.

##### Abstract (translated by Google)
增强现实（AR）显示器由于其对人类的高度直观性和高质量的头戴式显示器迅速发展而变得越来越流行。为了实现具有增强信息的这种显示，需要高度准确的图像配准或自我定位，但是对室外环境的关注很少。本文提出了一种利用低成本单目摄像机在室外环境中进行自我定位的方法。为了减少计算和存储需求以及通信开销，我们将模型压缩算法作为一个加权的k覆盖问题，以更好地保存模型结构。特别是对于真实世界的基于视觉的定位应用，我们考虑大场景变化的问题，并提出模型更新算法来解决这些问题。建立一个长达一个月以上的定位数据集，106个会话和14,275个图像。基于我们的方法构建的局部和最新模型，广泛的实验结果表明，可以实现高定位精度（平均30.9厘米，平均15.4厘米），优于现有的基于视觉的算法。

##### URL
[https://arxiv.org/abs/1506.05870](https://arxiv.org/abs/1506.05870)

##### PDF
[https://arxiv.org/pdf/1506.05870](https://arxiv.org/pdf/1506.05870)

