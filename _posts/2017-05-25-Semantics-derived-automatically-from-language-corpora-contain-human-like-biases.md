---
layout: post
title: "Semantics derived automatically from language corpora contain human-like biases"
date: 2017-05-25 17:50:31
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Aylin Caliskan, Joanna J. Bryson, Arvind Narayanan
mathjax: true
---

* content
{:toc}

##### Abstract
Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the {\em status quo} for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.

##### Abstract (translated by Google)
人工智能和机器学习正处于惊人的发展时期。然而，人们担心这些技术可能会被使用，无论有无意图，都会使许多人类机构不幸的特征所带来的偏见和不公平。在这里我们首次展示了人类的语义偏差是由于普通语言的应用标准机器学习的结果 - 人类每天都接触到同样的语言。我们复制了内隐联想测试和其他众所周知的心理学研究所揭示的一系列标准的人类偏见。我们使用广泛使用的纯粹的统计机器学习模型来复制这些模型，即GloVe词语嵌入 - 在Web上的文本语料库上进行训练。我们的研究结果表明，语言本身包含了我们历史偏见的可恢复和准确的印记，无论这些偏见是对昆虫或鲜花的道德中立，对于种族或性别问题，甚至仅仅是真实的，反映了分配的“现状”关于职业或名字的性别。机器学习和其他语义一起捕获这些规则。除了我们关于语言的实证研究之外，我们还提供了评估文本偏误，词语嵌入关联测试（WEAT）和词语嵌入事实关联测试（WEFAT）的新方法。我们的研究结果不仅对人工智能和机器学习有影响，而且对心理学，社会学和人类道德领域也有影响，因为它们提高了仅仅接触日常语言就可以解释我们在这里复制的偏见的可能性。

##### URL
[https://arxiv.org/abs/1608.07187](https://arxiv.org/abs/1608.07187)

##### PDF
[https://arxiv.org/pdf/1608.07187](https://arxiv.org/pdf/1608.07187)

