---
layout: post
title: "Deep Multimodal Representation Learning from Temporal Data"
date: 2017-04-11 05:47:42
categories: arXiv_CV
tags: arXiv_CV Attention Speech_Recognition Represenation_Learning RNN Classification Deep_Learning Relation Recognition
author: Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar A. Bernal, Jiebo Luo
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications. When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process. In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature. Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.

##### Abstract (translated by Google)
近年来，Deep Learning已经成功应用于多模式学习问题，目的是在数据融合应用中学习有用的联合表示。当可用的模态由视频，音频和传感器信号等时间序列数据组成时，在融合过程中考虑它们的时间结构变得势在必行。在本文中，我们提出了相关递归神经网络（CorrRNN），一种新颖的时间融合模型，用于融合固有的时间性质的多输入模式。我们提出的模型的关键特征包括：（1）同时学习联合表示和模态之间的时间依赖性;（2）在目标函数中使用多个损失项，包括最大相关损失项，以提高跨模态信息的学习（iii）使用注意模型来动态调整不同输入模态对联合表示的贡献。我们通过两个不同的任务实验验证我们的模型：基于视频和传感器的活动分类和视听语音识别。我们通过实证分析所提出的CorrRNN模型的不同组成部分的贡献，并展示其在多个数据集上的稳健性，有效性和最先进的性能。

##### URL
[https://arxiv.org/abs/1704.03152](https://arxiv.org/abs/1704.03152)

##### PDF
[https://arxiv.org/pdf/1704.03152](https://arxiv.org/pdf/1704.03152)

