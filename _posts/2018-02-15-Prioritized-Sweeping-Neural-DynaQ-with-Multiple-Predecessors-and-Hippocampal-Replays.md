---
layout: post
title: "Prioritized Sweeping Neural DynaQ with Multiple Predecessors, and Hippocampal Replays"
date: 2018-02-15 15:15:19
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Lise Aubin, Mehdi Khamassi (ISIR), Beno&#xee;t Girard (ISIR)
mathjax: true
---

* content
{:toc}

##### Abstract
During sleep and awake rest, the hippocampus replays sequences of place cells that have been activated during prior experiences. These have been interpreted as a memory consolidation process, but recent results suggest a possible interpretation in terms of reinforcement learning. The Dyna reinforcement learning algorithms use off-line replays to improve learning. Under limited replay budget, a prioritized sweeping approach, which requires a model of the transitions to the predecessors, can be used to improve performance. We investigate whether such algorithms can explain the experimentally observed replays. We propose a neural network version of prioritized sweeping Q-learning, for which we developed a growing multiple expert algorithm, able to cope with multiple predecessors. The resulting architecture is able to improve the learning of simulated agents confronted to a navigation task. We predict that, in animals, learning the world model should occur during rest periods, and that the corresponding replays should be shuffled.

##### Abstract (translated by Google)
在睡眠和清醒休息期间，海马回放在先前经历期间被激活的地点细胞序列。这些被解释为记忆巩固过程，但最近的结果表明在强化学习方面可能的解释。 Dyna强化学习算法使用离线重播来改善学习。在有限的重播预算下，可以使用优先扫描方法来改进性能，这种方法需要一个转换到前辈的模型。我们调查这些算法是否可以解释实验观察到的重放。我们提出了一个优先扫描Q-learning的神经网络版本，为此我们开发了一个不断增长的多专家算法，能够应对多个前辈。由此产生的体系结构能够提高面向导航任务的模拟代理的学习。我们预测，在动物中，学习世界模型应该在休息期间进行，并且相应的重放应该被洗牌。

##### URL
[http://arxiv.org/abs/1802.05594](http://arxiv.org/abs/1802.05594)

##### PDF
[http://arxiv.org/pdf/1802.05594](http://arxiv.org/pdf/1802.05594)

