---
layout: post
title: 'An Empirical Study of Language CNN for Image Captioning'
date: 2017-12-06 07:54:26
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption RNN
author: Jiuxiang Gu, Gang Wang, Jianfei Cai, Tsuhan Chen
---

* content
{:toc}

##### Abstract
Language Models based on recurrent neural networks have dominated recent image caption generation tasks. In this paper, we introduce a Language CNN model which is suitable for statistical language modeling tasks and shows competitive performance in image captioning. In contrast to previous models which predict next word based on one previous word and hidden state, our language CNN is fed with all the previous words and can model the long-range dependencies of history words, which are critical for image captioning. The effectiveness of our approach is validated on two datasets MS COCO and Flickr30K. Our extensive experimental results show that our method outperforms the vanilla recurrent neural network based language models and is competitive with the state-of-the-art methods.

##### Abstract (translated by Google)
基于递归神经网络的语言模型主导了最近的图像标题生成任务。本文介绍了一种适用于统计语言建模任务的语言CNN模型，并展示了图像字幕的竞争性表现。与先前的一个词根据前一个单词和隐藏状态预测下一个单词的模型相反，我们的语言CNN被提供了所有以前的单词，并且可以模拟历史单词的长程依赖性，这对于图像字幕是至关重要的。我们的方法的有效性验证了两个数据集MS COCO和Flickr30K。我们广泛的实验结果表明，我们的方法胜过基于香草回归神经网络的语言模型，并与最先进的方法竞争。

##### URL
[https://arxiv.org/abs/1612.07086](https://arxiv.org/abs/1612.07086)

