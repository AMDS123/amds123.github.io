---
layout: post
title: "Limitations in learning an interpreted language with recurrent models"
date: 2018-09-11 19:52:44
categories: arXiv_CL
tags: arXiv_CL RNN
author: Denis Paperno
mathjax: true
---

* content
{:toc}

##### Abstract
In this submission I report work in progress on learning simplified interpreted languages by means of recurrent models. The data is constructed to reflect core properties of natural language as modeled in formal syntax and semantics: recursive syntactic structure and compositionality. Preliminary results suggest that LSTM networks do generalise to compositional interpretation, albeit only in the most favorable learning setting, with a well-paced curriculum, extensive training data, and left-to-right (but not right-to-left) composition.

##### Abstract (translated by Google)
在本次提交中，我报告了通过循环模型学习简化解释语言的工作。构造数据以反映在形式语法和语义中建模的自然语言的核心属性：递归句法结构和组合性。初步结果表明，LSTM网络确实推广到成分解释，尽管只是在最有利的学习环境中，课程节奏良好，培训数据广泛，从左到右（但不是从右到左）构成。

##### URL
[http://arxiv.org/abs/1809.04128](http://arxiv.org/abs/1809.04128)

##### PDF
[http://arxiv.org/pdf/1809.04128](http://arxiv.org/pdf/1809.04128)

