---
layout: post
title: "PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding"
date: 2017-03-28 01:01:29
categories: arXiv_CV
tags: arXiv_CV Action_Recognition Deep_Learning Detection Recognition
author: Chunhui Liu, Yueyu Hu, Yanghao Li, Sijie Song, Jiaying Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Despite the fact that many 3D human activity benchmarks being proposed, most existing action datasets focus on the action recognition tasks for the segmented videos. There is a lack of standard large-scale benchmarks, especially for current popular data-hungry deep learning based methods. In this paper, we introduce a new large scale benchmark (PKU-MMD) for continuous multi-modality 3D human action understanding and cover a wide range of complex human activities with well annotated information. PKU-MMD contains 1076 long video sequences in 51 action categories, performed by 66 subjects in three camera views. It contains almost 20,000 action instances and 5.4 million frames in total. Our dataset also provides multi-modality data sources, including RGB, depth, Infrared Radiation and Skeleton. With different modalities, we conduct extensive experiments on our dataset in terms of two scenarios and evaluate different methods by various metrics, including a new proposed evaluation protocol 2D-AP. We believe this large-scale dataset will benefit future researches on action detection for the community.

##### Abstract (translated by Google)
尽管提出了许多3D人类活动基准，但大多数现有的行动数据集都集中在分割视频的动作识别任务上。缺乏标准的大规模基准测试，特别是对于目前流行的基于数据的深度学习方法。在本文中，我们引入了一个新的大规模基准（PKU-MMD），用于连续多模式三维人类行为的理解，涵盖了广泛的复杂的人类活动和注释良好的信息。 PKU-MMD在51个动作类别中包含1076个长视频序列，由三个相机视图中的66个主体执行。它包含近2万个动作实例和总计540万帧。我们的数据集还提供多模式数据源，包括RGB，深度，红外辐射和骨架。采用不同的模式，我们在两个场景下对数据集进行了广泛的实验，并通过各种度量评估了不同的方法，包括新提出的评估协议2D-AP。我们相信这个大规模的数据集将有利于未来对社区行为检测的研究。

##### URL
[https://arxiv.org/abs/1703.07475](https://arxiv.org/abs/1703.07475)

##### PDF
[https://arxiv.org/pdf/1703.07475](https://arxiv.org/pdf/1703.07475)

