---
layout: post
title: "Lip Reading Sentences in the Wild"
date: 2017-01-30 22:46:20
categories: arXiv_CV
tags: arXiv_CV Face Speech_Recognition Recognition
author: Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman
mathjax: true
---

* content
{:toc}

##### Abstract
The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television. The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that visual information helps to improve speech recognition performance even when the audio is available.

##### Abstract (translated by Google)
这项工作的目标是识别说话人脸上说的短语和句子，不管有没有音频。与以前着重于识别有限数量的单词或短语的作品不同，我们把唇读作为一个开放的世界性问题 - 不受约束的自然语言句子和野外视频。我们的主要贡献是：（1）学习将口述视频录制成角色的“观看，听，看，拼”（WLAS）网络; （2）加快培训和减少过度拟订的课程学习策略; （3）用于视觉语音识别的“唇读法”（LRS）数据集，由来自英国电视的超过100,000个自然语句组成。在LRS数据集上训练的WLAS模型超过了以往在标准唇读基准数据集上的所有工作的性能，往往是相当大的。这种嘴唇阅读性能比BBC电视的视频更胜一筹。我们还证明，即使音频可用，视觉信息也有助于提高语音识别性能。

##### URL
[https://arxiv.org/abs/1611.05358](https://arxiv.org/abs/1611.05358)

##### PDF
[https://arxiv.org/pdf/1611.05358](https://arxiv.org/pdf/1611.05358)

