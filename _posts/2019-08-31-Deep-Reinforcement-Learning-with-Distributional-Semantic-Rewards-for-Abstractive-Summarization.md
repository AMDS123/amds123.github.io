---
layout: post
title: "Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization"
date: 2019-08-31 06:13:33
categories: arXiv_AI
tags: arXiv_AI Face Summarization Reinforcement_Learning
author: Siyao Li, Deren Lei, Pengda Qin, William Yang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward Rouge-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of Rouge-L, we explore the practicability of utilizing the distributional semantics to measure the matching degrees. With distributional semantics, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1909.00141](http://arxiv.org/abs/1909.00141)

##### PDF
[http://arxiv.org/pdf/1909.00141](http://arxiv.org/pdf/1909.00141)

