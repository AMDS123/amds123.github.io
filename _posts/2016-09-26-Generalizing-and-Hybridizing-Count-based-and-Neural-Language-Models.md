---
layout: post
title: "Generalizing and Hybridizing Count-based and Neural Language Models"
date: 2016-09-26 01:48:57
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Graham Neubig, Chris Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.

##### Abstract (translated by Google)
语言模型（LM）是计算单词或其他离散符号序列的概率的统计模型。目前语言建模的两大主要模式有：基于计数的n-gram模型，其具有可伸缩性和测试时间速度的优势，以及神经LM，其经常实现优异的建模性能。我们演示了如何将这两种模型统一在一个单一的建模框架中，该框架定义了单词词汇表中的一组概率分布，然后动态计算这些分布上的混合权重。这个公式允许我们创造新的混合模型，结合基于计数和神经LM的理想特征，并且实验证明这些方法的优点。

##### URL
[https://arxiv.org/abs/1606.00499](https://arxiv.org/abs/1606.00499)

##### PDF
[https://arxiv.org/pdf/1606.00499](https://arxiv.org/pdf/1606.00499)

