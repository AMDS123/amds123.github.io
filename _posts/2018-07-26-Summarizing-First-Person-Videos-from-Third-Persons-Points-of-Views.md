---
layout: post
title: "Summarizing First-Person Videos from Third Persons' Points of Views"
date: 2018-07-26 13:28:51
categories: arXiv_CV
tags: arXiv_CV Summarization Quantitative
author: Hsuan-I Ho, Wei-Chen Chiu, Yu-Chiang Frank Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Video highlight or summarization is among interesting topics in computer vision, which benefits a variety of applications like viewing, searching, or storage. However, most existing studies rely on training data of third-person videos, which cannot easily generalize to highlight the first-person ones. With the goal of deriving an effective model to summarize first-person videos, we propose a novel deep neural network architecture for describing and discriminating vital spatiotemporal information across videos with different points of view. Our proposed model is realized in a semi-supervised setting, in which fully annotated third-person videos, unlabeled first-person videos, and a small number of annotated first-person ones are presented during training. In our experiments, qualitative and quantitative evaluations on both benchmarks and our collected first-person video datasets are presented.

##### Abstract (translated by Google)
视频突出显示或摘要是计算机视觉中的一个有趣主题，它有益于各种应用，如查看，搜索或存储。然而，大多数现有的研究都依赖于第三人称视频的训练数据，这些数据不容易概括为突出第一人称视频。为了得出有效的模型来总结第一人称视频，我们提出了一种新颖的深度神经网络架构，用于描述和区分具有不同视点的视频中的重要时空信息。我们提出的模型是在半监督环境中实现的，其中在训练期间呈现完全注释的第三人称视频，未标记的第一人称视频和少量注释的第一人称视频。在我们的实验中，我们展示了对基准和我们收集的第一人称视频数据集的定性和定量评估。

##### URL
[http://arxiv.org/abs/1711.08922](http://arxiv.org/abs/1711.08922)

##### PDF
[http://arxiv.org/pdf/1711.08922](http://arxiv.org/pdf/1711.08922)

