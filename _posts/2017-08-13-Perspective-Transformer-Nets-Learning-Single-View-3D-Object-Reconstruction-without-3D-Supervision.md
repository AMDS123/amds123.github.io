---
layout: post
title: "Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision"
date: 2017-08-13 02:40:50
categories: arXiv_CV
tags: arXiv_CV
author: Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, Honglak Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the perspective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.

##### Abstract (translated by Google)
了解3D世界是计算机视觉中的一个基本问题。然而，由于数据的高维度以及许多变化因素，学习3D对象的良好表示仍然是一个悬而未决的问题。在这项工作中，我们从学习代理的角度来研究单视图三维对象重构的任务。我们将学习过程制定为3D和2D表示之间的交互作用，并提出一种编码器 - 解码器网络，其具有由透视变换定义的新投影损失。更重要的是，投影损失使得无监督学习使用二维观测没有明确的3D监督。我们用三组实验来证明模型在从单个2D图像生成3D体积的能力：（1）从单类对象学习; （2）从多类对象学习，（3）对新类对象进行测试。结果表明，当涉及到投影丢失时，三维物体重建的性能优越，泛化能力更强。

##### URL
[https://arxiv.org/abs/1612.00814](https://arxiv.org/abs/1612.00814)

##### PDF
[https://arxiv.org/pdf/1612.00814](https://arxiv.org/pdf/1612.00814)

