---
layout: post
title: "Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing"
date: 2018-02-16 13:59:15
categories: arXiv_CL
tags: arXiv_CL Attention Transfer_Learning Classification
author: Somnath Basu Roy Chowdhury, K M Annervaz, Ambedkar Dukkipati
mathjax: true
---

* content
{:toc}

##### Abstract
Supervised learning models are typically trained on a single dataset and the performance of these models rely heavily on the size of the dataset, i.e., amount of data available with the ground truth. Learning algorithms try to generalize solely based on the data that is presented with during the training. In this work, we propose an inductive transfer learning method that can augment learning models by infusing similar instances from different learning tasks in the Natural Language Processing (NLP) domain. We propose to use instance representations from a source dataset, \textit{without inheriting anything} from the source learning model. Representations of the instances of \textit{source} \& \textit{target} datasets are learned, retrieval of relevant source instances is performed using soft-attention mechanism and \textit{locality sensitive hashing}, and then, augmented into the model during training on the target dataset. Our approach simultaneously exploits the local \textit{instance level information} as well as the macro statistical viewpoint of the dataset. Using this approach we have shown significant improvements for three major news classification datasets over the baseline. Experimental evaluations also show that the proposed approach reduces dependency on labeled data by a significant margin for comparable performance. With our proposed cross dataset learning procedure we show that one can achieve competitive/better performance than learning from a single dataset.

##### Abstract (translated by Google)
监督学习模型通常在单个数据集上进行训练，这些模型的性能在很大程度上依赖于数据集的大小，即数据量与地面真实情况。学习算法试图仅根据训练期间提供的数据进行概括。在这项工作中，我们提出了一种归纳转移学习方法，它可以通过在自然语言处理（NLP）领域注入来自不同学习任务的类似实例来增强学习模型。我们建议使用源数据集中的实例表示，\ textit {不需要从源学习模型继承任何东西}。学习\ textit {source} \＆\ textit {target}数据集实例的表示，使用软注意机制和\ textit {局部敏感散列}执行相关源实例的检索，然后在模型期间扩展到模型中对目标数据集进行培训。我们的方法同时利用本地\ textit {实例级信息}以及数据集的宏观统计视点。使用这种方法，我们已经显示出三个主要新闻分类数据集在基线上的显着改进。实验评估还表明，所提出的方法减少了对标记数据的依赖性，对于可比较的性能而言具有显着的余量。利用我们提出的交叉数据集学习过程，我们发现，与从单个数据集中学习相比，可以获得更好的竞争力/更好的性能。

##### URL
[https://arxiv.org/abs/1802.05934](https://arxiv.org/abs/1802.05934)

##### PDF
[https://arxiv.org/pdf/1802.05934](https://arxiv.org/pdf/1802.05934)

