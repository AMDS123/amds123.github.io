---
layout: post
title: "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing"
date: 2017-02-22 13:49:18
categories: arXiv_SD
tags: arXiv_SD Reinforcement_Learning
author: Minh Le, Antske Fokkens
mathjax: true
---

* content
{:toc}

##### Abstract
Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.

##### Abstract (translated by Google)
错误传播是NLP中的一个常见问题。强化学习在培训过程中探究错误的状态，因此在过程中提前发生错误时可以更加强大。在本文中，我们将强化学习应用于已知受到错误传播的贪婪依赖性解析。强化学习提高了斯坦福神经依赖解析器（一种高性能的贪婪解析器）的标记和未标记依赖的准确性，同时保持其效率。我们调查错误传播的结果部分，并确认强化学习减少错误传播的发生。

##### URL
[https://arxiv.org/abs/1702.06794](https://arxiv.org/abs/1702.06794)

##### PDF
[https://arxiv.org/pdf/1702.06794](https://arxiv.org/pdf/1702.06794)

