---
layout: post
title: "Visually grounded learning of keyword prediction from untranscribed speech"
date: 2017-05-25 20:49:15
categories: arXiv_CV
tags: arXiv_CV Caption Prediction
author: Herman Kamper, Shane Settle, Gregory Shakhnarovich, Karen Livescu
mathjax: true
---

* content
{:toc}

##### Abstract
During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance---acting as a spoken bag-of-words classifier---without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. "man" and "person", making it even more effective as a semantic keyword spotter.

##### Abstract (translated by Google)
在语言习得期间，婴儿可以获得对地面口语的视觉提示。机器人同样可以访问音频和视觉传感器。最近的工作表明，图像和语音标题可以映射到有意义的公共空间，允许使用语音检索图像，反之亦然。在这种与未转录的语音标题配对的图像设置中，我们考虑是否可以使用计算机视觉系统来获得语音的文本标签。具体地说，我们使用图像到单词的多标签视觉分类器来标记具有软文本标签的图像，然后训练神经网络以从语音映射到这些软目标。我们证明了由此产生的语音系统能够预测哪个词出现在一个话语中 - 充当一个口语词袋分类器---没有看到任何平行的语音和文本。我们发现该模型经常混淆语义相关的单词，例如： “man”和“person”，使其作为语义关键词观察者更有效。

##### URL
[https://arxiv.org/abs/1703.08136](https://arxiv.org/abs/1703.08136)

##### PDF
[https://arxiv.org/pdf/1703.08136](https://arxiv.org/pdf/1703.08136)

