---
layout: post
title: "Automating Interpretability: Discovering and Testing Visual Concepts Learned by Neural Networks"
date: 2019-02-07 03:18:54
categories: arXiv_CV
tags: arXiv_CV Prediction Relation
author: Amirata Ghorbani, James Wexler, Been Kim
mathjax: true
---

* content
{:toc}

##### Abstract
Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Due to it's complexity, i For high-stakes domains such as medical, providing intuitive explanations that can be consumed by domain experts without ML expertise becomes crucial. To this demand, concept-based methods (e.g., TCAV) were introduced to provide explanations using user-chosen high-level concepts rather than individual input features. While these methods successfully leverage rich representations learned by the networks to reveal how human-defined concepts are related to the prediction, they require users to select concepts of their choice and collect labeled examples of those concepts. In this work, we introduce DTCAV (Discovery TCAV) a global concept-based interpretability method that can automatically discover concepts as image segments, along with each concept's estimated importance for a deep neural network's predictions. We validate that discovered concepts are as coherent to humans as hand-labeled concepts. We also show that the discovered concepts carry significant signal for prediction by analyzing a network's performance with stitched/added/deleted concepts. DTCAV results revealed a number of undesirable correlations (e.g., a basketball player's jersey was a more important concept for predicting the basketball class than the ball itself) and show the potential shallow reasoning of these networks.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.03129](http://arxiv.org/abs/1902.03129)

##### PDF
[http://arxiv.org/pdf/1902.03129](http://arxiv.org/pdf/1902.03129)

