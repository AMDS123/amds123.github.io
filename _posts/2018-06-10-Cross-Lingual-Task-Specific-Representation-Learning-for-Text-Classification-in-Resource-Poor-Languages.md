---
layout: post
title: "Cross-Lingual Task-Specific Representation Learning for Text Classification in Resource Poor Languages"
date: 2018-06-10 06:09:57
categories: arXiv_CL
tags: arXiv_CL Sentiment Text_Classification Represenation_Learning RNN Classification Prediction
author: Nurendra Choudhary, Rajat Singh, Manish Shrivastava
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network models have shown promising results for text classification. However, these solutions are limited by their dependence on the availability of annotated data. 
 The prospect of leveraging resource-rich languages to enhance the text classification of resource-poor languages is fascinating. The performance on resource-poor languages can significantly improve if the resource availability constraints can be offset. To this end, we present a twin Bidirectional Long Short Term Memory (Bi-LSTM) network with shared parameters consolidated by a contrastive loss function (based on a similarity metric). The model learns the representation of resource-poor and resource-rich sentences in a common space by using the similarity between their assigned annotation tags. Hence, the model projects sentences with similar tags closer and those with different tags farther from each other. We evaluated our model on the classification tasks of sentiment analysis and emoji prediction for resource-poor languages - Hindi and Telugu and resource-rich languages - English and Spanish. Our model significantly outperforms the state-of-the-art approaches in both the tasks across all metrics.

##### Abstract (translated by Google)
神经网络模型已经显示出有希望的文本分类结果。但是，这些解决方案受限于其对注释数据可用性的依赖。
 利用资源丰富的语言来增强资源贫乏语言的文本分类的前景令人着迷。如果资源可用性约束可以抵消，资源匮乏语言的性能可以显着提高。为此，我们提出了一个双向双向长期短期记忆（Bi-LSTM）网络，该网络具有通过对比损失函数（基于相似性度量）整合的共享参数。该模型通过使用它们分配的注释标签之间的相似性来学习在公共空间中资源贫乏和资源丰富的句子的表示。因此，该模型将类似标签的句子投射得更近，不同标签的语句彼此远离。我们对资源贫乏语言 - 北印度语和泰卢固语以及资源丰富的语言 - 英语和西班牙语的情感分析和表情符号预测的分类任务进行了评估。我们的模型在所有指标的任务中都明显优于最先进的方法。

##### URL
[http://arxiv.org/abs/1806.03590](http://arxiv.org/abs/1806.03590)

##### PDF
[http://arxiv.org/pdf/1806.03590](http://arxiv.org/pdf/1806.03590)

