---
layout: post
title: "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates"
date: 2017-12-13 18:24:34
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Leslie N. Smith, Nicholay Topin
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we show a phenomenon, which we named "super-convergence", where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate. Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence.

##### Abstract (translated by Google)
在本文中，我们展示了一个我们称之为“超收敛”的现象，其中残余网络可以用比标准训练方法少的迭代次数训练。超融合的存在与理解深层网络泛化的关系。超融合的关键要素之一是具有周期性学习率和最大学习率的训练。此外，我们提出的证据表明，大规模学习率的培训通过规范网络来提高绩效。另外，我们证明，当标注的训练数据量有限时，超融合相对于标准训练提供了更大的性能提升。我们还推导了Hessian Free优化方法的简化来计算最优学习率的估计。复制本文中的图形的体系结构和代码可在github.com/lnsmith54/super-convergence上找到。

##### URL
[http://arxiv.org/abs/1708.07120](http://arxiv.org/abs/1708.07120)

##### PDF
[http://arxiv.org/pdf/1708.07120](http://arxiv.org/pdf/1708.07120)

