---
layout: post
title: "Scheduled Policy Optimization for Natural Language Communication with Intelligent Agents"
date: 2018-06-16 05:17:32
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning Optimization
author: Wenhan Xiong, Xiaoxiao Guo, Mo Yu, Shiyu Chang, Bowen Zhou, William Yang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate the task of learning to follow natural language instructions by jointly reasoning with visual observations and language inputs. In contrast to existing methods which start with learning from demonstrations (LfD) and then use reinforcement learning (RL) to fine-tune the model parameters, we propose a novel policy optimization algorithm which dynamically schedules demonstration learning and RL. The proposed training paradigm provides efficient exploration and better generalization beyond existing methods. Comparing to existing ensemble models, the best single model based on our proposed method tremendously decreases the execution error by over 50% on a block-world environment. To further illustrate the exploration strategy of our RL algorithm, We also include systematic studies on the evolution of policy entropy during training.

##### Abstract (translated by Google)
我们通过视觉观察和语言输入的联合推理来调查学习遵循自然语言指令的任务。与现有的从演示学习（LfD）开始，然后使用强化学习（RL）来微调模型参数的方法相比，我们提出了一种新的动态调度演示学习和RL的策略优化算法。所提出的培训范式提供了超越现有方法的有效探索和更好的概括。与现有的集成模型相比，基于我们提出的方法的最佳单一模型在块世界环境下极大地减少了超过50％的执行错误。为了进一步说明我们的RL算法的探索策略，我们还包括对培训期间策略熵演化的系统研究。

##### URL
[http://arxiv.org/abs/1806.06187](http://arxiv.org/abs/1806.06187)

##### PDF
[http://arxiv.org/pdf/1806.06187](http://arxiv.org/pdf/1806.06187)

