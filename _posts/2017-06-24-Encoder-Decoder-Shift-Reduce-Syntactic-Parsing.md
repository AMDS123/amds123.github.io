---
layout: post
title: "Encoder-Decoder Shift-Reduce Syntactic Parsing"
date: 2017-06-24 04:08:11
categories: arXiv_CL
tags: arXiv_CL NMT
author: Jiangming Liu, Yue Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Starting from NMT, encoder-decoder neu- ral networks have been used for many NLP problems. Graph-based models and transition-based models borrowing the en- coder components achieve state-of-the-art performance on dependency parsing and constituent parsing, respectively. How- ever, there has not been work empirically studying the encoder-decoder neural net- works for transition-based parsing. We apply a simple encoder-decoder to this end, achieving comparable results to the parser of Dyer et al. (2015) on standard de- pendency parsing, and outperforming the parser of Vinyals et al. (2015) on con- stituent parsing.

##### Abstract (translated by Google)
从NMT开始，编码器 - 解码器中性网络已被用于许多NLP问题。借助编码器组件的基于图形的模型和基于转换的模型分别实现了依赖性解析和组成解析的最新性能。然而，还没有经验性地研究基于转换的解析的编码器 - 解码器神经网络。为此，我们应用了一个简单的编码器 - 解码器，实现了与Dyer等人的解析器相媲美的结果。 （2015）的标准依赖性分析，并超越了Vinyals等人的解析器。 （2015年）的成分分析。

##### URL
[https://arxiv.org/abs/1706.07905](https://arxiv.org/abs/1706.07905)

##### PDF
[https://arxiv.org/pdf/1706.07905](https://arxiv.org/pdf/1706.07905)

