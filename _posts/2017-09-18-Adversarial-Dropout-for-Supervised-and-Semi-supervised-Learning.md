---
layout: post
title: "Adversarial Dropout for Supervised and Semi-supervised Learning"
date: 2017-09-18 00:36:45
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Sungrae Park, Jun-Keon Park, Su-Jin Shin, Il-Chul Moon
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, the training with adversarial examples, which are generated by adding a small but worst-case perturbation on input examples, has been proved to improve generalization performance of neural networks. In contrast to the individually biased inputs to enhance the generality, this paper introduces adversarial dropout, which is a minimal set of dropouts that maximize the divergence between the outputs from the network with the dropouts and the training supervisions. The identified adversarial dropout are used to reconfigure the neural network to train, and we demonstrated that training on the reconfigured sub-network improves the generalization performance of supervised and semi-supervised learning tasks on MNIST and CIFAR-10. We analyzed the trained model to reason the performance improvement, and we found that adversarial dropout increases the sparsity of neural networks more than the standard dropout does.

##### Abstract (translated by Google)
最近，通过在输入例子上增加一个小的但是最坏情况的扰动产生的对立例子的训练已经被证明可以提高神经网络的泛化性能。与个别偏见的投入相比，以增强普遍性，本文介绍了对抗性辍学，这是一个最小的辍学集，最大限度地发挥网络输出与辍学和训练监督之间的分歧。确定的敌对性丢失被用来重新配置神经网络进行训练，并且我们证明了对重构的子网络的训练提高了MNIST和CIFAR-10上的监督和半监督学习任务的泛化性能。我们分析了训练好的模型来推导性能的提高，我们发现对抗性辍学增加神经网络的稀疏性比标准辍学更多。

##### URL
[https://arxiv.org/abs/1707.03631](https://arxiv.org/abs/1707.03631)

##### PDF
[https://arxiv.org/pdf/1707.03631](https://arxiv.org/pdf/1707.03631)

