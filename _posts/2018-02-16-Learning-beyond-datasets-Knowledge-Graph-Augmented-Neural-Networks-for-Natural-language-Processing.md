---
layout: post
title: "Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing"
date: 2018-02-16 13:38:00
categories: arXiv_CL
tags: arXiv_CL Knowledge_Graph Knowledge Attention GAN Text_Classification Inference Classification Deep_Learning Relation
author: K M Annervaz, Somnath Basu Roy Chowdhury, Ambedkar Dukkipati
mathjax: true
---

* content
{:toc}

##### Abstract
Machine Learning has been the quintessential solution for many AI problems, but learning is still heavily dependent on the specific training data. Some learning models can be incorporated with a prior knowledge in the Bayesian set up, but these learning models do not have the ability to access any organised world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolution-based model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with News20, DBPedia datasets and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained well with substantially less amount of labeled training data, when it has access to organised world knowledge in the form of knowledge graph.

##### Abstract (translated by Google)
机器学习一直是许多AI问题的典型解决方案，但学习依然严重依赖于具体的培训数据。一些学习模型可以结合贝叶斯建立中的先验知识，但是这些学习模型不具备根据需要访问任何有组织的世界知识的能力。在这项工作中，我们建议以知识图（KG）的形式为世界知识增强学习模型，其实际上是自然语言处理（NLP）任务的三倍。我们的目标是开发一种深度学习模型，可以根据任务使用注意机制从知识图中提取相关的先前支持事实。为了减少注意空间，我们引入了基于卷积的模型来学习知识图实体和关系集的表示。我们表明，提出的方法是高度可扩展的先前信息的数量已被处理，并可应用于任何通用的NLP任务。使用这种方法，我们使用News20，DBPedia数据集和使用斯坦福自然语言推理（SNLI）数据集的自然语言推断，显示了文本分类性能的显着提高。我们还证明，当深度学习模型以知识图的形式访问有组织的世界知识时，可以用大大少量的标记训练数据进行良好的训练。

##### URL
[https://arxiv.org/abs/1802.05930](https://arxiv.org/abs/1802.05930)

##### PDF
[https://arxiv.org/pdf/1802.05930](https://arxiv.org/pdf/1802.05930)

