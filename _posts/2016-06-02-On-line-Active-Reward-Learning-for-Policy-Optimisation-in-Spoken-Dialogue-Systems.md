---
layout: post
title: "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems"
date: 2016-06-02 14:01:07
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.

##### Abstract (translated by Google)
计算准确奖励函数的能力对于通过强化学习来优化对话策略是至关重要的。在现实世界的应用中，使用明确的用户反馈作为奖励信号往往是不可靠和昂贵的收集。如果事先知道用户的意图或数据可用于预训练离线任务成功预测，则可以减轻此问题。实际上这些都不适用于大多数真实世界的应用。在这里，我们提出一个在线学习框架，通过主动学习和高斯过程模型，对话策略与奖励模式一起共同训练。该高斯过程使用循环神经网络编码器 - 解码器以无监督方式产生的连续空间对话表示操作。实验结果表明，所提出的框架能够显着降低数据标注成本，减轻对话政策学习中用户反馈的嘈杂程度。

##### URL
[https://arxiv.org/abs/1605.07669](https://arxiv.org/abs/1605.07669)

##### PDF
[https://arxiv.org/pdf/1605.07669](https://arxiv.org/pdf/1605.07669)

