---
layout: post
title: "Understanding Back-Translation at Scale"
date: 2018-08-28 16:05:40
categories: arXiv_CL
tags: arXiv_CL
author: Sergey Edunov, Myle Ott, Michael Auli, David Grangier
mathjax: true
---

* content
{:toc}

##### Abstract
An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set.

##### Abstract (translated by Google)
利用单语数据改进神经机器翻译的有效方法是利用目标语言句子的反向翻译来增强并行训练语料库。这项工作拓宽了对反向翻译的理解，并研究了许多生成合成源句的方法。我们发现，除了资源较差的设置之外，通过采样或噪声波束输出获得的反向翻译是最有效的。我们的分析表明，采样或噪声合成数据比通过波束或贪婪搜索生成的数据提供了更强的训练信号。我们还比较了合成数据与真正的bitext相比如何以及研究各种域效应。最后，我们扩展到数以亿计的单语句，并在WMT'14英语 - 德语测试集上实现了35 BLEU的新技术。

##### URL
[http://arxiv.org/abs/1808.09381](http://arxiv.org/abs/1808.09381)

##### PDF
[http://arxiv.org/pdf/1808.09381](http://arxiv.org/pdf/1808.09381)

