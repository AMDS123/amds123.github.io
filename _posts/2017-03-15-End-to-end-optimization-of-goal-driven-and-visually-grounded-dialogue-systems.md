---
layout: post
title: "End-to-end optimization of goal-driven and visually grounded dialogue systems"
date: 2017-03-15 23:34:20
categories: arXiv_SD
tags: arXiv_SD Reinforcement_Learning Optimization
author: Florian Strub, Harm de Vries, Jeremie Mary, Bilal Piot, Aaron Courville, Olivier Pietquin
mathjax: true
---

* content
{:toc}

##### Abstract
End-to-end design of dialogue systems has recently become a popular research topic thanks to powerful tools such as encoder-decoder architectures for sequence-to-sequence learning. Yet, most current approaches cast human-machine dialogue management as a supervised learning problem, aiming at predicting the next utterance of a participant given the full history of the dialogue. This vision is too simplistic to render the intrinsic planning problem inherent to dialogue as well as its grounded nature, making the context of a dialogue larger than the sole history. This is why only chit-chat and question answering tasks have been addressed so far using end-to-end architectures. In this paper, we introduce a Deep Reinforcement Learning method to optimize visually grounded task-oriented dialogues, based on the policy gradient algorithm. This approach is tested on a dataset of 120k dialogues collected through Mechanical Turk and provides encouraging results at solving both the problem of generating natural dialogues and the task of discovering a specific object in a complex picture.

##### Abstract (translated by Google)
对话系统的端到端设计最近成为了一个受欢迎的研究课题，这要归功于强大的工具，如用于序列到序列学习的编码器 - 解码器架构。然而，目前大多数方法将人机对话管理视为监督学习问题，旨在预测参与者的下一个话语，给出完整的对话历史。这种观点过于简单化，使得对话所固有的内在规划问题及其基础性质，使对话的背景大于单一的历史。这就是为什么只使用端到端体系结构来解决闲聊和问题解答任务的原因。在本文中，我们介绍了一种基于策略梯度算法的Deep Reinforcement Learning方法来优化基于视觉的面向任务的对话。这种方法在通过Mechanical Turk收集的12万对话的数据集上进行测试，在解决产生自然对话的问题和在复杂的图片中发现特定对象的任务方面提供了令人鼓舞的结果。

##### URL
[https://arxiv.org/abs/1703.05423](https://arxiv.org/abs/1703.05423)

##### PDF
[https://arxiv.org/pdf/1703.05423](https://arxiv.org/pdf/1703.05423)

