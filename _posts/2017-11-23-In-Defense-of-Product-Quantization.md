---
layout: post
title: "In Defense of Product Quantization"
date: 2017-11-23 06:40:28
categories: arXiv_CV
tags: arXiv_CV Knowledge Classification
author: Benjamin Klein, Lior Wolf
mathjax: true
---

* content
{:toc}

##### Abstract
Despite their widespread adoption, Product Quantization techniques were recently shown to be inferior to other hashing techniques. In this work, we present an improved Deep Product Quantization (DPQ) technique that leads to more accurate retrieval and classification than the latest state of the art methods, while having similar computational complexity and memory footprint as the Product Quantization method. To our knowledge, this is the first work to introduce a representation that is inspired by Product Quantization and which is learned end-to-end, and thus benefits from the supervised signal. DPQ explicitly learns soft and hard representations to enable an efficient and accurate asymmetric search, by using a straight-through estimator. A novel loss function, Joint Central Loss, is introduced, which both improves the retrieval performance, and decreases the discrepancy between the soft and the hard representations. Finally, by using a normalization technique, we improve the results for cross-domain category retrieval.

##### Abstract (translated by Google)
尽管被广泛采用，产品量化技术最近被证明不如其他哈希技术。在这项工作中，我们提出了一种改进的深度产品量化（DPQ）技术，其导致比最新的现有技术方法更精确的检索和分类，同时具有与产品量化方法相似的计算复杂性和内存占用。就我们所知，这是第一个引入受产品量化启发并且是端到端学习的表示的工作，从而受益于受监督的信号。 DPQ通过使用直通式估计器，明确地学习软硬表示以实现高效，准确的非对称搜索。提出了一种新的损失函数 - 联合中心损失（Joint Central Loss），既提高了检索性能，又减少了软硬表示之间的差异。最后，通过使用规范化技术，我们改进了跨域类别检索的结果。

##### URL
[https://arxiv.org/abs/1711.08589](https://arxiv.org/abs/1711.08589)

##### PDF
[https://arxiv.org/pdf/1711.08589](https://arxiv.org/pdf/1711.08589)

