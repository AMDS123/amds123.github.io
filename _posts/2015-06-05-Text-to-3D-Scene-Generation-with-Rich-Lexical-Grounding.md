---
layout: post
title: "Text to 3D Scene Generation with Rich Lexical Grounding"
date: 2015-06-05 01:13:17
categories: arXiv_CL
tags: arXiv_CL Quantitative
author: Angel Chang, Will Monroe, Manolis Savva, Christopher Potts, Christopher D. Manning
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.

##### Abstract (translated by Google)
将场景描述映射到三维几何表示的能力在诸如艺术，教育和机器人等领域具有许多应用。然而，之前关于文本到3D场景生成任务的工作已经使用手动指定的对象类别和识别它们的语言。我们介绍一个用自然语言描述注释的3D场景数据集，并从这些数据中学习如何将文本描述与物理对象建立关系。我们的方法成功地将各种词汇术语理解为具体的对象，并且我们定量地展示了我们的方法使用纯粹的基于规则的方法改善了以前工作中的3D场景生成。我们通过人为判断评估使用接地方法生成的3D场景的保真度和合理性。为了便于评估这个任务，我们还引入了一个与人类判断密切相关的自动化指标。

##### URL
[https://arxiv.org/abs/1505.06289](https://arxiv.org/abs/1505.06289)

##### PDF
[https://arxiv.org/pdf/1505.06289](https://arxiv.org/pdf/1505.06289)

