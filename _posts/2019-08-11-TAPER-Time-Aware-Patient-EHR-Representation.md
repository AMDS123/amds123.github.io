---
layout: post
title: "TAPER: Time-Aware Patient EHR Representation"
date: 2019-08-11 23:15:23
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning Language_Model
author: Sajad Darabi, Mohammad Kachuee, Shayan Fazeli, Majid Sarrafzadeh
mathjax: true
---

* content
{:toc}

##### Abstract
Effective representation learning of electronic health records is a challenging task and is becoming more important as the availability of such data is becoming pervasive. The data contained in these records are irregular and contain multiple modalities such as notes, and medical codes. They are preempted by medical conditions the patient may have, and are typically jotted down by medical staff. Accompanying codes are notes containing valuable information about patients beyond the structured information contained in EHR and medical codes. We use transformer networks and the recently proposed BERT language model to embed these data streams into a unified vector representation. The presented approach effectively encodes a patient's visit data into a single distributed representation, which can be used for downstream tasks. Our model demonstrates superior performance and generalization on mortality, readmission and length of stay tasks using the publicly available MIMIC-III ICU dataset.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.03971](https://arxiv.org/abs/1908.03971)

##### PDF
[https://arxiv.org/pdf/1908.03971](https://arxiv.org/pdf/1908.03971)

