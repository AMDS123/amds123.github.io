---
layout: post
title: "Continuous Learning in a Hierarchical Multiscale Neural Network"
date: 2018-05-15 13:37:33
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Thomas Wolf, Julien Chaumond, Clement Delangue
mathjax: true
---

* content
{:toc}

##### Abstract
We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework. We propose a hierarchical multi-scale language model in which short time-scale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer time-scale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion. We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework.

##### Abstract (translated by Google)
我们重新编写一个语言模型中一个序列的多尺度表示的编码问题，并将其转换为一个连续的学习框架。我们提出了一种分层多尺度语言模型，其中短时间尺度依赖性被编码在低级循环神经网络的隐藏状态中，而较长时间尺度依赖性被编入下级网络的动态中，元学习者以在线元学习的方式更新低级神经网络的权重。我们使用弹性权重合并作为更高级别来防止在我们的持续学习框架中发生灾难性遗忘。

##### URL
[http://arxiv.org/abs/1805.05758](http://arxiv.org/abs/1805.05758)

##### PDF
[http://arxiv.org/pdf/1805.05758](http://arxiv.org/pdf/1805.05758)

