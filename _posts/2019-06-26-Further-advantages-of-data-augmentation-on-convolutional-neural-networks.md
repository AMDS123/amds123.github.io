---
layout: post
title: "Further advantages of data augmentation on convolutional neural networks"
date: 2019-06-26 12:50:13
categories: arXiv_CV
tags: arXiv_CV Regularization CNN Classification Deep_Learning
author: Alex Hern&#xe1;ndez-Garc&#xed;a, Peter K&#xf6;nig
mathjax: true
---

* content
{:toc}

##### Abstract
Data augmentation is a popular technique largely used to enhance the training of convolutional neural networks. Although many of its benefits are well known by deep learning researchers and practitioners, its implicit regularization effects, as compared to popular explicit regularization techniques, such as weight decay and dropout, remain largely unstudied. As a matter of fact, convolutional neural networks for image object classification are typically trained with both data augmentation and explicit regularization, assuming the benefits of all techniques are complementary. In this paper, we systematically analyze these techniques through ablation studies of different network architectures trained with different amounts of training data. Our results unveil a largely ignored advantage of data augmentation: networks trained with just data augmentation more easily adapt to different architectures and amount of training data, as opposed to weight decay and dropout, which require specific fine-tuning of their hyperparameters.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.11052](http://arxiv.org/abs/1906.11052)

##### PDF
[http://arxiv.org/pdf/1906.11052](http://arxiv.org/pdf/1906.11052)

