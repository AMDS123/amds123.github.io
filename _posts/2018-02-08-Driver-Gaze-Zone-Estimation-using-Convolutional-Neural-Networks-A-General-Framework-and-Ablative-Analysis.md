---
layout: post
title: "Driver Gaze Zone Estimation using Convolutional Neural Networks: A General Framework and Ablative Analysis"
date: 2018-02-08 02:13:50
categories: arXiv_CV
tags: arXiv_CV Attention CNN
author: Sourabh Vora, Akshay Rangesh, Mohan M. Trivedi
mathjax: true
---

* content
{:toc}

##### Abstract
Driver gaze has been shown to be an excellent surrogate for driver attention in intelligent vehicles. With the recent surge of highly autonomous vehicles, driver gaze can be useful for determining the handoff time to a human driver. While there has been significant improvement in personalized driver gaze zone estimation systems, a generalized system which is invariant to different subjects, perspectives and scales is still lacking. We take a step towards this generalized system using Convolutional Neural Networks (CNNs). We finetune 4 popular CNN architectures for this task, and provide extensive comparisons of their outputs. We additionally experiment with different input image patches, and also examine how image size affects performance. For training and testing the networks, we collect a large naturalistic driving dataset comprising of 11 long drives, driven by 10 subjects in two different cars. Our best performing model achieves an accuracy of 95.18% during cross-subject testing, outperforming current state of the art techniques for this task. Finally, we evaluate our best performing model on the publicly available Columbia Gaze Dataset comprising of images from 56 subjects with varying head pose and gaze directions. Without any training, our model successfully encodes the different gaze directions on this diverse dataset, demonstrating good generalization capabilities.

##### Abstract (translated by Google)
驾驶者注视已被证明是智能车辆中的驾驶员注意力的极佳替代品。随着最近高度自动驾驶汽车的涌现，驾驶员注视对于确定对于驾驶员的交接时间是有用的。虽然个性化的驾驶注视区域估计系统已经有了显着的改进，但是对于不同的主题，视角和尺度不变的广义系统仍然缺乏。我们采用卷积神经网络（CNN）向这个广义系统迈出了一步。我们对这个任务进行了4种流行的CNN架构的微调，并对其输出进行了广泛的比较。我们另外尝试不同的输入图像补丁，并检查图像大小如何影响性能。为了训练和测试网络，我们收集了一个大型自然驾驶数据集，包括11个长驱动器，由两个不同的汽车中的10个主体驱动。我们表现​​最好的模型在跨学科测试中达到了95.18％的准确率，超越了目前这项任务的最新技术水平。最后，我们评估我们的最佳表演模型在公开可用的哥伦比亚凝视数据集，包括来自56个不同头部姿势和注视方向的受试者的图像。没有任何训练，我们的模型成功编码不同的凝视方向在这个不同的数据集，展示了良好的泛化能力。

##### URL
[http://arxiv.org/abs/1802.02690](http://arxiv.org/abs/1802.02690)

##### PDF
[http://arxiv.org/pdf/1802.02690](http://arxiv.org/pdf/1802.02690)

