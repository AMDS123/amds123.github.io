---
layout: post
title: "Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks"
date: 2015-07-29 03:04:19
categories: arXiv_CV
tags: arXiv_CV Knowledge Quantitative Recognition
author: Xiao Lin, Devi Parikh
mathjax: true
---

* content
{:toc}

##### Abstract
Artificial agents today can answer factual questions. But they fall short on questions that require common sense reasoning. Perhaps this is because most existing common sense databases rely on text to learn and represent knowledge. But much of common sense knowledge is unwritten - partly because it tends not to be interesting enough to talk about, and partly because some common sense is unnatural to articulate in text. While unwritten, it is not unseen. In this paper we leverage semantic common sense knowledge learned from images - i.e. visual common sense - in two textual tasks: fill-in-the-blank and visual paraphrasing. We propose to "imagine" the scene behind the text, and leverage visual cues from the "imagined" scenes in addition to textual cues while answering these questions. We imagine the scenes as a visual abstraction. Our approach outperforms a strong text-only baseline on these tasks. Our proposed tasks can serve as benchmarks to quantitatively evaluate progress in solving tasks that go "beyond recognition". Our code and datasets are publicly available.

##### Abstract (translated by Google)
今天的人工制剂可以回答事实问题。但是他们在需要常识推理的问题上没有做到。也许这是因为大多数现有的常识数据库依靠文本来学习和表示知识。但是许多常识性知识是不成文的 - 部分原因是它不够有趣，无法谈论，部分原因是某些常识在文本中表达不太自然。虽然不成文，但并不是看不见的。在本文中，我们利用从图像学习的语义常识知识 - 即视觉常识 - 在两个文本任务中：填空和视觉释义。我们建议“想象”文本背后的场景，并在回答这些问题的同时，利用“想象”场景中的视觉线索以及文本线索。我们将场景想象为一个视觉抽象。我们的方法比这些任务的纯文本基准要好。我们提出的任务可以作为基准来定量评估解决“超越认同”任务的进展。我们的代码和数据集是公开的。

##### URL
[https://arxiv.org/abs/1502.06108](https://arxiv.org/abs/1502.06108)

##### PDF
[https://arxiv.org/pdf/1502.06108](https://arxiv.org/pdf/1502.06108)

