---
layout: post
title: "Generalised Wasserstein Dice Score for Imbalanced Multi-class Segmentation using Holistic Convolutional Networks"
date: 2017-08-29 12:26:16
categories: arXiv_CV
tags: arXiv_CV Segmentation CNN Classification Prediction Relation
author: Lucas Fidon, Wenqi Li, Luis C. Garcia-Peraza-Herrera, Jinendra Ekanayake, Neil Kitchen, Sebastien Ourselin, Tom Vercauteren
mathjax: true
---

* content
{:toc}

##### Abstract
The Dice score is widely used for binary segmentation due to its robustness to class imbalance. Soft generalisations of the Dice score allow it to be used as a loss function for training convolutional neural networks (CNN). Although CNNs trained using mean-class Dice score achieve state-of-the-art results on multi-class segmentation, this loss function does neither take advantage of inter-class relationships nor multi-scale information. We argue that an improved loss function should balance misclassifications to favour predictions that are semantically meaningful. This paper investigates these issues in the context of multi-class brain tumour segmentation. Our contribution is threefold. 1) We propose a semantically-informed generalisation of the Dice score for multi-class segmentation based on the Wasserstein distance on the probabilistic label space. 2) We propose a holistic CNN that embeds spatial information at multiple scales with deep supervision. 3) We show that the joint use of holistic CNNs and generalised Wasserstein Dice scores achieves segmentations that are more semantically meaningful for brain tumour segmentation.

##### Abstract (translated by Google)
骰子分数由于其对类不平衡的鲁棒性而被广泛用于二进制分割。骰子得分的软归纳允许其被用作训练卷积神经网络（CNN）的损失函数。虽然使用均值级Dice分数训练的CNN在多级分割上达到了最新的结果，但这种损失函数既不利用类间关系也不利用多尺度信息。我们认为，改进的损失函数应该平衡错误分类，以支持语义上有意义的预测。本文在多级脑肿瘤分割的背景下研究这些问题。我们的贡献是三倍。 1）基于概率标签空间上的Wasserstein距离，我们提出了一种基于语义的多分类分类Dice分数的概括。 2）我们提出一个全面的CNN，在多个尺度上嵌入空间信息并进行深度监督。 3）我们表明，整体CNN和广义Wasserstein Dice分数的联合使用实现了对脑肿瘤分割在语义上更有意义的分割。

##### URL
[https://arxiv.org/abs/1707.00478](https://arxiv.org/abs/1707.00478)

##### PDF
[https://arxiv.org/pdf/1707.00478](https://arxiv.org/pdf/1707.00478)

