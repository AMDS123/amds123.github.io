---
layout: post
title: "Non-linear Convolution Filters for CNN-based Learning"
date: 2017-08-23 15:07:35
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Classification
author: Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas Vretos, Petros Daras
mathjax: true
---

* content
{:toc}

##### Abstract
During the last years, Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in image classification. Their architectures have largely drawn inspiration by models of the primate visual system. However, while recent research results of neuroscience prove the existence of non-linear operations in the response of complex visual cells, little effort has been devoted to extend the convolution technique to non-linear forms. Typical convolutional layers are linear systems, hence their expressiveness is limited. To overcome this, various non-linearities have been used as activation functions inside CNNs, while also many pooling strategies have been applied. We address the issue of developing a convolution method in the context of a computational model of the visual cortex, exploring quadratic forms through the Volterra kernels. Such forms, constituting a more rich function space, are used as approximations of the response profile of visual cells. Our proposed second-order convolution is tested on CIFAR-10 and CIFAR-100. We show that a network which combines linear and non-linear filters in its convolutional layers, can outperform networks that use standard linear filters with the same architecture, yielding results competitive with the state-of-the-art on these datasets.

##### Abstract (translated by Google)
在过去的几年中，卷积神经网络（CNN）在图像分类方面取得了最先进的性能。他们的架构在很大程度上从灵长类动物视觉系统的模型中汲取灵感。然而，尽管最近的神经科学研究结果证明了在复杂视觉细胞的响应中存在非线性操作，但是已经花费很少的努力将卷积技术扩展到非线性形式。典型的卷积层是线性系统，因此它们的表现力是有限的。为了克服这个问题，各种非线性已经被用作CNN内部的激活函数，同时也采用了许多汇集策略。我们解决了在视觉皮层的计算模型的背景下开发卷积方法的问题，通过沃尔泰拉内核探索二次形式。构成更丰富的功能空间的这种形式被用作视觉细胞的响应分布的近似值。我们提出的二阶卷积在CIFAR-10和CIFAR-100上进行了测试。我们表明，在卷积层中结合了线性滤波器和非线性滤波器的网络可以超越使用标准线性滤波器的网络，并且具有与这些数据集的最新技术相媲美的结果。

##### URL
[https://arxiv.org/abs/1708.07038](https://arxiv.org/abs/1708.07038)

##### PDF
[https://arxiv.org/pdf/1708.07038](https://arxiv.org/pdf/1708.07038)

