---
layout: post
title: "Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation"
date: 2018-03-21 03:21:38
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices---We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments, and the relative success rate is increased by 15.5% on the unseen test set.

##### Abstract (translated by Google)
现有的关于机器人导航的视觉和语言接地的研究集中于改进合成环境中的无模型深度强化学习（DRL）模型。然而，无模型DRL模型并不考虑现实环境中的动态，而且往往不能推广到新的场景。在本文中，我们采取了一种激进的方法来弥合合成研究和现实世界实践之间的差距---我们提出了一种新型的，计划先行的混合强化学习模型，该模型将无模型和基于模型的强化学习相结合，以解决真实世界的视觉语言导航任务。我们的预测模块将预见策略模型与预测下一个状态和奖励的环境模型紧密集成。实验结果表明，我们提出的方法明显优于基线，并且在实际的房间到房间数据集上达到最佳效果。而且，当转移到看不见的环境时，我们的可扩展方法更具普遍性，相对成功率提高了15.5％，看不到的测试集。

##### URL
[http://arxiv.org/abs/1803.07729](http://arxiv.org/abs/1803.07729)

##### PDF
[http://arxiv.org/pdf/1803.07729](http://arxiv.org/pdf/1803.07729)

