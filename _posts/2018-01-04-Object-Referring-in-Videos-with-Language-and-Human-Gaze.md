---
layout: post
title: "Object Referring in Videos with Language and Human Gaze"
date: 2018-01-04 23:31:20
categories: arXiv_CV
tags: arXiv_CV
author: Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate the problem of object referring (OR) i.e. to localize a target object in a visual scene coming with a language description. Humans perceive the world more as continued video snippets than as static images, and describe objects not only by their appearance, but also by their temporal-spatial contexts and motion features. Humans also gaze at the object when they issue a referring expression. Existing works for OR mostly focus on static images only, which fall short in providing many such cues. This paper addresses OR in videos with language and human gaze. To that end, we present a new video dataset for OR, with 30, 000 objects over 5, 000 stereo video sequences annotated for their descriptions and gaze. We further propose a novel network model for OR in videos, by integrating appearance, motion, gaze, and spatial-temporal contextual information all into one network. Experimental results shows that our method effectively utilizes motion cues, human gaze, and spatial-temporal context information. Our method outperforms previous OR methods. The dataset and code will be made available.

##### Abstract (translated by Google)
我们研究了对象引用（OR）的问题，即在具有语言描述的视觉场景中定位目标对象。人类认为世界更像视频片段，而不是静态的图像，并且不仅以其外观来描述对象，而且还以其时空背景和运动特征来描述对象。当人们发表一个指称表达时，人类也会注视这个对象。现有的“或”作品主要集中在静态图像上，这在提供很多这样的线索方面是不足的。这篇论文在语言和人类凝视的视频中谈到了“或”。为此，我们为OR提供了一个新的视频数据集，其中3000多个立体视频序列上的30,000个对象被注释用于描述和注视。通过将外观，运动，注视和时空背景信息整合到一个网络中，我们进一步提出了一个新的视频OR模型。实验结果表明，我们的方法有效利用运动线索，人类注视和时空背景信息。我们的方法胜过以前的OR方法。数据集和代码将被提供。

##### URL
[http://arxiv.org/abs/1801.01582](http://arxiv.org/abs/1801.01582)

##### PDF
[http://arxiv.org/pdf/1801.01582](http://arxiv.org/pdf/1801.01582)

