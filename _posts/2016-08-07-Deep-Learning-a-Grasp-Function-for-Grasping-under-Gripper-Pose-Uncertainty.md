---
layout: post
title: "Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty"
date: 2016-08-07 16:30:42
categories: arXiv_CV
tags: arXiv_CV CNN Deep_Learning
author: Edward Johns, Stefan Leutenegger, Andrew J. Davison
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a new method for parallel-jaw grasping of isolated objects from depth images, under large gripper pose uncertainty. Whilst most approaches aim to predict the single best grasp pose from an image, our method first predicts a score for every possible grasp pose, which we denote the grasp function. With this, it is possible to achieve grasping robust to the gripper's pose uncertainty, by smoothing the grasp function with the pose uncertainty function. Therefore, if the single best pose is adjacent to a region of poor grasp quality, that pose will no longer be chosen, and instead a pose will be chosen which is surrounded by a region of high grasp quality. To learn this function, we train a Convolutional Neural Network which takes as input a single depth image of an object, and outputs a score for each grasp pose across the image. Training data for this is generated by use of physics simulation and depth image simulation with 3D object meshes, to enable acquisition of sufficient data without requiring exhaustive real-world experiments. We evaluate with both synthetic and real experiments, and show that the learned grasp score is more robust to gripper pose uncertainty than when this uncertainty is not accounted for.

##### Abstract (translated by Google)
本文提出了一种新的方法，即在大的夹持器姿态不确定的情况下，从深度图像中抓取孤立的物体。虽然大多数方法的目标是从图像中预测单个最佳抓握姿势，但是我们的方法首先预测每个可能的抓握姿势的得分，其表示抓握功能。由此，通过利用姿态不确定性函数来平滑抓握功能，可以实现抓取器的姿态不确定性的鲁棒性。因此，如果单个最佳姿态与一个抓握质量较差的区域相邻，则不再选择该姿态，而是选择一个被抓握质量高的区域包围的姿势。为了学习这个功能，我们训练一个卷积神经网络，它将一个物体的单个深度图像作为输入，并输出图像上每个抓取姿势的得分。为此的训练数据是通过使用3D对象网格的物理模拟和深度图像模拟产生的，以使得能够获取足够的数据，而不需要详尽的真实世界的实验。我们评估与合成和真实的实验，并表明，学习的把握分数更强大的夹具姿态的不确定性比不确定性。

##### URL
[https://arxiv.org/abs/1608.02239](https://arxiv.org/abs/1608.02239)

##### PDF
[https://arxiv.org/pdf/1608.02239](https://arxiv.org/pdf/1608.02239)

