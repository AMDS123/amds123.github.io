---
layout: post
title: "Did you hear that? Adversarial Examples Against Automatic Speech Recognition"
date: 2018-01-02 05:24:30
categories: arXiv_CL
tags: arXiv_CL Adversarial Object_Detection Speech_Recognition Classification Deep_Learning Detection Recognition
author: Moustafa Alzantot, Bharathan Balaji, Mani Srivastava
mathjax: true
---

* content
{:toc}

##### Abstract
Speech is a common and effective way of communication between humans, and modern consumer devices such as smartphones and home hubs are equipped with deep learning based accurate automatic speech recognition to enable natural interaction between humans and machines. Recently, researchers have demonstrated powerful attacks against machine learning models that can fool them to produceincorrect results. However, nearly all previous research in adversarial attacks has focused on image recognition and object detection models. In this short paper, we present a first of its kind demonstration of adversarial attacks against speech classification model. Our algorithm performs targeted attacks with 87% success by adding small background noise without having to know the underlying model parameter and architecture. Our attack only changes the least significant bits of a subset of audio clip samples, and the noise does not change 89% the human listener's perception of the audio clip as evaluated in our human study.

##### Abstract (translated by Google)
语音是人与人之间沟通的一种常用而有效的方式，而智能手机和家庭中心等现代消费设备配备了深度学习的准确自动语音识别，使人与机器之间能够自然交互。最近，研究人员展示了强大的攻击机器学习模型，可以骗他们产生正确的结果。然而，几乎所有的对抗攻击研究都集中在图像识别和对象检测模型上。在这篇短文中，我们首先展示了对抗语音分类模型的对抗性攻击。我们的算法通过增加小的背景噪声来执行有针对性的攻击，成功率为87％，而不必知道底层模型参数和体系结构。我们的攻击只会改变音频剪辑样本子集的最低有效位，并且噪声不会改变89％的人类听众对音频剪辑的感知，正如我们在人类研究中所评估的那样。

##### URL
[http://arxiv.org/abs/1801.00554](http://arxiv.org/abs/1801.00554)

##### PDF
[http://arxiv.org/pdf/1801.00554](http://arxiv.org/pdf/1801.00554)

