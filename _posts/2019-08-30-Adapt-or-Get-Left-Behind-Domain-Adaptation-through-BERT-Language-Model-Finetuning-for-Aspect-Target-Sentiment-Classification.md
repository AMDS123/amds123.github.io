---
layout: post
title: "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification"
date: 2019-08-30 17:44:30
categories: arXiv_CL
tags: arXiv_CL Sentiment Review Sentiment_Classification Classification Language_Model
author: Alexander Rietzler, Sebastian Stabinger, Paul Opitz, Stefan Engl
mathjax: true
---

* content
{:toc}

##### Abstract
Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based Sentiment Analysis (ABSA), which has many applications e.g. in e-commerce, where data and insights from reviews can be leveraged to create value for businesses and customers. Recently, deep transfer-learning methods have been applied successfully to a myriad of Natural Language Processing (NLP) tasks, including ATSC. Building on top of the prominent the BERT language model, we approach ATSC by using a two-step procedure: Self-supervised domain-specific BERT language model finetuning, followed by supervised task-specific finetuning. Our findings on how to best exploit domain-specific language model finetuning enables us to produce new state-of-the-art performance on the SemEval 2014 Task 4 restaurants dataset. In addition, to explore the real-world robustness of our models, we perform cross-domain evaluation. We show that a cross-domain adapted BERT language model performs significantly better compared to strong baseline models like vanilla BERT-base and XLNet-base.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.11860](http://arxiv.org/abs/1908.11860)

##### PDF
[http://arxiv.org/pdf/1908.11860](http://arxiv.org/pdf/1908.11860)

