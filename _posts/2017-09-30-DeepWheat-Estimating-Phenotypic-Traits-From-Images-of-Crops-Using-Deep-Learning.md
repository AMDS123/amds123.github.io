---
layout: post
title: "DeepWheat: Estimating Phenotypic Traits From Images of Crops Using Deep Learning"
date: 2017-09-30 18:31:53
categories: arXiv_CV
tags: arXiv_CV Knowledge Segmentation CNN Deep_Learning
author: Shubhra Aich, Imran Ahmed, Ilya Obsyannikov, Ian Stavness, Anique Josuttes, Keegan Strueby, Hema Sudhakar Duddu, Curtis Pozniak, Steve Shirtliffe
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we investigate the problem of estimating the phenotypic traits of plants from color images and elevation maps of field plots. We focus on emergence and biomass traits - two important indicators of crop growth and health. We employ a state-of-the-art deconvolutional network for segmentation and convolutional architectures, with residual learning in the final stages, for trait estimation. Our intention was to design estimation architectures that behave like high dimensional nonlinear regression models. To the best of our knowledge, this is the first work on emergence counting and biomass estimation based on deep learning. Evaluation was performed on two different species of wheat, grown in field plots for an experimental plant breeding study. Our framework achieves satisfactory performance with mean and standard deviation of absolute difference of 1.20 and 1.53 counts for emergence and 1.45 and 2.05 for biomass estimation. Our results for counting wheat plants from field images is comparable to the accuracy reported for the similar, but arguably less difficult, task of counting leaves from pictures of rosettes grown in pots. Our results for biomass estimation improve upon all previously proposed approaches in the literature.

##### Abstract (translated by Google)
在本文中，我们调查从彩色图像和实地地形的高程图估计植物表型性状的问题。我们关注出苗和生物量特征 - 作物生长和健康的两个重要指标。我们采用最先进的去卷积网络进行分割和卷积架构，并在最后阶段进行残差学习，以进行特征估计。我们的目的是设计像高维非线性回归模型那样的估计体系结构。就我们所知，这是第一个基于深度学习的涌现计数和生物量估计的工作。在两个不同种类的小麦上进行评估，在实地植物育种研究中进行田间试验。我们的框架取得了令人满意的表现，绝对差异的平均值和标准差为1.20和1.53计数出现，生物量估计为1.45和2.05。我们从田间图像中计算小麦植物的结果与类似的报告的精确度相当，但是可以说不那么困难，从花盆中生长的玫瑰花图片计算叶片的任务。我们的生物量估计结果改善了文献中以前提出的所有方法。

##### URL
[https://arxiv.org/abs/1710.00241](https://arxiv.org/abs/1710.00241)

##### PDF
[https://arxiv.org/pdf/1710.00241](https://arxiv.org/pdf/1710.00241)

