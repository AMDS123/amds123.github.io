---
layout: post
title: "On Numerosity of Deep Convolutional Neural Networks"
date: 2018-07-11 03:14:50
categories: arXiv_CV
tags: arXiv_CV Knowledge CNN
author: Xiaolin Wu, Xi Zhang, Xiao Shu
mathjax: true
---

* content
{:toc}

##### Abstract
Subitizing, or the sense of small natural numbers, is a cognitive construct so primary and critical to the survival and well-being of humans and primates that is considered and proven to be innate; it responds to visual stimuli prior to the development of any symbolic skills, language or arithmetic. Given highly acclaimed successes of deep convolutional neural networks (DCNN) in tasks of visual intelligence, one would expect that DCNNs can learn subitizing. But somewhat surprisingly, our carefully crafted extensive experiments, which are similar to those of cognitive psychology, demonstrate that DCNNs cannot, even with strong supervision, see through superficial variations in visual representations and distill the abstract notion of natural number, a task that children perform with high accuracy and confidence. The DCNN black box learners driven by very large training sets are apparently still confused by geometric variations and fail to grasp the topological essence in subitizing. In sharp contrast to the failures of the black box learning, by incorporating a mechanism of mathematical morphology into convolutional kernels, we are able to construct a recurrent convolutional neural network that can perform subitizing deterministically. Our findings in this study of cognitive computing, without and with prior of human knowledge, are discussed; they are, we believe, significant and thought-provoking in the interests of AI research, because visual-based numerosity is a benchmark of minimum sort for human cognition.

##### Abstract (translated by Google)
次要化或小自然数的感觉是一种认知结构，对人类和灵长类动物的生存和福祉至关重要且被认为是天生的;它在任何符号技能，语言或算术的发展之前响应视觉刺激。鉴于深度卷积神经网络（DCNN）在视觉智能任务中获得高度评价的成功，人们可以预期DCNN可以学习次级化。但有些令人惊讶的是，我们精心设计的广泛实验，与认知心理学相似，证明DCNN即使在强有力的监督下也不能通过视觉表征的表面变化来看待并提炼出自然数的抽象概念，这是儿童所执行的任务。高精度和自信。由非常大的训练集驱动的DCNN黑匣子学习者显然仍然被几何变化所迷惑，并且未能掌握子化中的拓扑本质。与黑盒学习的失败形成鲜明对比的是，通过将数学形态学机制纳入卷积核，我们能够构建一个可以确定性地执行子化的循环卷积神经网络。我们在这项关于认知计算的研究中发现了我们在人类知识之前和之前的研究结果;我们认为，对于人工智能研究而言，它们具有重大意义和发人深省的意义，因为基于视觉的数字化是人类认知最小化的基准。

##### URL
[http://arxiv.org/abs/1802.05160](http://arxiv.org/abs/1802.05160)

##### PDF
[http://arxiv.org/pdf/1802.05160](http://arxiv.org/pdf/1802.05160)

