---
layout: post
title: "Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities"
date: 2017-01-16 22:11:51
categories: arXiv_SD
tags: arXiv_SD Knowledge Embedding Deep_Learning Language_Model
author: Yadollah Yaghoobzadeh, Hinrich Schütze
mathjax: true
---

* content
{:toc}

##### Abstract
Entities are essential elements of natural language. In this paper, we present methods for learning multi-level representations of entities on three complementary levels: character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for deep learning models, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level; for word2vec (Mikolov et al., 2013) on the word level; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of representation contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities.

##### Abstract (translated by Google)
实体是自然语言的基本要素。在本文中，我们提出了在三个互补层面学习实体的多级表示的方法：字符（实体名称中的字符模式，例如通过神经网络提取），单词（实体名称中的单词嵌入）和实体（实体嵌入）。我们调查了每个级别的最新学习方法，并且在角色层面上发现了很大的差异，例如深度学习模型，传统的ngram特征和fasttext的子词模型（Bojanowski等，2016）。对于word2vec（Mikolov等，2013）在单词级别;以及在实体层面上的订单感知模型wang2vec（Ling et al。，2015a）。我们通过实验确认每个表示级别都有助于补充信息，并且所有三个级别的联合表示都大大改善了现有的基于嵌入的细粒度实体类型基线。另外，我们还表明，从实体描述中添加信息进一步改进了实体的多级表示。

##### URL
[https://arxiv.org/abs/1701.02025](https://arxiv.org/abs/1701.02025)

##### PDF
[https://arxiv.org/pdf/1701.02025](https://arxiv.org/pdf/1701.02025)

