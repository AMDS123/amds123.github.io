---
layout: post
title: "Trivial Transfer Learning for Low-Resource Neural Machine Translation"
date: 2018-09-02 15:24:15
categories: arXiv_CL
tags: arXiv_CL Transfer_Learning
author: Tom Kocmi, Ond&#x159;ej Bojar
mathjax: true
---

* content
{:toc}

##### Abstract
Transfer learning has been proven as an effective technique for neural machine translation under low-resource conditions. Existing methods require a common target language, language relatedness, or specific training tricks and regimes. We present a simple transfer learning method, where we first train a "parent" model for a high-resource language pair and then continue the training on a lowresource pair only by replacing the training corpus. This "child" model performs significantly better than the baseline trained for lowresource pair only. We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.

##### Abstract (translated by Google)
转移学习已被证明是在低资源条件下进行神经机器翻译的有效技术。现有方法需要共同的目标语言，语言相关性或特定的训练技巧和制度。我们提出了一种简单的转移学习方法，我们首先为高资源语言对训练“父”模型，然后仅通过替换训练语料库继续对低资源对进行训练。这种“子”模型的性能明显优于仅针对低资源对训练的基线。我们是第一个针对不同语言展示这一点的人，我们观察到即使是使用不同字母表的不相关语言也会有所改进。

##### URL
[http://arxiv.org/abs/1809.00357](http://arxiv.org/abs/1809.00357)

##### PDF
[http://arxiv.org/pdf/1809.00357](http://arxiv.org/pdf/1809.00357)

