---
layout: post
title: "Wasserstein Divergence for GANs"
date: 2018-09-05 12:41:21
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Optimization Quantitative
author: Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, Luc Van Gool
mathjax: true
---

* content
{:toc}

##### Abstract
In many domains of computer vision, generative adversarial networks (GANs) have achieved great success, among which the family of Wasserstein GANs (WGANs) is considered to be state-of-the-art due to the theoretical contributions and competitive qualitative performance. However, it is very challenging to approximate the $k$-Lipschitz constraint required by the Wasserstein-1 metric~(W-met). In this paper, we propose a novel Wasserstein divergence~(W-div), which is a relaxed version of W-met and does not require the $k$-Lipschitz constraint. As a concrete application, we introduce a Wasserstein divergence objective for GANs~(WGAN-div), which can faithfully approximate W-div through optimization. Under various settings, including progressive growing training, we demonstrate the stability of the proposed WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we study the quantitative and visual performance of WGAN-div on standard image synthesis benchmarks of computer vision, showing the superior performance of WGAN-div compared to the state-of-the-art methods.

##### Abstract (translated by Google)
在计算机视觉的许多领域中，生成对抗网络（GAN）取得了巨大成功，其中Wasserstein GAN（WGAN）家族由于理论贡献和竞争性定性表现而被认为是最先进的。然而，近似Wasserstein-1 metric~（W-met）所需的$ k $ -Lipschitz约束是非常具有挑战性的。在本文中，我们提出了一个新的Wasserstein散度〜（W-div），它是W-met的宽松版本，不需要$ k $ -Lipschitz约束。作为一个具体的应用，我们引入了GANs~（WGAN-div）的Wasserstein发散目标，它可以通过优化忠实地逼近W-div。在各种环境下，包括渐进式培养，我们展示了WGAN-div的稳定性，因为它具有优于WGAN的理论和实践优势。此外，我们研究了WGAN-div在计算机视觉的标准图像合成基准上的定量和视觉性能，显示了WGAN-div与最先进方法相比的卓越性能。

##### URL
[http://arxiv.org/abs/1712.01026](http://arxiv.org/abs/1712.01026)

##### PDF
[http://arxiv.org/pdf/1712.01026](http://arxiv.org/pdf/1712.01026)

