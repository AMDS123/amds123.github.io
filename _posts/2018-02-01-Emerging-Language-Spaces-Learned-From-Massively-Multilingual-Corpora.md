---
layout: post
title: "Emerging Language Spaces Learned From Massively Multilingual Corpora"
date: 2018-02-01 12:58:16
categories: arXiv_CL
tags: arXiv_CL Relation
author: J&#xf6;rg Tiedemann
mathjax: true
---

* content
{:toc}

##### Abstract
Translations capture important information about languages that can be used as implicit supervision in learning linguistic properties and semantic representations. In an information-centric view, translated texts may be considered as semantic mirrors of the original text and the significant variations that we can observe across various languages can be used to disambiguate a given expression using the linguistic signal that is grounded in translation. Parallel corpora consisting of massive amounts of human translations with a large linguistic variation can be applied to increase abstractions and we propose the use of highly multilingual machine translation models to find language-independent meaning representations. Our initial experiments show that neural machine translation models can indeed learn in such a setup and we can show that the learning algorithm picks up information about the relation between languages in order to optimize transfer leaning with shared parameters. The model creates a continuous language space that represents relationships in terms of geometric distances, which we can visualize to illustrate how languages cluster according to language families and groups. Does this open the door for new ideas of data-driven language typology with promising models and techniques in empirical cross-linguistic research?

##### Abstract (translated by Google)
翻译捕捉有关语言的重要信息，在学习语言属性和语义表示时可以用作隐式监督。在以信息为中心的观点中，翻译后的文本可以被认为是原文的语义镜像，我们可以通过各种语言观察到的重要变化可以用于使用基于翻译的语言信号来消除给定表达的歧义。由大量具有大量语言变体的人类翻译组成的平行语料库可用于增加抽象，并且我们建议使用高度多语言的机器翻译模型来找到与语言无关的含义表示。我们最初的实验表明，神经机器翻译模型确实可以在这样的设置中学习，并且我们可以证明学习算法获取关于语言之间的关系的信息，以优化具有共享参数的传输。该模型创建了一个连续的语言空间，用几何距离表示关系，我们可以通过这个空间可视化来说明语言如何根据语言族和群组进行聚类。这是否为数据驱动的语言类型学的新思路打开了大门，在经验的跨语言学研究中有着有希望的模型和技术？

##### URL
[http://arxiv.org/abs/1802.00273](http://arxiv.org/abs/1802.00273)

##### PDF
[http://arxiv.org/pdf/1802.00273](http://arxiv.org/pdf/1802.00273)

