---
layout: post
title: "Which phoneme-to-viseme maps best improve visual-only computer lip-reading?"
date: 2017-10-03 11:44:40
categories: arXiv_CV
tags: arXiv_CV Speech_Recognition Recognition
author: Helen L. Bear, Richard W. Harvey, Barry-John Theobald, Yuxuan Lan
mathjax: true
---

* content
{:toc}

##### Abstract
A critical assumption of all current visual speech recognition systems is that there are visual speech units called visemes which can be mapped to units of acoustic speech, the phonemes. Despite there being a number of published maps it is infrequent to see the effectiveness of these tested, particularly on visual-only lip-reading (many works use audio-visual speech). Here we examine 120 mappings and consider if any are stable across talkers. We show a method for devising maps based on phoneme confusions from an automated lip-reading system, and we present new mappings that show improvements for individual talkers.

##### Abstract (translated by Google)
所有当前的视觉语音识别系统的一个关键假设是，存在称为视位的可视语音单元，其可以被映射到声学语音单元，音素。尽管有许多已发表的地图，但经常看到这些测试的效果，特别是在仅有视觉的唇读（许多作品使用视听说话）方面。在这里，我们研究了120个映射，并考虑了是否有任何人在谈话者中保持稳定。我们展示了一种基于来自自动唇读系统的音素混淆来设计地图的方法，并且我们提出了新的映射，其显示了对于个人讲话者的改进。

##### URL
[https://arxiv.org/abs/1710.01093](https://arxiv.org/abs/1710.01093)

##### PDF
[https://arxiv.org/pdf/1710.01093](https://arxiv.org/pdf/1710.01093)

