---
layout: post
title: "Overcoming Challenges in Fixed Point Training of Deep Convolutional Networks"
date: 2016-07-08 06:07:03
categories: arXiv_CV
tags: arXiv_CV Knowledge CNN Gradient_Descent
author: Darryl D. Lin, Sachin S. Talathi
mathjax: true
---

* content
{:toc}

##### Abstract
It is known that training deep neural networks, in particular, deep convolutional networks, with aggressively reduced numerical precision is challenging. The stochastic gradient descent algorithm becomes unstable in the presence of noisy gradient updates resulting from arithmetic with limited numeric precision. One of the well-accepted solutions facilitating the training of low precision fixed point networks is stochastic rounding. However, to the best of our knowledge, the source of the instability in training neural networks with noisy gradient updates has not been well investigated. This work is an attempt to draw a theoretical connection between low numerical precision and training algorithm stability. In doing so, we will also propose and verify through experiments methods that are able to improve the training performance of deep convolutional networks in fixed point.

##### Abstract (translated by Google)
众所周知，训练深度神经网络，尤其是深度卷积网络，积极降低数值精度是具有挑战性的。随机梯度下降算法在由于数值精度有限的算术而产生噪声梯度更新时变得不稳定。促进低精度定点网络训练的公认解决方案之一是随机舍入。然而，就我们所知，在噪声梯度更新训练神经网络中不稳定性的来源尚未得到很好的研究。这项工作是试图在低数值精度和训练算法稳定性之间建立理论联系。为此，我们还将通过实验方法提出并验证能够提高深度卷积网络定点训练性能的方法。

##### URL
[https://arxiv.org/abs/1607.02241](https://arxiv.org/abs/1607.02241)

##### PDF
[https://arxiv.org/pdf/1607.02241](https://arxiv.org/pdf/1607.02241)

