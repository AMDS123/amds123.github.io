---
layout: post
title: "Multi-Step Prediction of Dynamic Systems with Recurrent Neural Networks"
date: 2018-05-20 00:37:10
categories: arXiv_RO
tags: arXiv_RO RNN Prediction
author: Nima Mohajerin, Steven L. Waslander
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent Neural Networks (RNNs) can encode rich dynamics which makes them suitable for modeling dynamic systems. To train an RNN for multi-step prediction of dynamic systems, it is crucial to efficiently address the state initialization problem, which seeks proper values for the RNN initial states at the beginning of each prediction interval. In this work, the state initialization problem is addressed using Neural Networks (NNs) to effectively train a variety of RNNs for modeling two aerial vehicles, a helicopter and a quadrotor, from experimental data. It is shown that the RNN initialized by the NN-based initialization method outperforms the state of the art. Further, a comprehensive study of RNNs trained for multi-step prediction of the two aerial vehicles is presented. The multi-step prediction of the quadrotor is enhanced using a hybrid model which combines a simplified physics-based motion model of the vehicle with RNNs. While the maximum translational and rotational velocities in the quadrotor dataset are about 4 m/s and 3.8 rad/s, respectively, the hybrid model produces predictions, over 1.9 second, which remain within 9 cm/s and 0.12 rad/s of the measured translational and rotational velocities, with 99\% confidence on the test dataset

##### Abstract (translated by Google)
递归神经网络（RNN）可编码丰富的动态特性，使其适用于建模动态系统。为了训练RNN用于动态系统的多步预测，有效解决状态初始化问题至关重要，该问题在每个预测间隔开始时寻找RNN初始状态的适当值。在这项工作中，使用神经网络（NN）来解决状态初始化问题，以从实验数据中有效训练用于对两架飞行器，直升机和四旋翼飞行器进行建模的各种RNN。结果表明，基于NN的初始化方法初始化的RNN优于现有技术。此外，还介绍了对两架飞行器的多步预测进行训练的RNN的综合研究。使用将车辆的简化的基于物理学的运动模型与RNN相结合的混合模型来增强四旋翼飞行器的多步预测。虽然四旋翼数据集中最大平移和旋转速度分别约为4 m / s和3.8 rad / s，但混合模型产生的预测时间超过1.9秒，仍然保持在9 cm / s和0.12 rad / s的测量范围内平移和旋转速度，对测试数据集具有99％的置信度

##### URL
[http://arxiv.org/abs/1806.00526](http://arxiv.org/abs/1806.00526)

##### PDF
[http://arxiv.org/pdf/1806.00526](http://arxiv.org/pdf/1806.00526)

