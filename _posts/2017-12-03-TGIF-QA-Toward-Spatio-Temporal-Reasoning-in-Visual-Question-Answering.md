---
layout: post
title: "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"
date: 2017-12-03 04:46:42
categories: arXiv_CV
tags: arXiv_CV QA Attention RNN VQA
author: Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim
mathjax: true
---

* content
{:toc}

##### Abstract
Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.

##### Abstract (translated by Google)
视觉和语言理解已成为人工智能领域深入研究的课题。在这一系列研究中的许多任务中，视觉问答（VQA）是最成功的任务之一，其目标是学习一个理解区域级细节的视觉内容的模型，并发现它们与问题和以自然语言形式回答。尽管在过去几年中取得了快速进展，但VQA中的大多数现有工作主要集中在图像上。在本文中，我们专注于将VQA扩展到视频领域，并以三个重要方式为文献做出贡献。首先，我们提出了三个专门为视频VQA设计的新任务，这些任务需要视频的时空推理才能正确回答问题。接下来，我们为视频VQA引入了一个新的大型数据集，名为TGIF-QA，它通过我们的新任务扩展了现有的VQA工作。最后，我们提出了一种基于双LSTM的空间和时间关注方法​​，并通过实证评估显示其优于传统VQA技术的有效性。

##### URL
[https://arxiv.org/abs/1704.04497](https://arxiv.org/abs/1704.04497)

##### PDF
[https://arxiv.org/pdf/1704.04497](https://arxiv.org/pdf/1704.04497)

