---
layout: post
title: "Level Playing Field for Million Scale Face Recognition"
date: 2017-05-01 01:04:53
categories: arXiv_CV
tags: arXiv_CV Face Recognition Face_Recognition
author: Aaron Nech, Ira Kemelmacher-Shlizerman
mathjax: true
---

* content
{:toc}

##### Abstract
Face recognition has the perception of a solved problem, however when tested at the million-scale exhibits dramatic variation in accuracies across the different algorithms. Are the algorithms very different? Is access to good/big training data their secret weapon? Where should face recognition improve? To address those questions, we created a benchmark, MF2, that requires all algorithms to be trained on same data, and tested at the million scale. MF2 is a public large-scale set with 672K identities and 4.7M photos created with the goal to level playing field for large scale face recognition. We contrast our results with findings from the other two large-scale benchmarks MegaFace Challenge and MS-Celebs-1M where groups were allowed to train on any private/public/big/small set. Some key discoveries: 1) algorithms, trained on MF2, were able to achieve state of the art and comparable results to algorithms trained on massive private sets, 2) some outperformed themselves once trained on MF2, 3) invariance to aging suffers from low accuracies as in MegaFace, identifying the need for larger age variations possibly within identities or adjustment of algorithms in future testings.

##### Abstract (translated by Google)
人脸识别具有解决问题的感觉，但是当以百万级进行测试时，在不同的算法中准确度显着变化。算法是非常不同的？获得好/大训练数据是他们的秘密武器吗？脸部识别应该在哪里改进？为了解决这些问题，我们创建了一个基准测试平台MF2，要求所有的算法都使用相同的数据进行训练，并以百万级的速度进行测试。 MF2是一个公开的大规模集合，具有672K的身份和4.7M的照片，旨在为大规模人脸识别提供公平的竞争环境。我们将我们的结果与另外两个大型基准测试MegaFace Challenge和MS-Celebs-1M的结果进行了对比，在这些测试中，小组被允许在任何私人/公共/大/小组训练。一些重要的发现：1）在MF2上训练的算法能够达到现有技术水平，并且可以与在大规模私人集合上训练的算法相比较，2）一旦在MF2上训练，一些性能优于自身3）不变性老化遭受低精度就像在MegaFace中一样，在未来的测试中，可能会在身份识别或者算法调整的情况下，确定是否需要更大的年龄变化。

##### URL
[https://arxiv.org/abs/1705.00393](https://arxiv.org/abs/1705.00393)

##### PDF
[https://arxiv.org/pdf/1705.00393](https://arxiv.org/pdf/1705.00393)

