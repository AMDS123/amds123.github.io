---
layout: post
title: "Where is the Information in a Deep Neural Network?"
date: 2019-05-29 04:38:54
categories: arXiv_AI
tags: arXiv_AI Relation
author: Alessandro Achille, Stefano Soatto
mathjax: true
---

* content
{:toc}

##### Abstract
Whatever information a Deep Neural Network has gleaned from past data is encoded in its weights. How this information affects the response of the network to future data is largely an open question. In fact, even how to define and measure information in a network is still not settled. We introduce the notion of Information in the Weights as the optimal trade-off between accuracy of the network and complexity of the weights, relative to a prior. Depending on the prior, the definition reduces to known information measures such as Shannon Mutual Information and Fisher Information, but affords added flexibility that enables us to relate it to generalization, via the PAC-Bayes bound, and to invariance. This relation hinges not only on the architecture of the model, but surprisingly on how it is trained. We then introduce a notion of effective information in the activations, which are deterministic functions of future inputs, resolving inconsistencies in prior work. We relate this to the Information in the Weights, and use this result to show that models of low complexity not only generalize better, but are bound to learn invariant representations of future inputs.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.12213](http://arxiv.org/abs/1905.12213)

##### PDF
[http://arxiv.org/pdf/1905.12213](http://arxiv.org/pdf/1905.12213)

