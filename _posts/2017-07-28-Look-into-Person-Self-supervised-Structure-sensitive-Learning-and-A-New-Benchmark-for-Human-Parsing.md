---
layout: post
title: "Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing"
date: 2017-07-28 01:41:39
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, Liang Lin
mathjax: true
---

* content
{:toc}

##### Abstract
Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark "Look into Person (LIP)" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method.

##### Abstract (translated by Google)
人类解析因其巨大的应用潜力而近来引起了许多研究兴趣。然而，现有的数据集的图像和注释数量有限，在无约束的环境下，缺乏人性化的外表和挑战性的案例。在本文中，我们引入了一个新的基准“看人（LIP）”，在可扩展性，多样性和难度方面取得了显着的进步，我们认为这对未来在以人为本的分析方面的发展至关重要。这个全面的数据集包含超过50,000个精心注释的图像，包含19个语义部分标签，从更广泛的视点，遮挡和背景复杂度中捕获。鉴于这些丰富的注释，我们对主要的人类解析方法进行了详细的分析，深入了解这些方法的成功和失败。此外，与现有的改进特征判别能力的努力相反，我们通过探索一种新的自监督结构敏感学习方法来解决人类解析，这种方法将人体姿态结构强加于解析结果，而不需要额外的监督（即不需要额外的监督在模型训练中特别标记人体关节）。我们的自我监督学习框架可以注入到任何先进的神经网络中，从全局角度帮助整合丰富的关于人体关节的高级知识，并改善解析结果。对我们的LIP和公共PASCAL-Person-Part数据集的广泛评估证明了我们方法的优越性。

##### URL
[https://arxiv.org/abs/1703.05446](https://arxiv.org/abs/1703.05446)

##### PDF
[https://arxiv.org/pdf/1703.05446](https://arxiv.org/pdf/1703.05446)

