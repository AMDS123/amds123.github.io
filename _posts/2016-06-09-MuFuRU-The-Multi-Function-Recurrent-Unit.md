---
layout: post
title: "MuFuRU: The Multi-Function Recurrent Unit"
date: 2016-06-09 15:41:17
categories: arXiv_CL
tags: arXiv_CL Sentiment RNN Language_Model
author: Dirk Weissenborn, Tim Rocktäschel
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an input- and state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis.

##### Abstract (translated by Google)
像GRU和LSTM这样的递归神经网络在自然语言处理中被广泛采用，并为许多任务获得了最新的结果。这些模型的特点是可以通过将门控组合操作应用到当前输入和以前的状态来写入和读取内存状态。但是，它们只涵盖了一小部分可能有用的组合物。我们提出允许任意可微函数作为合成操作的多功能循环单元（MuFuRUs）。此外，MuFuRUs允许根据输入和状态选择这些组合操作。我们的实验表明，附加功能有助于不同的序列建模任务，包括命题逻辑公式的评估，语言建模和情感分析。

##### URL
[https://arxiv.org/abs/1606.03002](https://arxiv.org/abs/1606.03002)

##### PDF
[https://arxiv.org/pdf/1606.03002](https://arxiv.org/pdf/1606.03002)

