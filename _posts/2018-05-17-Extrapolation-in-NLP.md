---
layout: post
title: "Extrapolation in NLP"
date: 2018-05-17 08:29:09
categories: arXiv_CL
tags: arXiv_CL Attention Language_Model
author: Jeff Mitchell, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
mathjax: true
---

* content
{:toc}

##### Abstract
We argue that extrapolation to examples outside the training space will often be easier for models that capture global structures, rather than just maximise their local fit to the training data. We show that this is true for two popular models: the Decomposable Attention Model and word2vec.

##### Abstract (translated by Google)
我们认为，对于捕捉全球结构的模型而言，对训练空间以外的例子的外推通常会更容易，而不是最大限度地将它们与训练数据的本地适应性最大化。我们证明这对于两种流行的模型是正确的：可分解注意模型和word2vec。

##### URL
[http://arxiv.org/abs/1805.06648](http://arxiv.org/abs/1805.06648)

##### PDF
[http://arxiv.org/pdf/1805.06648](http://arxiv.org/pdf/1805.06648)

