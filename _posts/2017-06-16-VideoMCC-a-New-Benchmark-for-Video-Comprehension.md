---
layout: post
title: "VideoMCC: a New Benchmark for Video Comprehension"
date: 2017-06-16 19:50:46
categories: arXiv_CV
tags: arXiv_CV Video_Caption GAN Caption
author: Du Tran, Maksim Bolonkin, Manohar Paluri, Lorenzo Torresani
mathjax: true
---

* content
{:toc}

##### Abstract
While there is overall agreement that future technology for organizing, browsing and searching videos hinges on the development of methods for high-level semantic understanding of video, so far no consensus has been reached on the best way to train and assess models for this task. Casting video understanding as a form of action or event categorization is problematic as it is not fully clear what the semantic classes or abstractions in this domain should be. Language has been exploited to sidestep the problem of defining video categories, by formulating video understanding as the task of captioning or description. However, language is highly complex, redundant and sometimes ambiguous. Many different captions may express the same semantic concept. To account for this ambiguity, quantitative evaluation of video description requires sophisticated metrics, whose performance scores are typically hard to interpret by humans. This paper provides four contributions to this problem. First, we formulate Video Multiple Choice Caption (VideoMCC) as a new well-defined task with an easy-to-interpret performance measure. Second, we describe a general semi-automatic procedure to create benchmarks for this task. Third, we publicly release a large-scale video benchmark created with an implementation of this procedure and we include a human study that assesses human performance on our dataset. Finally, we propose and test a varied collection of approaches on this benchmark for the purpose of gaining a better understanding of the new challenges posed by video comprehension.

##### Abstract (translated by Google)
虽然大家普遍认为未来的视频组织，浏览和搜索技术取决于视频高层次语义理解方法的发展，但迄今为止还没有达成共识来培养和评估这个任务的模型。将视频理解作为一种行为或事件分类的形式是有问题的，因为它不完全清楚该领域中的语义类别或抽象应该是什么。语言已经被用来回避定义视频类别的问题，把视频理解作为字幕或描述的任务。但是，语言是非常复杂的，多余的，有时是模棱两可的。许多不同的标题可以表达相同的语义概念。为了解决这种模糊性，视频描述的定量评估需要复杂的度量标准，其性能评分通常难以被人类解读。本文提供了四个对这个问题的贡献。首先，我们制定视频多选标题（VideoMCC）作为一个新的明确的任务与一个易于解释的性能测量。其次，我们描述一个通用的半自动程序来为这个任务创建基准。第三，我们公开发布了一个大规模的视频基准，这个基准是通过实施这个程序而创建的，我们还包括一个人类研究，在人类的数据集上评估人类的表现。最后，为了更好地理解视频理解所带来的新挑战，我们提出并测试了这个基准的各种方法。

##### URL
[https://arxiv.org/abs/1606.07373](https://arxiv.org/abs/1606.07373)

