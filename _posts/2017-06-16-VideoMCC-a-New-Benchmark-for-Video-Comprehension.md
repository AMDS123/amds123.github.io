---
layout: post
title: "VideoMCC: a New Benchmark for Video Comprehension"
date: 2017-06-16 19:50:46
categories: arXiv_CV
tags: arXiv_CV Video_Caption GAN Caption Quantitative
author: Du Tran, Maksim Bolonkin, Manohar Paluri, Lorenzo Torresani
mathjax: true
---

* content
{:toc}

##### Abstract
While there is overall agreement that future technology for organizing, browsing and searching videos hinges on the development of methods for high-level semantic understanding of video, so far no consensus has been reached on the best way to train and assess models for this task. Casting video understanding as a form of action or event categorization is problematic as it is not fully clear what the semantic classes or abstractions in this domain should be. Language has been exploited to sidestep the problem of defining video categories, by formulating video understanding as the task of captioning or description. However, language is highly complex, redundant and sometimes ambiguous. Many different captions may express the same semantic concept. To account for this ambiguity, quantitative evaluation of video description requires sophisticated metrics, whose performance scores are typically hard to interpret by humans. This paper provides four contributions to this problem. First, we formulate Video Multiple Choice Caption (VideoMCC) as a new well-defined task with an easy-to-interpret performance measure. Second, we describe a general semi-automatic procedure to create benchmarks for this task. Third, we publicly release a large-scale video benchmark created with an implementation of this procedure and we include a human study that assesses human performance on our dataset. Finally, we propose and test a varied collection of approaches on this benchmark for the purpose of gaining a better understanding of the new challenges posed by video comprehension.

##### Abstract (translated by Google)
虽然总体上一致认为未来的视频组织，浏览和搜索技术取决于视频高级语义理解方法的发展，但迄今尚未就培养和评估此任务模型的最佳方法达成共识。将视频理解视为一种动作或事件分类形式是有问题的，因为它不完全清楚该域中的语义类或抽象应该是什么。通过将视频理解视为字幕或描述的任务，已经利用语言来回避定义视频类别的问题。但是，语言非常复杂，多余，有时含糊不清。许多不同的标题可以表达相同的语义概念。为了解释这种模糊性，视频描述的定量评估需要复杂的指标，其性能得分通常难以被人类解释。本文为这个问题提供了四个贡献。首先，我们将视频多选标题（VideoMCC）制定为一个新的明确定义的任务，具有易于理解的性能测量。其次，我们描述了为此任务创建基准的一般半自动过程。第三，我们公开发布了一个大规模的视频基准，该基准是通过实施这一程序而创建的，我们还包括一项人体研究，用于评估人类在我们数据集上的表现。最后，我们在此基准测试中提出并测试各种方法，以便更好地理解视频理解带来的新挑战。

##### URL
[https://arxiv.org/abs/1606.07373](https://arxiv.org/abs/1606.07373)

##### PDF
[https://arxiv.org/pdf/1606.07373](https://arxiv.org/pdf/1606.07373)

