---
layout: post
title: "Bilinear Attention Networks"
date: 2018-05-21 07:58:31
categories: arXiv_CV
tags: arXiv_CV QA Attention Quantitative VQA
author: Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.

##### Abstract (translated by Google)
多模式学习中的注意网络提供了有选择地利用给定视觉信息的有效方式。然而，学习每对多模式输入通道的注意力分布的计算成本非常昂贵。为了解决这个问题，共同注意力为每种模态建立了两个单独的注意力分布，忽略了多模态输入之间的相互作用。在本文中，我们提出双线性注意网络（BAN），发现双线性注意分布，以无缝地利用给定的视觉语言信息。 BAN考虑两组输入通道之间的双线性相互作用，而低秩双线性池化则提取每对通道的联合表示。此外，我们提出了多模态残差网络的变体，以有效地利用BAN的八个关注图。我们定量和定性地评估我们的视觉问答（VQA 2.0）和Flickr30k实体数据集模型，表明BAN明显优于以前的方法，并在两个数据集上实现了新的最新技术水平。

##### URL
[https://arxiv.org/abs/1805.07932](https://arxiv.org/abs/1805.07932)

##### PDF
[https://arxiv.org/pdf/1805.07932](https://arxiv.org/pdf/1805.07932)

