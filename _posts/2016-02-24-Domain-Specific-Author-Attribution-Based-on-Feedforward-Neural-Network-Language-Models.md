---
layout: post
title: "Domain Specific Author Attribution Based on Feedforward Neural Network Language Models"
date: 2016-02-24 04:32:34
categories: arXiv_SD
tags: arXiv_SD Classification Language_Model
author: Zhenhao Ge, Yufang Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model (NNLM) and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43% on average, compared with the N-gram methods using SRILM tools. An open source implementation of our methodology is freely available at this https URL

##### Abstract (translated by Google)
作者归属是指根据给定的文本样本自动确定作者的任务。这是一个历史悠久，应用范围广泛的问题。使用语言模型构建作者配置文件是使这一任务自动化的最成功的方法之一。基于神经网络的新的语言建模方法减轻了维数灾难，并且通常胜过传统的N-gram方法。然而，没有太多的研究将其应用于作者归属。在本文中，我们提出了一种神经网络语言模型（NNLM）的新颖设置，并将其应用于来自不同作者的文本样本的数据库。我们调查NNLM如何执行中等作者集大小和相对有限的训练和测试数据的任务，以及文本样本的主题如何影响准确性。 NNLM将困惑度降低了近2.5％，这是测试数据对训练好的语言模型的适应度的度量。给定5个随机测试句子，与使用SRILM工具的N-gram方法相比，平均提高了作者分类准确率3.43％。我们的方法的开源实现可以在https网址免费获得

##### URL
[https://arxiv.org/abs/1602.07393](https://arxiv.org/abs/1602.07393)

##### PDF
[https://arxiv.org/pdf/1602.07393](https://arxiv.org/pdf/1602.07393)

