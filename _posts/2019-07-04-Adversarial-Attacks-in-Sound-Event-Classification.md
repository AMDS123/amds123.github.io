---
layout: post
title: "Adversarial Attacks in Sound Event Classification"
date: 2019-07-04 16:15:35
categories: arXiv_SD
tags: arXiv_SD Adversarial CNN Classification Deep_Learning
author: Vinod Subramanian, Emmanouil Benetos, Ning Xu, SKoT McDonald, Mark Sandler
mathjax: true
---

* content
{:toc}

##### Abstract
Adversarial attacks refer to a set of methods that perturb the input to a classification model in order to fool the classifier. In this paper we apply different gradient based adversarial attack algorithms on five deep learning models trained for sound event classification. Four of the models use mel-spectrogram input and one model uses raw audio input. The models represent standard architectures such as convolutional, recurrent and dense networks. The dataset used for training is the Freesound dataset released for task 2 of the DCASE 2018 challenge and the models used are from participants of the challenge who open sourced their code. Our experiments show that adversarial attacks can be generated with high confidence and low perturbation. In addition, we show that the adversarial attacks are very effective across the different models.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.02477](http://arxiv.org/abs/1907.02477)

##### PDF
[http://arxiv.org/pdf/1907.02477](http://arxiv.org/pdf/1907.02477)

