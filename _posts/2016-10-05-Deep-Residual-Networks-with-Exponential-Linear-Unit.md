---
layout: post
title: "Deep Residual Networks with Exponential Linear Unit"
date: 2016-10-05 07:14:27
categories: arXiv_CV
tags: arXiv_CV CNN
author: Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde, Sandip Shingade
mathjax: true
---

* content
{:toc}

##### Abstract
Very deep convolutional neural networks introduced new problems like vanishing gradient and degradation. The recent successful contributions towards solving these problems are Residual and Highway Networks. These networks introduce skip connections that allow the information (from the input or those learned in earlier layers) to flow more into the deeper layers. These very deep models have lead to a considerable decrease in test errors, on benchmarks like ImageNet and COCO. In this paper, we propose the use of exponential linear unit instead of the combination of ReLU and Batch Normalization in Residual Networks. We show that this not only speeds up learning in Residual Networks but also improves the accuracy as the depth increases. It improves the test error on almost all data sets, like CIFAR-10 and CIFAR-100

##### Abstract (translated by Google)
非常深的卷积神经网络引入了像消失梯度和退化这样的新问题。最近对解决这些问题的成功贡献是残余和高速公路网络。这些网络引入了跳转连接，允许（从输入或在先前的层中学习的）信息更多地流入更深的层。这些非常深的模型已经导致测试错误的显着降低，像ImageNet和COCO这样的基准测试。在本文中，我们建议使用指数线性单位而不是ReLU和Batch Normalization在残差网络中的组合。我们表明，这不仅加速了剩余网络的学习，而且随着深度的增加而提高了准确度。它改善了几乎所有数据集的测试错误，如CIFAR-10和CIFAR-100

##### URL
[https://arxiv.org/abs/1604.04112](https://arxiv.org/abs/1604.04112)

##### PDF
[https://arxiv.org/pdf/1604.04112](https://arxiv.org/pdf/1604.04112)

