---
layout: post
title: "Learning Social Affordance Grammar from Videos: Transferring Human Interactions to Human-Robot Interactions"
date: 2017-03-01 21:05:10
categories: arXiv_CV
tags: arXiv_CV Weakly_Supervised Inference
author: Tianmin Shu, Xiaofeng Gao, Michael S. Ryoo, Song-Chun Zhu
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we present a general framework for learning social affordance grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human interactions, and transfer the grammar to humanoids to enable a real-time motion inference for human-robot interaction (HRI). Based on Gibbs sampling, our weakly supervised grammar learning can automatically construct a hierarchical representation of an interaction with long-term joint sub-tasks of both agents and short term atomic actions of individual agents. Based on a new RGB-D video dataset with rich instances of human interactions, our experiments of Baxter simulation, human evaluation, and real Baxter test demonstrate that the model learned from limited training data successfully generates human-like behaviors in unseen scenarios and outperforms both baselines.

##### Abstract (translated by Google)
在本文中，我们提出了一个通用框架，用于从人类相互作用的RGB-D视频中学习社交可供性语法作为时空AND-OR图（ST-AOG），并将语法转换为人形，以实现实时运动推断人机交互（HRI）。基于Gibbs抽样，我们的弱监督语法学习可以自动构建一个层次表示，这个层次表示包含两个Agent的长期联合子任务和个体代理的短期原子行为。基于一个具有丰富的人类交互实例的新的RGB-D视频数据集，我们的Baxter模拟，人类评估和真实的Baxter测试的实验证明，从有限的训练数据中学习的模型成功地在看不见的场景中产生类似人类的行为，基线。

##### URL
[https://arxiv.org/abs/1703.00503](https://arxiv.org/abs/1703.00503)

##### PDF
[https://arxiv.org/pdf/1703.00503](https://arxiv.org/pdf/1703.00503)

