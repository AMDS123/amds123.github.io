---
layout: post
title: "Attention-based Pyramid Aggregation Network for Visual Place Recognition"
date: 2018-08-01 12:10:40
categories: arXiv_CV
tags: arXiv_CV Image_Caption Image_Retrieval Attention Recognition
author: Yingying Zhu, Jiong Wang, Lingxi Xie, Liang Zheng
mathjax: true
---

* content
{:toc}

##### Abstract
Visual place recognition is challenging in the urban environment and is usually viewed as a large scale image retrieval task. The intrinsic challenges in place recognition exist that the confusing objects such as cars and trees frequently occur in the complex urban scene, and buildings with repetitive structures may cause over-counting and the burstiness problem degrading the image representations. To address these problems, we present an Attention-based Pyramid Aggregation Network (APANet), which is trained in an end-to-end manner for place recognition. One main component of APANet, the spatial pyramid pooling, can effectively encode the multi-size buildings containing geo-information. The other one, the attention block, is adopted as a region evaluator for suppressing the confusing regional features while highlighting the discriminative ones. When testing, we further propose a simple yet effective PCA power whitening strategy, which significantly improves the widely used PCA whitening by reasonably limiting the impact of over-counting. Experimental evaluations demonstrate that the proposed APANet outperforms the state-of-the-art methods on two place recognition benchmarks, and generalizes well on standard image retrieval datasets.

##### Abstract (translated by Google)
视觉位置识别在城市环境中具有挑战性，并且通常被视为大规模图像检索任务。在地方识别中存在的内在挑战存在于诸如汽车和树木之类的混乱对象经常出现在复杂的城市场景中，并且具有重复结构的建筑物可能导致过度计数并且突发性问题使图像表示降级。为了解决这些问题，我们提出了一个基于注意力的金字塔聚合网络（APANet），它以端到端的方式进行地点识别培训。 APANet的一个主要组成部分，即空间金字塔池，可以有效地编码包含地理信息的多尺寸建筑物。另一个，即注意块，被用作区域评估器，用于抑制混淆的区域特征，同时突出区别特征。在测试时，我们进一步提出了一种简单而有效的PCA功率白化策略，通过合理地限制过度计数的影响，显着改善了广泛使用的PCA白化。实验评估表明，所提出的APANet在两个位置识别基准上优于最先进的方法，并且很好地概括了标准图像检索数据集。

##### URL
[https://arxiv.org/abs/1808.00288](https://arxiv.org/abs/1808.00288)

##### PDF
[https://arxiv.org/pdf/1808.00288](https://arxiv.org/pdf/1808.00288)

