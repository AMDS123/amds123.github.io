---
layout: post
title: "Shortcut Sequence Tagging"
date: 2017-01-03 04:15:51
categories: arXiv_SD
tags: arXiv_SD RNN
author: Huijia Wu, Jiajun Zhang, Chengqing Zong
mathjax: true
---

* content
{:toc}

##### Abstract
Deep stacked RNNs are usually hard to train. Adding shortcut connections across different layers is a common way to ease the training of stacked networks. However, extra shortcuts make the recurrent step more complicated. To simply the stacked architecture, we propose a framework called shortcut block, which is a marriage of the gating mechanism and shortcuts, while discarding the self-connected part in LSTM cell. We present extensive empirical experiments showing that this design makes training easy and improves generalization. We propose various shortcut block topologies and compositions to explore its effectiveness. Based on this architecture, we obtain a 6% relatively improvement over the state-of-the-art on CCGbank supertagging dataset. We also get comparable results on POS tagging task.

##### Abstract (translated by Google)
深堆积的RNN通常很难训练。在不同层添加快捷方式连接是简化堆叠网络培训的常用方法。但是，额外的快捷方式使得经常性的步骤更加复杂。为了简化叠加的体系结构，我们提出了一种快捷方式的框架，它是门控机制和快捷方式的结合，而丢弃LSTM单元中的自连接部分。我们展示了广泛的经验性实验，表明这种设计使培训变得简单，并改进了泛化。我们提出各种快捷方式块拓扑和组合来探索其有效性。基于这种架构，我们获得了比CCGbank超级数据集最先进的6％的改进。 POS标签任务也可以得到类似的结果。

##### URL
[https://arxiv.org/abs/1701.00576](https://arxiv.org/abs/1701.00576)

##### PDF
[https://arxiv.org/pdf/1701.00576](https://arxiv.org/pdf/1701.00576)

