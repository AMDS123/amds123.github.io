---
layout: post
title: "Contextualize, Show and Tell: A Neural Visual Storyteller"
date: 2018-06-03 05:09:54
categories: arXiv_AI
tags: arXiv_AI Image_Caption Embedding RNN
author: Diana Gonzalez-Rico, Gibran Fuentes-Pineda
mathjax: true
---

* content
{:toc}

##### Abstract
We present a neural model for generating short stories from image sequences, which extends the image description model by Vinyals et al. (Vinyals et al., 2015). This extension relies on an encoder LSTM to compute a context vector of each story from the image sequence. This context vector is used as the first state of multiple independent decoder LSTMs, each of which generates the portion of the story corresponding to each image in the sequence by taking the image embedding as the first input. Our model showed competitive results with the METEOR metric and human ratings in the internal track of the Visual Storytelling Challenge 2018.

##### Abstract (translated by Google)
我们提出了一个用于从图像序列生成短篇故事的神经模型，它扩展了Vinyals等人的图像描述模型。 （Vinyals等，2015）。该扩展依赖于编码器LSTM根据图像序列计算每个故事的上下文向量。该上下文向量被用作多个独立解码器LSTM的第一状态，其中每个解码器通过将图像嵌入作为第一输入来生成与该序列中的每个图像相对应的故事的部分。我们的模型在2018年Visual Storytelling Challenge的内部轨迹中显示了METEOR度量和人员评分的竞争结果。

##### URL
[http://arxiv.org/abs/1806.00738](http://arxiv.org/abs/1806.00738)

##### PDF
[http://arxiv.org/pdf/1806.00738](http://arxiv.org/pdf/1806.00738)

