---
layout: post
title: "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size"
date: 2016-11-04 21:26:08
categories: arXiv_CV
tags: arXiv_CV
author: Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer
mathjax: true
---

* content
{:toc}

##### Abstract
Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: this https URL

##### Abstract (translated by Google)
最近对深度神经网络的研究主要集中在提高精度上。对于给定的准确度级别，通常可以确定达到该准确度级别的多个DNN体系结构。具有相同的准确性，较小的DNN体系结构提供至少三个优点：（1）较小的DNN在分布式培训期间在服务器之间需要较少的通信。 （2）较小的DNN需要较少的带宽将新模型从云端导出到自动驾驶汽车。 （3）较小的DNN更易于部署在FPGA和其他有限内存的硬件上。为了提供所有这些优势，我们提出了一个名为SqueezeNet的小DNN体系结构。 SqueezeNet在ImageNet上实现了AlexNet级别的准确性，参数少了50倍。另外，使用模型压缩技术，我们可以将SqueezeNet压缩到小于0.5MB（比AlexNet小510倍）。 SqueezeNet架构可以在这里下载：这个https URL

##### URL
[https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360)

##### PDF
[https://arxiv.org/pdf/1602.07360](https://arxiv.org/pdf/1602.07360)

