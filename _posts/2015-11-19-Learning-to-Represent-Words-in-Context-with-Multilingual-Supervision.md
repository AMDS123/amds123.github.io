---
layout: post
title: "Learning to Represent Words in Context with Multilingual Supervision"
date: 2015-11-19 23:35:42
categories: arXiv_CL
tags: arXiv_CL RNN Prediction
author: Kazuya Kawakami, Chris Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.

##### Abstract (translated by Google)
我们提出了一个基于双向LSTM的神经网络体系结构来计算句子语境中的单词表示。这些上下文相关的词表示适合于例如区分不同的词义和其他上下文调制的意义变化。为了学习模型的参数，我们使用跨语言监督，假设在上下文中对单词的良好表示将是足以将正确的翻译选择为第二语言的单词。我们评估我们的表示的质量，作为三个下游任务的特征：预测语义超验（将名词和动词分配到几十个语义类），低资源机器翻译和词汇替代任务，并获得状态的所有这些的开始结果。

##### URL
[https://arxiv.org/abs/1511.04623](https://arxiv.org/abs/1511.04623)

##### PDF
[https://arxiv.org/pdf/1511.04623](https://arxiv.org/pdf/1511.04623)

