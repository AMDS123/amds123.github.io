---
layout: post
title: "Rethink ReLU to Training Better CNNs"
date: 2018-08-31 08:25:33
categories: arXiv_CV
tags: arXiv_CV CNN
author: Gangming Zhao, Zhaoxiang Zhang, He Guan, Peng Tang, Jingdong Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Most of convolutional neural networks share the same characteristic: each convolutional layer is followed by a nonlinear activation layer where Rectified Linear Unit (ReLU) is the most widely used. In this paper, we argue that the designed structure with the equal ratio between these two layers may not be the best choice since it could result in the poor generalization ability. Thus, we try to investigate a more suitable method on using ReLU to explore the better network architectures. Specifically, we propose a proportional module to keep the ratio between convolution and ReLU amount to be N:M (N&gt;M). The proportional module can be applied in almost all networks with no extra computational cost to improve the performance. Comprehensive experimental results indicate that the proposed method achieves better performance on different benchmarks with different network architectures, thus verify the superiority of our work.

##### Abstract (translated by Google)
大多数卷积神经网络具有相同的特征：每个卷积层后面跟着一个非线性激活层，其中整流线性单元（ReLU）是最广泛使用的。在本文中，我们认为这两层之间具有相等比例的设计结构可能不是最佳选择，因为它可能导致较差的泛化能力。因此，我们尝试研究一种更合适的方法来使用ReLU来探索更好的网络架构。具体地，我们提出了一种比例模块，以使卷积和ReLU量之间的比率保持为N：M（N> M）。比例模块几乎可以应用于所有网络，无需额外的计算成本即可提高性能。综合实验结果表明，该方法在不同网络架构的不同基准测试中取得了较好的性能，验证了工作的优越性。

##### URL
[http://arxiv.org/abs/1709.06247](http://arxiv.org/abs/1709.06247)

##### PDF
[http://arxiv.org/pdf/1709.06247](http://arxiv.org/pdf/1709.06247)

