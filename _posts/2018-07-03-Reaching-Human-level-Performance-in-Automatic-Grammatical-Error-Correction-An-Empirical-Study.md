---
layout: post
title: "Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study"
date: 2018-07-03 16:37:05
categories: arXiv_CL
tags: arXiv_CL CNN Inference
author: Tao Ge, Furu Wei, Ming Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
Neural sequence-to-sequence (seq2seq) approaches have proven to be successful in grammatical error correction (GEC). Based on the seq2seq framework, we propose a novel fluency boost learning and inference mechanism. Fluency boosting learning generates diverse error-corrected sentence pairs during training, enabling the error correction model to learn how to improve a sentence's fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps. Combining fluency boost learning and inference with convolutional seq2seq models, our approach achieves the state-of-the-art performance: 75.72 (F_{0.5}) on CoNLL-2014 10 annotation dataset and 62.42 (GLEU) on JFLEG test set respectively, becoming the first GEC system that reaches human-level performance (72.58 for CoNLL and 62.37 for JFLEG) on both of the benchmarks.

##### Abstract (translated by Google)
已经证明，神经序列到序列（seq2seq）方法在语法纠错（GEC）中是成功的。基于seq2seq框架，我们提出了一种新颖的流畅性提升学习和推理机制。流畅性提升学习在训练期间生成多种纠错的句子对，使得纠错模型能够学习如何从更多实例中提高句子的流畅性，而流畅性提升推理允许模型通过多个推理步骤递增地校正句子。将流畅性提升学习和推理与卷积式seq2seq模型相结合，我们的方法实现了最先进的性能：CoNLL-2014 10注释数据集分别为75.72（F_ {0.5}）和JFLEG测试集分别为62.42（GLEU），在这两个基准测试中，第一个达到人类水平表现的GEC系统（CoNLL为72.58，JFLEG为62.37）。

##### URL
[https://arxiv.org/abs/1807.01270](https://arxiv.org/abs/1807.01270)

##### PDF
[https://arxiv.org/pdf/1807.01270](https://arxiv.org/pdf/1807.01270)

