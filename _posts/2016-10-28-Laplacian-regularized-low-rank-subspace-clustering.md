---
layout: post
title: "Laplacian regularized low rank subspace clustering"
date: 2016-10-28 01:24:55
categories: arXiv_CV
tags: arXiv_CV Regularization
author: Yu Song, Yiquan Wu
mathjax: true
---

* content
{:toc}

##### Abstract
The problem of fitting a union of subspaces to a collection of data points drawn from multiple subspaces is considered in this paper. In the traditional low rank representation model, the dictionary used to represent the data points is chosen as the data points themselves and thus the dictionary is corrupted with noise. This problem is solved in the low rank subspace clustering model which decomposes the corrupted data matrix as the sum of a clean and self-expressive dictionary plus a matrix of noise and gross errors. Also, the clustering results of the low rank representation model can be enhanced by using a graph of data similarity. This model is called Laplacian regularized low rank representation model with a graph regularization term added to the objective function. Inspired from the above two ideas, in this paper a Laplacian regularized low rank subspace clustering model is proposed. This model uses a clean dictionary to represent the data points and a graph regularization term is also incorporated in the objective function. Experimental results show that, compared with the traditional low rank representation model, low rank subspace clustering model and several other state-of-the-art subspace clustering model, the model proposed in this paper can get better subspace clustering results with lower clustering error.

##### Abstract (translated by Google)
在本文中考虑了将子空间的联合与从多个子空间中抽取的数据点的集合进行拟合的问题。在传统的低秩表示模型中，用于表示数据点的字典被选为数据点本身，因此字典被噪声破坏。这个问题是在低秩子空间聚类模型中解决的，该模型将损坏的数据矩阵分解为干净的自我表示字典加上噪声和粗差的矩阵的总和。而且，通过使用数据相似度图，可以增强低秩表示模型的聚类结果。该模型被称为拉普拉斯正则化低秩表示模型，其中图形正则化项被添加到目标函数中。受上述两个思想的启发，本文提出了拉普拉斯正则化低秩子空间聚类模型。该模型使用干净的字典来表示数据点，并且图形正则化项也被包含在目标函数中。实验结果表明，与传统的低秩表示模型，低秩子空间聚类模型和其他一些最新的子空间聚类模型相比，本文提出的模型能够获得更好的子空间聚类结果，聚类误差更低。

##### URL
[https://arxiv.org/abs/1610.07488](https://arxiv.org/abs/1610.07488)

##### PDF
[https://arxiv.org/pdf/1610.07488](https://arxiv.org/pdf/1610.07488)

