---
layout: post
title: "From Lifestyle Vlogs to Everyday Interactions"
date: 2017-12-06 18:07:57
categories: arXiv_CV
tags: arXiv_CV Prediction
author: David F. Fouhey, Wei-cheng Kuo, Alexei A. Efros, Jitendra Malik
mathjax: true
---

* content
{:toc}

##### Abstract
A major stumbling block to progress in understanding basic human interactions, such as getting out of bed or opening a refrigerator, is lack of good training data. Most past efforts have gathered this data explicitly: starting with a laundry list of action labels, and then querying search engines for videos tagged with each label. In this work, we do the reverse and search implicitly: we start with a large collection of interaction-rich video data and then annotate and analyze it. We use Internet Lifestyle Vlogs as the source of surprisingly large and diverse interaction data. We show that by collecting the data first, we are able to achieve greater scale and far greater diversity in terms of actions and actors. Additionally, our data exposes biases built into common explicitly gathered data. We make sense of our data by analyzing the central component of interaction -- hands. We benchmark two tasks: identifying semantic object contact at the video level and non-semantic contact state at the frame level. We additionally demonstrate future prediction of hands.

##### Abstract (translated by Google)
进一步了解基本的人际互动，如下床或打开冰箱，是一个主要的绊脚石，缺乏良好的训练数据。大多数过去的努力都明确地收集了这些数据：从行动标签的清单开始，然后向搜索引擎查询用每个标签标记的视频。在这项工作中，我们做了相反的事情，并且隐式地进行搜索：我们从大量交互式视频数据开始，然后对其进行注释和分析。我们使用互联网生活Vlogs作为惊人的大量和多样的互动数据的来源。我们表明，通过首先收集数据，我们能够在行动和行为方面实现更大的规模和更大的多样性。此外，我们的数据暴露出内置于通用显式收集数据的偏见。我们通过分析交互的主要组成部分来理解我们的数据。我们对两个任务进行了基准测试：识别视频级的语义对象联系和帧级的非语义联系状态。我们另外演示未来的手的预测。

##### URL
[http://arxiv.org/abs/1712.02310](http://arxiv.org/abs/1712.02310)

##### PDF
[http://arxiv.org/pdf/1712.02310](http://arxiv.org/pdf/1712.02310)

