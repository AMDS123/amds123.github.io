---
layout: post
title: "Fine-Grained Attention Mechanism for Neural Machine Translation"
date: 2018-03-30 10:38:33
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Heeyoul Choi, Kyunghyun Cho, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
Neural machine translation (NMT) has been a new paradigm in machine translation, and the attention mechanism has become the dominant approach with the state-of-the-art records in many language pairs. While there are variants of the attention mechanism, all of them use only temporal attention where one scalar value is assigned to one context vector corresponding to a source word. In this paper, we propose a fine-grained (or 2D) attention mechanism where each dimension of a context vector will receive a separate attention score. In experiments with the task of En-De and En-Fi translation, the fine-grained attention method improves the translation quality in terms of BLEU score. In addition, our alignment analysis reveals how the fine-grained attention mechanism exploits the internal structure of context vectors.

##### Abstract (translated by Google)
神经机器翻译（NMT）一直是机器翻译领域的一个新范例，关注机制已成为许多语言对中最先进记录的主流方法。虽然存在注意机制的变体，但是其中一个标量值被分配给对应于源单词的一个上下文矢量的情况下，它们都仅使用时间关注。在本文中，我们提出了一种细粒度（或二维）关注机制，其中上下文向量的每个维度都将获得单独的关注分数。在En-De和En-Fi翻译任务的实验中，细粒度关注方法在BLEU评分方面提高了翻译质量。另外，我们的对齐分析揭示了细粒度的关注机制如何利用上下文向量的内部结构。

##### URL
[http://arxiv.org/abs/1803.11407](http://arxiv.org/abs/1803.11407)

##### PDF
[http://arxiv.org/pdf/1803.11407](http://arxiv.org/pdf/1803.11407)

