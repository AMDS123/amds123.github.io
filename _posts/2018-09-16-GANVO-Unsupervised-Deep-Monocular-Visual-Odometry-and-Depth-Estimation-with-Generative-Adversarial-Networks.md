---
layout: post
title: "GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks"
date: 2018-09-16 00:27:09
categories: arXiv_CV
tags: arXiv_CV Adversarial Attention GAN Pose_Estimation CNN Deep_Learning Quantitative
author: Yasin Almalioglu, Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Andrew Markham, Niki Trigoni
mathjax: true
---

* content
{:toc}

##### Abstract
In the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI and Cityscapes datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.

##### Abstract (translated by Google)
在过去十年中，监督深度学习方法已广泛应用于视觉测距（VO）应用，这在标记数据不丰富的环境中是不可行的。另一方面，未经监督的深度学习方法用于未标记数据中未标记数据的定位和映射，在VO研究中受到的关注较少。在这项研究中，我们提出了一个生成无监督学习框架，使用深度卷积生成对抗网络（GAN）预测6-DoF姿势相机运动和来自未标记RGB图像序列的场景单眼深度图。我们通过扭曲视图序列并将重新投影最小化分配给在多视图姿态估计和单视图深度生成网络中采用的目标损失函数来创建监督信号。对KITTI和Cityscapes数据集的拟议框架进行详细的定量和定性评估表明，所提出的方法优于现有的传统和无监督的深VO方法，为姿态估计和深度恢复提供了更好的结果。

##### URL
[http://arxiv.org/abs/1809.05786](http://arxiv.org/abs/1809.05786)

##### PDF
[http://arxiv.org/pdf/1809.05786](http://arxiv.org/pdf/1809.05786)

