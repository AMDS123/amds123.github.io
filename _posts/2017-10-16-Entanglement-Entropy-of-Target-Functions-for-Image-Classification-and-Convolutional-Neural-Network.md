---
layout: post
title: "Entanglement Entropy of Target Functions for Image Classification and Convolutional Neural Network"
date: 2017-10-16 05:54:38
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Classification
author: Ya-Hui Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
The success of deep convolutional neural network (CNN) in computer vision especially image classification problems requests a new information theory for function of image, instead of image itself. In this article, after establishing a deep mathematical connection between image classification problem and quantum spin model, we propose to use entanglement entropy, a generalization of classical Boltzmann-Shannon entropy, as a powerful tool to characterize the information needed for representation of general function of image. We prove that there is a sub-volume-law bound for entanglement entropy of target functions of reasonable image classification problems. Therefore target functions of image classification only occupy a small subspace of the whole Hilbert space. As a result, a neural network with polynomial number of parameters is efficient for representation of such target functions of image. The concept of entanglement entropy can also be useful to characterize the expressive power of different neural networks. For example, we show that to maintain the same expressive power, number of channels $D$ in a convolutional neural network should scale with the number of convolution layers $n_c$ as $D\sim D_0^{\frac{1}{n_c}}$. Therefore, deeper CNN with large $n_c$ is more efficient than shallow ones.

##### Abstract (translated by Google)
深度卷积神经网络（CNN）在计算机视觉（特别是图像分类问题）中的成功需要一种新的图像信息的信息理论，而不是图像本身。在本文中，在图像分类问题和量子自旋模型之间建立了深刻的数学联系之后，我们提出使用经典玻尔兹曼 - 香农熵的广义熵纠缠熵作为表征一般函数所需信息的有力工具图片。证明了合理图像分类问题的目标函数的纠缠熵有一个子体积律。因此，图像分类的目标函数只占用整个希尔伯特空间的一个小子空间。结果，具有多项式参数的神经网络对于表示这种图像的目标函数是有效的。纠缠熵的概念也可以用来描述不同神经网络的表达能力。例如，为了保持相同的表达能力，卷积神经网络中的信道数目$ D $应该随着卷积层数$ n_c $按比例缩放，如$ D \ sim D_0 ^ {\ frac {1} {n_c }} $。因此，大$ n_c $的更深的CNN比浅的更有效。

##### URL
[https://arxiv.org/abs/1710.05520](https://arxiv.org/abs/1710.05520)

##### PDF
[https://arxiv.org/pdf/1710.05520](https://arxiv.org/pdf/1710.05520)

