---
layout: post
title: "Towards a Better Understanding of Predict and Count Models"
date: 2015-11-06 10:29:26
categories: arXiv_CL
tags: arXiv_CL Regularization Embedding Optimization Prediction
author: S. Sathiya Keerthi, Tobias Schnabel, Rajiv Khanna
mathjax: true
---

* content
{:toc}

##### Abstract
In a recent paper, Levy and Goldberg pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible.

##### Abstract (translated by Google)
在最近的一篇论文中，Levy和Goldberg指出基于预测的词嵌入模型和基于逐点互信息的计数模型之间有一个有趣的联系。在一定的条件下，他们表明两种模型都最终达到了优化等价的目标函数。本文将更详细地探讨这一关系，并列出导致这些模型之间差异的因素。我们发现从最优化角度来看，最相关的差异是：（i）预测模型在低维空间中工作，其中嵌入向量可以相互作用; （ii）由于预测模型具有较少的参数，所以它们不太容易过度拟合。受到我们分析的洞察力的启发，我们展示了如何以有原则的方式规则化计数模型，并为L1和L2正则化提供封闭形式的解决方案。最后，我们提出了一个新的凸目标的嵌入模型和额外的可理解的好处。

##### URL
[https://arxiv.org/abs/1511.02024](https://arxiv.org/abs/1511.02024)

##### PDF
[https://arxiv.org/pdf/1511.02024](https://arxiv.org/pdf/1511.02024)

