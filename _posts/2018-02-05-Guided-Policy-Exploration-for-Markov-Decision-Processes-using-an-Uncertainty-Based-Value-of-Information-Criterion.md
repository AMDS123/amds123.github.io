---
layout: post
title: "Guided Policy Exploration for Markov Decision Processes using an Uncertainty-Based Value-of-Information Criterion"
date: 2018-02-05 17:24:13
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Isaac J. Sledge, Matthew S. Emigh, Jose C. Principe
mathjax: true
---

* content
{:toc}

##### Abstract
Reinforcement learning in environments with many action-state pairs is challenging. At issue is the number of episodes needed to thoroughly search the policy space. Most conventional heuristics address this search problem in a stochastic manner. This can leave large portions of the policy space unvisited during the early training stages. In this paper, we propose an uncertainty-based, information-theoretic approach for performing guided stochastic searches that more effectively cover the policy space. Our approach is based on the value of information, a criterion that provides the optimal trade-off between expected costs and the granularity of the search process. The value of information yields a stochastic routine for choosing actions during learning that can explore the policy space in a coarse to fine manner. We augment this criterion with a state-transition uncertainty factor, which guides the search process into previously unexplored regions of the policy space.

##### Abstract (translated by Google)
在具有许多动作 - 状态对的环境中的强化学习是具有挑战性的。问题是彻底搜索政策空间所需的事件数量。大多数传统的启发式方法都是以随机的方式解决这个搜索问题。这可能会在早期培训阶段留下很大一部分政策空间。在本文中，我们提出了一种基于不确定性的信息论方法，用于执行更有效地覆盖政策空间的指导随机搜索。我们的方法是基于信息的价值，这个标准提供了预期成本和搜索过程粒度之间的最佳平衡。信息的价值产生一个随机的例行选择行动在学习，可以探索政策空间从粗到精的方式。我们用一个状态转移的不确定性因素来增强这个标准，这个因素将搜索过程引导到政策空间以前未被发现的地区。

##### URL
[http://arxiv.org/abs/1802.01518](http://arxiv.org/abs/1802.01518)

##### PDF
[http://arxiv.org/pdf/1802.01518](http://arxiv.org/pdf/1802.01518)

