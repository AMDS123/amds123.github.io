---
layout: post
title: "What is Wrong with Topic Modeling?"
date: 2017-11-08 04:49:42
categories: arXiv_CL
tags: arXiv_CL Classification
author: Amritanshu Agrawal, Wei Fu, Tim Menzies
mathjax: true
---

* content
{:toc}

##### Abstract
Context: Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeler is Latent Dirichlet allocation. When run on different datasets, LDA suffers from "order effects" i.e. different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results;specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results. Objective: To provide a method in which distributions generated by LDA are more stable and can be used for further analysis. Method: We use LDADE, a search-based software engineering tool that tunes LDA's parameters using DE (Differential Evolution). LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands ofSoftware Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using Gibbs sampling). Results were scored via topic stability and text mining classification accuracy. Results: In all treatments: (i) standard LDA exhibits very large topic instability; (ii) LDADE's tunings dramatically reduce cluster instability; (iii) LDADE also leads to improved performances for supervised as well as unsupervised learning. Conclusion: Due to topic instability, using standard LDA with its "off-the-shelf" settings should now be depreciated. Also, in future, we should require SE papers that use LDA to test and (if needed) mitigate LDA topic instability. Finally, LDADE is a candidate technology for effectively and efficiently reducing that instability.

##### Abstract (translated by Google)
上下文：主题建模在非结构化文本数据中查找可读结构。一个广泛使用的主题建模者是潜在Dirichlet分配。当在不同的数据集上运行时，LDA会遇到“顺序效应”，即如果训练数据的顺序被混洗，将会产生不同的主题。这种秩序效应为任何研究带来了系统性的错误。这个错误可能与错误的结果有关，特别是主题描述不准确，以及文本挖掘分类结果的效率降低。目的：提供一种由LDA产生的分布更加稳定的方法，可用于进一步的分析。方法：我们使用LDADE，一种基于搜索的软件工程工具，使用DE（Differential Evolution）来调整LDA的参数。 LDADE评估来自程序员信息交换站点（Stackoverflow）的数据，数千篇软件工程（SE）论文的标题和摘要文本以及来自NASA的软件缺陷报告。结果是在LDA（Python + Scikit-Learn，Scala + Spark）的不同实现中收集的;跨越不同的平台（Linux，Macintosh）以及不同类型的LDA（VEM或使用Gibbs采样）。结果通过主题稳定性和文本挖掘分类准确性进行评分。结果：在所有处理中：（i）标准LDA表现出非常大的话题不稳定性; （二）LDADE的调整大大减少了群集的不稳定性; （三）LDADE也导致改进的监督和无监督学习的表现。结论：由于主题不稳定，现在使用标准LDA及其“现成”设置应该折旧。而且，将来我们应该要求使用LDA的SE论文进行测试，并且（如果需要的话）减少LDA话题不稳定性。最后，LDADE是有效和有效地减少这种不稳定性的候选技术。

##### URL
[https://arxiv.org/abs/1608.08176](https://arxiv.org/abs/1608.08176)

##### PDF
[https://arxiv.org/pdf/1608.08176](https://arxiv.org/pdf/1608.08176)

