---
layout: post
title: "Learning to Separate Object Sounds by Watching Unlabeled Video"
date: 2018-07-26 04:46:24
categories: arXiv_CV
tags: arXiv_CV
author: Ruohan Gao, Rogerio Feris, Kristen Grauman
mathjax: true
---

* content
{:toc}

##### Abstract
Perceiving a scene most fully requires all the senses. Yet modeling how objects look and sound is challenging: most natural scenes and events contain multiple objects, and the audio track mixes all the sound sources together. We propose to learn audio-visual object models from unlabeled video, then exploit the visual context to perform audio source separation in novel videos. Our approach relies on a deep multi-instance multi-label learning framework to disentangle the audio frequency bases that map to individual visual objects, even without observing/hearing those objects in isolation. We show how the recovered disentangled bases can be used to guide audio source separation to obtain better-separated, object-level sounds. Our work is the first to learn audio source separation from large-scale "in the wild" videos containing multiple audio sources per video. We obtain state-of-the-art results on visually-aided audio source separation and audio denoising. Our video results: <a href="http://vision.cs.utexas.edu/projects/separating_object_sounds/">this http URL</a>

##### Abstract (translated by Google)
充分认识一个场景需要所有的感官。然而，对物体外观和声音进行建模具有挑战性：大多数自然场景和事件包含多个物体，音轨将所有声源混合在一起。我们建议从未标记的视频中学习视听对象模型，然后利用视觉上下文在新视频中进行音频源分离。我们的方法依赖于深度多实例多标签学习框架来解开映射到各个视觉对象的音频基础，即使没有孤立地观察/听到这些对象。我们展示了如何使用恢复的解缠结基座来指导音频源分离以获得更好分离的物体级声音。我们的工作是第一个从每个视频包含多个音频源的大规模“野外”视频中学习音频源分离的工作。我们在视觉辅助音频源分离和音频去噪方面获得了最先进的结果。我们的视频结果：<a href="http://vision.cs.utexas.edu/projects/separating_object_sounds/">此http网址</a>

##### URL
[http://arxiv.org/abs/1804.01665](http://arxiv.org/abs/1804.01665)

##### PDF
[http://arxiv.org/pdf/1804.01665](http://arxiv.org/pdf/1804.01665)

