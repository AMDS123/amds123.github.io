---
layout: post
title: "From Imitation to Prediction, Data Compression vs Recurrent Neural Networks for Natural Language Processing"
date: 2017-05-01 20:23:13
categories: arXiv_CL
tags: arXiv_CL RNN Prediction
author: Juan Andrés Laura, Gabriel Masi, Luis Argerich
mathjax: true
---

* content
{:toc}

##### Abstract
In recent studies [1][13][12] Recurrent Neural Networks were used for generative processes and their surprising performance can be explained by their ability to create good predictions. In addition, data compression is also based on predictions. What the problem comes down to is whether a data compressor could be used to perform as well as recurrent neural networks in natural language processing tasks. If this is possible,then the problem comes down to determining if a compression algorithm is even more intelligent than a neural network in specific tasks related to human language. In our journey we discovered what we think is the fundamental difference between a Data Compression Algorithm and a Recurrent Neural Network.

##### Abstract (translated by Google)
在最近的研究[1] [13] [12]中，递归神经网络被用于生成过程，其惊人的表现可以用它们创造良好预测的能力来解释。另外，数据压缩也基于预测。问题的关键在于数据压缩器是否可以用于执行自然语言处理任务中的递归神经网络。如果这是可能的，那么问题归结为确定在与人类语言相关的特定任务中压缩算法是否比神经网络更加智能。在我们的旅程中，我们发现了我们认为的数据压缩算法和递归神经网络之间的根本区别。

##### URL
[https://arxiv.org/abs/1705.00697](https://arxiv.org/abs/1705.00697)

##### PDF
[https://arxiv.org/pdf/1705.00697](https://arxiv.org/pdf/1705.00697)

