---
layout: post
title: "Decoupling Dynamics and Reward for Transfer Learning"
date: 2018-04-27 21:16:40
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning Transfer_Learning
author: Amy Zhang, Harsh Satija, Joelle Pineau
mathjax: true
---

* content
{:toc}

##### Abstract
Current reinforcement learning (RL) methods can successfully learn single tasks but often generalize poorly to modest perturbations in task domain or training procedure. In this work, we present a decoupled learning strategy for RL that creates a shared representation space where knowledge can be robustly transferred. We separate learning the task representation, the forward dynamics, the inverse dynamics and the reward function of the domain, and show that this decoupling improves performance within the task, transfers well to changes in dynamics and reward, and can be effectively used for online planning. Empirical results show good performance in both continuous and discrete RL domains.

##### Abstract (translated by Google)
目前的强化学习（RL）方法可以成功地学习单个任务，但往往不能很好地归纳为任务领域或训练过程中适度的扰动。在这项工作中，我们提出了一个RL的分离学习策略，创建一个共享表示空间，在这个空间中知识可以被强有力地转移。我们将领域的任务表示，前向动力学，逆动力学和奖励函数分开学习，并且表明这种解耦可以改善任务内的性能，良好地转换为动态和奖励的变化，并且可以有效地用于在线计划。实证结果表明在连续和离散的RL域中都有良好的性能。

##### URL
[https://arxiv.org/abs/1804.10689](https://arxiv.org/abs/1804.10689)

##### PDF
[https://arxiv.org/pdf/1804.10689](https://arxiv.org/pdf/1804.10689)

