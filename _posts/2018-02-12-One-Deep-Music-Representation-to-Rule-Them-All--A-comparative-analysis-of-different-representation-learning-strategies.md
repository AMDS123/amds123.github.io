---
layout: post
title: "One Deep Music Representation to Rule Them All? : A comparative analysis of different representation learning strategies"
date: 2018-02-12 14:08:54
categories: arXiv_SD
tags: arXiv_SD Transfer_Learning Represenation_Learning Deep_Learning
author: Jaehun Kim (1), Juli&#xe1;n Urbano (1), Cynthia C. S. Liem (1), Alan Hanjalic (1) ((1) Delft University of Technology)
mathjax: true
---

* content
{:toc}

##### Abstract
Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new, yet unseen learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of data (e.g. music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning task, the validity of the above hypothesis is questionable for an arbitrary new learning task. In this paper we present the results of our investigation of what the best ways are to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning tasks, as well as multiple deep learning architectures with varying levels of information sharing between tasks, in order to learn music representations. We then validate these representations considering multiple unseen learning tasks for evaluation. The results of our experiments yield several insights on how to approach the design of methods for learning widely deployable deep data representations in the music domain.

##### Abstract (translated by Google)
受到在计算机视觉和自然语言处理领域深入学习的成功的启发，这种学习范式也进入了音乐信息检索领域。为了从深度学习中受益，深度转移学习已成为一种常用方法。在这种方法中，可以重新使用预先训练的神经网络的输出作为新的但看不见的学习任务的基础。基本的假设是，如果初始和新的学习任务表现出共性并且被应用于相同类型的数据（例如音乐音频），则所产生的数据的深度表示对于新任务也是信息性的。然而，由于大部分用于生成深度表示的网络都是使用单个初始学习任务进行训练的，所以上述假设对于任意新的学习任务的有效性是有问题的。在本文中，我们将介绍我们调查什么是最好的方法来为音乐领域的数据和学习任务生成深度表示的结果。我们通过广泛的实证研究进行调查，涉及多个学习任务，以及多个深度学习架构，在任务之间共享不同级别的信息，以便学习音乐表现形式。然后，我们验证这些表示考虑多个看不见的学习任务进行评估。我们的实验结果对如何在音乐领域中学习广泛部署的深度数据表示方法的设计提供了一些见解。

##### URL
[http://arxiv.org/abs/1802.04051](http://arxiv.org/abs/1802.04051)

##### PDF
[http://arxiv.org/pdf/1802.04051](http://arxiv.org/pdf/1802.04051)

