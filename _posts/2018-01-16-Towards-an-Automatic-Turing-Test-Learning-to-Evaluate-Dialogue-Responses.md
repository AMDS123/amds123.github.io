---
layout: post
title: "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses"
date: 2018-01-16 23:29:14
categories: arXiv_AI
tags: arXiv_AI Prediction
author: Ryan Lowe, Michael Noseworthy, Iulian V. Serban, Nicolas Angelard-Gontier, Yoshua Bengio, Joelle Pineau
mathjax: true
---

* content
{:toc}

##### Abstract
Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.

##### Abstract (translated by Google)
自动评估非结构化域对话响应的质量是一个具有挑战性的问题。不幸的是，现有的自动评估指标存在偏差，并且与人为判断的响应质量相关性很差。然而，准确的自动评估程序对于对话研究来说至关重要，因为它允许用较少昂贵的人类评估来快速建立原型并测试新模型。为了应对这一挑战，我们制定了自动对话评估作为一个学习问题。我们提出一个评估模型（ADEM），学习使用人类反应分数的新数据集来预测类人分数来输入反应。我们表明，ADEM模型的预测显着相关，并且在比BLEU等词重叠度量高很多的水平上，在话语和系统层面都有人类判断。我们还表明，ADEM可以推广到评估训练期间看不见的对话模型，这是自动对话评估的一个重要步骤。

##### URL
[http://arxiv.org/abs/1708.07149](http://arxiv.org/abs/1708.07149)

##### PDF
[http://arxiv.org/pdf/1708.07149](http://arxiv.org/pdf/1708.07149)

