---
layout: post
title: "Beyond the One Step Greedy Approach in Reinforcement Learning"
date: 2018-02-10 22:22:03
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning
author: Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor
mathjax: true
---

* content
{:toc}

##### Abstract
The famous Policy Iteration algorithm alternates between policy improvement and policy evaluation. Implementations of this algorithm with several variants of the latter evaluation stage, e.g, $n$-step and trace-based returns, have been analyzed in previous works. However, the case of multiple-step lookahead policy improvement, despite the recent increase in empirical evidence of its strength, has to our knowledge not been carefully analyzed yet. In this work, we introduce the first such analysis. Namely, we formulate variants of multiple-step policy improvement, derive new algorithms using these definitions and prove their convergence. Moreover, we show that recent prominent Reinforcement Learning algorithms are, in fact, instances of our framework. We thus shed light on their empirical success and give a recipe for deriving new algorithms for future study.

##### Abstract (translated by Google)
着名的Policy Iteration算法在策略改进和策略评估之间交替进行。这个算法在后面的评估阶段的几个变体中的实现，例如$ n $ -step和基于跟踪的回报，已经在之前的作品中进行了分析。然而，尽管最近实证证据表明其实力有所增加，但多步前瞻政策改善的情况仍未得到仔细分析。在这项工作中，我们介绍了第一个这样的分析。即，我们制定多步政策改进的变体，使用这些定义推导新算法并证明它们的收敛性。此外，我们显示最近突出的强化学习算法实际上是我们框架的实例。因此，我们阐明了他们的经验成功，并为未来研究提供了推导新算法的配方。

##### URL
[http://arxiv.org/abs/1802.03654](http://arxiv.org/abs/1802.03654)

##### PDF
[http://arxiv.org/pdf/1802.03654](http://arxiv.org/pdf/1802.03654)

