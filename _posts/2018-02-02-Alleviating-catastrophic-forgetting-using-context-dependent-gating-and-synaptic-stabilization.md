---
layout: post
title: "Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization"
date: 2018-02-02 23:49:44
categories: arXiv_AI
tags: arXiv_AI Sparse
author: Nicolas Y. Masse, Gregory D. Grant, David J. Freedman
mathjax: true
---

* content
{:toc}

##### Abstract
Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks (ANNs) on new tasks typically cause it to forget previously learned tasks. This phenomenon is the result of "catastrophic forgetting", in which training an ANN disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of ANNs that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly non-overlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows ANNs to maintain high performance across large numbers of sequentially presented tasks when combined with weight stabilization. This work provides another example of how neuroscience-inspired algorithms can benefit ANN design and capability.

##### Abstract (translated by Google)
人类和大多数动物可以学习新的任务，而不会忘记旧的任务。然而，对新任务训练人工神经网络（ANN）通常会导致它忘记以前学过的任务。这种现象是“灾难性遗忘”的结果，在这种情况下，人工神经网络破坏了对于解决以前的任务很重要的连接权重，降低了任务绩效。最近的一些研究提出了稳定人工神经网络的连接权重的方法，这些方法被认为是解决任务最重要的问题，这有助于减轻灾难性的遗忘。在这里，从被认为在体内实施的算法中汲取灵感，我们提出一种补充方法：增加依赖于上下文的门控信号，使得对于任何一个任务只有稀疏的，大部分不重叠的单元模式是活动的。这种方法易于实现，只需要很少的计算开销，并且允许ANN在结合重量稳定的情况下在大量顺序呈现的任务中保持高性能。这项工作提供了另一个例子，说明神经科学启发算法如何能够使人工神经网络设计和能力受益

##### URL
[http://arxiv.org/abs/1802.01569](http://arxiv.org/abs/1802.01569)

##### PDF
[http://arxiv.org/pdf/1802.01569](http://arxiv.org/pdf/1802.01569)

