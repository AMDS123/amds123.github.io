---
layout: post
title: "Inverse Reinforce Learning with Nonparametric Behavior Clustering"
date: 2017-12-15 03:13:23
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Siddharthan Rajasekaran, Jinwei Zhang, Jie Fu
mathjax: true
---

* content
{:toc}

##### Abstract
Inverse Reinforcement Learning (IRL) is the task of learning a single reward function given a Markov Decision Process (MDP) without defining the reward function, and a set of demonstrations generated by humans/experts. However, in practice, it may be unreasonable to assume that human behaviors can be explained by one reward function since they may be inherently inconsistent. Also, demonstrations may be collected from various users and aggregated to infer and predict user's behaviors. In this paper, we introduce the Non-parametric Behavior Clustering IRL algorithm to simultaneously cluster demonstrations and learn multiple reward functions from demonstrations that may be generated from more than one behaviors. Our method is iterative: It alternates between clustering demonstrations into different behavior clusters and inverse learning the reward functions until convergence. It is built upon the Expectation-Maximization formulation and non-parametric clustering in the IRL setting. Further, to improve the computation efficiency, we remove the need of completely solving multiple IRL problems for multiple clusters during the iteration steps and introduce a resampling technique to avoid generating too many unlikely clusters. We demonstrate the convergence and efficiency of the proposed method through learning multiple driver behaviors from demonstrations generated from a grid-world environment and continuous trajectories collected from autonomous robot cars using the Gazebo robot simulator.

##### Abstract (translated by Google)
逆向强化学习（IRL）是在给定马尔可夫决策过程（MDP）的情况下学习单一的奖励函数的任务，没有定义奖励函数以及由人类/专家产生的一组示例。然而，在实践中，假设人类行为可以用一种奖励函数来解释，这可能是不合理的，因为它们本质上是不一致的。另外，可以从各种用户收集演示并汇总以推断和预测用户的行为。在本文中，我们引入非参数行为聚类IRL算法来同时聚类演示，并从多个行为中可能产生的演示中学习多个奖励函数。我们的方法是迭代的：在不同的行为集群中交替进行聚类演示，反向学习奖励函数直到收敛。它建立在期望最大化公式和IRL设置中的非参数聚类上。此外，为了提高计算效率，我们在迭代步骤中不需要完全解决多个簇的多个IRL问题，并且引入重采样技术以避免产生太多不太可能的簇。我们通过学习从网格世界环境中产生的演示以及使用Gazebo机器人模拟器从自主机器人汽车收集的连续轨迹来学习多个驾驶员行为，来证明所提出的方法的收敛性和效率。

##### URL
[https://arxiv.org/abs/1712.05514](https://arxiv.org/abs/1712.05514)

##### PDF
[https://arxiv.org/pdf/1712.05514](https://arxiv.org/pdf/1712.05514)

