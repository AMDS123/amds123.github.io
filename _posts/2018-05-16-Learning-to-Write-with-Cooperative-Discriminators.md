---
layout: post
title: "Learning to Write with Cooperative Discriminators"
date: 2018-05-16 01:27:58
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, Yejin Choi
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models, but when used to generate natural language their output tends to be overly generic, repetitive, and self-contradictory. We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the notion of communicative goals described by linguistic principles such as Grice's Maxims. We propose learning a mixture of multiple discriminative models that can be used to complement the RNN generator and guide the decoding process. Human evaluation demonstrates that text generated by our system is preferred over that of baselines by a large margin and significantly enhances the overall coherence, style, and information content of the generated text.

##### Abstract (translated by Google)
递归神经网络（RNN）是强大的自回归序列模型，但是当用于生成自然语言时，它们的输出往往过于一般化，重复性和自相矛盾。我们假设RNN语言模型优化的目标函数（相当于文本的整体困惑）不足以表达由语言学原理（如格莱斯的格言）描述的交际目标的概念。我们建议学习多种区分模型的混合，这些模型可以用来补充RNN生成器并指导解码过程。人类评估表明，我们系统生成的文本优于基准文本，大大增强了生成文本的整体一致性，风格和信息内容。

##### URL
[http://arxiv.org/abs/1805.06087](http://arxiv.org/abs/1805.06087)

##### PDF
[http://arxiv.org/pdf/1805.06087](http://arxiv.org/pdf/1805.06087)

