---
layout: post
title: "Additive Margin Softmax for Face Verification"
date: 2018-01-17 09:13:05
categories: arXiv_CV
tags: arXiv_CV Face
author: Feng Wang, Weiyang Liu, Haijun Liu, Jian Cheng
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax

##### Abstract (translated by Google)
在本文中，我们提出了一个概念上简单和几何可解释的目标函数，即用于深度验证的附加裕度Softmax（AM-Softmax）。一般情况下，人脸验证任务可以看作是一个度量学习问题，因此学习类内变异较小，类间差异较大的大边缘人脸特征对于实现良好的性能具有重要意义。最近，大边缘Softmax和角度Softmax已经被提出来将角边缘以乘法方式结合起来。在这项工作中，我们为Softmax损失引入了一个新的角度余量，这比现有的作品具有直观的吸引力和可解释性。本文也强调和讨论了特征归一化的重要性。最重要的是，我们在LFW BLUFR和MegaFace上的实验表明，我们的附加容限softmax损失始终比使用相同网络架构和训练数据集的当前最先进的方法执行得更好。我们的代码也可以在https://github.com/happynear/AMSoftmax上找到

##### URL
[http://arxiv.org/abs/1801.05599](http://arxiv.org/abs/1801.05599)

##### PDF
[http://arxiv.org/pdf/1801.05599](http://arxiv.org/pdf/1801.05599)

