---
layout: post
title: "Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation"
date: 2018-06-12 11:21:16
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention
author: Haoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin, Ting Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Classic pipeline models for task-oriented dialogue system require explicit modeling the dialogue states and hand-crafted action spaces to query a domain-specific knowledge base. Conversely, sequence-to-sequence models learn to map dialogue history to the response in current turn without explicit knowledge base querying. In this work, we propose a novel framework that leverages the advantages of classic pipeline and sequence-to-sequence models. Our framework models a dialogue state as a fixed-size distributed representation and use this representation to query a knowledge base via an attention mechanism. Experiment on Stanford Multi-turn Multi-domain Task-oriented Dialogue Dataset shows that our framework significantly outperforms other sequence-to-sequence based baseline models on both automatic and human evaluation.

##### Abstract (translated by Google)
面向任务的对话系统的经典流水线模型需要显式建模对话状态和手工制作的动作空间以查询特定于领域的知识库。相反，序列到序列模型学习将对话历史映射到当前回答中，而不需要明确的知识库查询。在这项工作中，我们提出了一个新颖的框架，它利用了经典流水线和序列到序列模型的优点。我们的框架将对话状态建模为固定大小的分布式表示，并使用此表示通过注意机制查询知识库。斯坦福多回合多领域任务型对话的实验数据集显示，我们的框架在自动评估和人工评估方面都明显优于其他基于序列的序列基准模型。

##### URL
[http://arxiv.org/abs/1806.04441](http://arxiv.org/abs/1806.04441)

##### PDF
[http://arxiv.org/pdf/1806.04441](http://arxiv.org/pdf/1806.04441)

