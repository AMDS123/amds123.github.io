---
layout: post
title: "Clustering by Hierarchical Nearest Neighbor Descent"
date: 2016-03-04 15:50:58
categories: arXiv_CV
tags: arXiv_CV Salient Face
author: Teng Qiu, Yongjie Li
mathjax: true
---

* content
{:toc}

##### Abstract
Previously in 2014, we proposed the Nearest Descent (ND) method, capable of generating an efficient Graph, called the in-tree (IT). Due to some beautiful and effective features, this IT structure proves well suited for data clustering. Although there exist some redundant edges in IT, they usually have salient features and thus it is not hard to remove them. Subsequently, in order to prevent the seemingly redundant edges from occurring, we proposed the Nearest Neighbor Descent (NND) by adding the "Neighborhood" constraint on ND. Consequently, clusters automatically emerged, without the additional requirement of removing the redundant edges. However, NND proved still not perfect, since it brought in a new yet worse problem, the "over-partitioning" problem. Now, in this paper, we propose a method, called the Hierarchical Nearest Neighbor Descent (H-NND), which overcomes the over-partitioning problem of NND via using the hierarchical strategy. Specifically, H-NND uses ND to effectively merge the over-segmented sub-graphs or clusters that NND produces. Like ND, H-NND also generates the IT structure, in which the redundant edges once again appear. This seemingly comes back to the situation that ND faces. However, compared with ND, the redundant edges in the IT structure generated by H-NND generally become more salient, thus being much easier and more reliable to be identified even by the simplest edge-removing method which takes the edge length as the only measure. In other words, the IT structure constructed by H-NND becomes more fitted for data clustering. We prove this on several clustering datasets of varying shapes, dimensions and attributes. Besides, compared with ND, H-NND generally takes less computation time to construct the IT data structure for the input data.

##### Abstract (translated by Google)
之前在2014年，我们提出了最近下降（ND）方法，能够生成一个有效的图，称为树内（IT）。由于一些美丽和有效的功能，这个IT结构证明非常适合数据聚类。 IT部门虽然存在一些冗余边缘，但通常具有显着的特点，因此不难将其删除。随后，为了防止看似冗余的边缘发生，我们通过在ND上增加“邻居”约束来提出最近邻居下降（NND）。因此，集群自动出现，没有额外的要求去除多余的边缘。然而，NND证明还不够完善，因为它带来了一个新的更严重的问题，即“过度分割”问题。在本文中，我们提出了一种称为层次最近邻下降（H-NND）的方法，通过层次策略克服了NND的超分割问题。具体而言，H-NND使用ND来有效地合并NND产生的过度分割的子图或群集。与ND一样，H-NND也生成IT结构，其中冗余边再次出现。这貌似回到了ND面临的局面。然而，与ND相比，由H-NND生成的IT结构中的冗余边缘通常变得更加突出，即使通过以边缘长度为唯一度量的最简单的边缘去除方法，也更容易被识别和更可靠地识别。换句话说，由H-NND构造的IT结构变得更适合于数据聚类。我们在不同形状，尺寸和属性的几个聚类数据集上证明了这一点。此外，与ND相比，H-NND一般花费较少的计算时间来构建输入数据的IT数据结构。

##### URL
[https://arxiv.org/abs/1509.02805](https://arxiv.org/abs/1509.02805)

##### PDF
[https://arxiv.org/pdf/1509.02805](https://arxiv.org/pdf/1509.02805)

