---
layout: post
title: "Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance"
date: 2018-05-26 00:47:14
categories: arXiv_AI
tags: arXiv_AI Regularization Represenation_Learning Deep_Learning
author: Neal Jean, Sang Michael Xie, Stefano Ermon
mathjax: true
---

* content
{:toc}

##### Abstract
Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.

##### Abstract (translated by Google)
通常需要大量的标记数据来训练深度学习模型。但是，对于许多现实世界的问题，获取附加数据可能很昂贵，甚至不可能。我们提出半监督深核学习（SSDKL），一种基于最小化后验正则化框架预测方差的半监督回归模型。 SSDKL将神经网络的分层表示学习与高斯过程的概率建模能力相结合。通过利用未标记的数据，我们展示了对监督的深度内核学习和半监督方法（如增值税和适合回归的平均值教师）的各种实际回归任务的改进。

##### URL
[http://arxiv.org/abs/1805.10407](http://arxiv.org/abs/1805.10407)

##### PDF
[http://arxiv.org/pdf/1805.10407](http://arxiv.org/pdf/1805.10407)

