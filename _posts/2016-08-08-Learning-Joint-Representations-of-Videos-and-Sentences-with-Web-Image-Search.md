---
layout: post
title: "Learning Joint Representations of Videos and Sentences with Web Image Search"
date: 2016-08-08 09:54:15
categories: arXiv_CV
tags: arXiv_CV Embedding
author: Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkilä, Naokazu Yokoya
mathjax: true
---

* content
{:toc}

##### Abstract
Our objective is video retrieval based on natural language queries. In addition, we consider the analogous problem of retrieving sentences or generating descriptions given an input video. Recent work has addressed the problem by embedding visual and textual inputs into a common space where semantic similarities correlate to distances. We also adopt the embedding approach, and make the following contributions: First, we utilize web image search in sentence embedding process to disambiguate fine-grained visual concepts. Second, we propose embedding models for sentence, image, and video inputs whose parameters are learned simultaneously. Finally, we show how the proposed model can be applied to description generation. Overall, we observe a clear improvement over the state-of-the-art methods in the video and sentence retrieval tasks. In description generation, the performance level is comparable to the current state-of-the-art, although our embeddings were trained for the retrieval tasks.

##### Abstract (translated by Google)
我们的目标是基于自然语言查询的视频检索。此外，我们考虑类似的问题，检索句子或生成输入视频的描述。最近的工作已经通过将视觉和文本输入嵌入语义相似性与距离相关的公共空间来解决该问题。我们也采用了嵌入方法，做出如下贡献：首先，我们利用句子嵌入过程中的网页图像搜索来消除细粒度的视觉概念。其次，我们提出了同时学习参数的句子，图像和视频输入的嵌入模型。最后，我们展示了如何将所提出的模型应用于描述生成。总的来说，我们观察到视频和句子检索任务中最先进的方法有了明显的改进。在描述生成中，性能水平与当前的最新水平相当，尽管我们的嵌入已经被训练用于检索任务。

##### URL
[https://arxiv.org/abs/1608.02367](https://arxiv.org/abs/1608.02367)

##### PDF
[https://arxiv.org/pdf/1608.02367](https://arxiv.org/pdf/1608.02367)

