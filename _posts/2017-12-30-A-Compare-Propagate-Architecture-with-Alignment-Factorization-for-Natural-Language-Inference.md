---
layout: post
title: "A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference"
date: 2017-12-30 08:54:16
categories: arXiv_AI
tags: arXiv_AI Represenation_Learning Inference Deep_Learning
author: Yi Tay, Luu Anh Tuan, Siu Cheung Hui
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new compare-propagate architecture where alignments pairs are compared and then propagated to upper layers for enhanced representation learning. Secondly, we adopt novel factorization layers for efficient compression of alignment vectors into scalar valued features, which are then be used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving state-of-the-art performance on all. A lightweight parameterization of our model enjoys a $\approx 300\%$ reduction in parameter size compared to the ESIM and DIIN, while maintaining competitive performance. Visual analysis shows that our propagated features are highly interpretable, opening new avenues to explainability in neural NLI models.

##### Abstract (translated by Google)
本文提出了一种新的自然语言推理（NLI）的深度学习架构。首先，我们介绍一种新的比较传播架构，比较对齐对，然后传播到上层以增强表示学习。其次，我们采用新的分解层来将对齐矢量有效地压缩成标量值特征，然后将其用于扩充基本表示。我们的方法的设计旨在从概念上简单，紧凑而且功能强大。我们在三个流行的基准SNLI，MultiNLI和SciTail上进行了实验，实现了最先进的性能。与ESIM和DIIN相比，我们的模型的轻量化参数化可以使参数尺寸减少约300％，同时保持竞争性能。视觉分析表明，我们的传播特征是高度可解释的，为神经NLI模型的可解释性开辟了新的途径。

##### URL
[http://arxiv.org/abs/1801.00102](http://arxiv.org/abs/1801.00102)

##### PDF
[http://arxiv.org/pdf/1801.00102](http://arxiv.org/pdf/1801.00102)

