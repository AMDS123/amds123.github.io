---
layout: post
title: "Collaborative Quantization for Cross-Modal Similarity Search"
date: 2019-02-02 02:20:25
categories: arXiv_CV
tags: arXiv_CV
author: Ting Zhang, Jingdong Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Cross-modal similarity search is a problem about designing a search system supporting querying across content modalities, e.g., using an image to search for texts or using a text to search for images. This paper presents a compact coding solution for efficient search, with a focus on the quantization approach which has already shown the superior performance over the hashing solutions in the single-modal similarity search. We propose a cross-modal quantization approach, which is among the early attempts to introduce quantization into cross-modal search. The major contribution lies in jointly learning the quantizers for both modalities through aligning the quantized representations for each pair of image and text belonging to a document. In addition, our approach simultaneously learns the common space for both modalities in which quantization is conducted to enable efficient and effective search using the Euclidean distance computed in the common space with fast distance table lookup. Experimental results compared with several competitive algorithms over three benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.00623](http://arxiv.org/abs/1902.00623)

##### PDF
[http://arxiv.org/pdf/1902.00623](http://arxiv.org/pdf/1902.00623)

