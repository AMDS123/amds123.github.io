---
layout: post
title: "Enhanced LSTM for Natural Language Inference"
date: 2017-04-26 17:37:13
categories: arXiv_CL
tags: arXiv_CL Inference RNN
author: Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Diana Inkpen
mathjax: true
---

* content
{:toc}

##### Abstract
Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.

##### Abstract (translated by Google)
推理和推理是人类和人工智能的核心。人类语言的建模推理非常具有挑战性。随着大量注释数据的可用性（Bowman等，2015），训练基于神经网络的推理模型最近已经变得可行，这已经证明是非常有效的。在本文中，我们提出了一个新的最新的结果，达到斯坦福自然语言推理数据集的88.6％的准确性。与以前使用非常复杂的网络体系结构的顶级模型不同，我们首先证明，仔细设计基于链式LSTM的顺序推理模型可以胜过以前的所有模型。基于此，我们进一步表明，通过在局部推理建模和推理组合中明确考虑递归体系结构，我们实现了额外的改进。特别是，结合语法分析信息有助于达到最佳效果 - 即使添加到已经非常强大的模型中，也可以进一步提高性能。

##### URL
[https://arxiv.org/abs/1609.06038](https://arxiv.org/abs/1609.06038)

##### PDF
[https://arxiv.org/pdf/1609.06038](https://arxiv.org/pdf/1609.06038)

