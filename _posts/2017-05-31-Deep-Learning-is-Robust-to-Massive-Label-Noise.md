---
layout: post
title: "Deep Learning is Robust to Massive Label Noise"
date: 2017-05-31 02:02:56
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.

##### Abstract (translated by Google)
在大型监督数据集上训练的深度神经网络近年来取得了令人印象深刻的成果。但是，由于注释良好的数据集可能过于昂贵而且费时，所以最近的工作已经探索使用更容易获得的更大但噪声更大的数据集。在本文中，我们研究深度神经网络在具有大量噪声标签的训练集上的行为。我们表明，即使有本质上的任意数量的噪音，成功的学习也是可能的。例如，在MNIST上，我们发现，即使数据集已被每个干净示例的100个噪声样本稀释，仍然可以达到90％以上的准确度。这种行为可以保持多种标签噪声模式，即使有噪声的标签对混淆类也有偏见。此外，我们还展示了所需数据集大小对于成功训练如何随着标签噪声的增加而增加。最后，我们提出简单的可操作的技术来改善高标签噪音制度的学习。

##### URL
[https://arxiv.org/abs/1705.10694](https://arxiv.org/abs/1705.10694)

##### PDF
[https://arxiv.org/pdf/1705.10694](https://arxiv.org/pdf/1705.10694)

