---
layout: post
title: "Discriminative Label Consistent Domain Adaptation"
date: 2018-02-21 13:30:52
categories: arXiv_CV
tags: arXiv_CV Sparse Knowledge Image_Classification Transfer_Learning Classification Prediction Recognition
author: Lingkun Luo, Liming Chen, Ying lu, Shiqiang Hu
mathjax: true
---

* content
{:toc}

##### Abstract
Domain adaptation (DA) is transfer learning which aims to learn an effective predictor on target data from source data despite data distribution mismatch between source and target. We present in this paper a novel unsupervised DA method for cross-domain visual recognition which simultaneously optimizes the three terms of a theoretically established error bound. Specifically, the proposed DA method iteratively searches a latent shared feature subspace where not only the divergence of data distributions between the source domain and the target domain is decreased as most state-of-the-art DA methods do, but also the inter-class distances are increased to facilitate discriminative learning. Moreover, the proposed DA method sparsely regresses class labels from the features achieved in the shared subspace while minimizing the prediction errors on the source data and ensuring label consistency between source and target. Data outliers are also accounted for to further avoid negative knowledge transfer. Comprehensive experiments and in-depth analysis verify the effectiveness of the proposed DA method which consistently outperforms the state-of-the-art DA methods on standard DA benchmarks, i.e., 12 cross-domain image classification tasks.

##### Abstract (translated by Google)
领域适应（DA）是转移学习，旨在从源数据中学习目标数据的有效预测器，尽管源和目标之间的数据分布不匹配。我们在本文中提出了一种新的无监督DA方法的跨域视觉识别，同时优化了理论上建立的误差界限的三个术语。具体而言，所提出的DA方法迭代地搜索潜在共享特征子空间，其中不仅大多数先进DA方法所做的源域和目标域之间的数据分布的散度减少，而且类间距离增加以促进歧视性学习。此外，所提出的DA方法从共享子空间中实现的特征中稀疏地回归类别标签，同时最小化源数据上的预测误差并确保源与目标之间的标签一致性。数据异常值也是为了进一步避免负面知识转移。全面的实验和深入的分析验证了DA方法的有效性，该方法在标准DA基准测试中始终优于最先进的DA方法，即12个跨域图像分类任务。

##### URL
[http://arxiv.org/abs/1802.08077](http://arxiv.org/abs/1802.08077)

##### PDF
[http://arxiv.org/pdf/1802.08077](http://arxiv.org/pdf/1802.08077)

