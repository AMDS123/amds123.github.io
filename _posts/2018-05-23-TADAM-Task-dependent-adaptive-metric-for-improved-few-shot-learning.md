---
layout: post
title: "TADAM: Task dependent adaptive metric for improved few-shot learning"
date: 2018-05-23 20:17:59
categories: arXiv_AI
tags: arXiv_AI Optimization Classification
author: Boris N. Oreshkin, Alexandre Lacoste, Pau Rodriguez
mathjax: true
---

* content
{:toc}

##### Abstract
Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100.

##### Abstract (translated by Google)
少量的学习已经成为生成模型的必要条件，这些模型通过几个例子来概括。在这项工作中，我们确定度量缩放和度量任务调节对于提高少数算法的性能非常重要。我们的分析表明，简单的度量标准缩放彻底改变了少数算法参数更新的性质。度量标度缩放可以在迷你Imagenet 5路5镜头分类任务的某些度量标准上提高高达14％的精度。我们进一步提出了一种简单而有效的方式来调节学习者对任务样本集的了解，从而导致学习任务依赖度量空间。此外，我们提出并经验地测试基于辅助任务协同训练的实际端到端优化过程，以学习任务相关度量空间。由此产生的基于任务依赖的缩放量度的少量学习模型实现了小型Imagenet上的最新技术。我们在本文中基于CIFAR100介绍的另一个少数数据集上确认了这些结果。

##### URL
[http://arxiv.org/abs/1805.10123](http://arxiv.org/abs/1805.10123)

##### PDF
[http://arxiv.org/pdf/1805.10123](http://arxiv.org/pdf/1805.10123)

