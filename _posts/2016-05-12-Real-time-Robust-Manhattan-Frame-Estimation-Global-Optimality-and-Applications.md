---
layout: post
title: "Real-time Robust Manhattan Frame Estimation: Global Optimality and Applications"
date: 2016-05-12 08:55:06
categories: arXiv_CV
tags: arXiv_CV Face Quantitative
author: Kyungdon Joo, Tae-Hyun Oh, Junsik Kim, In So Kweon
mathjax: true
---

* content
{:toc}

##### Abstract
Most man-made environments, such as urban and indoor scenes, consist of a set of parallel and orthogonal planar structures. These structures are approximated by Manhattan world assumption and be referred to Manhattan Frame (MF). Given a set of inputs such as surface normals or vanishing points, we pose an MF estimation problem as a consensus set maximization that maximizes the number of inliers over the rotation search space. Conventionally this problem can be solved by a branch-and-bound framework which mathematically guarantees global optimality. However, the computational time of the conventional branch-and-bound algorithms is rather far from real-time performance. In this paper, we propose a novel bound computation method on an efficient measurement domain for MF estimation, i.e., the extended Gaussian image (EGI). By relaxing the original problem, we can compute the bounds in real-time performance, while preserving global optimality. Furthermore, we quantitatively and qualitatively demonstrate the performance of the proposed method for various synthetic and real-world data. We also show the versatility of our approach through three different applications: extension to multiple MF estimation, video stabilization and line clustering.

##### Abstract (translated by Google)
大多数人造环境，如城市和室内场景，由一组平行和正交的平面结构组成。这些结构近似于曼哈顿的世界假设，并被称为曼哈顿框架（MF）。给定一组输入如曲面法线或消失点，我们提出一个MF估计问题作为最大化共识集，以最大化旋转搜索空间的内点数量。通常，这个问题可以通过数学上保证全局最优性的分支定界框架来解决。然而，传统的分支定界算法的计算时间与实时性相差甚远。在本文中，我们提出了一种新的边界计算方法，用于MF估计的高效测量域，即扩展的高斯图像（EGI）。通过放宽原始问题，我们可以在保持全局最优性的同时计算实时性能的边界。此外，我们定量和定性地展示了提出的方法对各种合成和现实世界的数据的性能。我们还通过三种不同的应用展示了我们的方法的多功能性：扩展到多个MF估计，视频稳定和线聚类。

##### URL
[https://arxiv.org/abs/1605.03730](https://arxiv.org/abs/1605.03730)

##### PDF
[https://arxiv.org/pdf/1605.03730](https://arxiv.org/pdf/1605.03730)

