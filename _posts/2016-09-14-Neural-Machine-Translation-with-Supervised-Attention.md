---
layout: post
title: "Neural Machine Translation with Supervised Attention"
date: 2016-09-14 09:31:40
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Lemao Liu, Masao Utiyama, Andrew Finch, Eiichiro Sumita
mathjax: true
---

* content
{:toc}

##### Abstract
The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.

##### Abstract (translated by Google)
关注机制对于神经机器翻译是有吸引力的，因为它能够通过在目标词和源词之间产生一个对齐来动态地编码一个源语句。不幸的是，已经证明，在对准精度上比传统的对准模型差。在本文中，我们从重新排序的角度分析和解释这个问题，并提出一个在传统的对准模型的指导下学习的监督注意力。对两个中英文翻译任务的实验表明，超级的注意机制可以产生更好的对齐，从而比标准的基于注意力的NMT带来更多的收益。

##### URL
[https://arxiv.org/abs/1609.04186](https://arxiv.org/abs/1609.04186)

##### PDF
[https://arxiv.org/pdf/1609.04186](https://arxiv.org/pdf/1609.04186)

