---
layout: post
title: "XNLI: Evaluating Cross-lingual Sentence Representations"
date: 2018-09-13 16:39:53
categories: arXiv_AI
tags: arXiv_AI Inference RNN
author: Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk, Veselin Stoyanov
mathjax: true
---

* content
{:toc}

##### Abstract
State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.

##### Abstract (translated by Google)
最先进的自然语言处理系统依靠注释数据形式的监督来学习有能力的模型。这些模型通常使用单一语言（通常是英语）对数据进行培训，并且不能直接在该语言之外使用。由于收集每种语言的数据是不现实的，因此人们越来越关注跨语言理解（XLU）和低资源的跨语言转移。在这项工作中，我们通过将多类型自然语言推理语料库（MultiNLI）的开发和测试集扩展到15种语言（包括斯瓦希里语和乌尔都语等低资源语言）来构建XLU评估集。我们希望我们的数据集（称为XNLI）将通过提供信息性的标准评估任务来促进跨语言句子理解的研究。此外，我们为多语言句子理解提供了几个基线，其中两个基于机器翻译系统，两个使用并行数据来训练对齐的多语言词袋和LSTM编码器。我们发现XNLI代表了一个实用且具有挑战性的评估套件，直接翻译测试数据可以在可用的基线中产生最佳性能。

##### URL
[http://arxiv.org/abs/1809.05053](http://arxiv.org/abs/1809.05053)

##### PDF
[http://arxiv.org/pdf/1809.05053](http://arxiv.org/pdf/1809.05053)

