---
layout: post
title: "Learning deep representation of multityped objects and tasks"
date: 2016-03-04 06:34:24
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval Sparse Prediction Relation
author: Truyen Tran, Dinh Phung, Svetha Venkatesh
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a deep multitask architecture to integrate multityped representations of multimodal objects. This multitype exposition is less abstract than the multimodal characterization, but more machine-friendly, and thus is more precise to model. For example, an image can be described by multiple visual views, which can be in the forms of bag-of-words (counts) or color/texture histograms (real-valued). At the same time, the image may have several social tags, which are best described using a sparse binary vector. Our deep model takes as input multiple type-specific features, narrows the cross-modality semantic gaps, learns cross-type correlation, and produces a high-level homogeneous representation. At the same time, the model supports heterogeneously typed tasks. We demonstrate the capacity of the model on two applications: social image retrieval and multiple concept prediction. The deep architecture produces more compact representation, naturally integrates multiviews and multimodalities, exploits better side information, and most importantly, performs competitively against baselines.

##### Abstract (translated by Google)
我们引入了一个深入的多任务架构来整合多模式对象的多重表示。这种多种展示比多模式表征更抽象，但是更加机器友好，因此模型更精确。例如，图像可以通过多个视觉视图来描述，其可以是字袋（计数）或颜色/纹理直方图（实值）的形式。同时，图像可能有多个社交标签，最好使用稀疏二元向量来描述。我们的深层模型将输入的多个特定类型的特征作为输入，缩小了跨模式的语义差距，学习了交叉类型的相关性，并产生了一个高层次的均匀表示。同时，该模型支持异构类型的任务。我们证明了模型在两个应用程序上的能力：社交图像检索和多重概念预测。深层架构产生更紧凑的代表性，自然地结合了多视图和多模态，利用更好的辅助信息，最重要的是，与基线进行竞争性的对抗。

##### URL
[https://arxiv.org/abs/1603.01359](https://arxiv.org/abs/1603.01359)

##### PDF
[https://arxiv.org/pdf/1603.01359](https://arxiv.org/pdf/1603.01359)

