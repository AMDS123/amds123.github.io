---
layout: post
title: "Using $k$-way Co-occurrences for Learning Word Embeddings"
date: 2017-09-05 00:25:58
categories: arXiv_CL
tags: arXiv_CL Embedding Relation
author: Danushka Bollegala, Yuichi Yoshida, Ken-ichi Kawarabayashi
mathjax: true
---

* content
{:toc}

##### Abstract
Co-occurrences between two words provide useful insights into the semantics of those words. Consequently, numerous prior work on word embedding learning have used co-occurrences between two words as the training signal for learning word embeddings. However, in natural language texts it is common for multiple words to be related and co-occurring in the same context. We extend the notion of co-occurrences to cover $k(\geq\!\!2)$-way co-occurrences among a set of $k$-words. Specifically, we prove a theoretical relationship between the joint probability of $k(\geq\!\!2)$ words, and the sum of $\ell_2$ norms of their embeddings. Next, we propose a learning objective motivated by our theoretical result that utilises $k$-way co-occurrences for learning word embeddings. Our experimental results show that the derived theoretical relationship does indeed hold empirically, and despite data sparsity, for some smaller $k$ values, $k$-way embeddings perform comparably or better than $2$-way embeddings in a range of tasks.

##### Abstract (translated by Google)
两个词之间的共现为这些词的语义提供了有用的见解。因此，大量的关于词嵌入学习的研究已经将两个词之间的共现用作用于学习词嵌入的训练信号。然而，在自然语言文本中，多个单词在相同的上下文中是相关的并且共同发生的。我们扩展同现的概念来涵盖$ k $（\ geq \！\！2）$  - 方式中的一组$ k $  - 词的同现。具体来说，我们证明了$ k（\ geq \！\！2）$ words的联合概率与它们嵌入的$ \ ell_2 $标准之和之间的理论关系。接下来，我们提出了一个学习目标，其动机是我们的理论结果，它利用$ k $ -way同现来学习单词嵌入。我们的实验结果表明，派生的理论关系确实是经验性的，尽管有数据稀疏性，对于一些较小的$ k $值，$ k $ -way嵌入在一系列任务中表现比$ 2 $更好地嵌入。

##### URL
[https://arxiv.org/abs/1709.01199](https://arxiv.org/abs/1709.01199)

##### PDF
[https://arxiv.org/pdf/1709.01199](https://arxiv.org/pdf/1709.01199)

