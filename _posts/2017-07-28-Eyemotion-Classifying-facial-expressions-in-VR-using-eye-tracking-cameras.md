---
layout: post
title: "Eyemotion: Classifying facial expressions in VR using eye-tracking cameras"
date: 2017-07-28 19:05:50
categories: arXiv_CV
tags: arXiv_CV Face Tracking Inference
author: Steven Hickson, Nick Dufour, Avneesh Sud, Vivek Kwatra, Irfan Essa
mathjax: true
---

* content
{:toc}

##### Abstract
One of the main challenges of social interaction in virtual reality settings is that head-mounted displays occlude a large portion of the face, blocking facial expressions and thereby restricting social engagement cues among users. Hence, auxiliary means of sensing and conveying these expressions are needed. We present an algorithm to automatically infer expressions by analyzing only a partially occluded face while the user is engaged in a virtual reality experience. Specifically, we show that images of the user's eyes captured from an IR gaze-tracking camera within a VR headset are sufficient to infer a select subset of facial expressions without the use of any fixed external camera. Using these inferences, we can generate dynamic avatars in real-time which function as an expressive surrogate for the user. We propose a novel data collection pipeline as well as a novel approach for increasing CNN accuracy via personalization. Our results show a mean accuracy of 74% ($F1$ of 0.73) among 5 `emotive' expressions and a mean accuracy of 70% ($F1$ of 0.68) among 10 distinct facial action units, outperforming human raters.

##### Abstract (translated by Google)
虚拟现实环境中社交互动的主要挑战之一是头戴式显示器遮挡了大部分脸部，阻塞了脸部表情，从而限制了用户之间的社交参与线索。因此，需要传感和传达这些表达的辅助手段。我们提出了一种算法，当用户参与虚拟现实体验时，通过仅分析部分遮挡的脸部来自动推断表情。具体来说，我们展示从VR头戴式耳机中的IR注视跟踪相机捕获的用户眼睛的图像足以推断面部表情的选择子集，而不使用任何固定的外部相机。使用这些推论，我们可以实时生成动态头像，作为用户的表达替代者。我们提出了一种新颖的数据收集管道，以及通过个性化提高CNN准确度的新方法。我们的结果显示，在10个不同的面部动作单元中，5个“情感”表达式中的平均准确率为74％（$ F1 $ 0.73），平均准确率为70％（$ F1 $ 0.68），表现优于人类评分者。

##### URL
[https://arxiv.org/abs/1707.07204](https://arxiv.org/abs/1707.07204)

##### PDF
[https://arxiv.org/pdf/1707.07204](https://arxiv.org/pdf/1707.07204)

