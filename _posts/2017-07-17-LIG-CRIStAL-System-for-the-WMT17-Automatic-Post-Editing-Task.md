---
layout: post
title: "LIG-CRIStAL System for the WMT17 Automatic Post-Editing Task"
date: 2017-07-17 12:25:52
categories: arXiv_CL
tags: arXiv_CL Attention
author: Alexandre Berard, Olivier Pietquin, Laurent Besacier
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents the LIG-CRIStAL submission to the shared Automatic Post- Editing task of WMT 2017. We propose two neural post-editing models: a monosource model with a task-specific attention mechanism, which performs particularly well in a low-resource scenario; and a chained architecture which makes use of the source sentence to provide extra context. This latter architecture manages to slightly improve our results when more training data is available. We present and discuss our results on two datasets (en-de and de-en) that are made available for the task.

##### Abstract (translated by Google)
本文将LIG-CRIStAL提交给WMT 2017的共享自动后期编辑任务。我们提出了两个神经后期编辑模型：一个具有特定任务注意机制的单源模型，在低资源情况下表现特别好;以及利用源语句提供额外语境的链式结构。当更多的训练数据可用时，后一种体系结构可以稍微改善我们的结果。我们在两个数据集（en-de和de-en）上展示和讨论我们的结果，这些数据集可用于任务。

##### URL
[https://arxiv.org/abs/1707.05118](https://arxiv.org/abs/1707.05118)

##### PDF
[https://arxiv.org/pdf/1707.05118](https://arxiv.org/pdf/1707.05118)

