---
layout: post
title: "Refining Architectures of Deep Convolutional Neural Networks"
date: 2016-04-22 22:39:55
categories: arXiv_CV
tags: arXiv_CV CNN Deep_Learning Recognition
author: Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, Roberto Cipolla
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Convolutional Neural Networks (CNNs) have recently evinced immense success for various image recognition tasks. However, a question of paramount importance is somewhat unanswered in deep learning research - is the selected CNN optimal for the dataset in terms of accuracy and model size? In this paper, we intend to answer this question and introduce a novel strategy that alters the architecture of a given CNN for a specified dataset, to potentially enhance the original accuracy while possibly reducing the model size. We use two operations for architecture refinement, viz. stretching and symmetrical splitting. Our procedure starts with a pre-trained CNN for a given dataset, and optimally decides the stretch and split factors across the network to refine the architecture. We empirically demonstrate the necessity of the two operations. We evaluate our approach on two natural scenes attributes datasets, SUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, that are quite contrasting in their construction. We justify our choice of datasets, and show that they are interestingly distinct from each other, and together pose a challenge to our architectural refinement algorithm. Our results substantiate the usefulness of the proposed method.

##### Abstract (translated by Google)
深度卷积神经网络（CNN）最近在各种图像识别任务中取得了巨大的成功。然而，在深度学习研究中，一个至关重要的问题有些没有答案 - 就精确度和模型大小而言，选择的CNN是否最适合数据集？在本文中，我们打算回答这个问题，并介绍一种新的策略，改变给定的数据集的CNN的体系结构，可能提高原有的准确性，同时可能减少模型的大小。我们使用两个操作进行架构细化，伸展和对称分裂。我们的程序从给定数据集的预先训练的CNN开始，最优地决定整个网络的拉伸和分裂因子以改进架构。我们凭经验证明这两项行动的必要性。我们在两个自然场景属性数据集，SUN属性和CAMIT-NSAD上评估了我们的方法，它们的构造与GoogleNet和VGG-11的构造相当对比。我们证明了我们对数据集的选择是正确的，并且证明它们有趣的是彼此不同，并且共同对我们的架构优化算法构成挑战。我们的结果证实了所提出的方法的有用性。

##### URL
[https://arxiv.org/abs/1604.06832](https://arxiv.org/abs/1604.06832)

##### PDF
[https://arxiv.org/pdf/1604.06832](https://arxiv.org/pdf/1604.06832)

