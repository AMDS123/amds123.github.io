---
layout: post
title: "MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels"
date: 2017-12-14 00:02:37
categories: arXiv_CV
tags: arXiv_CV
author: Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei
mathjax: true
---

* content
{:toc}

##### Abstract
Recent studies have discovered that deep networks are capable of memorizing the entire data even when the labels are completely random. Since deep models are trained on big data where labels are often noisy, the ability to overfit noise can lead to poor performance. To overcome the overfitting on corrupted training data, we propose a novel technique to regularize deep networks in the data dimension. This is achieved by learning a neural network called MentorNet to supervise the training of the base network, namely, StudentNet. Our work is inspired by curriculum learning and advances the theory by learning a curriculum from data by neural networks. We demonstrate the efficacy of MentorNet on several benchmarks. Comprehensive experiments show that it is able to significantly improve the generalization performance of the state-of-the-art deep networks on corrupted training data.

##### Abstract (translated by Google)
最近的研究发现，即使标签是完全随机的，深层网络也能够记住整个数据。由于深层模型是在大数据的情况下训练的，而大数据的标签通常是嘈杂的，所以过度装配噪声的能力可能导致性能下降。为了克服对训练数据的过度拟合，我们提出了一种在数据维度上调整深度网络的新技术。这是通过学习称为MentorNet的神经网络来监督基础网络即StudentNet的训练来实现的。我们的工作受到课程学习的启发，并通过神经网络从数据中学习课程来推进理论。我们展示了MentorNet在几个基准测试中的功效。综合实验表明，能够显着提高腐蚀训练数据的深度网络的泛化性能。

##### URL
[http://arxiv.org/abs/1712.05055](http://arxiv.org/abs/1712.05055)

##### PDF
[http://arxiv.org/pdf/1712.05055](http://arxiv.org/pdf/1712.05055)

