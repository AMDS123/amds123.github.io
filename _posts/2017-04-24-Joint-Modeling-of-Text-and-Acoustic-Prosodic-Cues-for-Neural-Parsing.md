---
layout: post
title: "Joint Modeling of Text and Acoustic-Prosodic Cues for Neural Parsing"
date: 2017-04-24 15:33:26
categories: arXiv_CL
tags: arXiv_CL Attention CNN
author: Trang Tran, Shubham Toshniwal, Mohit Bansal, Kevin Gimpel, Karen Livescu, Mari Ostendorf
mathjax: true
---

* content
{:toc}

##### Abstract
In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing a spoken utterance, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and word-based prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together improve parse F1 scores significantly over a strong text-only baseline. For this study with known sentence boundaries, error analysis shows that the main benefit of acoustic-prosodic features is in sentences with disfluencies and that attachment errors are most improved.

##### Abstract (translated by Google)
在对话式语音中，声音信号提供了帮助听众消除疑难解析的线索。为了自动解析口头发言，我们引入了一个模型，该模型使用能量和音高轨迹上的卷积神经网络结合转录的文本和声韵律特征，再加上基于注意的递归神经网络，接受基于文本和基于单词的韵律特征。我们发现，不同类型的声韵律特征是个别有帮助的，并且共同提高了在强文本基线上解析F1得分。对于这个具有已知句子边界的研究，误差分析表明，声韵律特征的主要好处在于不流利的句子，附着错误得到最大的改善。

##### URL
[https://arxiv.org/abs/1704.07287](https://arxiv.org/abs/1704.07287)

##### PDF
[https://arxiv.org/pdf/1704.07287](https://arxiv.org/pdf/1704.07287)

