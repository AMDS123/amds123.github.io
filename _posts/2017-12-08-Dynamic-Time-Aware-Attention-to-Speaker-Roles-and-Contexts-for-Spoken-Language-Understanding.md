---
layout: post
title: "Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding"
date: 2017-12-08 10:47:58
categories: arXiv_CL
tags: arXiv_CL Attention Tracking
author: Po-Chun Chen, Ta-Chung Chi, Shang-Yu Su, Yun-Nung Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Spoken language understanding (SLU) is an essential component in conversational systems. Most SLU component treats each utterance independently, and then the following components aggregate the multi-turn information in the separate phases. In order to avoid error propagation and effectively utilize contexts, prior work leveraged history for contextual SLU. However, the previous model only paid attention to the content in history utterances without considering their temporal information and speaker roles. In the dialogues, the most recent utterances should be more important than the least recent ones. Furthermore, users usually pay attention to 1) self history for reasoning and 2) others' utterances for listening, the speaker of the utterances may provides informative cues to help understanding. Therefore, this paper proposes an attention-based network that additionally leverages temporal information and speaker role for better SLU, where the attention to contexts and speaker roles can be automatically learned in an end-to-end manner. The experiments on the benchmark Dialogue State Tracking Challenge 4 (DSTC4) dataset show that the time-aware dynamic role attention networks significantly improve the understanding performance.

##### Abstract (translated by Google)
口语理解（SLU）是会话系统中的一个重要组成部分。大多数SLU组件独立处理每个话语，然后下面的组件在不同的阶段中聚合多转信息。为了避免错误传播并有效地利用上下文，以前的工作利用了上下文SLU的历史。然而，以往的模型只关注历史话语的内容，却没有考虑其时间信息和说话人角色。在对话中，最近的话语应该比最近的话语更重要。此外，用户通常注意1）自我推理的历史和2）其他人的倾听话语，说话者可以提供有助于理解的信息提示。因此，本文提出了一种基于注意的网络，该网络另外利用时间信息和说话者的角色来实现更好的SLU，其中关注上下文和说话者角色可以以端到端的方式自动学习。基准对话状态追踪挑战4（DSTC4）数据集的实验表明，时间感知的动态角色关注网络显着提高了理解性能。

##### URL
[http://arxiv.org/abs/1710.00165](http://arxiv.org/abs/1710.00165)

##### PDF
[http://arxiv.org/pdf/1710.00165](http://arxiv.org/pdf/1710.00165)

