---
layout: post
title: "Path-SGD: Path-Normalized Optimization in Deep Neural Networks"
date: 2015-06-08 19:01:33
categories: arXiv_CV
tags: arXiv_CV Regularization Optimization
author: Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro
mathjax: true
---

* content
{:toc}

##### Abstract
We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.

##### Abstract (translated by Google)
我们重新考虑用于训练深度神经网络的SGD的选择，通过重新考虑优化权重的适当几何。我们认为几何不变的权重的重新缩放不会影响网络的输出，并建议路径SGD，这是一个近似最速下降方法相对于最大范数正则化路径明智正规化。 Path-SGD实施起来既容易又高效，并导致经济收益超过SGD和AdaGrad。

##### URL
[https://arxiv.org/abs/1506.02617](https://arxiv.org/abs/1506.02617)

##### PDF
[https://arxiv.org/pdf/1506.02617](https://arxiv.org/pdf/1506.02617)

