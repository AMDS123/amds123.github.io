---
layout: post
title: "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation"
date: 2017-03-23 14:20:52
categories: arXiv_CV
tags: arXiv_CV Image_Caption Attention Caption VQA
author: Jean-Benoit Delbrouck, Stephane Dupont
mathjax: true
---

* content
{:toc}

##### Abstract
In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.

##### Abstract (translated by Google)
在最先进的神经机器翻译中，在解码期间使用注意机制来增强翻译。在每个步骤中，解码器使用该机制聚焦于源句子的不同部分，以在输出其目标词之前收集最有用的信息。最近，对于多模式任务也已经探索了注意机制的有效性，其中可以将注意力集中在句子部分和图像区域上。汇集两种方式的方法通常包括元素产品，总和或连接。在本文中，我们评估了更先进的Multimodal Compact双线性池化方法，它采用两个向量的外积来结合两种方式的注意特征。以前已经对视觉问题回答进行过调查。我们尝试使用这种方法进行多模态图像标题翻译，并与基本组合方法相比显示出改进。

##### URL
[https://arxiv.org/abs/1703.08084](https://arxiv.org/abs/1703.08084)

##### PDF
[https://arxiv.org/pdf/1703.08084](https://arxiv.org/pdf/1703.08084)

