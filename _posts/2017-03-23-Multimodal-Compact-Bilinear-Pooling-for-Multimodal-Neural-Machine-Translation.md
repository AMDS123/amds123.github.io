---
layout: post
title: "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation"
date: 2017-03-23 14:20:52
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption VQA
author: Jean-Benoit Delbrouck, Stephane Dupont
---

* content
{:toc}

##### Abstract
In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.

##### Abstract (translated by Google)
在最先进的神经机器翻译中，在解码期间使用注意机制来增强翻译。在每一步中，解码器都使用这种机制来聚焦源句子的不同部分，以在输出其目标词之前收集最有用的信息。最近，关注机制的有效性也在多模式任务中得到了探索，在这些任务中，句子部分和图像区域都成为可能。汇集两种模式的方法通常包括基于元素的产品，总和或级联。在本文中，我们评估更先进的多模式紧凑双线性池化方法，它将两个向量的外积结合在两种模式的注意力特征上。之前已经对视觉问题回答进行了调查。我们试用这种方法进行多模式图像标题翻译，并且与基本组合方法相比显示出改进。

##### URL
[https://arxiv.org/abs/1703.08084](https://arxiv.org/abs/1703.08084)

