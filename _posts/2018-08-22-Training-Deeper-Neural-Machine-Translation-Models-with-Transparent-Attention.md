---
layout: post
title: "Training Deeper Neural Machine Translation Models with Transparent Attention"
date: 2018-08-22 20:53:37
categories: arXiv_AI
tags: arXiv_AI Attention CNN Optimization NMT RNN
author: Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, Yonghui Wu
mathjax: true
---

* content
{:toc}

##### Abstract
While current state-of-the-art NMT models, such as RNN seq2seq and Transformers, possess a large number of parameters, they are still shallow in comparison to convolutional models used for both text and vision applications. In this work we attempt to train significantly (2-3x) deeper Transformer and Bi-RNN encoders for machine translation. We propose a simple modification to the attention mechanism that eases the optimization of deeper models, and results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT'14 English-German and WMT'15 Czech-English tasks for both architectures.

##### Abstract (translated by Google)
虽然目前最先进的NMT模型，如RNN seq2seq和变形金刚，拥有大量参数，但与用于文本和视觉应用的卷积模型相比，它们仍然很浅。在这项工作中，我们尝试训练显着（2-3x）更深的Transformer和Bi-RNN编码器进行机器翻译。我们建议对注意机制进行简单的修改，以简化更深层模型的优化，并在两种架构的基准WMT'14英语 - 德语和WMT'15捷克 - 英语任务中获得0.7-1.1 BLEU的一致增益。

##### URL
[http://arxiv.org/abs/1808.07561](http://arxiv.org/abs/1808.07561)

##### PDF
[http://arxiv.org/pdf/1808.07561](http://arxiv.org/pdf/1808.07561)

