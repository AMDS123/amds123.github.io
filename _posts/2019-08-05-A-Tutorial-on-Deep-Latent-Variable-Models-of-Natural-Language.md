---
layout: post
title: "A Tutorial on Deep Latent Variable Models of Natural Language"
date: 2019-08-05 01:14:32
categories: arXiv_CL
tags: arXiv_CL Inference Deep_Learning
author: Yoon Kim, Sam Wiseman, Alexander M. Rush
mathjax: true
---

* content
{:toc}

##### Abstract
There has been much recent, exciting work on combining the complementary strengths of latent variable models and deep learning. Latent variable modeling makes it easy to explicitly specify model constraints through conditional independence properties, while deep learning makes it possible to parameterize these conditional likelihoods with powerful function approximators. While these "deep latent variable" models provide a rich, flexible framework for modeling many real-world phenomena, difficulties exist: deep parameterizations of conditional likelihoods usually make posterior inference intractable, and latent variable objectives often complicate backpropagation by introducing points of non-differentiability. This tutorial explores these issues in depth through the lens of variational inference.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.06834](http://arxiv.org/abs/1812.06834)

##### PDF
[http://arxiv.org/pdf/1812.06834](http://arxiv.org/pdf/1812.06834)

