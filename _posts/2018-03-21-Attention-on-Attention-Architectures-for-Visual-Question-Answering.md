---
layout: post
title: "Attention on Attention: Architectures for Visual Question Answering"
date: 2018-03-21 03:05:58
categories: arXiv_CV
tags: arXiv_CV QA Attention Deep_Learning VQA
author: Jasdeep Singh, Vincent Ying, Alex Nutkiewicz
mathjax: true
---

* content
{:toc}

##### Abstract
Visual Question Answering (VQA) is an increasingly popular topic in deep learning research, requiring coordination of natural language processing and computer vision modules into a single architecture. We build upon the model which placed first in the VQA Challenge by developing thirteen new attention mechanisms and introducing a simplified classifier. We performed 300 GPU hours of extensive hyperparameter and architecture searches and were able to achieve an evaluation score of 64.78%, outperforming the existing state-of-the-art single model's validation score of 63.15%.

##### Abstract (translated by Google)
视觉问题回答（VQA）是深度学习研究中越来越受欢迎的主题，需要将自然语言处理和计算机视觉模块协调为单一架构。我们通过开发十三种新的注意机制并引入简化的分类器，建立在VQA挑战中首位的模型。我们进行了300小时GPU广泛的超参数和架构搜索，并且能够获得64.78％的评估分数，优于现有的最先进的单一模型的验证分数63.15％。

##### URL
[https://arxiv.org/abs/1803.07724](https://arxiv.org/abs/1803.07724)

##### PDF
[https://arxiv.org/pdf/1803.07724](https://arxiv.org/pdf/1803.07724)

