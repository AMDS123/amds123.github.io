---
layout: post
title: "VisDA: The Visual Domain Adaptation Challenge"
date: 2017-11-29 04:04:18
categories: arXiv_CV
tags: arXiv_CV Segmentation Image_Classification Classification
author: Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, Kate Saenko
mathjax: true
---

* content
{:toc}

##### Abstract
We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a large-scale testbed for unsupervised domain adaptation across visual domains. Unsupervised domain adaptation aims to solve the real-world problem of domain shift, where machine learning models trained on one domain must be transferred and adapted to a novel visual domain without additional supervision. The VisDA2017 challenge is focused on the simulation-to-reality shift and has two associated tasks: image classification and image segmentation. The goal in both tracks is to first train a model on simulated, synthetic data in the source domain and then adapt it to perform well on real image data in the unlabeled test domain. Our dataset is the largest one to date for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation and testing domains. The image segmentation dataset is also large-scale with over 30K images across 18 categories in the three domains. We compare VisDA to existing cross-domain adaptation datasets and provide a baseline performance analysis using various domain adaptation models that are currently popular in the field.

##### Abstract (translated by Google)
我们提出了2017年的视觉域适配（VisDA）数据集和挑战，一个跨视觉域的无监督域适应的大型测试平台。无监督领域适应的目的是解决现实世界中的领域转移问题，在这个领域中，在一个领域上训练的机器学习模型必须被转移并适应新的视觉领域，而不需要额外的监督。 VisDA2017的挑战集中在模拟到现实的转变，并有两个相关的任务：图像分类和图像分割。这两个方面的目标是首先在源域中训练模拟的合成数据的模型，然后使其适应在未标记的测试域中的真实图像数据上的良好性能。我们的数据集是迄今为止跨域对象分类的最大数据集，在整合的培训，验证和测试领域，共有12个类别的超过280K个图像。图像分割数据集也是大规模的，在三个领域的18个类别中具有超过30K的图像。我们将VisDA与现有的跨域适配数据集进行比较，并使用当前在该领域当前流行的各种域适配模型提供基线性能分析。

##### URL
[https://arxiv.org/abs/1710.06924](https://arxiv.org/abs/1710.06924)

##### PDF
[https://arxiv.org/pdf/1710.06924](https://arxiv.org/pdf/1710.06924)

