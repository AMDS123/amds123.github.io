---
layout: post
title: "Excitation Backprop for RNNs"
date: 2018-03-08 16:49:13
categories: arXiv_CV
tags: arXiv_CV Salient Video_Caption Caption Action_Recognition RNN Classification Prediction Recognition
author: Sarah Adel Bargal, Andrea Zunino, Donghyun Kim, Jianming Zhang, Vittorio Murino, Stan Sclaroff
mathjax: true
---

* content
{:toc}

##### Abstract
Deep models are state-of-the-art for many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks.

##### Abstract (translated by Google)
深度模型是许多视觉任务的最新技术，包括视频动作识别和视频字幕。对模型进行培训以对视频中的活动进行标题或分类，但对用于做出此类决策的证据知之甚少。已经在空间视觉内容中研究了由深度网络做出的接地决策，从而更深入地了解图像的模型预测。然而，这些研究相对缺乏时空视觉内容模型 - 视频。在这项工作中，我们设计了一个配方，使用自上而下的显着性，在一次通过中同时在空间和时间上证明证据。我们使用模型的内部表示来显示有助于深度模型的分类/字幕输出的时空线索。基于这些时空线索，我们能够在视频中本地化与特定动作或来自标题的短语相对应的片段，而无需明确优化/训练这些任务。

##### URL
[https://arxiv.org/abs/1711.06778](https://arxiv.org/abs/1711.06778)

##### PDF
[https://arxiv.org/pdf/1711.06778](https://arxiv.org/pdf/1711.06778)

