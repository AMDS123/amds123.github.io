---
layout: post
title: "Topic Modeling over Short Texts by Incorporating Word Embeddings"
date: 2016-09-27 15:26:07
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Relation
author: Jipeng Qiang, Ping Chen, Tong Wang, Xindong Wu
mathjax: true
---

* content
{:toc}

##### Abstract
Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest profiling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this prob- lem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn se- mantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo- texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.

##### Abstract (translated by Google)
从大量短文本中推断主题对于许多内容分析任务（诸如内容特征，用户兴趣分析和新兴主题检测）而言成为关键但是具有挑战性的任务。如概率潜在语义分析（PLSA）和潜在狄利克雷分配（LDA）等现有方法不能很好地解决这个问题，因为在短文本中只有非常有限的单词共现信息。本文研究如何将外部词汇相关知识整合到短文本中，以提高主题建模的一致性。基于最近在大型语料库中学习单词表示的词嵌入的结果，我们引入了一种基于嵌入的主题模型（ETM）的新方法来从短文中学习潜在的主题。 ETM不仅通过将短文本聚合成长伪文本来解决非常有限的单词共现信息的问题，而且还利用马尔科夫随机场正则化模型给相关单词提供更好的机会被放入同一主题。在真实世界的数据集上的实验验证了我们的模型与最先进的模型相比的有效性。

##### URL
[https://arxiv.org/abs/1609.08496](https://arxiv.org/abs/1609.08496)

##### PDF
[https://arxiv.org/pdf/1609.08496](https://arxiv.org/pdf/1609.08496)

