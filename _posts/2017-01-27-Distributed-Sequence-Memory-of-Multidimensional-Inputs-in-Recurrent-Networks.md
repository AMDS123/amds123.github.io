---
layout: post
title: "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks"
date: 2017-01-27 15:03:53
categories: arXiv_CV
tags: arXiv_CV RNN
author: Adam Charles, Dong Yin, Christopher Rozell
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the ambient input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs.

##### Abstract (translated by Google)
递归神经网络（RNN）已经引起了机器学习研究者的兴趣，因为它们有效地保留了过去的时变数据处理任务的输入。为了理解RNNs的成功和局限性，我们推进对基本记忆属性的分析是至关重要的。我们关注的是回声状态网络（ESN），它们是具有简单无记忆节点和随机连接的RNN。在大多数现有的分析中，短期存储容量结果表明ESN网络的大小必须与非结构化输入的输入大小成线性关系。本文的主要贡献是在输入具有共同的低维结构时提供表征具有多维输入流的线性ESN的STM容量的一般结果：基础的稀疏性或输入之间的显着的统计相关性。在这两种情况下，我们都表明，网络中的节点数量必须与信息速率成线性关系，并与环境输入维度呈多元对数关系。分析依赖于随机矩阵理论的高级应用，并导致恢复误差的显式非渐近界。总而言之，这一分析为我们理解RNN中的STM性质迈出了重要的一步。

##### URL
[https://arxiv.org/abs/1605.08346](https://arxiv.org/abs/1605.08346)

##### PDF
[https://arxiv.org/pdf/1605.08346](https://arxiv.org/pdf/1605.08346)

