---
layout: post
title: "Learning Video-Story Composition via Recurrent Neural Network"
date: 2018-01-31 02:35:30
categories: arXiv_CV
tags: arXiv_CV Optimization RNN
author: Guangyu Zhong, Yi-Hsuan Tsai, Sifei Liu, Zhixun Su, Ming-Hsuan Yang
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a learning-based method to compose a video-story from a group of video clips that describe an activity or experience. We learn the coherence between video clips from real videos via the Recurrent Neural Network (RNN) that jointly incorporates the spatial-temporal semantics and motion dynamics to generate smooth and relevant compositions. We further rearrange the results generated by the RNN to make the overall video-story compatible with the storyline structure via a submodular ranking optimization process. Experimental results on the video-story dataset show that the proposed algorithm outperforms the state-of-the-art approach.

##### Abstract (translated by Google)
在本文中，我们提出了一种基于学习的方法，用一组描述活动或体验的视频剪辑来组成视频故事。我们通过循环神经网络（RNN）学习了来自真实视频的视频片段之间的连贯性，它们共同结合了时空语义和运动动力学，以生成平滑和相关的组合。我们进一步重新排列由RNN生成的结果，使整个视频故事通过子模块排名优化过程与故事情节结构相兼容。在视频故事数据集上的实验结果表明，所提出的算法优于最先进的方法。

##### URL
[http://arxiv.org/abs/1801.10281](http://arxiv.org/abs/1801.10281)

##### PDF
[http://arxiv.org/pdf/1801.10281](http://arxiv.org/pdf/1801.10281)

