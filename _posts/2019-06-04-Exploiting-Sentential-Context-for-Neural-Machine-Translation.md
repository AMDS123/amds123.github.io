---
layout: post
title: "Exploiting Sentential Context for Neural Machine Translation"
date: 2019-06-04 08:29:33
categories: arXiv_CL
tags: arXiv_CL NMT
author: Xing Wang, Zhaopeng Tu, Longyue Wang, Shuming Shi
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we first show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-to-German and English-to-French benchmarks show that our model consistently improves performance over the strong TRANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.01268](http://arxiv.org/abs/1906.01268)

##### PDF
[http://arxiv.org/pdf/1906.01268](http://arxiv.org/pdf/1906.01268)

