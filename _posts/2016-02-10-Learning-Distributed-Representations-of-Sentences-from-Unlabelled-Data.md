---
layout: post
title: "Learning Distributed Representations of Sentences from Unlabelled Data"
date: 2016-02-10 18:49:58
categories: arXiv_SD
tags: arXiv_SD
author: Felix Hill, Kyunghyun Cho, Anna Korhonen
mathjax: true
---

* content
{:toc}

##### Abstract
Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.

##### Abstract (translated by Google)
在今天的NLP研究中，无监督学习方法用于学习词语的分布式表示是无处不在的，但是从未标记的数据中学习分布式短语或句子表示的最佳方法的知识要少得多。本文是对学习这种表示的模型进行系统的比较。我们发现最佳的方法关键取决于预期的应用。对于在监督系统中使用的表示，更深层的更复杂的模型是优选的，但浅对数线性模型最适合于构建可以用简单的空间距离度量来解码的表示空间。我们还提出了两个新的无监督表示学习目标，旨在优化培训时间，领域可移植性和性能之间的权衡。

##### URL
[https://arxiv.org/abs/1602.03483](https://arxiv.org/abs/1602.03483)

##### PDF
[https://arxiv.org/pdf/1602.03483](https://arxiv.org/pdf/1602.03483)

