---
layout: post
title: "Knowledge Distillation by On-the-Fly Native Ensemble"
date: 2018-06-12 15:28:53
categories: arXiv_CV
tags: arXiv_CV Knowledge Image_Classification Classification
author: Xu Lan, Xiatian Zhu, Shaogang Gong
mathjax: true
---

* content
{:toc}

##### Abstract
Knowledge distillation is effective to train small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a highcapacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) strategy for one-stage online distillation. Specifically, ONE trains only a single multi-branch network while simultaneously establishing a strong teacher on-the- fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.

##### Abstract (translated by Google)
知识精馏有效地训练小型和一般性网络模型，以满足低内存和快速运行的需求。现有的离线蒸馏方法依赖于强大的预先训练的教师，这使得有利的知识发现和转移，但需要复杂的两阶段培训程序。在线同行以缺乏高容量教师的代价来解决这一限制。在这项工作中，我们提出了一个在线蒸馏的在线原生合奏（ONE）战略。具体来说，ONE只训练一个单一的多分支网络，同时建立一个强大的教师即时增强目标网络的学习。广泛的评估表明，与四种图像分类数据集上的替代方法相比，ONE可以更好地改善各种深度神经网络的泛化性能：CIFAR10，CIFAR100，SVHN和ImageNet，同时具有计算效率优势。

##### URL
[http://arxiv.org/abs/1806.04606](http://arxiv.org/abs/1806.04606)

##### PDF
[http://arxiv.org/pdf/1806.04606](http://arxiv.org/pdf/1806.04606)

