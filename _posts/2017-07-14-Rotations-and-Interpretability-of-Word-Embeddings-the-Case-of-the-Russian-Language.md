---
layout: post
title: "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language"
date: 2017-07-14 23:26:35
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Alexey Zobnin
mathjax: true
---

* content
{:toc}

##### Abstract
Consider a continuous word embedding model. Usually, the cosines between word vectors are used as a measure of similarity of words. These cosines do not change under orthogonal transformations of the embedding space. We demonstrate that, using some canonical orthogonal transformations from SVD, it is possible both to increase the meaning of some components and to make the components more stable under re-learning. We study the interpretability of components for publicly available models for the Russian language (RusVectores, fastText, RDT).

##### Abstract (translated by Google)
考虑一个连续的词嵌入模型。通常，词向量之间的余弦用作词的相似度量度。这些余弦在嵌入空间的正交变换下不会改变。我们证明，使用SVD的一些正则正交变换，既可以增加一些组件的含义，又可以在重新学习的情况下使组件更稳定。我们研究公开可用的俄语模型的组件的可解释性（RusVectores，fastText，RDT）。

##### URL
[https://arxiv.org/abs/1707.04662](https://arxiv.org/abs/1707.04662)

##### PDF
[https://arxiv.org/pdf/1707.04662](https://arxiv.org/pdf/1707.04662)

