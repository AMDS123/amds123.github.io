---
layout: post
title: "Interpretable Policies for Reinforcement Learning by Genetic Programming"
date: 2017-12-12 08:31:51
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Daniel Hein, Steffen Udluft, Thomas A. Runkler
mathjax: true
---

* content
{:toc}

##### Abstract
The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL) approach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy equations from pre-existing default state-action trajectory samples. GPRL is compared to a straight-forward method which utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but non-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression method. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data.

##### Abstract (translated by Google)
寻求可解释的强化学习政策具有很高的学术和工业兴趣。特别是对于工业系统，如果领域专家可以理解且便于评估，他们更可能部署自主学习的控制器。基本的代数方程式应该满足这些要求，只要它们被限制在适当的复杂度。在这里，我们介绍基于基于模型的批量强化学习和遗传编程的强化学习（GPRL）方法的遗传编程，它从先前存在的默认状态行为轨迹样本自主学习政策方程。 GPRL与一种直接的方法相比，该方法利用遗传编程进行符号回归，产生了模仿现有表现良好但不可解释的政策的政策。对三种强化学习基准的实验，即山地车，推车平衡和行业基准，证明了我们的GPRL方法与符号回归方法相比的优越性。 GPRL能够根据预先存在的默认轨迹数据生成性能良好的可解释强化学习策略。

##### URL
[https://arxiv.org/abs/1712.04170](https://arxiv.org/abs/1712.04170)

##### PDF
[https://arxiv.org/pdf/1712.04170](https://arxiv.org/pdf/1712.04170)

