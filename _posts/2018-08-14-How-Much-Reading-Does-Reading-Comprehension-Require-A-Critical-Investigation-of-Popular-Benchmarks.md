---
layout: post
title: "How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks"
date: 2018-08-14 23:59:26
categories: arXiv_AI
tags: arXiv_AI Prediction
author: Divyansh Kaushik, Zachary C. Lipton
mathjax: true
---

* content
{:toc}

##### Abstract
Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On $14$ out of $20$ bAbI tasks, passage-only models achieve greater than $50\%$ accuracy, sometimes matching the full model. Interestingly, while CBT provides $20$-sentence stories only the last is needed for comparably accurate prediction. By comparison, SQuAD and CNN appear better-constructed.

##### Abstract (translated by Google)
最近的许多论文都涉及阅读理解，其中的例子包括（问题，段落，答案）元组。据推测，模型必须结合来自问题和段落的信息来预测相应的答案。然而，尽管对这一主题非常感兴趣，有数百篇已发表的论文争夺排行榜的主导地位，但许多流行基准测试难度的基本问题仍未得到解决。在本文中，我们为bAbI，SQuAD，CBT，CNN和Who-did-What数据集建立了合理的基线，发现只有问题和通过的模型通常表现得非常好。 20美元bAbI任务中的$ 14 $，仅限通道模型的准确度达到50美元以上，有时与完整模型相匹配。有趣的是，虽然CBT提供了20美元的$ _sentence故事，但只有最后一个才能进行相对准确的预测。相比之下，SQuAD和CNN似乎构造得更好。

##### URL
[http://arxiv.org/abs/1808.04926](http://arxiv.org/abs/1808.04926)

##### PDF
[http://arxiv.org/pdf/1808.04926](http://arxiv.org/pdf/1808.04926)

