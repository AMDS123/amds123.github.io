---
layout: post
title: "Learning to Follow Language Instructions with Adversarial Reward Induction"
date: 2018-06-05 22:01:51
categories: arXiv_AI
tags: arXiv_AI Adversarial Relation
author: Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, Edward Grefenstette
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, for many real-world natural language commands that involve a degree of underspecification or ambiguity, such as "tidy the room", it would be challenging or impossible to program an appropriate reward function. To overcome this, we present a method for learning to follow commands from a training set of instructions and corresponding example goal-states, rather than an explicit reward function. Importantly, the example goal-states are not seen at test time. The approach effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, the method enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new training examples.

##### Abstract (translated by Google)
最近的研究表明，深度强化学习机构可以学习如何按照语言般的指示从罕见的环境奖励中获益。然而，对于许多涉及一定程度的不明确性或模糊性的真实世界的自然语言命令，例如“整理房间”，要编制适当的奖励功能将是具有挑战性的或不可能的。为了克服这一点，我们提出了一种学习遵循指令和相应的示例目标状态的命令的方法，而不是明确的奖励函数。重要的是，测试时没有看到示例目标状态。该方法有效地将指令所需的表示与如何执行分开。在简单的网格世界中，该方法使代理能够学习一系列需要与块交互以及理解空间关系和不明确的抽象安排的命令。我们进一步显示该方法可以让我们的代理人适应环境的变化，而不需要新的培训实例。

##### URL
[http://arxiv.org/abs/1806.01946](http://arxiv.org/abs/1806.01946)

##### PDF
[http://arxiv.org/pdf/1806.01946](http://arxiv.org/pdf/1806.01946)

