---
layout: post
title: "Transfer with Model Features in Reinforcement Learning"
date: 2018-07-04 18:41:27
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning Optimization
author: Lucas Lehnert, Michael L. Littman
mathjax: true
---

* content
{:toc}

##### Abstract
A key question in Reinforcement Learning is which representation an agent can learn to efficiently reuse knowledge between different tasks. Recently the Successor Representation was shown to have empirical benefits for transferring knowledge between tasks with shared transition dynamics. This paper presents Model Features: a feature representation that clusters behaviourally equivalent states and that is equivalent to a Model-Reduction. Further, we present a Successor Feature model which shows that learning Successor Features is equivalent to learning a Model-Reduction. A novel optimization objective is developed and we provide bounds showing that minimizing this objective results in an increasingly improved approximation of a Model-Reduction. Further, we provide transfer experiments on randomly generated MDPs which vary in their transition and reward functions but approximately preserve behavioural equivalence between states. These results demonstrate that Model Features are suitable for transfer between tasks with varying transition and reward functions.

##### Abstract (translated by Google)
强化学习中的一个关键问题是代理可以学习如何在不同任务之间有效地重用知识。最近，继任者代表被证明在具有共同过渡动态的任务之间传递知识具有经验益处。本文介绍了模型特征：一种特征表示，它集群行为等效状态，相当于模型简化。此外，我们提出了一个后继特征模型，该模型显示学习后继特征等同于学习模型简化。我们开发了一个新的优化目标，并且我们提供了界限，表明最小化该目标导致模型减少的近似得到越来越多的改进。此外，我们提供随机生成的MDP的转移实验，这些MDP的转换和奖励功能各不相同，但大致保持了状态之间的行为等同性。这些结果表明，模型特征适用于具有不同过渡和奖励功能的任务之间的转移。

##### URL
[http://arxiv.org/abs/1807.01736](http://arxiv.org/abs/1807.01736)

##### PDF
[http://arxiv.org/pdf/1807.01736](http://arxiv.org/pdf/1807.01736)

