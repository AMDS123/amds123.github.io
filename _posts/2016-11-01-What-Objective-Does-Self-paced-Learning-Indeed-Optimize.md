---
layout: post
title: "What Objective Does Self-paced Learning Indeed Optimize?"
date: 2016-11-01 13:59:27
categories: arXiv_CV
tags: arXiv_CV Embedding Relation Recognition
author: Deyu Meng, Qian Zhao, Lu Jiang
mathjax: true
---

* content
{:toc}

##### Abstract
Self-paced learning (SPL) is a recently raised methodology designed through simulating the learning principle of humans/animals. A variety of SPL realization schemes have been designed for different computer vision and pattern recognition tasks, and empirically substantiated to be effective in these applications. However, the investigation on its theoretical insight is still a blank. To this issue, this study attempts to provide some new theoretical understanding under the SPL scheme. Specifically, we prove that the solving strategy on SPL accords with a majorization minimization algorithm implemented on a latent objective function. Furthermore, we find that the loss function contained in this latent objective has a similar configuration with non-convex regularized penalty (NSPR) known in statistics and machine learning. Such connection inspires us discovering more intrinsic relationship between SPL regimes and NSPR forms, like SCAD, LOG and EXP. The robustness insight under SPL can then be finely explained. We also analyze the capability of SPL on its easy loss prior embedding property, and provide an insightful interpretation to the effectiveness mechanism under previous SPL variations. Besides, we design a group-partial-order loss prior, which is especially useful to weakly labeled large-scale data processing tasks. Through applying SPL with this loss prior to the FCVID dataset, which is currently one of the biggest manually annotated video dataset, our method achieves state-of-the-art performance beyond previous methods, which further helps supports the proposed theoretical arguments.

##### Abstract (translated by Google)
自主学习（SPL）是最近通过模拟人类/动物学习原理而设计的一种方法。已经针对不同的计算机视觉和模式识别任务设计了各种SPL实现方案，并且经验证实在这些应用中是有效的。然而，对其理论洞察力的调查仍然是一个空白。针对这个问题，本研究试图在SPL方案下提供一些新的理论认识。具体来说，我们证明了SPL上的求解策略符合潜在目标函数上实现的主要化最小化算法。此外，我们发现包含在这个潜在目标中的损失函数与统计和机器学习中已知的非凸正则化惩罚（NSPR）类似。这种联系激发了我们发现SPL制度与NSPR形式（如SCAD，LOG和EXP）之间更加内在的关系。 SPL下的健壮性洞察力可以得到很好的解释。我们还分析了SPL在嵌入属性之前容易丢失的能力，并对以前的SPL变化下的有效性机制提供了深刻的解释。此外，我们设计了一个群体偏序损失先验分布，这对弱标号的大规模数据处理任务特别有用。通过在FCVID数据集（这是目前最大的手动注释的视频数据集之一）之前应用此损失的SPL，我们的方法实现了超越以前方法的最先进的性能，这进一步有助于支持所提出的理论论证。

##### URL
[https://arxiv.org/abs/1511.06049](https://arxiv.org/abs/1511.06049)

##### PDF
[https://arxiv.org/pdf/1511.06049](https://arxiv.org/pdf/1511.06049)

