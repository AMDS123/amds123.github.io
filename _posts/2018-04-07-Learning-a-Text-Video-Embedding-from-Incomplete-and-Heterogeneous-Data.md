---
layout: post
title: "Learning a Text-Video Embedding from Incomplete and Heterogeneous Data"
date: 2018-04-07 06:59:45
categories: arXiv_CV
tags: arXiv_CV Face Caption Embedding
author: Antoine Miech, Ivan Laptev, Josef Sivic
mathjax: true
---

* content
{:toc}

##### Abstract
Joint understanding of video and language is an active research area with many applications. Prior work in this domain typically relies on learning text-video embeddings. One difficulty with this approach, however, is the lack of large-scale annotated video-caption datasets for training. To address this issue, we aim at learning text-video embeddings from heterogeneous data sources. To this end, we propose a Mixture-of-Embedding-Experts (MEE) model with ability to handle missing input modalities during training. As a result, our framework can learn improved text-video embeddings simultaneously from image and video datasets. We also show the generalization of MEE to other input modalities such as face descriptors. We evaluate our method on the task of video retrieval and report results for the MPII Movie Description and MSR-VTT datasets. The proposed MEE model demonstrates significant improvements and outperforms previously reported methods on both text-to-video and video-to-text retrieval tasks. Code is available at: this https URL

##### Abstract (translated by Google)
对视频和语言的共同理解是一个活跃的研究领域，有许多应用。此领域的先前工作通常依赖于学习文本视频嵌入。然而，这种方法的一个困难是缺乏用于训练的大规模注释视频字幕数据集。为了解决这个问题，我们的目标是从异构数据源学习文本视频嵌入。为此，我们提出了一种嵌入专家混合（MEE）模型，能够在训练期间处理缺失的输入模态。因此，我们的框架可以同时从图像和视频数据集中学习改进的文本视频嵌入。我们还展示了MEE对其他输入模态（如面部描述符）的推广。我们评估了视频检索任务的方法，并报告了MPII电影描述和MSR-VTT数据集的结果。建议的MEE模型显示出显着的改进，并且在文本到视频和视频到文本检索任务方面都优于以前报告的方法。代码位于：此https URL

##### URL
[https://arxiv.org/abs/1804.02516](https://arxiv.org/abs/1804.02516)

##### PDF
[https://arxiv.org/pdf/1804.02516](https://arxiv.org/pdf/1804.02516)

