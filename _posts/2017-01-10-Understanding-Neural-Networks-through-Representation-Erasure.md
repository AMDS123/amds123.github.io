---
layout: post
title: "Understanding Neural Networks through Representation Erasure"
date: 2017-01-10 01:40:51
categories: arXiv_CL
tags: arXiv_CL Sentiment Reinforcement_Learning Classification Prediction
author: Jiwei Li, Will Monroe, Dan Jurafsky
mathjax: true
---

* content
{:toc}

##### Abstract
While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.

##### Abstract (translated by Google)
虽然神经网络已经成功应用于许多自然语言处理任务，但它们的代价是可解释性的。在本文中，我们提出了一个通用的方法来分析和解释来自神经模型的决定，通过观察消除表达的各个部分，如输入的单词向量维度，中间隐藏单位或输入单词的模型的影响。我们提出了几种方法来分析这种擦除的影响，从计算评估指标的相对差异，到使用强化学习来擦除最小输入字集，以翻转神经模型的决定。在对多个NLP任务（包括语言特征分类，句子层次情感分析和文档层次情感层面预测）的综合分析中，我们发现所提出的方法不仅对神经模型决策提供了明确的解释，而且提供了一种进行神经模型的误差分析。

##### URL
[https://arxiv.org/abs/1612.08220](https://arxiv.org/abs/1612.08220)

##### PDF
[https://arxiv.org/pdf/1612.08220](https://arxiv.org/pdf/1612.08220)

