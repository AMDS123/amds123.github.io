---
layout: post
title: "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"
date: 2018-11-02 21:04:24
categories: arXiv_CL
tags: arXiv_CL Transfer_Learning Language_Model
author: Jason Phang, Thibault F&#xe9;vry, Samuel R. Bowman
mathjax: true
---

* content
{:toc}

##### Abstract
Pretraining with language modeling and related unsupervised tasks has recently been shown to be a very effective enabling technology for the development of neural network models for language understanding tasks. In this work, we show that although language model-style pretraining is extremely effective at teaching models about language, it does not yield an ideal starting point for efficient transfer learning. By supplementing language model-style pretraining with further training on data-rich supervised tasks, we are able to achieve substantial additional performance improvements across the nine target tasks in the GLUE benchmark. We obtain an overall score of 76.9 on GLUE--a 2.3 point improvement over our baseline system adapted from Radford et al. (2018) and a 4.1 point improvement over Radford et al.'s reported score. We further use training data downsampling to show that the benefits of this supplementary training are even more pronounced in data-constrained regimes.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.01088](http://arxiv.org/abs/1811.01088)

##### PDF
[http://arxiv.org/pdf/1811.01088](http://arxiv.org/pdf/1811.01088)

