---
layout: post
title: "Learning to Perform Physics Experiments via Deep Reinforcement Learning"
date: 2017-08-17 19:51:29
categories: arXiv_CV
tags: arXiv_CV Reinforcement_Learning
author: Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas
mathjax: true
---

* content
{:toc}

##### Abstract
When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.

##### Abstract (translated by Google)
当遇到新颖的物体时，人类能够以目标驱动的方式通过与它们相互作用来推断大范围的物理特性，例如质量，摩擦和可变形性。这种积极互动的过程与进行实验发现隐藏事实的科学家具有同样的精神。最近在人工智能方面的进步已经产生了可以在Go，Atari，自然语言处理和复杂的控制问题中实现超人的性能的机器;然而，这些系统甚至不能与年幼的科学直觉相媲美。在这项工作中，我们介绍了一组基本的任务，这些任务要求代理人在一个交互式的模拟环境中估计物体的质量和内聚力等属性，在这个交互的模拟环境中他们可以操纵物体并观察结果。我们发现艺术深度强化学习方法可以学习执行必要的实验来发现这种隐藏的属性。通过系统地操纵代理人进行实验所遇到的问题难度和代价，我们发现代理人学习不同的策略来平衡收集信息的成本和不同情况下犯错误的代价。

##### URL
[https://arxiv.org/abs/1611.01843](https://arxiv.org/abs/1611.01843)

##### PDF
[https://arxiv.org/pdf/1611.01843](https://arxiv.org/pdf/1611.01843)

