---
layout: post
title: "Certified Robustness to Adversarial Examples with Differential Privacy"
date: 2018-06-26 15:54:44
categories: arXiv_AI
tags: arXiv_AI Adversarial
author: Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana
mathjax: true
---

* content
{:toc}

##### Abstract
Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best-effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks, but they either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired technique, that provides a rigorous, generic, and flexible foundation for defense.

##### Abstract (translated by Google)
欺骗机器学习模型，尤其是深度神经网络的对抗性例子一直是研究热点问题之一，攻击和防御正在紧密地来回发展。过去的大多数防御措施都是尽力而为，并且已经证明它们容易受到复杂的攻击。最近引入了一组经认证的防御措施，可以保证对有规范的攻击具有鲁棒性，但是它们要么不能扩展到大型数据集，要么限制了它们可以支持的模型类型。本白皮书介绍了首个可以扩展到大型网络和数据集（例如Google的ImageNet Inception网络）的认证防御系统，并广泛适用于任意模型类型。我们的防守基于鲁棒性与对抗性示例之间的新颖联系以及差分隐私，这是一种密码启发技术，为防御提供了严谨，通用和灵活的基础。

##### URL
[http://arxiv.org/abs/1802.03471](http://arxiv.org/abs/1802.03471)

##### PDF
[http://arxiv.org/pdf/1802.03471](http://arxiv.org/pdf/1802.03471)

