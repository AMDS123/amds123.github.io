---
layout: post
title: "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks"
date: 2015-08-02 06:18:10
categories: arXiv_CL
tags: arXiv_CL Attention Embedding CNN Represenation_Learning Deep_Learning
author: Jian Tang, Meng Qu, Qiaozhu Mei
mathjax: true
---

* content
{:toc}

##### Abstract
Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the \textit{predictive text embedding} (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.

##### Abstract (translated by Google)
无监督文本嵌入方法，如Skip-gram和Paragraph Vector，由于其简单性，可伸缩性和有效性而受到越来越多的关注。然而，与复杂的深度学习架构（如卷积神经网络）相比，这些方法通常在应用于特定的机器学习任务时产生较差的结果。一个可能的原因是这些文本嵌入方法以完全无监督的方式学习文本的表示，而没有利用可用于任务的标记信息。尽管所学的低维表征适用于许多不同的任务，但对于任何任务他们都没有特别的调整。在本文中，我们通过提出一种文本数据的半监督表示学习方法填补这个空白，我们称之为\ textit {预测文本嵌入}（PTE）。预测文本嵌入利用标记和未标记的数据来学习文本的嵌入。标注信息和不同级别的词同现信息首先表示为大规模的异构文本网络，然后通过原理性，高效的算法嵌入到低维空间。这种低维嵌入不仅保留了单词和文档的语义接近性，而且对于特定的任务具有很强的预测能力。与最近基于卷积神经网络的监督方法相比，预测文本嵌入是可比的或更有效的，效率更高，并且具有更少的调整参数。

##### URL
[https://arxiv.org/abs/1508.00200](https://arxiv.org/abs/1508.00200)

##### PDF
[https://arxiv.org/pdf/1508.00200](https://arxiv.org/pdf/1508.00200)

