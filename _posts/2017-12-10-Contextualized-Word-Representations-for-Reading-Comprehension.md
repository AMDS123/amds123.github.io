---
layout: post
title: "Contextualized Word Representations for Reading Comprehension"
date: 2017-12-10 23:16:02
categories: arXiv_CL
tags: arXiv_CL Attention Language_Model
author: Shimi Salant, Jonathan Berant
mathjax: true
---

* content
{:toc}

##### Abstract
Reading a document and extracting an answer to a question about its content has attracted substantial attention recently, where most work has focused on the interaction between the question and the document. In this work we evaluate the importance of context when the question and the document are each read on their own. We take a standard neural architecture for the task of reading comprehension, and show that by providing rich contextualized word representations from a large language model, and allowing the model to choose between context dependent and context independent word representations, we can dramatically improve performance and reach state-of-the-art performance on the competitive SQuAD dataset.

##### Abstract (translated by Google)
阅读文档并提取关于其内容的问题的答案最近引起了大量的关注，大多数工作集中于问题和文档之间的交互。在这项工作中，我们评估问题和文档各自阅读时的上下文的重要性。我们采用一种标准的神经体系结构来完成阅读理解任务，并且表明通过从大型语言模型提供丰富的上下文语言表示，并允许模型在上下文相关和上下文无关的词表示之间进行选择，我们可以显着提高性能和覆盖范围最具竞争力的SQUAD数据集上的最新性能。

##### URL
[http://arxiv.org/abs/1712.03609](http://arxiv.org/abs/1712.03609)

##### PDF
[http://arxiv.org/pdf/1712.03609](http://arxiv.org/pdf/1712.03609)

