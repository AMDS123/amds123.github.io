---
layout: post
title: "Analysis of adversarial attacks against CNN-based image forgery detectors"
date: 2018-08-25 13:40:36
categories: arXiv_CV
tags: arXiv_CV Adversarial Object_Detection Classification Deep_Learning Detection
author: Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva
mathjax: true
---

* content
{:toc}

##### Abstract
With the ubiquitous diffusion of social networks, images are becoming a dominant and powerful communication channel. Not surprisingly, they are also increasingly subject to manipulations aimed at distorting information and spreading fake news. In recent years, the scientific community has devoted major efforts to contrast this menace, and many image forgery detectors have been proposed. Currently, due to the success of deep learning in many multimedia processing tasks, there is high interest towards CNN-based detectors, and early results are already very promising. Recent studies in computer vision, however, have shown CNNs to be highly vulnerable to adversarial attacks, small perturbations of the input data which drive the network towards erroneous classification. In this paper we analyze the vulnerability of CNN-based image forensics methods to adversarial attacks, considering several detectors and several types of attack, and testing performance on a wide range of common manipulations, both easily and hardly detectable.

##### Abstract (translated by Google)
随着社交网络的普及，图像正在成为一种主导且强大的通信渠道。毫不奇怪，他们也越来越多地受到旨在歪曲信息和传播假新闻的操纵。近年来，科学界一直致力于对比这一威胁，并提出了许多图像伪造探测器。目前，由于在许多多媒体处理任务中深度学习的成功，对基于CNN的检测器存在高度兴趣，并且早期结果已经非常有希望。然而，最近对计算机视觉的研究表明，CNN非常容易受到对抗性攻击，输入数据的微小扰动会导致网络错误分类。在本文中，我们分析了基于CNN的图像取证方法对对抗性攻击的脆弱性，考虑了几种探测器和几种类型的攻击，并且在广泛的常见操作上测试性能，这些操作既容易又难以检测。

##### URL
[http://arxiv.org/abs/1808.08426](http://arxiv.org/abs/1808.08426)

##### PDF
[http://arxiv.org/pdf/1808.08426](http://arxiv.org/pdf/1808.08426)

