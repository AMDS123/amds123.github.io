---
layout: post
title: "Deriving Neural Architectures from Sequence and Graph Kernels"
date: 2017-10-30 13:56:23
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Tao Lei, Wengong Jin, Regina Barzilay, Tommi Jaakkola
mathjax: true
---

* content
{:toc}

##### Abstract
The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.

##### Abstract (translated by Google)
结构化对象的神经体系结构的设计通常由实验性见解而非正式过程来指导。在这项工作中，我们呼吁核心超过组合结构，如序列和图表，以派生适当的神经操作。我们引入一类深循环神经运算，并形式化地描述它们相关的内核空间。我们经常使用的模块通过内核比较输入和虚拟参考对象（比较CNN中的过滤器）。类似于传统的神经操作，这些参考对象被参数化并且在端对端训练中被直接优化。我们在标准应用程序（如语言建模和分子图回归）上实验性地评估建议的神经架构类，在这些应用程序中实现最先进的结果。

##### URL
[https://arxiv.org/abs/1705.09037](https://arxiv.org/abs/1705.09037)

##### PDF
[https://arxiv.org/pdf/1705.09037](https://arxiv.org/pdf/1705.09037)

