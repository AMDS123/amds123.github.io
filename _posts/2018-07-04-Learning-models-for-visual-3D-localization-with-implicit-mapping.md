---
layout: post
title: "Learning models for visual 3D localization with implicit mapping"
date: 2018-07-04 15:50:58
categories: arXiv_CV
tags: arXiv_CV Attention
author: Dan Rosenbaum, Frederic Besse, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a formulation of visual localization that does not require construction of explicit maps in the form of point clouds or voxels. The goal is to learn an implicit representation of the environment at a higher, more abstract level, for instance that of objects. To study this approach we consider procedurally generated Minecraft worlds, for which we can generate visually rich images along with camera pose coordinates. We first show that Generative Query Networks (GQNs) enhanced with a novel attention mechanism can capture the visual structure of 3D scenes in Minecraft, as evidenced by their samples. We then apply the models to the localization problem, investigating both generative and discriminative approaches, and compare the different ways in which they each capture task uncertainty. Our results show that models with implicit mapping are able to capture the underlying 3D structure of visually complex scenes, and use this to accurately localize new observations, paving the way towards future applications in sequential localization. Supplementary video available at https://youtu.be/iHEXX5wXbCI.

##### Abstract (translated by Google)
我们提出了一种视觉定位的公式，不需要以点云或体素的形式构建显式地图。目标是在更高，更抽象的层面（例如对象层面）学习环境的隐式表示。为了研究这种方法，我们考虑程序生成的Minecraft世界，为此我们可以生成视觉上丰富的图像以及相机姿势坐标。我们首先表明，使用新颖的注意机制增强的生成查询网络（GQN）可以捕获Minecraft中3D场景的视觉结构，正如他们的样本所证明的那样。然后，我们将模型应用于定位问题，研究生成和判别方法，并比较它们各自捕获任务不确定性的不同方式。我们的结果表明，具有隐式映射的模型能够捕获视觉复杂场景的基础3D结构，并使用它来准确定位新观察，为顺序定位中的未来应用铺平道路。补充视频可在https://youtu.be/iHEXX5wXbCI获得。

##### URL
[http://arxiv.org/abs/1807.03149](http://arxiv.org/abs/1807.03149)

##### PDF
[http://arxiv.org/pdf/1807.03149](http://arxiv.org/pdf/1807.03149)

