---
layout: post
title: "From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction"
date: 2018-04-29 18:27:43
categories: arXiv_CL
tags: arXiv_CL Regularization Reinforcement_Learning Prediction
author: Zihang Dai, Qizhe Xie, Eduard Hovy
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection, we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.

##### Abstract (translated by Google)
在这项工作中，我们研究了奖励扩充最大似然（RAML）学习中的信用分配问题，并建立了RAML的标记级对应和熵正则化强化学习之间的理论等价性。受连接的启发，我们提出了两种序列预测算法，一种是用细粒度信用分配扩展RAML，另一种是用系统熵正则化改进Actor-Critic。在两个基准数据集上，我们展示了所提出的算法分别优于RAML和Actor-Critic，为序列预测提供了新的替代方案。

##### URL
[https://arxiv.org/abs/1804.10974](https://arxiv.org/abs/1804.10974)

##### PDF
[https://arxiv.org/pdf/1804.10974](https://arxiv.org/pdf/1804.10974)

