---
layout: post
title: "Simplified Long Short-term Memory Recurrent Neural Networks: part II"
date: 2017-07-14 19:59:18
categories: arXiv_CV
tags: arXiv_CV RNN
author: Atra Akandeh, Fathi M. Salem
mathjax: true
---

* content
{:toc}

##### Abstract
This is part II of three-part work. Here, we present a second set of inter-related five variants of simplified Long Short-term Memory (LSTM) recurrent neural networks by further reducing adaptive parameters. Two of these models have been introduced in part I of this work. We evaluate and verify our model variants on the benchmark MNIST dataset and assert that these models are comparable to the base LSTM model while use progressively less number of parameters. Moreover, we observe that in case of using the ReLU activation, the test accuracy performance of the standard LSTM will drop after a number of epochs when learning parameter become larger. However all of the new model variants sustain their performance.

##### Abstract (translated by Google)
这是三部分工作的第二部分。在这里，我们通过进一步减少自适应参数提出了第二组简化的长期短期记忆（LSTM）递归神经网络的相互关联的五个变体。这项工作的第一部分已经介绍了其中的两个模型。我们在基准MNIST数据集上评估和验证我们的模型变体，并断言这些模型与基本LSTM模型相当，同时使用逐渐减少的参数数量。此外，我们观察到，在使用ReLU激活的情况下，当学习参数变大时，标准LSTM的测试精度性能将在多个时期后下降。不过，所有新车型都会保持其性能。

##### URL
[https://arxiv.org/abs/1707.04623](https://arxiv.org/abs/1707.04623)

##### PDF
[https://arxiv.org/pdf/1707.04623](https://arxiv.org/pdf/1707.04623)

