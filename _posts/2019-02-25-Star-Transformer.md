---
layout: post
title: "Star-Transformer"
date: 2019-02-25 07:07:38
categories: arXiv_CL
tags: arXiv_CL Attention
author: Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, Zheng Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Although the fully-connected attention-based model Transformer has achieved great successes on many NLP tasks, it has heavy structure and usually requires large training data. In this paper, we present the Star-Transformer, an alternative and light-weighted model of the Transformer. To reduce the model complexity, we replace the fully-connected structure with a star-shaped structure, in which every two non-adjacent nodes are connected through a shared relay node. Thus, the Star-Transformer has lower complexity than the standard Transformer (from quadratic to linear according to the input length) and preserves the ability to handle with the long-range dependencies. The experiments on four tasks (22 datasets) show the Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.09113](http://arxiv.org/abs/1902.09113)

##### PDF
[http://arxiv.org/pdf/1902.09113](http://arxiv.org/pdf/1902.09113)

