---
layout: post
title: "Adversarial Learning of Task-Oriented Neural Dialog Models"
date: 2018-05-30 00:48:44
categories: arXiv_CL
tags: arXiv_CL Adversarial Reinforcement_Learning
author: Bing Liu, Ian Lane
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we propose an adversarial learning method for reward estimation in reinforcement learning (RL) based task-oriented dialog models. Most of the current RL based task-oriented dialog systems require the access to a reward signal from either user feedback or user ratings. Such user ratings, however, may not always be consistent or available in practice. Furthermore, online dialog policy learning with RL typically requires a large number of queries to users, suffering from sample efficiency problem. To address these challenges, we propose an adversarial learning method to learn dialog rewards directly from dialog samples. Such rewards are further used to optimize the dialog policy with policy gradient based RL. In the evaluation in a restaurant search domain, we show that the proposed adversarial dialog learning method achieves advanced dialog success rate comparing to strong baseline methods. We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback.

##### Abstract (translated by Google)
在这项工作中，我们提出了一种基于强化学习（RL）的基于任务的对话模型中的对抗性学习方法。目前基于任务的面向任务的对话系统大多数需要从用户反馈或用户评分中获得奖励信号。然而，这样的用户评级在实践中可能并不总是一致的或可用的。此外，使用RL进行在线对话策略学习通常需要向用户提供大量查询，这些问题都存在样本效率问题。为了应对这些挑战，我们提出了一种敌对学习方法，以直接从对话样本中学习对话奖励。这些奖励进一步用于优化基于策略梯度的RL对话策略。在餐厅搜索领域的评估中，我们表明，与强基准方法相比，提议的对抗对话学习方法实现了高级对话成功率。我们进一步讨论在线对抗对话学习中的协变量转移问题，并展示我们如何通过部分访问用户反馈来解决这个问题。

##### URL
[http://arxiv.org/abs/1805.11762](http://arxiv.org/abs/1805.11762)

##### PDF
[http://arxiv.org/pdf/1805.11762](http://arxiv.org/pdf/1805.11762)

