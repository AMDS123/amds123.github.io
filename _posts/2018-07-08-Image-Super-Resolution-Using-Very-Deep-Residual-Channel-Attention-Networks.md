---
layout: post
title: "Image Super-Resolution Using Very Deep Residual Channel Attention Networks"
date: 2018-07-08 05:45:45
categories: arXiv_CV
tags: arXiv_CV Super_Resolution Attention CNN
author: Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.

##### Abstract (translated by Google)
卷积神经网络（CNN）深度对于图像超分辨率（SR）至关重要。但是，我们观察到图像SR的更深层网络更难以训练。低分辨率输入和特征包含丰富的低频信息，这些信息在信道上被平等对待，因此阻碍了CNN的表示能力。为了解决这些问题，我们提出了非常深的残留信道关注网络（RCAN）。具体而言，我们提出残余残余（RIR）结构以形成非常深的网络，其由具有长跳过连接的若干残余组组成。每个残差组包含一些具有短跳过连接的残余块。同时，RIR允许通过多个跳过连接绕过丰富的低频信息，使主网络专注于学习高频信息。此外，我们提出了一种通道关注机制，通过考虑通道之间的相互依赖性来自适应地重新调整通道方面的特征。大量实验表明，我们的RCAN可以在最先进的方法中实现更好的准确性和视觉改进。

##### URL
[http://arxiv.org/abs/1807.02758](http://arxiv.org/abs/1807.02758)

##### PDF
[http://arxiv.org/pdf/1807.02758](http://arxiv.org/pdf/1807.02758)

