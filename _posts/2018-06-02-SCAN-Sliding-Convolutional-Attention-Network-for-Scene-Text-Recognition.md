---
layout: post
title: "SCAN: Sliding Convolutional Attention Network for Scene Text Recognition"
date: 2018-06-02 03:28:43
categories: arXiv_CV
tags: arXiv_CV Attention CNN RNN Recognition
author: Yi-Chao Wu, Fei Yin, Xu-Yao Zhang, Li Liu, Cheng-Lin Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Scene text recognition has drawn great attentions in the community of computer vision and artificial intelligence due to its challenges and wide applications. State-of-the-art recurrent neural networks (RNN) based models map an input sequence to a variable length output sequence, but are usually applied in a black box manner and lack of transparency for further improvement, and the maintaining of the entire past hidden states prevents parallel computation in a sequence. In this paper, we investigate the intrinsic characteristics of text recognition, and inspired by human cognition mechanisms in reading texts, we propose a scene text recognition method with sliding convolutional attention network (SCAN). Similar to the eye movement during reading, the process of SCAN can be viewed as an alternation between saccades and visual fixations. Compared to the previous recurrent models, computations over all elements of SCAN can be fully parallelized during training. Experimental results on several challenging benchmarks, including the IIIT5k, SVT and ICDAR 2003/2013 datasets, demonstrate the superiority of SCAN over state-of-the-art methods in terms of both the model interpretability and performance.

##### Abstract (translated by Google)
场景文本识别由于其挑战和广泛的应用，在计算机视觉和人工智能领域备受关注。现有技术的基于循环神经网络（RNN）的模型将输入序列映射到可变长度输出序列，但通常以黑盒方式应用，并且缺乏透明度以进一步改进，并且维持整个过去隐藏状态阻止了一个序列中的并行计算。在本文中，我们研究了文本识别的内在特征，并且在阅读文本时受到人类认知机制的启发，我们提出了一种带有滑动卷积关注网络（SCAN）的场景文本识别方法。与阅读过程中的眼球运动类似，SCAN的过程可以被视为扫视和视觉注视之间的交替。与以前的复发模型相比，SCAN的所有元素的计算可以在训练期间完全并行化。在包括IIIT5k，SVT和ICDAR 2003/2013数据集在内的几个具有挑战性的基准测试中的实验结果证明了SCAN相对于最新的方法在模型可解释性和性能方面的优越性。

##### URL
[http://arxiv.org/abs/1806.00578](http://arxiv.org/abs/1806.00578)

##### PDF
[http://arxiv.org/pdf/1806.00578](http://arxiv.org/pdf/1806.00578)

