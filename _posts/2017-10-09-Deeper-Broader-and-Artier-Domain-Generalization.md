---
layout: post
title: "Deeper, Broader and Artier Domain Generalization"
date: 2017-10-09 13:19:27
categories: arXiv_CV
tags: arXiv_CV Sparse Deep_Learning Recognition
author: Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M. Hospedales
mathjax: true
---

* content
{:toc}

##### Abstract
The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them. In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research.

##### Abstract (translated by Google)
领域泛化的问题是从多个训练领域学习，并提取一个领域不可知的模型，然后可以应用到一个看不见的领域。在具有不同特征的目标领域，但训练数据稀少的情况下，领域概括（DG）具有明确的动机。例如在草图图像中的识别，其明显比照片更抽象和稀少。尽管如此，DG方法主要是在仅有照片的基准上进行评估，其重点在于减轻数据集偏差，因为在领域独特性和数据稀疏性方面的问题都很小。我们认为这些基准过于简单，表明简单的深度学习基线表现出色。在本文中，我们做出了两个主要贡献：首先，我们建立在深度学习方法的有利域偏移 - 鲁棒性质的基础上，开发了一个端到端DG学习的低阶参数化CNN模型。其次，我们开发了一个涵盖照片，素描，卡通和绘画领域的DG基准数据集。这与现有的基准相比更加相关，更难（更大的域名转换）。结果表明，我们的方法优于现有的DG替代方案，而我们的数据集提供了更重要的DG挑战来推动未来的研究。

##### URL
[https://arxiv.org/abs/1710.03077](https://arxiv.org/abs/1710.03077)

##### PDF
[https://arxiv.org/pdf/1710.03077](https://arxiv.org/pdf/1710.03077)

