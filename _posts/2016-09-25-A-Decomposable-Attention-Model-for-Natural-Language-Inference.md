---
layout: post
title: "A Decomposable Attention Model for Natural Language Inference"
date: 2016-09-25 23:52:45
categories: arXiv_CL
tags: arXiv_CL Attention Inference
author: Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.

##### Abstract (translated by Google)
我们提出了一个简单的自然语言推理的神经架构。我们的方法使用注意力将问题分解成可以分开解决的子问题，从而使其平行化。在斯坦福自然语言推理（SNLI）数据集上，我们获得最先进的结果，比以前的工作几乎少一个数量级的参数，并且不依赖于任何字序信息。增加考虑到最小订单量的句子内关注会产生进一步的改进。

##### URL
[https://arxiv.org/abs/1606.01933](https://arxiv.org/abs/1606.01933)

##### PDF
[https://arxiv.org/pdf/1606.01933](https://arxiv.org/pdf/1606.01933)

