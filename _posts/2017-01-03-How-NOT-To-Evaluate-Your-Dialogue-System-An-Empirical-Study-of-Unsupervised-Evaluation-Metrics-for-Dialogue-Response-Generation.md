---
layout: post
title: "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation"
date: 2017-01-03 18:28:32
categories: arXiv_SD
tags: arXiv_SD Quantitative Recommendation
author: Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.

##### Abstract (translated by Google)
我们调查对话响应生成系统的评估指标，其中监督标签，如任务完成，不可用。最近在响应代中的作品采用了机器翻译的指标来比较模型生成的响应和单个目标响应。我们表明，这些指标与非技术性的Twitter领域中的人为判断相关性很​​弱，而在技术性的Ubuntu领域则完全没有。我们提供定量和定性的结果，强调现有指标的具体弱点，并为未来开发更好的对话系统自动评估指标提供建议。

##### URL
[https://arxiv.org/abs/1603.08023](https://arxiv.org/abs/1603.08023)

##### PDF
[https://arxiv.org/pdf/1603.08023](https://arxiv.org/pdf/1603.08023)

