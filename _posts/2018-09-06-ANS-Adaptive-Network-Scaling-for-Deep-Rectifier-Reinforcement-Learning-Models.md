---
layout: post
title: "ANS: Adaptive Network Scaling for Deep Rectifier Reinforcement Learning Models"
date: 2018-09-06 17:39:18
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Deep_Learning
author: Yeah-Hua Wu, Fan-Yun Sun, Yen-Yu Chang, Should-De Lin
mathjax: true
---

* content
{:toc}

##### Abstract
This work provides a thorough study on how reward scaling can affect performance of deep reinforcement learning agents. In particular, we would like to answer the question that how does reward scaling affect non-saturating ReLU networks in RL? This question matters because ReLU is one of the most effective activation functions for deep learning models. We also propose an Adaptive Network Scaling framework to find a suitable scale of the rewards during learning for better performance. We conducted empirical studies to justify the solution.

##### Abstract (translated by Google)
这项工作提供了一个彻底的研究，如何奖励扩展可以影响深层强化学习代理的表现。特别是，我们想回答一个问题，即奖励缩放如何影响RL中的非饱和ReLU网络？这个问题很重要，因为ReLU是深度学习模型最有效的激活函数之一。我们还提出了一种自适应网络扩展框架，以便在学习期间找到合适的奖励等级以获得更好的性能。我们进行了实证研究以证明解决方案的合理性。

##### URL
[http://arxiv.org/abs/1809.02112](http://arxiv.org/abs/1809.02112)

##### PDF
[http://arxiv.org/pdf/1809.02112](http://arxiv.org/pdf/1809.02112)

