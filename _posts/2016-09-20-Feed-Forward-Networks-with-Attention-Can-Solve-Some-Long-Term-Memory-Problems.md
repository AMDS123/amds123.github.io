---
layout: post
title: "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems"
date: 2016-09-20 05:02:22
categories: arXiv_CV
tags: arXiv_CV Attention
author: Colin Raffel, Daniel P. W. Ellis
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic "addition" and "multiplication" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.

##### Abstract (translated by Google)
我们提出了一个适用于前馈神经网络的简化的注意模型，并且证明了所得到的模型能够解决序列长度的合成“加法”和“乘法”长期记忆问题。这些任务的最佳发布结果。

##### URL
[https://arxiv.org/abs/1512.08756](https://arxiv.org/abs/1512.08756)

##### PDF
[https://arxiv.org/pdf/1512.08756](https://arxiv.org/pdf/1512.08756)

