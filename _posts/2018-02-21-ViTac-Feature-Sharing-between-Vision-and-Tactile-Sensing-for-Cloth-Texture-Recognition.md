---
layout: post
title: "ViTac: Feature Sharing between Vision and Tactile Sensing for Cloth Texture Recognition"
date: 2018-02-21 10:06:14
categories: arXiv_CV
tags: arXiv_CV Recognition
author: Shan Luo, Wenzhen Yuan, Edward Adelson, Anthony G. Cohn, Raul Fuentes
mathjax: true
---

* content
{:toc}

##### Abstract
Vision and touch are two of the important sensing modalities for humans and they offer complementary information for sensing the environment. Robots are also envisioned to be of such multi-modal sensing ability. In this paper, we propose a new fusion method named Deep Maximum Covariance Analysis (DMCA) to learn a joint latent space for sharing features through vision and tactile sensing. The features of camera images and tactile data acquired from a GelSight sensor are learned by deep neural networks. But the learned features are of a high dimensionality and are redundant due to the differences in the two sensing modalities, which deteriorates the perception performance. To solve this, the learned features are paired using maximum covariance analysis. Results of the algorithm on a newly collected dataset of paired visual and tactile data relating to cloth textures show that a good recognition performance of greater than 90% can be achieved by using the proposed DMCA framework. In addition, we find that the perception performance of either vision or tactile sensing can be improved by employing the shared representation space, compared to learning from unimodal data.

##### Abstract (translated by Google)
视觉和触觉是人类的两种重要感应模式，它们提供了感应环境的补充信息。机器人也被认为具有这种多模态感知能力。在本文中，我们提出了一种名为深度最大协方差分析（DMCA）的新融合方法，以学习通过视觉和触觉感应共享特征的共同潜在空间。通过深度神经网络学习从GelSight传感器获取的摄像头图像和触觉数据的特征。但是学习到的特征具有高度的维度，并且由于两种感知模式的差异而变得冗余，这使感知性能恶化。为了解决这个问题，使用最大协方差分析将学习的特征配对。在新收集的关于布料纹理的成对视觉和触觉数据的数据集上的算法结果表明，使用所提出的DMCA框架可以实现大于90％的良好识别性能。此外，我们发现，与从单峰数据中学习相比，使用共享表示空间可以提高视觉或触觉感知的感知性能。

##### URL
[http://arxiv.org/abs/1802.07490](http://arxiv.org/abs/1802.07490)

##### PDF
[http://arxiv.org/pdf/1802.07490](http://arxiv.org/pdf/1802.07490)

