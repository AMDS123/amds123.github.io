---
layout: post
title: "Decoding-History-Based Adaptive Control of Attention for Neural Machine Translation"
date: 2018-02-06 06:18:56
categories: arXiv_AI
tags: arXiv_AI Attention NMT
author: Junyang Lin, Shuming Ma, Qi Su, Xu Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Attention-based sequence-to-sequence model has proved successful in Neural Machine Translation (NMT). However, the attention without consideration of decoding history, which includes the past information in the decoder and the attention mechanism, often causes much repetition. To address this problem, we propose the decoding-history-based Adaptive Control of Attention (ACA) for the NMT model. ACA learns to control the attention by keeping track of the decoding history and the current information with a memory vector, so that the model can take the translated contents and the current information into consideration. Experiments on Chinese-English translation and the English-Vietnamese translation have demonstrated that our model significantly outperforms the strong baselines. The analysis shows that our model is capable of generating translation with less repetition and higher accuracy. The code will be available at https://github.com/lancopku

##### Abstract (translated by Google)
基于注意的序列 - 序列模型在神经机器翻译（NMT）中被证明是成功的。然而，不考虑包括解码器中的过去信息和关注机制的解码历史的注意力常常导致很多重复。为了解决这个问题，我们提出了NMT模型的基于解码历史的注意适应控制（Adaptive Control of Attention，ACA）。 ACA通过使用存储器矢量跟踪解码历史和当前信息来学习控制注意力，使得模型能够考虑翻译的内容和当前的信息。汉英翻译和英 - 越翻译实验表明，我们的模型显着优于强基线。分析表明，我们的模型能够以较少的重复和较高的准确性生成翻译。代码将在https://github.com/lancopku上提供

##### URL
[http://arxiv.org/abs/1802.01812](http://arxiv.org/abs/1802.01812)

##### PDF
[http://arxiv.org/pdf/1802.01812](http://arxiv.org/pdf/1802.01812)

