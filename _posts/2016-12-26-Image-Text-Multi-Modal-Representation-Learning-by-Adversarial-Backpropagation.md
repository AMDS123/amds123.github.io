---
layout: post
title: "Image-Text Multi-Modal Representation Learning by Adversarial Backpropagation"
date: 2016-12-26 09:51:18
categories: arXiv_CL
tags: arXiv_CL Adversarial Knowledge Embedding Represenation_Learning Prediction
author: Gwangbeen Park, Woobin Im
mathjax: true
---

* content
{:toc}

##### Abstract
We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding. In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work.

##### Abstract (translated by Google)
我们提出了图像文本多模式表示学习的新方法。据我们所知，这项工作是第一种将敌对学习概念应用于多模态学习的方法，而不是利用图像 - 文本对信息来学习多模式特征。我们只使用类别信息，与之前大多数使用图像 - 文本对信息的方法进行多模式嵌入。在本文中，我们表明，在没有图像 - 文本对信息的情况下，可以实现多模式特征，并且与其他使用图像 - 文本对信息的方法相比，我们的方法在多模态特征空间中与图像和文本进行更加类似的分布。而且我们展示了我们的多模态特征具有通用的语义信息，即使它是为类别预测而训练的。我们的模型是端到端的反向传播，直观且容易扩展到其他多模式学习工作。

##### URL
[https://arxiv.org/abs/1612.08354](https://arxiv.org/abs/1612.08354)

##### PDF
[https://arxiv.org/pdf/1612.08354](https://arxiv.org/pdf/1612.08354)

