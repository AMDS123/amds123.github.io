---
layout: post
title: "Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection"
date: 2018-02-19 07:16:39
categories: arXiv_CL
tags: arXiv_CL Relation_Extraction Embedding Detection Relation
author: Haw-Shiuan Chang, ZiYun Wang, Luke Vilnis, Andrew McCallum
mathjax: true
---

* content
{:toc}

##### Abstract
Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, coreference, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limits the coverage of these models, which can be addressed by learning hypernyms from unlabeled text. Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts in a low-dimensional and interpretable space. In experimental evaluations more comprehensive than any previous literature of which we are aware-evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions-we find that our method provides up to double the precision of previous unsupervised embeddings, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results.

##### Abstract (translated by Google)
建模hypernymy，如狮子狗是一只狗，是许多NLP任务的重要概括性辅助工具，如蕴涵，共因，关系提取和问题回答。从带标签的上位词源（如WordNet）进行监督学习限制了这些模式的覆盖范围，这可以通过从未标记的文本中学习上位词来解决。现有的无监督方法要么不能扩展到大的词汇表，要么产生不可接受的低精度。本文介绍了分布式包含矢量嵌入（DIVE），这是一种简单实现的无监督方法，通过每个词的非负向量嵌入来保留词语上下文在低维和可解释空间中的包含特性。在比我们意识到的任何以前的文献更全面的实验评估中 - 使用多个现有评估函数和新提出的评分函数对11个数据集进行评估 - 我们发现，我们的方法提供了以前无监督嵌入的精度的两倍，并且平均值最高性能，使用更加紧凑的单词表示，并产生许多新的最新技术成果。

##### URL
[http://arxiv.org/abs/1710.00880](http://arxiv.org/abs/1710.00880)

##### PDF
[http://arxiv.org/pdf/1710.00880](http://arxiv.org/pdf/1710.00880)

