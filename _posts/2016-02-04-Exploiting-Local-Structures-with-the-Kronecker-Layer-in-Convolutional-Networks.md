---
layout: post
title: "Exploiting Local Structures with the Kronecker Layer in Convolutional Networks"
date: 2016-02-04 01:19:38
categories: arXiv_CV
tags: arXiv_CV CNN Recognition
author: Shuchang Zhou, Jia-Nan Wu, Yuxin Wu, Xinyu Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose and study a technique to reduce the number of parameters and computation time in convolutional neural networks. We use Kronecker product to exploit the local structures within convolution and fully-connected layers, by replacing the large weight matrices by combinations of multiple Kronecker products of smaller matrices. Just as the Kronecker product is a generalization of the outer product from vectors to matrices, our method is a generalization of the low rank approximation method for convolution neural networks. We also introduce combinations of different shapes of Kronecker product to increase modeling capacity. Experiments on SVHN, scene text recognition and ImageNet dataset demonstrate that we can achieve $3.3 \times$ speedup or $3.6 \times$ parameter reduction with less than 1\% drop in accuracy, showing the effectiveness and efficiency of our method. Moreover, the computation efficiency of Kronecker layer makes using larger feature map possible, which in turn enables us to outperform the previous state-of-the-art on both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character recognition) datasets.

##### Abstract (translated by Google)
在本文中，我们提出并研究了一种减少卷积神经网络参数和计算时间的技术。我们使用Kronecker乘积来利用卷积和完全连接层内的局部结构，通过用较小矩阵的多个Kronecker乘积的组合代替大的权矩阵。就像Kronecker乘积是从向量到矩阵的外积的推广一样，我们的方法是卷积神经网络的低秩逼近方法的推广。我们还介绍了不同形状的Kronecker产品的组合，以提高建模能力。在SVHN，场景文本识别和ImageNet数据集上的实验表明，我们可以实现$ 3.3倍的加速或$ 3.6倍的参数减少，精度下降不到1％，显示了我们方法的有效性和效率。此外，Kronecker层的计算效率使得可以使用更大的特征映射，这使得我们能够在SVHN（数字识别）和CASIA-HWDB（手写汉字识别）数据集上超越先前的最新技术水平。

##### URL
[https://arxiv.org/abs/1512.09194](https://arxiv.org/abs/1512.09194)

##### PDF
[https://arxiv.org/pdf/1512.09194](https://arxiv.org/pdf/1512.09194)

