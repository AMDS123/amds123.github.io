---
layout: post
title: "It was the training data pruning too!"
date: 2018-03-12 23:59:37
categories: arXiv_CL
tags: arXiv_CL
author: Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, Kedar Dhamdhere
mathjax: true
---

* content
{:toc}

##### Abstract
We study the current best model (KDG) for question answering on tabular data evaluated over the WikiTableQuestions dataset. Previous ablation studies performed against this model attributed the model's performance to certain aspects of its architecture. In this paper, we find that the model's performance also crucially depends on a certain pruning of the data used to train the model. Disabling the pruning step drops the accuracy of the model from 43.3% to 36.3%. The large impact on the performance of the KDG model suggests that the pruning may be a useful pre-processing step in training other semantic parsers as well.

##### Abstract (translated by Google)
我们研究当前最佳模型（KDG），用于回答在WikiTableQuestions数据集上评估的表格数据的问题。之前针对该模型进行的消融研究将模型的性能归因于其架构的某些方面。在本文中，我们发现模型的性能也很关键取决于对用于训练模型的数据进行某种修剪。禁用修剪步骤会将模型的准确度从43.3％降至36.3％。对KDG模型性能的巨大影响表明修剪在训练其他语义解析器时可能是一个有用的预处理步骤。

##### URL
[http://arxiv.org/abs/1803.04579](http://arxiv.org/abs/1803.04579)

##### PDF
[http://arxiv.org/pdf/1803.04579](http://arxiv.org/pdf/1803.04579)

