---
layout: post
title: 'Visually Grounded Word Embeddings and Richer Visual Features for Improving Multimodal Neural Machine Translation'
date: 2017-07-26 13:42:46
categories: arXiv_CV
tags: arXiv_CV Image_Caption Object_Detection Caption CNN NMT Detection
author: Jean-Benoit Delbrouck, Stéphane Dupont, Omar Seddati
---

* content
{:toc}

##### Abstract
In Multimodal Neural Machine Translation (MNMT), a neural model generates a translated sentence that describes an image, given the image itself and one source descriptions in English. This is considered as the multimodal image caption translation task. The images are processed with Convolutional Neural Network (CNN) to extract visual features exploitable by the translation model. So far, the CNNs used are pre-trained on object detection and localization task. We hypothesize that richer architecture, such as dense captioning models, may be more suitable for MNMT and could lead to improved translations. We extend this intuition to the word-embeddings, where we compute both linguistic and visual representation for our corpus vocabulary. We combine and compare different confi

##### Abstract (translated by Google)
在多模态神经机器翻译（MNMT）中，神经模型生成描述图像的翻译句子，给出图像本身和一个英文源描述。这被认为是多模式图像字幕翻译任务。利用卷积神经网络（CNN）对图像进行处理，提取翻译模型可利用的视觉特征。到目前为止，所使用的CNN都是在物体检测和定位任务上进行预先训练的。我们假设更丰富的体系结构，如密集字幕模型，可能更适合于MNMT，并可能导致改进的翻译。我们将这个直觉扩展到词嵌入，在这里我们为语料库词汇表计算语言和视觉表示。我们结合并比较不同的配置

##### URL
[https://arxiv.org/abs/1707.01009](https://arxiv.org/abs/1707.01009)

