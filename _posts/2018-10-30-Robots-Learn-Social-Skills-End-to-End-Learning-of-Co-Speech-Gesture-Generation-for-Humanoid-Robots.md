---
layout: post
title: "Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots"
date: 2018-10-30 06:19:58
categories: arXiv_RO
tags: arXiv_RO Knowledge
author: Youngwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, Geehyuk Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Co-speech gestures enhance interaction experiences between humans as well as between humans and robots. Existing robots use rule-based speech-gesture association, but this requires human labor and prior knowledge of experts to be implemented. We present a learning-based co-speech gesture generation that is learned from 52 h of TED talks. The proposed end-to-end neural network model consists of an encoder for speech text understanding and a decoder to generate a sequence of gestures. The model successfully produces various gestures including iconic, metaphoric, deictic, and beat gestures. In a subjective evaluation, participants reported that the gestures were human-like and matched the speech content. We also demonstrate a co-speech gesture with a NAO robot working in real time.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.12541](http://arxiv.org/abs/1810.12541)

##### PDF
[http://arxiv.org/pdf/1810.12541](http://arxiv.org/pdf/1810.12541)

