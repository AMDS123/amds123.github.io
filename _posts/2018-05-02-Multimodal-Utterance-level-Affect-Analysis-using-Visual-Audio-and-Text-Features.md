---
layout: post
title: "Multimodal Utterance-level Affect Analysis using Visual, Audio and Text Features"
date: 2018-05-02 05:05:32
categories: arXiv_CV
tags: arXiv_CV Sentiment Inference Relation Recognition
author: Didan Deng, Yuqian Zhou, Jimin Pi, Bertram E.Shi
mathjax: true
---

* content
{:toc}

##### Abstract
Affective computing models are essential for human behavior analysis. A promising trend of affective system is enhancing the recognition performance by analyzing the contextual information over time and across modalities. To overcome the limitations of instantaneous emotion recognition, the 2018 IJCNN challenge on One-Minute Gradual-Emotion Recognition (OMG-Emotion) encourages the participants to address long-term emotion recognition using multiple modalities data like facial expression, audio and language context. Compared with single modality models given by the baseline method, a multi-modal inference network can leverage the information from each modality and their correlations to improve the performance of recognition. In this paper, we propose a multi-modal architecture which uses facial, audio and language context features to recognize human sentiment from utterances. Our model outperforms the provided unimodal baseline, and achieves the concordance correlation coefficients (CCC) 0.400 of arousal task, and 0.353 of valence task.

##### Abstract (translated by Google)
情感计算模型对人类行为分析至关重要。情感系统的一个有希望的趋势是通过分析随着时间和各种模式的情境信息来提高识别性能。为了克服即时情绪识别的局限性，2018年IJCNN对一分钟渐进式情绪识别（OMG-Emotion）的挑战鼓励参与者使用多种模态数据（如面部表情，音频和语言环境）来解决长期情绪识别问题。与基准方法给出的单模态模型相比，多模态推理网络可以利用来自每种模态的信息及其相关性来提高识别性能。在本文中，我们提出了一种多模态架构，它使用面部，音频和语言环境特征来识别来自话语的人类情绪。我们的模型胜过提供的单峰基线，并且实现了唤醒任务的一致性相关系数（CCC）0.400和价值任务的0.353。

##### URL
[https://arxiv.org/abs/1805.00625](https://arxiv.org/abs/1805.00625)

##### PDF
[https://arxiv.org/pdf/1805.00625](https://arxiv.org/pdf/1805.00625)

