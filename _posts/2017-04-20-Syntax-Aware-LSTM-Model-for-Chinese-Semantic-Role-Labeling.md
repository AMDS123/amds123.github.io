---
layout: post
title: "Syntax Aware LSTM Model for Chinese Semantic Role Labeling"
date: 2017-04-20 01:55:26
categories: arXiv_SD
tags: arXiv_SD RNN
author: Feng Qian, Lei Sha, Baobao Chang, Lu-chen Liu, Ming Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
As for semantic role labeling (SRL) task, when it comes to utilizing parsing information, both traditional methods and recent recurrent neural network (RNN) based methods use the feature engineering way. In this paper, we propose Syntax Aware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies according to dependency parsing information in order to model parsing information directly in an architecture engineering way instead of feature engineering way. We experimentally demonstrate that SA-LSTM gains more improvement from the model architecture. Furthermore, SA-LSTM outperforms the state-of-the-art on CPB 1.0 significantly according to Student t-test ($p<0.05$).

##### Abstract (translated by Google)
对于语义角色标注（SRL）任务，在利用解析信​​息时，传统方法和最近的递归神经网络（RNN）方法均采用特征工程方法。在本文中，我们提出了句法感知长时间短时记忆（SA-LSTM）。 SA-LSTM的结构根据依赖关系解析信息进行修改，以直接以体系结构工程的方式建模解析信息，而不是采用特征工程的方式。我们通过实验证明SA-LSTM从模型架构中获得了更多的改进。此外，根据学生t检验，SA-LSTM显着优于CPB 1.0的最新技术（$ p <0.05 $）。

##### URL
[https://arxiv.org/abs/1704.00405](https://arxiv.org/abs/1704.00405)

##### PDF
[https://arxiv.org/pdf/1704.00405](https://arxiv.org/pdf/1704.00405)

