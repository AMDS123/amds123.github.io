---
layout: post
title: "Multilingual Hierarchical Attention Networks for Document Classification"
date: 2017-09-15 10:47:26
categories: arXiv_CL
tags: arXiv_CL Attention Classification
author: Nikolaos Pappas, Andrei Popescu-Belis
mathjax: true
---

* content
{:toc}

##### Abstract
Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.

##### Abstract (translated by Google)
分级关注网络最近在给定语言中对文档分类取得了显着的性能。然而，在考虑多语言文档集合时，分别针对每种语言对这些模型进行培训需要线性参数增长和缺乏跨语言传输。学习一个参数较少的单一多语言模型因此是一个具有挑战性但潜在有益的目标。为此，我们针对学习文档结构提出了多语言层次关注网络，使用多任务学习和对齐的语义空间作为输入，共享编码器和/或跨语言的共享关注机制。我们使用不相交的标签集，在我们提供的大型数据集上，使用8种语言的600k个新闻文档和5k个标签来评估所提出的多语言文档分类模型。多语言模型在低资源和全资源环境下表现优于单语模型，参数使用较少，从而验证了它们的计算效率和跨语言迁移的实用性。

##### URL
[https://arxiv.org/abs/1707.00896](https://arxiv.org/abs/1707.00896)

##### PDF
[https://arxiv.org/pdf/1707.00896](https://arxiv.org/pdf/1707.00896)

