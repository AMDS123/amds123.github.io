---
layout: post
title: "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality"
date: 2018-05-11 22:49:10
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Alexandre Salle, Aline Villavicencio
mathjax: true
---

* content
{:toc}

##### Abstract
Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, with significant increase of computational cost. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory.

##### Abstract (translated by Google)
递增递归神经网络（RNN）的容量通常涉及增加隐藏层的大小，并且计算成本显着增加。递归神经张量网络（RNTN）使用每个词的不同隐藏层权重来增加容量，但是存储器使用成本更高。在本文中，我们引入限制递归神经张量网络（r-RNTN），它为频繁的词汇单词保留不同的隐藏层权重，同时为不频繁的词汇分配一组权重。困惑性评估表明，对于固定隐藏层大小，r-RNTN仅使用非限制RNTN的一小部分参数来提高RNN上的语言模型性能。这些结果适用于使用门控重复单元和长期短期记忆的r-RNTNs。

##### URL
[http://arxiv.org/abs/1704.00774](http://arxiv.org/abs/1704.00774)

##### PDF
[http://arxiv.org/pdf/1704.00774](http://arxiv.org/pdf/1704.00774)

