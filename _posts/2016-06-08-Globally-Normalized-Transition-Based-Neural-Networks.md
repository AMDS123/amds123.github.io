---
layout: post
title: "Globally Normalized Transition-Based Neural Networks"
date: 2016-06-08 13:43:30
categories: arXiv_SD
tags: arXiv_SD
author: Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, Michael Collins
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.

##### Abstract (translated by Google)
我们引入了全局归一化的基于转换的神经网络模型，实现了最先进的词性标注，依存分析和句子压缩结果。我们的模型是一个简单的前馈神经网络，它在一个任务特定的过渡系统上运行，但却比循环模型具有可比较的或更好的精度。我们讨论全局而非局部归一化的重要性：一个关键的洞察是标签偏差问题意味着全局归一化模型可以比局部归一化模型更严格地表达。

##### URL
[https://arxiv.org/abs/1603.06042](https://arxiv.org/abs/1603.06042)

##### PDF
[https://arxiv.org/pdf/1603.06042](https://arxiv.org/pdf/1603.06042)

