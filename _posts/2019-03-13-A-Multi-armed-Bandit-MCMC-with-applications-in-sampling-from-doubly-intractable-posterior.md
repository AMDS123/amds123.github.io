---
layout: post
title: "A Multi-armed Bandit MCMC, with applications in sampling from doubly intractable posterior"
date: 2019-03-13 21:38:48
categories: arXiv_AI
tags: arXiv_AI Inference
author: Wang Guanyang
mathjax: true
---

* content
{:toc}

##### Abstract
Markov chain Monte Carlo (MCMC) algorithms are widely used to sample from complicated distributions, especially to sample from the posterior distribution in Bayesian inference. However, MCMC is not directly applicable when facing the doubly intractable problem. In this paper, we discussed and compared two existing solutions -- Pseudo-marginal Monte Carlo and Exchange Algorithm. This paper also proposes a novel algorithm: Multi-armed Bandit MCMC (MABMC), which chooses between two (or more) randomized acceptance ratios in each step. MABMC could be applied directly to incorporate Pseudo-marginal Monte Carlo and Exchange algorithm, with higher average acceptance probability.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1903.05726](http://arxiv.org/abs/1903.05726)

##### PDF
[http://arxiv.org/pdf/1903.05726](http://arxiv.org/pdf/1903.05726)

