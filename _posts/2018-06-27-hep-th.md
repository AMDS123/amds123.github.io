---
layout: post
title: "hep-th"
date: 2018-06-27 11:44:35
categories: arXiv_CL
tags: arXiv_CL Classification Language_Model
author: Yang-Hui He, Vishnu Jejjala, Brent D. Nelson
mathjax: true
---

* content
{:toc}

##### Abstract
We apply techniques in natural language processing, computational linguistics, and machine-learning to investigate papers in hep-th and four related sections of the arXiv: hep-ph, hep-lat, gr-qc, and math-ph. All of the titles of papers in each of these sections, from the inception of the arXiv until the end of 2017, are extracted and treated as a corpus which we use to train the neural network Word2Vec. A comparative study of common n-grams, linear syntactical identities, word cloud and word similarities is carried out. We find notable scientific and sociological differences between the fields. In conjunction with support vector machines, we also show that the syntactic structure of the titles in different sub-fields of high energy and mathematical physics are sufficiently different that a neural network can perform a binary classification of formal versus phenomenological sections with 87.1% accuracy, and can perform a finer five-fold classification across all sections with 65.1% accuracy.

##### Abstract (translated by Google)
我们应用自然语言处理，计算语言学和机器学习技术来研究arXiv的第四和相关部分的论文：hep-ph，hep-lat，gr-qc和math-ph。从arXiv开始到2017年底，这些部分中的所有论文标题都被提取并作为语料库处理，我们用它来训练神经网络Word2Vec。进行了常见n-gram，线性句法身份，词云和单词相似性的比较研究。我们发现这些领域之间存在显着的科学和社会学差异。结合支持向量机，我们还表明，高能量和数学物理学的不同子领域中的标题的句法结构是完全不同的，神经网络可以执行形式与现象学部分的二元分类，准确度为87.1％，并且可以在所有部分执行更精细的五重分类，准确率为65.1％。

##### URL
[http://arxiv.org/abs/1807.00735](http://arxiv.org/abs/1807.00735)

##### PDF
[http://arxiv.org/pdf/1807.00735](http://arxiv.org/pdf/1807.00735)

