---
layout: post
title: "The loss surface and expressivity of deep convolutional neural networks"
date: 2017-10-30 13:24:28
categories: arXiv_CV
tags: arXiv_CV Face CNN Optimization Deep_Learning
author: Quynh Nguyen, Matthias Hein
mathjax: true
---

* content
{:toc}

##### Abstract
We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a "wide" layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer, we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with potentially no bad local minima.

##### Abstract (translated by Google)
我们分析了具有共享权重和最大共享层次的实际深度卷积神经网络（CNN）的表达性和损失表面。我们表明，这样的CNN在“广”层产生线性独立的特征，其具有比训练样本的数目更多的神经元。这个条件例如为VGG网络。而且，我们为全球最小的CNN提供了这么宽的CNN的充分必要条件，并且没有训练错误。对于宽层跟随完全连通层的情况，我们表明几乎每个经验损失的临界点都是全局最小值，零训练误差。我们的分析表明深度和宽度在深度学习中是非常重要的。虽然深度带来更多的代表性的权力，并允许网络学习高水平的功能，宽度平滑损失函数的优化格局，意义上说，一个足够宽的网络有一个良好的损失表面，可能没有不良的局部最小值。

##### URL
[https://arxiv.org/abs/1710.10928](https://arxiv.org/abs/1710.10928)

##### PDF
[https://arxiv.org/pdf/1710.10928](https://arxiv.org/pdf/1710.10928)

