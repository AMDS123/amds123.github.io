---
layout: post
title: "Variational Memory Encoder-Decoder"
date: 2018-07-26 04:41:30
categories: arXiv_CL
tags: arXiv_CL
author: Hung Le, Truyen Tran, Thin Nguyen, Svetha Venkatesh
mathjax: true
---

* content
{:toc}

##### Abstract
Introducing variability while maintaining coherence is a core task in learning to generate utterances in conversation. Standard neural encoder-decoder models and their extensions using conditional variational autoencoder often result in either trivial or digressive responses. To overcome this, we explore a novel approach that injects variability into neural encoder-decoder via the use of external memory as a mixture model, namely Variational Memory Encoder-Decoder (VMED). By associating each memory read with a mode in the latent mixture distribution at each timestep, our model can capture the variability observed in sequential data such as natural conversations. We empirically compare the proposed model against other recent approaches on various conversational datasets. The results show that VMED consistently achieves significant improvement over others in both metric-based and qualitative evaluations.

##### Abstract (translated by Google)
在保持一致性的同时引入可变性是学习在对话中产生话语的核心任务。标准神经编码器 - 解码器模型及其使用条件变分自动编码器的扩展通常导致无关紧要或偏离的响应。为了克服这个问题，我们探索了一种新方法，通过使用外部存储器作为混合模型，即变分存储器编码器 - 解码器（VMED），将可变性注入神经编码器 - 解码器。通过在每个时间步将每个存储器读取与潜在混合物分布中的模式相关联，我们的模型可以捕获在顺序数据（例如自然对话）中观察到的可变性。我们根据经验将所提出的模型与各种会话数据集上的其他近期方法进行比较。结果表明，在基于度量的评估和定性评估中，VMED始终比其他人有显着的改进。

##### URL
[http://arxiv.org/abs/1807.09950](http://arxiv.org/abs/1807.09950)

##### PDF
[http://arxiv.org/pdf/1807.09950](http://arxiv.org/pdf/1807.09950)

