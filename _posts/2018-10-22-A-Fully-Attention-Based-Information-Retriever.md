---
layout: post
title: "A Fully Attention-Based Information Retriever"
date: 2018-10-22 22:10:46
categories: arXiv_CL
tags: arXiv_CL Attention Inference RNN
author: Alvaro Henrique Chaim Correia, Jorge Luiz Moreira Silva, Thiago de Castro Martins, Fabio Gagliardi Cozman
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks are now the state-of-the-art in natural language processing because they can build rich contextual representations and process texts of arbitrary length. However, recent developments on attention mechanisms have equipped feedforward networks with similar capabilities, hence enabling faster computations due to the increase in the number of operations that can be parallelized. We explore this new type of architecture in the domain of question-answering and propose a novel approach that we call Fully Attention Based Information Retriever (FABIR). We show that FABIR achieves competitive results in the Stanford Question Answering Dataset (SQuAD) while having fewer parameters and being faster at both learning and inference than rival methods.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.09580](http://arxiv.org/abs/1810.09580)

##### PDF
[http://arxiv.org/pdf/1810.09580](http://arxiv.org/pdf/1810.09580)

