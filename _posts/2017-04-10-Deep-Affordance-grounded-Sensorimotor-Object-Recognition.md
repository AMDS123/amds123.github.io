---
layout: post
title: "Deep Affordance-grounded Sensorimotor Object Recognition"
date: 2017-04-10 10:06:53
categories: arXiv_CV
tags: arXiv_CV Deep_Learning Recognition
author: Spyridon Thermos, Georgios Th. Papadopoulos, Petros Daras, Gerasimos Potamianos
mathjax: true
---

* content
{:toc}

##### Abstract
It is well-established by cognitive neuroscience that human perception of objects constitutes a complex process, where object appearance information is combined with evidence about the so-called object "affordances", namely the types of actions that humans typically perform when interacting with them. This fact has recently motivated the "sensorimotor" approach to the challenging task of automatic object recognition, where both information sources are fused to improve robustness. In this work, the aforementioned paradigm is adopted, surpassing current limitations of sensorimotor object recognition research. Specifically, the deep learning paradigm is introduced to the problem for the first time, developing a number of novel neuro-biologically and neuro-physiologically inspired architectures that utilize state-of-the-art neural networks for fusing the available information sources in multiple ways. The proposed methods are evaluated using a large RGB-D corpus, which is specifically collected for the task of sensorimotor object recognition and is made publicly available. Experimental results demonstrate the utility of affordance information to object recognition, achieving an up to 29% relative error reduction by its inclusion.

##### Abstract (translated by Google)
认知神经科学认为人类对物体的感知构成了一个复杂的过程，在这个过程中，物体外观信息与关于所谓的物体“可供性”的证据相结合，即人类与它们相互作用时典型的行为类型。这个事实最近激发了“感觉运动”的方法来处理自动对象识别的挑战性任务，其中两个信息源被融合以提高鲁棒性。在这项工作中，采用了上述的范式，超越了目前对感觉运动物体识别研究的局限性。具体而言，深度学习范式首次引入到这个问题中，开发了许多新颖的神经生物学和神经生理学启发体系结构，利用最先进的神经网络以多种方式融合可用的信息源。所提出的方法使用大的RGB-D语料库进行评估，该语料库专门收集用于感知运动物体识别的任务并被公开可用。实验结果证明了可供性信息对物体识别的效用，通过包含来减少高达29％的相对误差。

##### URL
[https://arxiv.org/abs/1704.02787](https://arxiv.org/abs/1704.02787)

##### PDF
[https://arxiv.org/pdf/1704.02787](https://arxiv.org/pdf/1704.02787)

