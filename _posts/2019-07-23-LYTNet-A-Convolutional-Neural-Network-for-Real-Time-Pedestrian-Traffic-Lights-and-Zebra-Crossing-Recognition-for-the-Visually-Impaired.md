---
layout: post
title: "LYTNet: A Convolutional Neural Network for Real-Time Pedestrian Traffic Lights and Zebra Crossing Recognition for the Visually Impaired"
date: 2019-07-23 05:56:43
categories: arXiv_CV
tags: arXiv_CV CNN Classification Deep_Learning Recognition
author: Samuel Yu, Heon Lee, John Kim
mathjax: true
---

* content
{:toc}

##### Abstract
Currently, the visually impaired rely on either a sighted human, guide dog, or white cane to safely navigate. However, the training of guide dogs is extremely expensive, and canes cannot provide essential information regarding the color of traffic lights and direction of crosswalks. In this paper, we propose a deep learning based solution that provides information regarding the traffic light mode and the position of the zebra crossing. Previous solutions that utilize machine learning only provide one piece of information and are mostly binary: only detecting red or green lights. The proposed convolutional neural network, LYTNet, is designed for comprehensiveness, accuracy, and computational efficiency. LYTNet delivers both of the two most important pieces of information for the visually impaired to cross the road. We provide five classes of pedestrian traffic lights rather than the commonly seen three or four, and a direction vector representing the midline of the zebra crossing that is converted from the 2D image plane to real-world positions. We created our own dataset of pedestrian traffic lights containing over 5000 photos taken at hundreds of intersections in Shanghai. The experiments carried out achieve a classification accuracy of 94%, average angle error of 6.35 degrees, with a frame rate of 20 frames per second when testing the network on an iPhone 7 with additional post-processing steps.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.09706](http://arxiv.org/abs/1907.09706)

##### PDF
[http://arxiv.org/pdf/1907.09706](http://arxiv.org/pdf/1907.09706)

