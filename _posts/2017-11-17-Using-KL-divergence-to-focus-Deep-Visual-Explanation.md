---
layout: post
title: "Using KL-divergence to focus Deep Visual Explanation"
date: 2017-11-17 06:53:17
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Classification Deep_Learning Prediction
author: Housam Khalifa Bashier Babiker, Randy Goebel
mathjax: true
---

* content
{:toc}

##### Abstract
We present a method for explaining the image classification predictions of deep convolution neural networks, by highlighting the pixels in the image which influence the final class prediction. Our method requires the identification of a heuristic method to select parameters hypothesized to be most relevant in this prediction, and here we use Kullback-Leibler divergence to provide this focus. Overall, our approach helps in understanding and interpreting deep network predictions and we hope contributes to a foundation for such understanding of deep learning networks. In this brief paper, our experiments evaluate the performance of two popular networks in this context of interpretability.

##### Abstract (translated by Google)
我们提出一种解释深度卷积神经网络的图像分类预测的方法，通过突出显示影响最终类别预测的图像中的像素。我们的方法需要识别启发式方法来选择假设在这个预测中最相关的参数，在这里我们使用Kullback-Leibler散度来提供这个焦点。总的来说，我们的方法有助于理解和解释深度网络预测，我们希望为深入学习网络的理解奠定基础。在这篇简短的论文中，我们的实验在这种可解释性的背景下评估了两个流行网络的性能。

##### URL
[https://arxiv.org/abs/1711.06431](https://arxiv.org/abs/1711.06431)

##### PDF
[https://arxiv.org/pdf/1711.06431](https://arxiv.org/pdf/1711.06431)

