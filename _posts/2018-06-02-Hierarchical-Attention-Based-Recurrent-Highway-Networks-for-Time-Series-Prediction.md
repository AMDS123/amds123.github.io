---
layout: post
title: "Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction"
date: 2018-06-02 18:46:50
categories: arXiv_CV
tags: arXiv_CV Attention Deep_Learning Prediction Relation
author: Yunzhe Tao, Lin Ma, Weizhong Zhang, Jian Liu, Wei Liu, Qiang Du
mathjax: true
---

* content
{:toc}

##### Abstract
Time series prediction has been studied in a variety of domains. However, it is still challenging to predict future series given historical observations and past exogenous data. Existing methods either fail to consider the interactions among different components of exogenous variables which may affect the prediction accuracy, or cannot model the correlations between exogenous data and target data. Besides, the inherent temporal dynamics of exogenous data are also related to the target series prediction, and thus should be considered as well. To address these issues, we propose an end-to-end deep learning model, i.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which incorporates spatio-temporal feature extraction of exogenous variables and temporal dynamics modeling of target variables into a single framework. Moreover, by introducing the hierarchical attention mechanism, HRHN can adaptively select the relevant exogenous features in different semantic levels. We carry out comprehensive empirical evaluations with various methods over several datasets, and show that HRHN outperforms the state of the arts in time series prediction, especially in capturing sudden changes and sudden oscillations of time series.

##### Abstract (translated by Google)
时间序列预测已经在各种领域进行了研究。然而，根据历史观察和过去的外生数据预测未来的系列仍然具有挑战性。现有方法要么不考虑可能影响预测准确性的外生变量的不同组分之间的相互作用，要么不能模拟外生数据与目标数据之间的相关性。此外，外生数据的固有时间动态也与目标序列预测有关，因此也应予以考虑。为了解决这些问题，我们提出了一种端到端的深度学习模型，即基于分层注意的循环高速公路网络（HRHN），它将外生变量的时空特征提取和目标变量的时间动态建模合并为一个单一的框架。此外，通过引入层次关注机制，HRHN可以自适应地选择不同语义层次的相关外生特征。我们用几种数据集上的各种方法进行全面的实证评估，表明HRHN在时间序列预测中优于艺术状态，特别是捕捉时间序列的突然变化和突然振荡。

##### URL
[http://arxiv.org/abs/1806.00685](http://arxiv.org/abs/1806.00685)

##### PDF
[http://arxiv.org/pdf/1806.00685](http://arxiv.org/pdf/1806.00685)

