---
layout: post
title: "On the Evaluation of Dialogue Systems with Next Utterance Classification"
date: 2016-07-23 00:00:36
categories: arXiv_CL
tags: arXiv_CL Classification
author: Ryan Lowe, Iulian V. Serban, Mike Noseworthy, Laurent Charlin, Joelle Pineau
mathjax: true
---

* content
{:toc}

##### Abstract
An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed Next-Utterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.

##### Abstract (translated by Google)
构建对话系统面临的一个公开挑战是开发从大量未标记的数据中自动学习对话策略的方法。最近的工作提出了下一步话语分类（NUC）作为从文本数据建立对话系统的替代任务。在本文中，我们调查人类在这个任务上的表现，以验证NUC作为一种评估方法的相关性。我们的研究结果显示了三个主要的发现：（1）人类能够以比机会好得多的速率对反应进行正确的分类，从而确认任务是可行的;（2）人的绩效水平在任务领域（我们考虑3个数据集） （新手vs专家），从而表明在这种类型的任务中可以有一系列的性能表现，（3）使用最先进的机器学习方法构建的自动对话系统与人类新手具有相似的性能，但是比专家更差，从而证实了这类任务的实用性，以推动自动对话系统的进一步研究。

##### URL
[https://arxiv.org/abs/1605.05414](https://arxiv.org/abs/1605.05414)

##### PDF
[https://arxiv.org/pdf/1605.05414](https://arxiv.org/pdf/1605.05414)

