---
layout: post
title: "End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy Gradient"
date: 2017-12-07 19:52:50
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Li Zhou, Kevin Small, Oleg Rokhlenko, Charles Elkan
mathjax: true
---

* content
{:toc}

##### Abstract
Learning a goal-oriented dialog policy is generally performed offline with supervised learning algorithms or online with reinforcement learning (RL). Additionally, as companies accumulate massive quantities of dialog transcripts between customers and trained human agents, encoder-decoder methods have gained popularity as agent utterances can be directly treated as supervision without the need for utterance-level annotations. However, one potential drawback of such approaches is that they myopically generate the next agent utterance without regard for dialog-level considerations. To resolve this concern, this paper describes an offline RL method for learning from unannotated corpora that can optimize a goal-oriented policy at both the utterance and dialog level. We introduce a novel reward function and use both on-policy and off-policy policy gradient to learn a policy offline without requiring online user interaction or an explicit state space definition.

##### Abstract (translated by Google)
学习一个面向目标的对话策略通常是使用监督学习算法离线执行的，或者使用强化学习（RL）在线进行。另外，由于公司在客户和受过训练的人员之间积累了大量的对话成绩单，因此编码器 - 译码器方法越来越受欢迎，因为代理人的话语可以直接作为监督而不需要话语级别的注释。然而，这种方法的一个潜在的缺点是它们在不考虑对话层考虑的情况下在近视下产生下一个代理话语。为了解决这个问题，本文描述了一种离线的语料库学习方法，可以在语言和对话层面优化目标导向的政策。我们引入了一种新颖的奖励功能，并使用在线策略和非策略策略梯度来离线学习策略，而不需要在线用户交互或明确的状态空间定义。

##### URL
[https://arxiv.org/abs/1712.02838](https://arxiv.org/abs/1712.02838)

##### PDF
[https://arxiv.org/pdf/1712.02838](https://arxiv.org/pdf/1712.02838)

