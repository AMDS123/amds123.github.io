---
layout: post
title: "3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks"
date: 2017-08-04 19:30:13
categories: arXiv_CV
tags: arXiv_CV RNN
author: Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, Derek Hoiem
mathjax: true
---

* content
{:toc}

##### Abstract
The success of various applications including robotics, digital content creation, and visualization demand a structured and abstract representation of the 3D world from limited sensor data. Inspired by the nature of human perception of 3D shapes as a collection of simple parts, we explore such an abstract shape representation based on primitives. Given a single depth image of an object, we present 3D-PRNN, a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives. Our generative model encodes symmetry characteristics of common man-made objects, preserves long-range structural coherence, and describes objects of varying complexity with a compact representation. We also propose a method based on Gaussian Fields to generate a large scale dataset of primitive-based shape representations to train our network. We evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxel-based generative models while using a significantly reduced parameter space.

##### Abstract (translated by Google)
包括机器人技术，数字内容创建和可视化在内的各种应用的成功需要有限的传感器数据对3D世界进行结构化和抽象的表示。受人类对三维形状感知的本质的启发，我们探索了一种基于图元的抽象形状表示。给定一个对象的深度图像，我们提出一个生成的循环神经网络3D-PRNN，它可以合成由一组基元组成的多个合理的形状。我们的生成模型编码普通人造物体的对称特性，保留远距离的结构连贯性，并用紧凑的表示描述不同复杂度的物体。我们还提出了一种基于高斯场的方法来生成基于原始形状表示的大规模数据集来训练我们的网络。我们在广泛的例子中评估我们的方法，并且表明它优于基于最邻近的形状检索方法，并且与基于体素的生成模型相当，同时使用显着减少的参数空间。

##### URL
[https://arxiv.org/abs/1708.01648](https://arxiv.org/abs/1708.01648)

##### PDF
[https://arxiv.org/pdf/1708.01648](https://arxiv.org/pdf/1708.01648)

