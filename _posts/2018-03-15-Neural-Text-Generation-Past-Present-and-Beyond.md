---
layout: post
title: "Neural Text Generation: Past, Present and Beyond"
date: 2018-03-15 07:54:30
categories: arXiv_AI
tags: arXiv_AI Adversarial GAN Text_Generation Reinforcement_Learning Survey Language_Model
author: Sidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, Yong Yu
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a systematic survey on recent development of neural text generation models. Specifically, we start from recurrent neural network language models with the traditional maximum likelihood estimation training scheme and point out its shortcoming for text generation. We thus introduce the recently proposed methods for text generation based on reinforcement learning, re-parametrization tricks and generative adversarial nets (GAN) techniques. We compare different properties of these models and the corresponding techniques to handle their common problems such as gradient vanishing and generation diversity. Finally, we conduct a benchmarking experiment with different types of neural text generation models on two well-known datasets and discuss the empirical results along with the aforementioned model properties.

##### Abstract (translated by Google)
本文提出了对神经文本生成模型近期发展的系统调查。具体来说，我们从传统的神经网络语言模型开始，用传统的最大似然估计训练方案，并指出它的文本生成的缺点。因此，我们介绍了最近提出的基于强化学习，重新参数化技巧和生成对抗网（GAN）技术的文本生成方法。我们比较了这些模型的不同属性和相应的技术，以处理它们的常见问题，如梯度消失和生成多样性。最后，我们在两个众所周知的数据集上对不同类型的神经文本生成模型进行了基准实验，并讨论了实验结果以及上述模型属性。

##### URL
[http://arxiv.org/abs/1803.07133](http://arxiv.org/abs/1803.07133)

##### PDF
[http://arxiv.org/pdf/1803.07133](http://arxiv.org/pdf/1803.07133)

