---
layout: post
title: "Reducing Gender Bias in Abusive Language Detection"
date: 2018-08-22 06:00:56
categories: arXiv_CL
tags: arXiv_CL Embedding Detection
author: Ji Ho Park, Jamin Shin, Pascale Fung
mathjax: true
---

* content
{:toc}

##### Abstract
Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, "You are a good woman" was considered "sexist" when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure gender biases on models trained with different abusive language datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios.

##### Abstract (translated by Google)
由于训练数据集不平衡，滥用语言检测模型往往存在偏向某一群体的身份词的问题。例如，在现有数据集上训练时，“你是一个好女人”被认为是“性别歧视”。这种模型偏差是模型足够坚固以供实际使用的障碍。在这项工作中，我们测量了使用不同的滥用语言数据集训练的模型的性别偏差，同时分析了不同的预训练词嵌入和模型架构的效果。我们还尝试了三种偏差缓解方法：（1）去除字嵌入，（2）性别交换数据增加，以及（3）使用更大的语料库进行微调。这些方法可以有效地将性别偏差降低90-98％，并可以扩展到其他情景中的模型偏差。

##### URL
[http://arxiv.org/abs/1808.07231](http://arxiv.org/abs/1808.07231)

##### PDF
[http://arxiv.org/pdf/1808.07231](http://arxiv.org/pdf/1808.07231)

