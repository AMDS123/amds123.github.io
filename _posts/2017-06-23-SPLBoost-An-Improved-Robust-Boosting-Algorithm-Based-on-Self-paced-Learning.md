---
layout: post
title: "SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced Learning"
date: 2017-06-23 14:04:46
categories: arXiv_CV
tags: arXiv_CV Gradient_Descent
author: Kaidong Wang, Yao Wang, Qian Zhao, Deyu Meng, Zongben Xu
mathjax: true
---

* content
{:toc}

##### Abstract
It is known that Boosting can be interpreted as a gradient descent technique to minimize an underlying loss function. Specifically, the underlying loss being minimized by the traditional AdaBoost is the exponential loss, which is proved to be very sensitive to random noise/outliers. Therefore, several Boosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to improve the robustness of AdaBoost by replacing the exponential loss with some designed robust loss functions. In this work, we present a new way to robustify AdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning (SPL) into Boosting framework. Specifically, we design a new robust Boosting algorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented by slightly modifying off-the-shelf Boosting packages. Extensive experiments and a theoretical characterization are also carried out to illustrate the merits of the proposed SPLBoost.

##### Abstract (translated by Google)
众所周知，Boosting可以被解释为梯度下降技术，以使潜在的损失函数最小化。具体来说，传统的AdaBoost最小化的潜在损失是指数损失，这被证明对随机噪声/异常值非常敏感。因此，已经提出了几种Boosting算法，例如LogitBoost和SavageBoost，通过用一些设计的鲁棒损失函数代替指数损失来提高AdaBoost的鲁棒性。在这项工作中，我们提出了一种强化AdaBoost的新方法，即将自主学习（SPL）的强大学习理念融入到Boosting框架中。具体来说，我们设计了一个基于SPL机制的新的稳健Boosting算法，即SPLBoost，通过稍微修改现成的Boosting包可以轻松实现。还进行了大量的实验和理论表征来说明所提出的SPLBoost的优点。

##### URL
[https://arxiv.org/abs/1706.06341](https://arxiv.org/abs/1706.06341)

##### PDF
[https://arxiv.org/pdf/1706.06341](https://arxiv.org/pdf/1706.06341)

