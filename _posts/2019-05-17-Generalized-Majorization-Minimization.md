---
layout: post
title: "Generalized Majorization-Minimization"
date: 2019-05-17 17:13:53
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Sobhan Naderi Parizi, Kun He, Reza Aghajani, Stan Sclaroff, Pedro Felzenszwalb
mathjax: true
---

* content
{:toc}

##### Abstract
Non-convex optimization is ubiquitous in machine learning. Majorization-Minimization (MM) is a powerful iterative procedure for optimizing non-convex functions that works by optimizing a sequence of bounds on the function. In MM, the bound at each iteration is required to \emph{touch} the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new optimization framework, named Generalized Majorization-Minimization (G-MM), that is more flexible. For instance, G-MM can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show empirically that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1506.07613](http://arxiv.org/abs/1506.07613)

##### PDF
[http://arxiv.org/pdf/1506.07613](http://arxiv.org/pdf/1506.07613)

