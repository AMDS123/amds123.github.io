---
layout: post
title: "Deep Matching Autoencoders"
date: 2017-11-16 11:50:41
categories: arXiv_CV
tags: arXiv_CV Image_Caption GAN Caption Represenation_Learning
author: Tanmoy Mukherjee, Makoto Yamada, Timothy M. Hospedales
mathjax: true
---

* content
{:toc}

##### Abstract
Increasingly many real world tasks involve data in multiple modalities or views. This has motivated the development of many effective algorithms for learning a common latent space to relate multiple domains. However, most existing cross-view learning algorithms assume access to paired data for training. Their applicability is thus limited as the paired data assumption is often violated in practice: many tasks have only a small subset of data available with pairing annotation, or even no paired data at all. In this paper we introduce Deep Matching Autoencoders (DMAE), which learn a common latent space and pairing from unpaired multi-modal data. Specifically we formulate this as a cross-domain representation learning and object matching problem. We simultaneously optimise parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data. This framework elegantly spans the full regime from fully supervised, semi-supervised, and unsupervised (no paired data) multi-modal learning. We show promising results in image captioning, and on a new task that is uniquely enabled by our methodology: unsupervised classifier learning.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1711.06047](https://arxiv.org/abs/1711.06047)

##### PDF
[https://arxiv.org/pdf/1711.06047](https://arxiv.org/pdf/1711.06047)

