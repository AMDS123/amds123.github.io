---
layout: post
title: "Deep Matching Autoencoders"
date: 2017-11-16 11:50:41
categories: arXiv_CV
tags: arXiv_CV Image_Caption GAN Caption
author: Tanmoy Mukherjee, Makoto Yamada, Timothy M. Hospedales
mathjax: true
---

* content
{:toc}

##### Abstract
Increasingly many real world tasks involve data in multiple modalities or views. This has motivated the development of many effective algorithms for learning a common latent space to relate multiple domains. However, most existing cross-view learning algorithms assume access to paired data for training. Their applicability is thus limited as the paired data assumption is often violated in practice: many tasks have only a small subset of data available with pairing annotation, or even no paired data at all. In this paper we introduce Deep Matching Autoencoders (DMAE), which learn a common latent space and pairing from unpaired multi-modal data. Specifically we formulate this as a cross-domain representation learning and object matching problem. We simultaneously optimise parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data. This framework elegantly spans the full regime from fully supervised, semi-supervised, and unsupervised (no paired data) multi-modal learning. We show promising results in image captioning, and on a new task that is uniquely enabled by our methodology: unsupervised classifier learning.

##### Abstract (translated by Google)
越来越多的现实世界的任务涉及多种形式或观点的数据。这激发了许多有效的算法的发展，学习一个共同的潜在空间，以涉及多个领域。但是，大多数现有的交叉视图学习算法假定访问配对数据进行训练。它们的适用性因此受到限制，因为配对数据假设在实践中经常被违反：许多任务只有一小部分数据可用于配对注释，甚至根本没有配对数据。在本文中，我们介绍了深度匹配自动编码器（DMAE），它学习了一个常见的潜在空间，并从不成对的多模式数据中进行配对。具体而言，我们将其制定为跨域表示学习和对象匹配问题。我们同时优化表示学习自动编码器的参数和不成对的多模态数据的配对。该框架完美地跨越了完全监督，半监督和无监督（无配对数据）多模式学习的完整体系。我们在图像字幕中显示出有希望的结果，并且在我们的方法中独特地启用了一项新的任务：无监督的分类器学习。

##### URL
[https://arxiv.org/abs/1711.06047](https://arxiv.org/abs/1711.06047)

##### PDF
[https://arxiv.org/pdf/1711.06047](https://arxiv.org/pdf/1711.06047)

