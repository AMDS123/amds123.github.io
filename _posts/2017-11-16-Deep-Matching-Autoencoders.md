---
layout: post
title: "Deep Matching Autoencoders"
date: 2017-11-16 11:50:41
categories: arXiv_CV
tags: arXiv_CV Image_Caption GAN Caption Represenation_Learning
author: Tanmoy Mukherjee, Makoto Yamada, Timothy M. Hospedales
mathjax: true
---

* content
{:toc}

##### Abstract
Increasingly many real world tasks involve data in multiple modalities or views. This has motivated the development of many effective algorithms for learning a common latent space to relate multiple domains. However, most existing cross-view learning algorithms assume access to paired data for training. Their applicability is thus limited as the paired data assumption is often violated in practice: many tasks have only a small subset of data available with pairing annotation, or even no paired data at all. In this paper we introduce Deep Matching Autoencoders (DMAE), which learn a common latent space and pairing from unpaired multi-modal data. Specifically we formulate this as a cross-domain representation learning and object matching problem. We simultaneously optimise parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data. This framework elegantly spans the full regime from fully supervised, semi-supervised, and unsupervised (no paired data) multi-modal learning. We show promising results in image captioning, and on a new task that is uniquely enabled by our methodology: unsupervised classifier learning.

##### Abstract (translated by Google)
越来越多的现实世界任务涉及多种模态或视图中的数据。这促使开发了许多有效的算法，用于学习共同的潜在空间以关联多个域。然而，大多数现有的跨视图学习算法假设访问配对数据以进行训练。因此，它们的适用性受到限制，因为配对数据假设在实践中经常被违反：许多任务只有一小部分数据可用于配对注释，甚至根本没有配对数据。在本文中，我们介绍了深度匹配自动编码器（DMAE），它可以从不成对的多模态数据中学习常见的潜在空间和配对。具体来说，我们将其表述为跨域表示学习和对象匹配问题。我们同时优化表示学习自动编码器的参数和不成对的多模态数据的配对。这个框架优雅地涵盖了完全监督，半监督和无监督（无配对数据）多模式学习的完整制度。我们在图像字幕以及由我们的方法独特支持的新任务中展示了有希望的结果：无监督分类器学习。

##### URL
[https://arxiv.org/abs/1711.06047](https://arxiv.org/abs/1711.06047)

##### PDF
[https://arxiv.org/pdf/1711.06047](https://arxiv.org/pdf/1711.06047)

