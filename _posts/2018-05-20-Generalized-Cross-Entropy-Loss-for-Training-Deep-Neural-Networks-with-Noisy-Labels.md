---
layout: post
title: "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels"
date: 2018-05-20 23:01:49
categories: arXiv_CV
tags: arXiv_CV
author: Zhilu Zhang, Mert R. Sabuncu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.

##### Abstract (translated by Google)
深度神经网络（DNN）在许多学科的各种应用中取得了巨大的成功。然而，它们卓越的性能伴随着需要正确注释的大规模数据集的昂贵成本。此外，由于DNN的容量丰富，培训标签中的错误可能会影响性能。为了解决这个问题，最近已经提出平均绝对误差（MAE）作为对常用分类交叉熵（CCE）损失的噪声鲁棒替代。但是，正如我们在本文中所展示的，MAE可能在DNN和具有挑战性的数据集方面表现不佳。在这里，我们提出了一组理论上可以看作是MAE和CCE泛化的噪声鲁棒损失函数。建议的损失函数可以很容易地应用于任何现有的DNN架构和算法，同时在广泛的噪声标签方案中产生良好的性能。我们报告了使用CIFAR-10，CIFAR-100和FASHION-MNIST数据集进行实验的结果以及合成生成的噪声标签。

##### URL
[https://arxiv.org/abs/1805.07836](https://arxiv.org/abs/1805.07836)

##### PDF
[https://arxiv.org/pdf/1805.07836](https://arxiv.org/pdf/1805.07836)

