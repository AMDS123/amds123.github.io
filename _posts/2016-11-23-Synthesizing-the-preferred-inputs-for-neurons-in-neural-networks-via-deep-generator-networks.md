---
layout: post
title: "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks"
date: 2016-11-23 18:41:12
categories: arXiv_CV
tags: arXiv_CV Classification Recognition
author: Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).

##### Abstract (translated by Google)
深度神经网络（DNN）已经在许多模式识别任务上展示了最新的结果，特别是视觉分类问题。了解这种计算脑的内部工作既是迷人的基本科学，它本身是有趣的 - 类似于为什么我们研究人类大脑 - 并将使研究人员进一步改善DNNs。理解神经网络如何在内部起作用的一条途径是研究它的每个神经元学到了什么。一种这样的方法被称为激活最大化（AM），其合成高度激活神经元的输入（例如图像）。在这里，我们通过利用一个强大的，学到的先验：深发电机网络（DGN），极大地改善了激活最大化的艺术的定性状态。该算法（1）生成质量最先进的合成图像，看起来几乎是真实的，（2）以可解释的方式揭示每个神经元学习的特征，（3）很好地推广到新的数据集，并且对不同的网络（4）可以被认为是高质量的生成方法（在这种情况下，通过产生新颖，创造性，有趣的，可识别的图像）。

##### URL
[https://arxiv.org/abs/1605.09304](https://arxiv.org/abs/1605.09304)

##### PDF
[https://arxiv.org/pdf/1605.09304](https://arxiv.org/pdf/1605.09304)

