---
layout: post
title: "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks"
date: 2018-10-22 20:37:46
categories: arXiv_CL
tags: arXiv_CL Inference RNN Language_Model
author: Yikang Shen, Shawn Tan, Alessandro Sordoni, Aaron Courville
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural network (RNN) models are widely used for processing sequential data governed by a latent tree structure. Previous work shows that RNN models (especially Long Short-Term Memory (LSTM) based models) could learn to exploit the underlying tree structure. However, its performance consistently lags behind that of tree-based models. This work proposes a new inductive bias Ordered Neurons, which enforces an order of updating frequencies between hidden state neurons. We show that the ordered neurons could explicitly integrate the latent tree structure into recurrent models. To this end, we propose a new RNN unit: ON-LSTM, which achieve good performances on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.09536](http://arxiv.org/abs/1810.09536)

##### PDF
[http://arxiv.org/pdf/1810.09536](http://arxiv.org/pdf/1810.09536)

