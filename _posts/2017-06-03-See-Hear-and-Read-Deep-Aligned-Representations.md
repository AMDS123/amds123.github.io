---
layout: post
title: "See, Hear, and Read: Deep Aligned Representations"
date: 2017-06-03 11:11:13
categories: arXiv_CV
tags: arXiv_CV CNN Represenation_Learning
author: Yusuf Aytar, Carl Vondrick, Antonio Torralba
mathjax: true
---

* content
{:toc}

##### Abstract
We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality.

##### Abstract (translated by Google)
我们利用大量随时可用的同步数据来学习三种主要自然形式（视觉，声音和语言）中的深层差异表征。通过利用一年以上的视频声音和数百万个与图像配对的句子，我们联合训练了一个深度卷积网络来进行对齐表示学习。我们的实验表明，这种表示对于多种任务是有用的，例如跨模态检索或在模态之间转移分类器。而且，虽然我们的网络只是训练了图像+文本和图像+声音对，它还可以在文本和声音之间转换，转换网络在训练期间从未观察到。我们表示的可视化揭示了许多隐藏的单元，它们自动出现以检测概念，而与模式无关。

##### URL
[https://arxiv.org/abs/1706.00932](https://arxiv.org/abs/1706.00932)

##### PDF
[https://arxiv.org/pdf/1706.00932](https://arxiv.org/pdf/1706.00932)

