---
layout: post
title: "ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions"
date: 2018-09-05 05:15:14
categories: arXiv_CV
tags: arXiv_CV Sparse CNN Classification
author: Hongyang Gao, Zhengyang Wang, Shuiwang Ji
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which re- place dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. Channel- Nets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolu- tional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods.

##### Abstract (translated by Google)
卷积神经网络（CNN）已经显示出解决各种人工智能任务的强大能力。然而，不断增加的模型尺寸在资源有限的应用中使用它们带来了挑战。在这项工作中，我们建议通过使用通道方式的卷积来压缩深度模型，这些卷积在CNN中稀疏的特征映射之间替换密集连接。基于这种新颖的操作，我们构建了称为ChannelNets的轻量级CNN。 Channel-Nets使用三个通道信道卷积实例;即群组通道卷积，深度可分离通道卷积和卷积分类层。与为移动设备设计的现有CNN相比，ChannelNets在参数数量和计算成本方面实现了显着降低，而不会降低精度。值得注意的是，我们的工作代表了压缩完全连接的分类层的第一次尝试，该分层通常占紧凑CNN中总参数的约25％。 ImageNet数据集上的实验结果表明，与现有方法相比，ChannelNets可以实现始终如一的更好性能。

##### URL
[http://arxiv.org/abs/1809.01330](http://arxiv.org/abs/1809.01330)

##### PDF
[http://arxiv.org/pdf/1809.01330](http://arxiv.org/pdf/1809.01330)

