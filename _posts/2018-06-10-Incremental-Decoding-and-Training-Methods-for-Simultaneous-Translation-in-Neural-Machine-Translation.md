---
layout: post
title: "Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation"
date: 2018-06-10 13:50:17
categories: arXiv_CL
tags: arXiv_CL Segmentation Attention
author: Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Stephan Vogel
mathjax: true
---

* content
{:toc}

##### Abstract
We address the problem of simultaneous translation by modifying the Neural MT decoder to operate with dynamically built encoder and attention. We propose a tunable agent which decides the best segmentation strategy for a user-defined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to Neural MT training to better match the incremental decoding framework.

##### Abstract (translated by Google)
我们通过修改神经MT解码器以动态构建编码器和注意力来解决同时翻译的问题。我们提出了一个可调代理，它决定了用户定义的BLEU损失和平均比例（AP）约束的最佳分割策略。我们的代理在BLEU上的延迟时间更低，优于以前提出的Wait-if-diff和Wait-if-worst代理（Cho和Esipova，2016）。其次，我们提出了数据驱动的Neural MT训练变化，以更好地匹配增量解码框架。

##### URL
[http://arxiv.org/abs/1806.03661](http://arxiv.org/abs/1806.03661)

##### PDF
[http://arxiv.org/pdf/1806.03661](http://arxiv.org/pdf/1806.03661)

