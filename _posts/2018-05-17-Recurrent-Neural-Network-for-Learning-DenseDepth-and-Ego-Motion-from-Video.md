---
layout: post
title: "Recurrent Neural Network for Learning DenseDepth and Ego-Motion from Video"
date: 2018-05-17 00:18:08
categories: arXiv_CV
tags: arXiv_CV RNN
author: Rui Wang, Jan-Michael Frahm, Stephen M. Pizer
mathjax: true
---

* content
{:toc}

##### Abstract
Learning-based, single-view depth estimation often generalizes poorly to unseen datasets. While learning-based, two-frame depth estimation solves this problem to some extent by learning to match features across frames, it performs poorly at large depth where the uncertainty is high. There exists few learning-based, multi-view depth estimation methods. In this paper, we present a learning-based, multi-view dense depth map and ego-motion estimation method that uses Recurrent Neural Networks (RNN). Our model is designed for 3D reconstruction from video where the input frames are temporally correlated. It is generalizable to single- or two-view dense depth estimation. Compared to recent single- or two-view CNN-based depth estimation methods, our model leverages more views and achieves more accurate results, especially at large distances. Our method produces superior results to the state-of-the-art learning-based, single- or two-view depth estimation methods on both indoor and outdoor benchmark datasets. We also demonstrate that our method can even work on extremely difficult sequences, such as endoscopic video, where none of the assumptions (static scene, constant lighting, Lambertian reflection, etc.) from traditional 3D reconstruction methods hold.

##### Abstract (translated by Google)
基于学习的单视点深度估计通常对于看不见的数据集进行很差的概括。虽然基于学习的双帧深度估计在一定程度上通过学习跨帧匹配特征来解决这个问题，但是在不确定性高的深度处它表现不佳。存在很少的基于学习的多视点深度估计方法。在本文中，我们提出了一种基于学习的多视点密集深度图和使用递归神经网络（RNN）的自我运动估计方法。我们的模型设计用于从输入帧时间相关的视频进行三维重建。它可以推广到单视点或双视点密集深度估计。与最近的单视图或双视图CNN深度估计方法相比，我们的模型利用更多视图并获得更准确的结果，尤其是在远距离情况下。我们的方法可为室内和室外基准数据集上最先进的基于学习的单视点或双视点深度估计方法带来出众的效果。我们还证明，我们的方法甚至可以处理极其困难的序列，例如内窥镜视频，其中没有任何来自传统三维重建方法的假设（静态场景，恒定光照，朗伯反射等）。

##### URL
[http://arxiv.org/abs/1805.06558](http://arxiv.org/abs/1805.06558)

##### PDF
[http://arxiv.org/pdf/1805.06558](http://arxiv.org/pdf/1805.06558)

