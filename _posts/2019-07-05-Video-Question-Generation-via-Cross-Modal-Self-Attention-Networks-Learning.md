---
layout: post
title: "Video Question Generation via Cross-Modal Self-Attention Networks Learning"
date: 2019-07-05 23:47:04
categories: arXiv_CV
tags: arXiv_CV QA Attention Deep_Learning
author: Yu-Siang Wang, Hung-Ting Su, Chen-Hsi Chang, Winston Hsu
mathjax: true
---

* content
{:toc}

##### Abstract
Video Question Answering (Video QA) is a critical and challenging task in multimedia comprehension. While deep learning based models are extremely capable of representing and understanding videos, these models heavily rely on massive data, which is expensive to label. In this paper, we introduce a novel task for automatically generating questions given a sequence of video frames and the corresponding subtitles from a clip of video to reduce the huge annotation cost. Learning to ask a question based on a video requires the model to comprehend the rich semantics in the scene and the interplay between the vision and the language. To address this, we propose a novel cross-modal self-attention (CMSA) network to aggregate the diverse features from video frames and subtitles. Excitingly, we demonstrate that our proposed model can improve the (strong) baseline from 0.0738 to 0.1374 in BLEU4 score -- more than 0.063 improvement (i.e., 85\% relatively). Most of all, We arguably pave a novel path toward solving the challenging Video QA task and provide detailed analysis which ushers the avenues for future investigations.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.03049](http://arxiv.org/abs/1907.03049)

##### PDF
[http://arxiv.org/pdf/1907.03049](http://arxiv.org/pdf/1907.03049)

