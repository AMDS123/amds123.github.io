---
layout: post
title: "Recurrent knowledge distillation"
date: 2018-05-18 12:32:16
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: Silvia L. Pintea, Yue Liu, Jan C. van Gemert
mathjax: true
---

* content
{:toc}

##### Abstract
Knowledge distillation compacts deep networks by letting a small student network learn from a large teacher network. The accuracy of knowledge distillation recently benefited from adding residual layers. We propose to reduce the size of the student network even further by recasting multiple residual layers in the teacher network into a single recurrent student layer. We propose three variants of adding recurrent connections into the student network, and show experimentally on CIFAR-10, Scenes and MiniPlaces, that we can reduce the number of parameters at little loss in accuracy.

##### Abstract (translated by Google)
通过让小型学生网络从大型教师网络中学习，知识蒸馏可以压缩深度网络。知识蒸馏的准确性最近受益于添加残留层。我们建议通过将教师网络中的多个残留层重新划分为单个经常性学生层来进一步缩小学生网络的规模。我们提出了三种将经常性连接添加到学生网络中的变体，并在CIFAR-10，场景和MiniPlaces上进行实验性演示，以便我们可以在精度损失很少的情况下减少参数数量。

##### URL
[http://arxiv.org/abs/1805.07170](http://arxiv.org/abs/1805.07170)

##### PDF
[http://arxiv.org/pdf/1805.07170](http://arxiv.org/pdf/1805.07170)

