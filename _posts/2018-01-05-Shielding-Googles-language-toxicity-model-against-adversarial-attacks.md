---
layout: post
title: "Shielding Google's language toxicity model against adversarial attacks"
date: 2018-01-05 16:45:59
categories: arXiv_CL
tags: arXiv_CL Adversarial Detection Recognition
author: Nestor Rodriguez, Sergio Rojas-Galeano
mathjax: true
---

* content
{:toc}

##### Abstract
Lack of moderation in online communities enables participants to incur in personal aggression, harassment or cyberbullying, issues that have been accentuated by extremist radicalisation in the contemporary post-truth politics scenario. This kind of hostility is usually expressed by means of toxic language, profanity or abusive statements. Recently Google has developed a machine-learning-based toxicity model in an attempt to assess the hostility of a comment; unfortunately, it has been suggested that said model can be deceived by adversarial attacks that manipulate the text sequence of the comment. In this paper we firstly characterise such adversarial attacks as using obfuscation and polarity transformations. The former deceives by corrupting toxic trigger content with typographic edits, whereas the latter deceives by grammatical negation of the toxic content. Then, we propose a two--stage approach to counter--attack these anomalies, bulding upon a recently proposed text deobfuscation method and the toxicity scoring model. Lastly, we conducted an experiment with approximately 24000 distorted comments, showing how in this way it is feasible to restore toxicity of the adversarial variants, while incurring roughly on a twofold increase in processing time. Even though novel adversary challenges would keep coming up derived from the versatile nature of written language, we anticipate that techniques combining machine learning and text pattern recognition methods, each one targeting different layers of linguistic features, would be needed to achieve robust detection of toxic language, thus fostering aggression--free digital interaction.

##### Abstract (translated by Google)
网络社区缺乏适度的参与，使得参与者在当代后真理政治情景中遭受到极端主义激进化所强化的个人侵略，骚扰或网络欺凌。这种敌意通常通过有毒的语言，亵渎或辱骂的言论来表达。最近，Google开发了一种基于机器学习的毒性模型，试图评估评论的敌意;不幸的是，有人提出，可以通过操纵评论的文本序列的对抗性攻击来欺骗所述模型。在本文中，我们首先将这种敌对攻击描述为使用混淆和极性转换。前者通过印刷编辑破坏有毒触发内容来欺骗，而后者则通过语法上否定有毒内容而蒙骗。然后，我们提出了一个两阶段的方法来反击这些异常，建立在最近提出的文本去混淆方法和毒性评分模型。最后，我们进行了大约24000个扭曲的评论的实验，显示如何以这种方式恢复敌对变体的毒性，同时使处理时间大致增加两倍。尽管来自书面语言的多样化本质的新的对手挑战将不断涌现，但我们预计将需要结合机器学习和文本模式识别方法的技术，每种技术都针对不同层次的语言特征，以实现有毒语言的强大检测从而促进无侵略的数字互动。

##### URL
[http://arxiv.org/abs/1801.01828](http://arxiv.org/abs/1801.01828)

##### PDF
[http://arxiv.org/pdf/1801.01828](http://arxiv.org/pdf/1801.01828)

