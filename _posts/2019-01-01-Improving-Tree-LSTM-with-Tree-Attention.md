---
layout: post
title: "Improving Tree-LSTM with Tree Attention"
date: 2019-01-01 00:15:45
categories: arXiv_CL
tags: arXiv_CL Attention RNN
author: Mahtab Ahmed, Muhammad Rifayat Samee, Robert E. Mercer
mathjax: true
---

* content
{:toc}

##### Abstract
In Natural Language Processing (NLP), we often need to extract information from tree topology. Sentence structure can be represented via a dependency tree or a constituency tree structure. For this reason, a variant of LSTMs, named Tree-LSTM, was proposed to work on tree topology. In this paper, we design a generalized attention framework for both dependency and constituency trees by encoding variants of decomposable attention inside a Tree-LSTM cell. We evaluated our models on a semantic relatedness task and achieved notable results compared to Tree-LSTM based methods with no attention as well as other neural and non-neural methods and good results compared to Tree-LSTM based methods with attention.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1901.00066](https://arxiv.org/abs/1901.00066)

##### PDF
[https://arxiv.org/pdf/1901.00066](https://arxiv.org/pdf/1901.00066)

