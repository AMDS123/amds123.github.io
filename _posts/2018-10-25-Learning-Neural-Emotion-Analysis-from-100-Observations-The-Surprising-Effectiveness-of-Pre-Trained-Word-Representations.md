---
layout: post
title: "Learning Neural Emotion Analysis from 100 Observations: The Surprising Effectiveness of Pre-Trained Word Representations"
date: 2018-10-25 16:08:18
categories: arXiv_CL
tags: arXiv_CL Embedding Deep_Learning
author: Sven Buechel, Jo√£o Sedoc, H. Andrew Schwartz, Lyle Ungar
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Learning has drastically reshaped virtually all areas of NLP. Yet on the downside, it is commonly thought to be dependent on vast amounts of training data. As such, these techniques appear ill-suited for areas where annotated data is limited, like emotion analysis, with its many nuanced and hard-to-acquire annotation formats, or other low-data scenarios encountered in under-resourced languages. In contrast to this popular notion, we provide empirical evidence from three typologically diverse languages that today's favorite neural architectures can be trained on a few hundred observations only. Our results suggest that high-quality, pre-trained word embeddings are crucial for achieving high performance despite such strong data limitations.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1810.10949](https://arxiv.org/abs/1810.10949)

##### PDF
[https://arxiv.org/pdf/1810.10949](https://arxiv.org/pdf/1810.10949)

