---
layout: post
title: "Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning"
date: 2018-05-31 19:48:00
categories: arXiv_AI
tags: arXiv_AI Survey
author: Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, Lalana Kagal
mathjax: true
---

* content
{:toc}

##### Abstract
There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.

##### Abstract (translated by Google)
最近在解释性人工智能（XAI）方面出现了大量的工作。这个研究领域解决了复杂的机器和算法往往不能提供洞察他们的行为和思维过程的重要问题。 XAI允许内部系统的用户和部分更加透明，从某种程度上详细解释他们的决策。这些解释对于确保算法的公平性，确定训练数据中潜在的偏见/问题以及确保算法按预期执行很重要。但是，这些系统产生的解释既没有标准化，也没有系统评估。为了创造最佳实践并确定未来挑战，我们提供了解释性的定义，并说明如何将其用于对现有文献进行分类。我们讨论为什么目前的解释方法特别是深度神经网络方法不足。最后，根据我们的调查，我们最后提出了解释性人工智能的未来研究方向。

##### URL
[http://arxiv.org/abs/1806.00069](http://arxiv.org/abs/1806.00069)

##### PDF
[http://arxiv.org/pdf/1806.00069](http://arxiv.org/pdf/1806.00069)

