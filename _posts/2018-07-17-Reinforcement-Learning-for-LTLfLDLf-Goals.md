---
layout: post
title: "Reinforcement Learning for LTLf/LDLf Goals"
date: 2018-07-17 10:51:04
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Giuseppe De Giacomo, Luca Iocchi, Marco Favorito, Fabio Patrizi
mathjax: true
---

* content
{:toc}

##### Abstract
MDPs extended with LTLf/LDLf non-Markovian rewards have recently attracted interest as a way to specify rewards declaratively. In this paper, we discuss how a reinforcement learning agent can learn policies fulfilling LTLf/LDLf goals. In particular we focus on the case where we have two separate representations of the world: one for the agent, using the (predefined, possibly low-level) features available to it, and one for the goal, expressed in terms of high-level (human-understandable) fluents. We formally define the problem and show how it can be solved. Moreover, we provide experimental evidence that keeping the RL agent feature space separated from the goal's can work in practice, showing interesting cases where the agent can indeed learn a policy that fulfills the LTLf/LDLf goal using only its features (augmented with additional memory).

##### Abstract (translated by Google)
利用LTLf / LDLf非马尔可夫奖励扩展的MDP最近引起了人们的兴趣，作为一种以声明方式指定奖励的方式。在本文中，我们将讨论强化学习代理如何学习实现LTLf / LDLf目标的策略。特别是我们关注的情况是，我们有两个独立的世界表示：一个用于代理，使用（预定义的，可能是低级别）可用的功能，一个用于目标，用高级表示（人类可以理解的）流水。我们正式定义问题并展示如何解决问题。此外，我们提供了实验证据，保持RL代理特征空间与目标分离可以在实践中工作，显示有趣的情况，代理确实可以学习仅使用其功能（增加额外内存）来实现LTLf / LDLf目标的策略。

##### URL
[http://arxiv.org/abs/1807.06333](http://arxiv.org/abs/1807.06333)

##### PDF
[http://arxiv.org/pdf/1807.06333](http://arxiv.org/pdf/1807.06333)

