---
layout: post
title: "Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks"
date: 2018-02-25 21:52:37
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: R. Thomas McCoy, Tal Linzen, Robert Frank
mathjax: true
---

* content
{:toc}

##### Abstract
Syntactic rules in human language usually refer to the hierarchical structure of sentences. However, the input during language acquisition can often be explained equally well with rules based on linear order. The fact that children consistently ignore these linear explanations to instead settle on hierarchical explanations has been used to argue for an innate hierarchical bias in humans. We revisit this argument by using recurrent neural networks (RNNs), which have no hierarchical bias, to simulate the acquisition of question formation, a hierarchical transformation, in an artificial language modeled after English. Even though this transformation could be explained with a linear rule, we find that some RNN architectures consistently learn the correct hierarchical rule instead. This finding suggests that hierarchical cues within the language are sufficient to induce a preference for hierarchical generalization. This conclusion is strengthened by the finding that adding an additional hierarchical cue, namely syntactic agreement, further improves performance.

##### Abstract (translated by Google)
人类语言中的句法规则通常指的是句子的层次结构。然而，语言习得过程中的输入通常可以用基于线性顺序的规则来解释。儿童一直忽视这些线性解释而不是解决层次解释的事实已经被用来争论人类内在的先天分层偏见。我们通过使用没有等级偏差的递归神经网络（RNN）来重新审视这个论点，以模仿在英语之后建模的人工语言中的问题形成（分层转换）的获得。尽管这种转换可以用线性规则来解释，但我们发现一些RNN体系结构一贯学习正确的分层规则。这一发现表明，语言中的分层线索足以引起对分层泛化的偏好。这一结论得到加强，发现增加一个额外的层次提示，即语法协议，可以进一步提高性能。

##### URL
[http://arxiv.org/abs/1802.09091](http://arxiv.org/abs/1802.09091)

##### PDF
[http://arxiv.org/pdf/1802.09091](http://arxiv.org/pdf/1802.09091)

