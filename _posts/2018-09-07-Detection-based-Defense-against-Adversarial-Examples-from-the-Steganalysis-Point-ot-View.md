---
layout: post
title: "Detection based Defense against Adversarial Examples from the Steganalysis Point ot View"
date: 2018-09-07 01:37:46
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Detection
author: Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Hongyue Zha, Nenghai Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks (DNNs) have recently led to significant improvements in many fields. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Moreover, adversarial examples can be used to perform an attack on various kinds of DNN based systems, even if the adversary has no access to the underlying model. Many defense methods have been proposed, such as obfuscating gradients of the networks or detecting adversarial examples. However it is proved out that these defense methods are not effective or cannot resist secondary adversarial attacks. In this paper, we point out that steganalysis can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modifications caused by adversarial attacks. Experimental results show that the proposed method can accurately detect adversarial examples. Moreover, secondary adversarial attacks cannot be directly performed to our method because our method is not based on a neural network but based on high-dimensional artificial features and FLD (Fisher Linear Discriminant) ensemble.

##### Abstract (translated by Google)
深度神经网络（DNN）最近在许多领域取得了重大进展。然而，DNN容易受到对抗性示例的影响，这些示例是具有难以察觉的扰动的样本，同时极大地误导了DNN。此外，即使对手无法访问底层模型，也可以使用对抗性示例对各种基于DNN的系统执行攻击。已经提出了许多防御方法，例如混淆网络的梯度或检测对抗性示例。然而事实证明，这些防御方法无效或无法抵抗次要对抗性攻击。在本文中，我们指出隐写分析可以应用于对抗性实例检测，并提出一种通过估计由对抗性攻击引起的修改概率来增强隐写分析特征的方法。实验结果表明，该方法能够准确地检测出敌对实例。此外，二次对抗性攻击不能直接对我们的方法进行，因为我们的方法不是基于神经网络，而是基于高维人工特征和FLD（Fisher线性判别）集合。

##### URL
[http://arxiv.org/abs/1806.09186](http://arxiv.org/abs/1806.09186)

##### PDF
[http://arxiv.org/pdf/1806.09186](http://arxiv.org/pdf/1806.09186)

