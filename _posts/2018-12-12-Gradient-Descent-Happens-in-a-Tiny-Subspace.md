---
layout: post
title: "Gradient Descent Happens in a Tiny Subspace"
date: 2018-12-12 00:36:17
categories: arXiv_AI
tags: arXiv_AI Optimization Classification Deep_Learning Gradient_Descent
author: Guy Gur-Ari, Daniel A. Roberts, Ethan Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.04754](http://arxiv.org/abs/1812.04754)

##### PDF
[http://arxiv.org/pdf/1812.04754](http://arxiv.org/pdf/1812.04754)

