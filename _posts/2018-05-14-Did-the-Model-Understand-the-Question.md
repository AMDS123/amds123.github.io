---
layout: post
title: "Did the Model Understand the Question?"
date: 2018-05-14 23:10:28
categories: arXiv_AI
tags: arXiv_AI Adversarial Face Deep_Learning VQA
author: Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, Kedar Dhamdhere
mathjax: true
---

* content
{:toc}

##### Abstract
We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of \emph{attribution} (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from $61.1\%$ to $19\%$, and that of a tabular question answering model from $33.5\%$ to $3.3\%$. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.

##### Abstract (translated by Google)
我们分析了三个任务的最新深度学习模型：对（1）图像，（2）表格和（3）文本段落的问题回答。使用\ emph {attribution}（单词重要性）这个概念，我们发现这些深度网络经常会忽略重要的问题术语。利用这样的行为，我们扰乱问题来制作各种对抗性的例子。我们最强的攻击将视觉问答模型的准确性从61.1 \％$降到19美元\％$，并将表格问答模型的准确性从$ 33.5 \％$降到$ 3.3 \％$。此外，我们还展示了归因如何加强贾和梁（2017）对段落理解模型的攻击。我们的研究结果表明，归因可以增加准确度的标准度量并赋予模型性能的调查权。当模型准确但出于错误原因时，归因可能会在模型中显示错误的逻辑，表明测试数据中存在不足。

##### URL
[http://arxiv.org/abs/1805.05492](http://arxiv.org/abs/1805.05492)

##### PDF
[http://arxiv.org/pdf/1805.05492](http://arxiv.org/pdf/1805.05492)

