---
layout: post
title: "Variational Neural Networks: Every Layer and Neuron Can Be Unique"
date: 2018-10-14 22:41:11
categories: arXiv_AI
tags: arXiv_AI Gradient_Descent
author: Yiwei Li, Enzhi Li
mathjax: true
---

* content
{:toc}

##### Abstract
The choice of activation function can significantly influence the performance of neural networks. The lack of guiding principles for the selection of activation function is lamentable. We try to address this issue by introducing our variational neural networks, where the activation function is represented as a linear combination of possible candidate functions, and an optimal activation is obtained via minimization of a loss function using gradient descent method. The gradient formulae for the loss function with respect to these expansion coefficients are central for the implementation of gradient descent algorithm, and here we derive these gradient formulae.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1810.06120](https://arxiv.org/abs/1810.06120)

##### PDF
[https://arxiv.org/pdf/1810.06120](https://arxiv.org/pdf/1810.06120)

