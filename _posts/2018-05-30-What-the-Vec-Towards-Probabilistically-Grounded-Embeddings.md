---
layout: post
title: "What the Vec? Towards Probabilistically Grounded Embeddings"
date: 2018-05-30 18:19:38
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model
author: Carl Allen, Ivana Bala&#x17e;evi&#x107;, Timothy Hospedales
mathjax: true
---

* content
{:toc}

##### Abstract
Vector representation, or embedding, of words is commonly achieved with neural network methods, in particular word2vec (W2V). It has been shown that certain statistics of word co-occurrences are implicitly captured by properties of W2V vectors, but much remains unknown of them, e.g. any meaning of length, or more generally how it is that statistics can be reliably framed as vectors at all. By deriving a mathematical link between probabilities and vectors, we justify why W2V works and are able to create embeddings with probabilistically interpretable properties.

##### Abstract (translated by Google)
词的向量表示或嵌入通常用神经网络方法来实现，特别是word2vec（W2V）。已经表明，词汇共现的某些统计量被W2V向量的属性隐含地捕获，但是它们的许多仍然是未知的，例如，任何长度的含义，或更一般地说，统计数据如何可靠地构建为矢量。通过推导概率和向量之间的数学关系，我们证明为什么W2V能够工作并能够创建具有概率可解释性质的嵌入。

##### URL
[http://arxiv.org/abs/1805.12164](http://arxiv.org/abs/1805.12164)

##### PDF
[http://arxiv.org/pdf/1805.12164](http://arxiv.org/pdf/1805.12164)

