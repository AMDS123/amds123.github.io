---
layout: post
title: "Emergence of Selective Invariance in Hierarchical Feed Forward Networks"
date: 2017-01-30 21:44:27
categories: arXiv_CV
tags: arXiv_CV Attention Recognition
author: Dipan K. Pal, Vishnu Boddeti, Marios Savvides
mathjax: true
---

* content
{:toc}

##### Abstract
Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling. The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. We con- jecture that hierarchically building selective invariance (i.e. carefully choosing the range of the transformation to be in- variant to at each layer of a hierarchical network) is im- portant for pattern recognition. We utilize a novel pooling layer called adaptive pooling to find linear pooling weights within networks. These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks. In- terestingly, adaptive pooling can converge to mean pooling (when initialized with random pooling weights), find more general linear pooling schemes or even decide not to pool at all. We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012.

##### Abstract (translated by Google)
已经出现了许多理论来研究如何通过简单的方案，如最大和平均汇集，在分层网络中产生方差。在理论和实证研究中对最大/平均汇集的限制已经将注意力从更一般的产生不变性的方法转移到滋扰转变。我们认为，分层构建选择性不变性（即谨慎地选择不同层次的变换范围到分层网络的每一层）对于模式识别是非常重要的。我们利用一种称为自适应池的新型池化层来查找网络中的线性池化权重。具有学习池化权重的这些网络在与最大/平均池化网络相当的对象分类任务上具有性能。有趣的是，适应性汇集可以收敛到意味着汇集（当用随机汇集权重初始化时），找到更一般的线性汇集方案，甚至决定不汇集。我们通过SVHN和ILSVRC 2012等大规模数据集上的对象分类实验来说明选择性不变性的一般概念。

##### URL
[https://arxiv.org/abs/1701.08837](https://arxiv.org/abs/1701.08837)

##### PDF
[https://arxiv.org/pdf/1701.08837](https://arxiv.org/pdf/1701.08837)

