---
layout: post
title: "Inference, Learning and Attention Mechanisms that Exploit and Preserve Sparsity in Convolutional Networks"
date: 2018-01-31 18:12:24
categories: arXiv_CV
tags: arXiv_CV Sparse Attention CNN Inference
author: Timo Hackel, Mikhail Usvyatsov, Silvano Galliani, Jan D. Wegner, Konrad Schindler
mathjax: true
---

* content
{:toc}

##### Abstract
While CNNs naturally lend themselves to densely sampled data, and sophisticated implementations are available, they lack the ability to efficiently process sparse data. In this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of the back-propagation algorithm, which makes it possible to combine our approach with standard learning frameworks, while still exploiting sparsity in the data and the model.

##### Abstract (translated by Google)
虽然有线电视网络自然适合采集密集采样的数据，并且有复杂的实施方法，但缺乏有效处理稀疏数据的能力。在这项工作中，我们介绍了一套利用特征映射和过滤器权重的稀疏性的工具，从而在处理高度稀疏的数据时，比传统的密集框架能够显着减少内存占用和计算时间。我们的方案提供（i）基于直接，稀疏卷积的卷积层的高效GPU实现; （ii）卷积层内的滤波步骤，我们称之为注意，防止填充，即卷积趋势快速降低稀疏性，并保证计算资源的上限; （iii）反向传播算法的改进，使得我们可以将我们的方法与标准的学习框架结合起来，同时仍然利用数据和模型的稀疏性。

##### URL
[http://arxiv.org/abs/1801.10585](http://arxiv.org/abs/1801.10585)

##### PDF
[http://arxiv.org/pdf/1801.10585](http://arxiv.org/pdf/1801.10585)

