---
layout: post
title: "Mixed Membership Word Embeddings for Computational Social Science"
date: 2018-02-20 00:34:49
categories: arXiv_AI
tags: arXiv_AI Embedding Language_Model Relation
author: James Foulds
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. Despite their success in many applications, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. I show how to train the model using a combination of state-of-the-art training techniques for word embeddings and topic models. The experimental results show an improvement in predictive language modeling of up to 63% in MRR over the skip-gram, and demonstrate that the representations are beneficial for supervised learning. I illustrate the interpretability of the models with computational social science case studies on State of the Union addresses and NIPS articles.

##### Abstract (translated by Google)
词嵌入通过揭示词之间隐藏的结构关系来改善NLP系统的性能。尽管他们在许多应用程序中取得了成功，但字嵌入在计算社会科学NLP任务中几乎没有用处，可能是由于它们依赖大数据，并且缺乏可解释性。我提出了一种基于概率模型的词嵌入方法，它可以恢复可解释的嵌入，而不需要大数据。关键的洞察力是利用混合成员资格建模，其中全局表示是共享的，但是单个实体（即字典单词）可以自由地使用这些表示以独特的不同程度。我展示了如何使用针对词嵌入和主题模型的最先进训练技术的组合来训练模型。实验结果显示，预测语言模型的改进在MRR上可达63％，并证明这些表示对于监督式学习是有益的。我用关于国情咨文和NIPS文章的计算社会科学案例研究来说明模型的可解释性。

##### URL
[http://arxiv.org/abs/1705.07368](http://arxiv.org/abs/1705.07368)

##### PDF
[http://arxiv.org/pdf/1705.07368](http://arxiv.org/pdf/1705.07368)

