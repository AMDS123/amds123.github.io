---
layout: post
title: "Soft-Robust Actor-Critic Policy-Gradient"
date: 2018-03-11 09:43:20
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Esther Derman, Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor
mathjax: true
---

* content
{:toc}

##### Abstract
Robust Reinforcement Learning aims to derive an optimal behavior that accounts for model uncertainty in dynamical systems. However, previous studies have shown that by considering the worst case scenario, robust policies can be overly conservative. Our \textit{soft-robust} framework is an attempt to overcome this issue. In this paper, we present a novel Soft-Robust Actor-Critic algorithm (SR-AC). It learns an optimal policy with respect to a distribution over an uncertainty set and stays robust to model uncertainty but avoids the conservativeness of robust strategies. We show convergence of the SR-AC and test the efficiency of our approach on different domains by comparing it against regular learning methods and their robust formulations.

##### Abstract (translated by Google)
稳健的强化学习旨在推导出最优的行为，解释动态系统中的模型不确定性。然而，之前的研究表明，通过考虑最坏的情况，强健的政策可能过于保守。我们的\ textit {soft-robust}框架试图解决这个问题。在本文中，我们提出了一种新颖的软健壮演员 - 评论者算法（SR-AC）。它学习了一个关于不确定性集合的分布的最优策略，并且保持模型不确定性的稳健性，但是避免了鲁棒策略的保守性。我们展示了SR-AC的收敛性，并通过将其与常规学习方法及其稳健公式进行比较，来测试我们在不同领域的方法的效率。

##### URL
[http://arxiv.org/abs/1803.04848](http://arxiv.org/abs/1803.04848)

##### PDF
[http://arxiv.org/pdf/1803.04848](http://arxiv.org/pdf/1803.04848)

