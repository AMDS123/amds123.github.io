---
layout: post
title: "MEx: Multi-modal Exercises Dataset for Human Activity Recognition"
date: 2019-08-13 16:09:53
categories: arXiv_AI
tags: arXiv_AI Classification Recognition
author: Anjana Wijekoon, Nirmalie Wiratunga, Kay Cooper
mathjax: true
---

* content
{:toc}

##### Abstract
MEx: Multi-modal Exercises Dataset is a multi-sensor, multi-modal dataset, implemented to benchmark Human Activity Recognition(HAR) and Multi-modal Fusion algorithms. Collection of this dataset was inspired by the need for recognising and evaluating quality of exercise performance to support patients with Musculoskeletal Disorders(MSD). We select 7 exercises regularly recommended for MSD patients by physiotherapists and collected data with four sensors a pressure mat, a depth camera and two accelerometers. The dataset contains three data modalities; numerical time-series data, video data and pressure sensor data posing interesting research challenges when reasoning for HAR and Exercise Quality Assessment. This paper presents our evaluation of the dataset on number of standard classification algorithms for the HAR task by comparing different feature representation algorithms for each sensor. These results set a reference performance for each individual sensor that expose their strengths and weaknesses for the future tasks. In addition we visualise pressure mat data to explore the potential of the sensor to capture exercise performance quality. With the recent advancement in multi-modal fusion, we also believe MEx is a suitable dataset to benchmark not only HAR algorithms, but also fusion algorithms of heterogeneous data types in multiple application domains.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.08992](http://arxiv.org/abs/1908.08992)

##### PDF
[http://arxiv.org/pdf/1908.08992](http://arxiv.org/pdf/1908.08992)

