---
layout: post
title: "Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks"
date: 2018-01-23 09:57:10
categories: arXiv_CV
tags: arXiv_CV Sparse CNN
author: Yuechao Gao, Nianhong Liu, Sheng Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks (CNNs), in this paper we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), and also a three dimensional Single Instruction Multiple Data (3D-SIMD) processor architecture to take full advantage of these two features. Comparing with the state-of-the-art result (Han et al., 2016b), our method achieved 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using this approach, chip area for logics handling irregular sparse data access can be saved.

##### Abstract (translated by Google)
为了解决深度卷积神经网络（CNN）面向硬件加速的内存和计算资源限制，本文提出了一种计算流，堆栈滤波器静态流（SFS）和相应的数据编码格式，相对索引压缩稀疏滤波格式（CSF）以及三维单指令多数据（3D-SIMD）处理器架构来充分利用这两个特征。与最先进的结果（Han等，2016b）相比，我们的方法在减少AlexNet所需的存储方面获得了1.11倍的提高，在减少了SqueezeNet所需的存储方面，提高了1.09倍，同时又不损失精度在ImageNet数据集上。而且，采用这种方法，可以节省处理不规则稀疏数据访问的逻辑芯片面积。

##### URL
[http://arxiv.org/abs/1801.07459](http://arxiv.org/abs/1801.07459)

##### PDF
[http://arxiv.org/pdf/1801.07459](http://arxiv.org/pdf/1801.07459)

