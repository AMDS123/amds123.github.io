---
layout: post
title: "Deep Micro-Dictionary Learning and Coding Network"
date: 2018-09-11 22:36:36
categories: arXiv_CV
tags: arXiv_CV CNN Deep_Learning
author: Hao Tang, Heng Wei, Wei Xiao, Wei Wang, Dan Xu, Yan Yan, Nicu Sebe
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a novel Deep Micro-Dictionary Learning and Coding Network (DDLCN). DDLCN has most of the standard deep learning layers (pooling, fully, connected, input/output, etc.) but the main difference is that the fundamental convolutional layers are replaced by novel compound dictionary learning and coding layers. The dictionary learning layer learns an over-complete dictionary for the input training data. At the deep coding layer, a locality constraint is added to guarantee that the activated dictionary bases are close to each other. Next, the activated dictionary atoms are assembled together and passed to the next compound dictionary learning and coding layers. In this way, the activated atoms in the first layer can be represented by the deeper atoms in the second dictionary. Intuitively, the second dictionary is designed to learn the fine-grained components which are shared among the input dictionary atoms. In this way, a more informative and discriminative low-level representation of the dictionary atoms can be obtained. We empirically compare the proposed DDLCN with several dictionary learning methods and deep learning architectures. The experimental results on four popular benchmark datasets demonstrate that the proposed DDLCN achieves competitive results compared with state-of-the-art approaches.

##### Abstract (translated by Google)
在本文中，我们提出了一种新颖的深度微词典学习和编码网络（DDLCN）。 DDLCN具有大多数标准深度学习层（汇集，完全，连接，输入/输出等），但主要区别在于基本卷积层被新的复合字典学习和编码层所取代。字典学习层学习输入训练数据的过完整字典。在深度编码层，添加局部性约束以保证激活的字典基础彼此接近。接下来，将激活的字典原子组装在一起并传递给下一个复合字典学习和编码层。以这种方式，第一层中的活化原子可以由第二字典中的较深原子表示。直观地说，第二个字典旨在学习在输入字典原子之间共享的细粒度组件。以这种方式，可以获得字典原子的更具信息性和辨别力的低级表示。我们根据经验将提出的DDLCN与几种字典学习方法和深度学习架构进行比较。四个流行的基准数据集的实验结果表明，与最先进的方法相比，所提出的DDLCN实现了有竞争力的结果。

##### URL
[http://arxiv.org/abs/1809.04185](http://arxiv.org/abs/1809.04185)

##### PDF
[http://arxiv.org/pdf/1809.04185](http://arxiv.org/pdf/1809.04185)

