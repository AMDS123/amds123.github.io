---
layout: post
title: "Randomness in Deconvolutional Networks for Visual Representation"
date: 2018-02-19 09:04:21
categories: arXiv_CV
tags: arXiv_CV CNN Classification
author: Kun He, Jingbo Wang, Haochuan Li, Yao Shu, Mengxiao Zhang, Man Zhu, Liwei Wang, John E. Hopcroft
mathjax: true
---

* content
{:toc}

##### Abstract
Toward a deeper understanding on the inner work of deep neural networks, we investigate CNN (convolutional neural network) using DCN (deconvolutional network) and randomization technique, and gain new insights for the intrinsic property of this network architecture. For the random representations of an untrained CNN, we train the corresponding DCN to reconstruct the input images. Compared with the image inversion on pre-trained CNN, our training converges faster and the yielding network exhibits higher quality for image reconstruction. It indicates there is rich information encoded in the random features; the pre-trained CNN may discard information irrelevant for classification and encode relevant features in a way favorable for classification but harder for reconstruction. We further explore the property of the overall random CNN-DCN architecture. Surprisingly, images can be inverted with satisfactory quality. Extensive empirical evidence as well as theoretical analysis are provided.

##### Abstract (translated by Google)
为了深入理解深层神经网络的内在工作，我们使用DCN（去卷积网络）和随机化技术研究了CNN（卷积神经网络），并对这种网络结构的内在属性有了新的认识。对于未经训练的CNN的随机表示，我们训练相应的DCN以重建输入图像。与预先训练的CNN图像反演相比，我们的训练收敛速度更快，并且屈服网络对于图像重建具有更高的质量。它表示在随机特征中编码了丰富的信息;预先训练的CNN可丢弃与分类无关的信息并以有利于分类但更难重建的方式编码相关特征。我们进一步探索整体随机CNN-DCN架构的特性。令人惊讶的是，图像可以以令人满意的质量倒置。提供了广泛的经验证据和理论分析。

##### URL
[http://arxiv.org/abs/1704.00330](http://arxiv.org/abs/1704.00330)

##### PDF
[http://arxiv.org/pdf/1704.00330](http://arxiv.org/pdf/1704.00330)

