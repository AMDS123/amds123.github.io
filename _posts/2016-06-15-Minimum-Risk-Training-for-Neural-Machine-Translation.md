---
layout: post
title: "Minimum Risk Training for Neural Machine Translation"
date: 2016-06-15 00:07:05
categories: arXiv_CL
tags: arXiv_CL
author: Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu
mathjax: true
---

* content
{:toc}

##### Abstract
We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.

##### Abstract (translated by Google)
我们提出端到端神经机器翻译的最低风险培训。与传统的最大似然估计不同，最小风险训练能够直接针对任意评估度量优化模型参数，这些评估度量不一定是可区分的。实验表明，我们的方法在各种语言对的最先进的神经机器翻译系统的最大似然估计上实现了显着的改进。对架构透明，我们的方法可以应用于更多的神经网络，并有可能获益更多的NLP任务。

##### URL
[https://arxiv.org/abs/1512.02433](https://arxiv.org/abs/1512.02433)

##### PDF
[https://arxiv.org/pdf/1512.02433](https://arxiv.org/pdf/1512.02433)

