---
layout: post
title: "Unsupervised Cross-Domain Word Representation Learning"
date: 2015-05-27 04:02:56
categories: arXiv_CL
tags: arXiv_CL Sentiment Sentiment_Classification Represenation_Learning Classification
author: Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi
mathjax: true
---

* content
{:toc}

##### Abstract
Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of \emph{source}-\emph{target} domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domain-specific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as \emph{pivots}. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-the-art domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset.

##### Abstract (translated by Google)
一个词的含义因领域而异。尽管这种重要的领域依赖在词语义，现有的词表示学习方法是绑定到一个单一的领域。给定一对\ emph {源}  -  \ emph {目标}域，我们提出一种无监督的方法来学习领域特定的词表示，准确地捕捉词语义领域特定的方面。首先，我们选择在两个域中出现的频繁词的子集作为\ emph {pivot}。接下来，我们优化一个强制执行两个约束的目标函数：（a）对于源域和目标域文档，出现在文档中的枢轴必须精确地预测同时发生的非枢轴;（b）针对枢轴学习的文字表示必须在两个领域是相似的。此外，我们提出了一种方法来执行领域适应使用学习词表示。我们提出的方法明显优于包括最先进的领域不敏感词表示在内的竞争基线，并报告基准数据集中所有域对的最佳情感分类精度。

##### URL
[https://arxiv.org/abs/1505.07184](https://arxiv.org/abs/1505.07184)

##### PDF
[https://arxiv.org/pdf/1505.07184](https://arxiv.org/pdf/1505.07184)

