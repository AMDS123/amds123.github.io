---
layout: post
title: "Understanding the Loss Surface of Neural Networks for Binary Classification"
date: 2018-02-19 02:13:38
categories: arXiv_AI
tags: arXiv_AI Face Classification
author: Shiyu Liang, Ruoyu Sun, Yixuan Li, R. Srikant
mathjax: true
---

* content
{:toc}

##### Abstract
It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance, for example, see (LeCun et al., 2015, Choromanska et al., 2015, Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.

##### Abstract (translated by Google)
人们普遍推测，神经网络的训练算法是成功的，因为所有局部极小值导致相似的性能，例如参见（LeCun等，2015，Choromanska等，2015，Dauphin等，2014） 。绩效通常以两个指标来衡量：培训绩效和泛化绩效。在这里，我们重点关注二元分类的单层神经网络的训练性能，并提供在平滑铰链损失函数的所有局部最小值处训练误差为零的条件。我们的条件大致如下：神经元必须严格凸起，替代损失函数应该是铰链损失的平滑版本。我们还提供了反例来说明当损失函数被二次损失或逻辑损失所替代时，结果可能不成立。

##### URL
[http://arxiv.org/abs/1803.00909](http://arxiv.org/abs/1803.00909)

##### PDF
[http://arxiv.org/pdf/1803.00909](http://arxiv.org/pdf/1803.00909)

