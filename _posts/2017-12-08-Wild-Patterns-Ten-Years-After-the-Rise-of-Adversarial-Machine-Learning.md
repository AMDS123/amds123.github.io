---
layout: post
title: "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning"
date: 2017-12-08 15:59:41
categories: arXiv_CV
tags: arXiv_CV Adversarial Deep_Learning Prediction
author: Battista Biggio, Fabio Roli
mathjax: true
---

* content
{:toc}

##### Abstract
Learning-based pattern classifiers, including deep networks, have demonstrated impressive performance in several application domains, ranging from computer vision to computer security. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to adversarial inputs (also known as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this interdisciplinary research area over the last ten years, starting from pioneering, earlier work up to more recent work aimed at understanding the security properties of deep learning algorithms, in the context of different applications. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the evaluation of the security of machine-learning algorithms. We finally discuss the main limitations of current work, along with the corresponding future research challenges towards the design of more secure learning algorithms.

##### Abstract (translated by Google)
包括深度网络在内的基于学习的模式分类器已经在从计算机视觉到计算机安全的多个应用领域中表现出令人印象深刻的性能。然而，也已经表明，在训练或在测试时精心设计的敌对输入扰动可能容易地颠覆他们的预测。在对抗机器学习的研究领域中，已经研究了机器学习对抗性输入（也称为对抗性例子）的脆弱性以及适当对策的设计。在这项工作中，我们提供了这个跨学科研究领域在过去十年的发展的一个彻底的概述，从先驱，早期的工作，到最近的工作，旨在了解深度学习算法的安全属性，在不同的背景下应用。我们报告了这些明显不同的工作线之间有趣的联系，突出了与评估机器学习算法安全性相关的常见误解。我们最后讨论了当前工作的主要局限性，以及在设计更安全的学习算法时相应的未来研究挑战。

##### URL
[http://arxiv.org/abs/1712.03141](http://arxiv.org/abs/1712.03141)

##### PDF
[http://arxiv.org/pdf/1712.03141](http://arxiv.org/pdf/1712.03141)

