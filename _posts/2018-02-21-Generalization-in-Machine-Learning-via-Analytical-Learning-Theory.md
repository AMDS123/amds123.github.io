---
layout: post
title: "Generalization in Machine Learning via Analytical Learning Theory"
date: 2018-02-21 05:03:52
categories: arXiv_AI
tags: arXiv_AI Represenation_Learning Deep_Learning
author: Kenji Kawaguchi, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
This paper introduces a novel measure-theoretic learning theory to analyze generalization behaviors of practical interest. The proposed learning theory has the following abilities: 1) to utilize the qualities of each learned representation on the path from raw inputs to outputs in representation learning, 2) to guarantee good generalization errors possibly with arbitrarily rich hypothesis spaces (e.g., arbitrarily large capacity and Rademacher complexity) and non-stable/non-robust learning algorithms, and 3) to clearly distinguish each individual problem instance from each other. Our generalization bounds are relative to a representation of the data, and hold true even if the representation is learned. We discuss several consequences of our results on deep learning, one-shot learning and curriculum learning. Unlike statistical learning theory, the proposed learning theory analyzes each problem instance individually via measure theory, rather than a set of problem instances via statistics. Because of the differences in the assumptions and the objectives, the proposed learning theory is meant to be complementary to previous learning theory and is not designed to compete with it.

##### Abstract (translated by Google)
本文引入了一种新的测度理论学习理论来分析实际兴趣的泛化行为。所提出的学习理论具有以下能力：1）利用表示学习中从原始输入到输出的路径上每个学习表示的质量，2）保证可能具有任意丰富假设空间的良好泛化误差（例如，任意大容量和Rademacher复杂性）和非稳定/非稳健学习算法，以及3）清楚地将每个单独的问题实例彼此区分开来。我们的泛化边界与数据的表示有关，并且即使学习表示也是成立的。我们讨论了我们在深度学习，一次学习和课程学习方面取得的成果的几个后果。与统计学习理论不同，所提出的学习理论分别通过测量理论分析每个问题实例，而不是通过统计学分析一组问题实例。由于假设和目标的不同，所提出的学习理论是为了补充以前的学习理论，而不是为了与之竞争。

##### URL
[http://arxiv.org/abs/1802.07426](http://arxiv.org/abs/1802.07426)

##### PDF
[http://arxiv.org/pdf/1802.07426](http://arxiv.org/pdf/1802.07426)

