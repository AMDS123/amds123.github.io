---
layout: post
title: "Improved Descriptors for Patch Matching and Reconstruction"
date: 2017-08-27 17:45:22
categories: arXiv_CV
tags: arXiv_CV CNN
author: Rahul Mitra, Jiakai Zhang, Sanath Narayan, Shuaib Ahmed, Sharat Chandran, Arjun Jain
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a convolutional neural network (ConvNet) based approach for learning local image descriptors which can be used for significantly improved patch matching and 3D reconstructions. A multi-resolution ConvNet is used for learning keypoint descriptors. We also propose a new dataset consisting of an order of magnitude more number of scenes, images, and positive and negative correspondences compared to the currently available Multi-View Stereo (MVS) [18] dataset. The new dataset also has better coverage of the overall viewpoint, scale, and lighting changes in comparison to the MVS dataset. We evaluate our approach on publicly available datasets, such as Oxford Affine Covariant Regions Dataset (ACRD) [12], MVS [18], Synthetic [6] and Strecha [15] datasets to quantify the image descriptor performance. Scenes from the Oxford ACRD, MVS and Synthetic datasets are used for evaluating the patch matching performance of the learnt descriptors while the Strecha dataset is used to evaluate the 3D reconstruction task. Experiments show that the proposed descriptor outperforms the current state-of-the-art descriptors in both the evaluation tasks.

##### Abstract (translated by Google)
我们提出了一种基于卷积神经网络（ConvNet）的学习局部图像描述符的方法，可用于显着改进的补丁匹配和三维重建。多分辨率ConvNet用于学习关键点描述符。我们还提出了一个新的数据集，其中包括数量级更多的场景，图像以及与当前可用的多视点立体（MVS）[18]数据集相比的正负对应关系。与MVS数据集相比，新数据集还可以更好地覆盖整个视点，比例和光照变化。我们在公开可用的数据集上评估我们的方法，如牛津仿射协变区域数据集（ACRD）[12]，MVS [18]，合成[6]和Strecha [15]数据集以量化图像描述符的性能。使用牛津ACRD，MVS和合成数据集的场景评估学习描述符的补丁匹配性能，而使用Strecha数据集评估三维重建任务。实验表明，在两个评估任务中，所提出的描述符优于当前的最新描述符。

##### URL
[https://arxiv.org/abs/1701.06854](https://arxiv.org/abs/1701.06854)

##### PDF
[https://arxiv.org/pdf/1701.06854](https://arxiv.org/pdf/1701.06854)

