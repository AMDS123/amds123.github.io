---
layout: post
title: "Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification"
date: 2016-11-16 17:31:44
categories: arXiv_CV
tags: arXiv_CV Classification Deep_Learning Prediction
author: Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-task learning aims to improve generalization performance of multiple prediction tasks by appropriately sharing relevant information across them. In the context of deep neural networks, this idea is often realized by hand-designed network architectures with layers that are shared across tasks and branches that encode task-specific features. However, the space of possible multi-task deep architectures is combinatorially large and often the final architecture is arrived at by manual exploration of this space subject to designer's bias, which can be both error-prone and tedious. In this work, we propose a principled approach for designing compact multi-task deep learning architectures. Our approach starts with a thin network and dynamically widens it in a greedy manner during training using a novel criterion that promotes grouping of similar tasks together. Our Extensive evaluation on person attributes classification tasks involving facial and clothing attributes suggests that the models produced by the proposed method are fast, compact and can closely match or exceed the state-of-the-art accuracy from strong baselines by much more expensive models.

##### Abstract (translated by Google)
多任务学习旨在通过适当地共享相关信息来提高多个预测任务的泛化性能。在深度神经网络的情况下，这个想法通常是通过手工设计的网络架构来实现的，这些网络架构在任务和分支之间共享，这些任务和分支对特定任务的特征进行编码。然而，可能的多任务深层体系结构的空间是相当大的，通常最终的体系结构是通过人为地探索这个空间而受到设计人员的偏见，这可能是容易出错和乏味的。在这项工作中，我们提出了一个原则性的方法来设计紧凑的多任务深度学习体系结构。我们的方法是从一个精简的网络开始，在训练过程中以一种贪婪的方式动态地扩展，使用一种新的标准来促进将相似的任务组合在一起。我们对涉及面部和服装属性的人物属性分类任务进行了广泛的评估，这表明由所提出的方法产生的模型快速，紧凑并且可以通过更昂贵的模型与强基线紧密匹配或超过最新的精度。

##### URL
[https://arxiv.org/abs/1611.05377](https://arxiv.org/abs/1611.05377)

##### PDF
[https://arxiv.org/pdf/1611.05377](https://arxiv.org/pdf/1611.05377)

