---
layout: post
title: "Attending to Characters in Neural Sequence Labeling Models"
date: 2016-11-14 12:36:07
categories: arXiv_CL
tags: arXiv_CL Attention Embedding
author: Marek Rei, Gamal K.O. Crichton, Sampo Pyysalo
mathjax: true
---

* content
{:toc}

##### Abstract
Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.

##### Abstract (translated by Google)
序列标签体系结构使用单词嵌入来捕获相似性，但在处理先前看不见的或罕见的单词时会受到影响。我们调查这些模型的字符级扩展，并提出了一种新颖的结构来结合替代词表示。通过使用注意机制，模型能够动态地决定从单词或字符级组件中使用多少信息。我们对一系列序列标签数据集评估了不同的体系结构，发现字符级扩展可以提高每个基准的性能。此外，即使使用较少的可训练参数，所提出的基于关注的架构也能提供最好的结果。

##### URL
[https://arxiv.org/abs/1611.04361](https://arxiv.org/abs/1611.04361)

##### PDF
[https://arxiv.org/pdf/1611.04361](https://arxiv.org/pdf/1611.04361)

