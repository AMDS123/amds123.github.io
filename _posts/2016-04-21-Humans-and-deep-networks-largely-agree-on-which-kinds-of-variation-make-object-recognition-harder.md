---
layout: post
title: "Humans and deep networks largely agree on which kinds of variation make object recognition harder"
date: 2016-04-21 20:53:00
categories: arXiv_CV
tags: arXiv_CV Attention CNN Recognition
author: Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, Timothée Masquelier
mathjax: true
---

* content
{:toc}

##### Abstract
View-invariant object recognition is a challenging problem, which has attracted much attention among the psychology, neuroscience, and computer vision communities. Humans are notoriously good at it, even if some variations are presumably more difficult to handle than others (e.g. 3D rotations). Humans are thought to solve the problem through hierarchical processing along the ventral stream, which progressively extracts more and more invariant visual features. This feed-forward architecture has inspired a new generation of bio-inspired computer vision systems called deep convolutional neural networks (DCNN), which are currently the best algorithms for object recognition in natural images. Here, for the first time, we systematically compared human feed-forward vision and DCNNs at view-invariant object recognition using the same images and controlling for both the kinds of transformation as well as their magnitude. We used four object categories and images were rendered from 3D computer models. In total, 89 human subjects participated in 10 experiments in which they had to discriminate between two or four categories after rapid presentation with backward masking. We also tested two recent DCNNs on the same tasks. We found that humans and DCNNs largely agreed on the relative difficulties of each kind of variation: rotation in depth is by far the hardest transformation to handle, followed by scale, then rotation in plane, and finally position. This suggests that humans recognize objects mainly through 2D template matching, rather than by constructing 3D object models, and that DCNNs are not too unreasonable models of human feed-forward vision. Also, our results show that the variation levels in rotation in depth and scale strongly modulate both humans' and DCNNs' recognition performances. We thus argue that these variations should be controlled in the image datasets used in vision research.

##### Abstract (translated by Google)
视觉不变的物体识别是一个具有挑战性的问题，在心理学，神经科学和计算机视觉领域引起了很大的关注。人类的臭名昭着，即使一些变化比其他人更难处理（例如3D旋转）。人类被认为通过沿着腹部流的分层处理来解决问题，逐渐提取越来越多的不变的视觉特征。这种前馈架构激发了新一代的生物启发计算机视觉系统，称为深度卷积神经网络（DCNN），它是目前在自然图像中进行物体识别的最佳算法。在这里，我们首次系统地比较了人类前馈视觉和DCNN在视觉不变的物体识别中使用相同的图像，并控制这两种变换以及它们的大小。我们使用了四个对象类别，图像是从三维计算机模型渲染的。总共有89名人类受试者参加了10次实验，他们在快速呈现后向遮蔽后不得不区分两类或四类。我们还在相同的任务上测试了两个最近的DCNN。我们发现人类和DCNN在很大程度上同意了各种变化的相对困难：深度旋转是目前最难处理的变换，其次是尺度，然后是平面旋转，最后是位置。这表明人类主要通过二维模板匹配来识别物体，而不是通过构建三维物体模型，DCNN并不是过于不合理的人类前视视觉模型。另外，我们的研究结果表明，深度和尺度上的旋转变化水平强烈地调节了人类和DCNN的识别性能。因此我们认为这些变化应该在视觉研究中使用的图像数据集中进行控制。

##### URL
[https://arxiv.org/abs/1604.06486](https://arxiv.org/abs/1604.06486)

##### PDF
[https://arxiv.org/pdf/1604.06486](https://arxiv.org/pdf/1604.06486)

