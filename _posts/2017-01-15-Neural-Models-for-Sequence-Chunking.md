---
layout: post
title: "Neural Models for Sequence Chunking"
date: 2017-01-15 11:08:28
categories: arXiv_SD
tags: arXiv_SD
author: Feifei Zhai, Saloni Potdar, Bing Xiang, Bowen Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-Outside-Beginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.

##### Abstract (translated by Google)
许多自然语言理解（NLU）任务（如浅解析（即文本组块）和语义槽填充）需要将代表性标签分配给句子中有意义的块。目前大多数基于深度神经网络（DNN）的方法认为这些任务是一个序列标签问题，其中一个单词而不是一个块被视为标签的基本单位。然后这些块被标准的IOB（Inside-Outside-Beginning）标签推断出来。在本文中，我们提出了一种替代方法，通过研究使用DNN进行序列组块，并提出了三个神经模型，以便每个块可以被视为一个完整的单位进行标记。实验结果表明，所提出的神经序列分块模型可以在文本分块和槽填充任务上实现最先进的性能。

##### URL
[https://arxiv.org/abs/1701.04027](https://arxiv.org/abs/1701.04027)

##### PDF
[https://arxiv.org/pdf/1701.04027](https://arxiv.org/pdf/1701.04027)

