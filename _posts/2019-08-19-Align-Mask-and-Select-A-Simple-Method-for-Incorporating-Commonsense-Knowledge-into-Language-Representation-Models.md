---
layout: post
title: "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models"
date: 2019-08-19 12:08:27
categories: arXiv_CL
tags: arXiv_CL Knowledge QA Inference Classification
author: Zhi-Xiu Ye, Qian Chen, Wen Wang, Zhen-Hua Ling
mathjax: true
---

* content
{:toc}

##### Abstract
Neural language representation models such as Bidirectional Encoder Representations from Transformers (BERT) pre-trained on large-scale corpora can well capture rich semantics from plain text, and can be fine-tuned to consistently improve the performance on various natural language processing (NLP) tasks. However, the existing pre-trained language representation models rarely consider explicitly incorporating commonsense knowledge or other knowledge. In this paper, we develop a pre-training approach for incorporating commonsense knowledge into language representation models. We construct a commonsense-related multi-choice question answering dataset for pre-training a neural language representation model. The dataset is created automatically by our proposed "align, mask, and select" (AMS) method. We also investigate different pre-training tasks. Experimental results demonstrate that pre-training models using the proposed approach followed by fine-tuning achieves significant improvements on various commonsense-related tasks, such as CommonsenseQA and Winograd Schema Challenge, while maintaining comparable performance on other NLP tasks, such as sentence classification and natural language inference (NLI) tasks, compared to the original BERT models.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.06725](http://arxiv.org/abs/1908.06725)

##### PDF
[http://arxiv.org/pdf/1908.06725](http://arxiv.org/pdf/1908.06725)

