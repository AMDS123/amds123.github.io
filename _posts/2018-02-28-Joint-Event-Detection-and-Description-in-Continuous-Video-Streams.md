---
layout: post
title: "Joint Event Detection and Description in Continuous Video Streams"
date: 2018-02-28 03:40:05
categories: arXiv_CV
tags: arXiv_CV Video_Caption Segmentation Caption CNN RNN Detection Relation
author: Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
mathjax: true
---

* content
{:toc}

##### Abstract
As a fine-grained video understanding task, dense video captioning involves first localizing events in a video and then generating captions for the identified events. We present the Joint Event Detection and Description Network (JEDDi-Net) that solves the dense captioning task in an end-to-end fashion. Our model continuously encodes the input video stream with three-dimensional convolutional layers and proposes variable-length temporal events based on pooled features. In order to explicitly model temporal relationships between visual events and their captions in a single video, we propose a two-level hierarchical LSTM module that transcribes the event proposals into captions. Unlike existing dense video captioning approaches, our proposal generation and language captioning networks are trained end-to-end, allowing for improved temporal segmentation. On the large-scale ActivityNet Captions dataset, JEDDi-Net demonstrates improved results as measured by most language generation metrics. We also present the first dense captioning results on the TACoS-MultiLevel dataset.

##### Abstract (translated by Google)
作为一项细致的视频理解任务，密集的视频字幕涉及首先在视频中本地化事件，然后为识别的事件生成字幕。我们提出联合事件检测和描述网络（JEDDi-Net），以端到端的方式解决密集字幕任务。我们的模型使用三维卷积层连续编码输入视频流，并基于合并的特征提出可变长度的时间事件。为了明确地建模一个视频中的视觉事件及其标题之间的时间关系，我们提出了一个将事件提议转录成标题的两级分层LSTM模块。与现有的密集视频字幕方法不同，我们的提案生成和语言字幕网络是端对端培训，可以改善时间分割。在大型ActivityNet Captions数据集上，JEDDi-Net通过大多数语言生成度量标准来衡量改进后的结果。我们还在TACoS-MultiLevel数据集中提供了第一个密集字幕结果。

##### URL
[http://arxiv.org/abs/1802.10250](http://arxiv.org/abs/1802.10250)

##### PDF
[http://arxiv.org/pdf/1802.10250](http://arxiv.org/pdf/1802.10250)

