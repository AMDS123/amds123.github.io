---
layout: post
title: "Learning to Ask: Neural Question Generation for Reading Comprehension"
date: 2017-04-29 01:08:48
categories: arXiv_CL
tags: arXiv_CL Attention
author: Xinya Du, Junru Shao, Claire Cardie
mathjax: true
---

* content
{:toc}

##### Abstract
We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).

##### Abstract (translated by Google)
我们在阅读理解中研究文本段落中的句子自动生成问题。我们引入一个基于注意力的序列学习模型来研究任务，并研究句子级别信息的编码效果。与之前的所有工作相比，我们的模型不依赖手工制定的规则或复杂的NLP管道。而是通过序列到序列的学习来进行可训练的端对端。自动评估结果显示，我们的系统明显优于最先进的基于规则的系统。在人的评价中，由我们的系统产生的问题也被评定为更自然（即，语法性，流利性），并且更难以回答（从原始文本的句法和词汇分歧和需要回答的推理方面）。

##### URL
[https://arxiv.org/abs/1705.00106](https://arxiv.org/abs/1705.00106)

##### PDF
[https://arxiv.org/pdf/1705.00106](https://arxiv.org/pdf/1705.00106)

