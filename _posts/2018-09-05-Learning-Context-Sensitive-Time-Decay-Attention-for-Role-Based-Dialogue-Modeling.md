---
layout: post
title: "Learning Context-Sensitive Time-Decay Attention for Role-Based Dialogue Modeling"
date: 2018-09-05 14:53:14
categories: arXiv_CL
tags: arXiv_CL Attention Tracking
author: Shang-Yu Su, Pei-Chieh Yuan, Yun-Nung Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Spoken language understanding (SLU) is an essential component in conversational systems. Considering that contexts provide informative cues for better understanding, history can be leveraged for contextual SLU. However, most prior work only paid attention to the related content in history utterances and ignored the temporal information. In dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, hence time-aware attention should be in a decaying manner. Therefore, this paper allows the model to automatically learn a time-decay attention function based on the content of each role's contexts, which effectively integrates both content-aware and time-aware perspectives and demonstrates remarkable flexibility to complex dialogue contexts. The experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed role-based context-sensitive time-decay attention mechanisms significantly improve the state-of-the-art model for contextual understanding performance.

##### Abstract (translated by Google)
口语理解（SLU）是会话系统中的重要组成部分。考虑到上下文提供了更好理解的信息提示，历史可以用于上下文SLU。然而，大多数先前的工作只关注历史话语中的相关内容而忽略了时间信息。在对话中，直观的是最近的话语比最近的话语更重要，因此时间意识的注意力应该是腐朽的方式。因此，本文允许模型基于每个角色的上下文的内容自动学习时间衰减注意功能，这有效地整合了内容感知和时间感知视角，并展示了复杂对话环境的显着灵活性。基准对话状态跟踪挑战（DSTC4）数据集的实验表明，所提出的基于角色的上下文敏感时间衰减注意机制显着改善了上下文理解性能的最新模型。

##### URL
[http://arxiv.org/abs/1809.01557](http://arxiv.org/abs/1809.01557)

##### PDF
[http://arxiv.org/pdf/1809.01557](http://arxiv.org/pdf/1809.01557)

