---
layout: post
title: "Neural Paraphrase Generation with Stacked Residual LSTM Networks"
date: 2016-10-13 00:37:33
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention Embedding RNN Deep_Learning
author: Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek Datla, Ashequl Qadir, Joey Liu, Oladimeji Farri
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a novel neural approach for paraphrase generation. Conventional para- phrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi- directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric.

##### Abstract (translated by Google)
在这篇文章中，我们提出了一种新的释义方法来解释生成。常规的段落生成方法要么利用手写规则和基于叙词表的对齐，要么使用统计机器学习原则。就我们所知，这项工作是第一个探索深层次的复述生成模式。我们的主要贡献是堆栈剩余LSTM网络，我们在LSTM层之间添加剩余连接。这允许对深LSTM进行有效的培训。我们在三个不同的数据集上评估我们的模型和其他最先进的深度学习模型：PPDB，WikiAnswers和MSCOCO。评估结果表明，我们的模型在BLEU，METEOR，TER和基于嵌入的句子相似性度量方面优于序列，基于注意的和双向的LSTM模型。

##### URL
[https://arxiv.org/abs/1610.03098](https://arxiv.org/abs/1610.03098)

##### PDF
[https://arxiv.org/pdf/1610.03098](https://arxiv.org/pdf/1610.03098)

