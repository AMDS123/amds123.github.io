---
layout: post
title: "Zero-Shot Relation Extraction via Reading Comprehension"
date: 2017-06-13 15:17:42
categories: arXiv_CL
tags: arXiv_CL Relation_Extraction Relation
author: Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer
mathjax: true
---

* content
{:toc}

##### Abstract
We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.

##### Abstract (translated by Google)
我们表明，关系提取可以减少回答简单的阅读理解问题，通过关联一个或多个自然语言问题与每个关系槽。 （1）通过扩展最近的神经阅读理解技术来学习关系提取模型，（2）通过将关系特定的众包问题与远程监督相结合来为这些模型构建非常大的训练集，以及即使（3）通过提取仅在测试时间指定的新的关系类型来进行零点学习，对此我们没有标记的训练例子。在维基百科填充任务上的实验表明，这种方法可以推广到高准确度的已知关系类型的新问题，并且可以在较低的准确性水平上实现对于看不见的关系类型的零炮概化，为将来的工作设定条件这个任务。

##### URL
[https://arxiv.org/abs/1706.04115](https://arxiv.org/abs/1706.04115)

##### PDF
[https://arxiv.org/pdf/1706.04115](https://arxiv.org/pdf/1706.04115)

