---
layout: post
title: "An AGI with Time-Inconsistent Preferences"
date: 2019-06-23 21:22:19
categories: arXiv_AI
tags: arXiv_AI
author: James D. Miller, Roman Yampolskiy
mathjax: true
---

* content
{:toc}

##### Abstract
This paper reveals a trap for artificial general intelligence (AGI) theorists who use economists' standard method of discounting. This trap is implicitly and falsely assuming that a rational AGI would have time-consistent preferences. An agent with time-inconsistent preferences knows that its future self will disagree with its current self concerning intertemporal decision making. Such an agent cannot automatically trust its future self to carry out plans that its current self considers optimal.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.10536](http://arxiv.org/abs/1906.10536)

##### PDF
[http://arxiv.org/pdf/1906.10536](http://arxiv.org/pdf/1906.10536)

