---
layout: post
title: "How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?"
date: 2017-04-14 14:43:44
categories: arXiv_CL
tags: arXiv_CL Embedding CNN RNN
author: Georg Heigold, Günter Neumann, Josef van Genabith
mathjax: true
---

* content
{:toc}

##### Abstract
This paper investigates the robustness of NLP against perturbed word forms. While neural approaches can achieve (almost) human-like accuracy for certain tasks and conditions, they often are sensitive to small changes in the input such as non-canonical input (e.g., typos). Yet both stability and robustness are desired properties in applications involving user-generated content, and the more as humans easily cope with such noisy or adversary conditions. In this paper, we study the impact of noisy input. We consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions for training and testing. Moreover, we empirically evaluate the robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different NLP tasks (morphological tagging, machine translation).

##### Abstract (translated by Google)
本文研究了NLP对干扰词形式的鲁棒性。虽然神经方法可以在某些任务和条件下达到（几乎）类似人类的精确度，但它们通常对输入中的小变化（例如非规范输入（例如，拼写错误））敏感。然而，稳定性和健壮性在涉及用户生成的内容的应用中是期望的特性，并且人类更容易应对这样的嘈杂或对手的情况。在本文中，我们研究噪声输入的影响。我们考虑不同的噪声分布（噪声的一种类型，噪声类型的组合）和用于训练和测试的不匹配的噪声分布。此外，我们还经验性地评估了不同模型（卷积神经网络，递归神经网络，非神经模型），不同基本单元（字符，字节对编码单元）和不同NLP任务（形态标记，机器翻译）的鲁棒性。

##### URL
[https://arxiv.org/abs/1704.04441](https://arxiv.org/abs/1704.04441)

##### PDF
[https://arxiv.org/pdf/1704.04441](https://arxiv.org/pdf/1704.04441)

