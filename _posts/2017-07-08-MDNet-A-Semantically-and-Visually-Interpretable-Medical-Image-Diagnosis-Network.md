---
layout: post
title: "MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network"
date: 2017-07-08 19:48:30
categories: arXiv_CV
tags: arXiv_CV Attention Optimization Language_Model Prediction
author: Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, Lin Yang
mathjax: true
---

* content
{:toc}

##### Abstract
The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet to establish a direct multimodal mapping between medical images and diagnostic reports that can read images, generate diagnostic reports, retrieve images by symptom descriptions, and visualize attention, to provide justifications of the network diagnosis process. MDNet includes an image model and a language model. The image model is proposed to enhance multi-scale feature ensembles and utilization efficiency. The language model, integrated with our improved attention mechanism, aims to read and explore discriminative image feature descriptions from reports to learn a direct mapping from sentence words to image pixels. The overall network is trained end-to-end by using our developed optimization strategy. Based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we conduct sufficient experiments to demonstrate that MDNet outperforms comparative baselines. The proposed image model obtains state-of-the-art performance on two CIFAR datasets as well.

##### Abstract (translated by Google)
以语义和视觉上有意义的方式无法解释模型预测是大多数现有的计算机辅助诊断方法的众所周知的缺点。在本文中，我们建议MDNet建立医学图像和诊断报告之间的直接的多模式映射，可以读取图像，生成诊断报告，通过症状描述检索图像，可视化注意力，为网络诊断过程提供合理性。 MDNet包含一个图像模型和一个语言模型。提出了图像模型来提高多尺度特征集合和利用效率。语言模型与我们改进的关注机制相结合，旨在从报告中读取和探索识别性图像特征描述，以学习从句子到图像像素的直接映射。整个网络使用我们开发的优化策略进行端对端培训。基于病理学膀胱癌图像及其诊断报告（BCIDR）数据集，我们进行充分的实验来证明MDNet优于比较基线。所提出的图像模型也在两个CIFAR数据集上获得了最新的性能。

##### URL
[https://arxiv.org/abs/1707.02485](https://arxiv.org/abs/1707.02485)

##### PDF
[https://arxiv.org/pdf/1707.02485](https://arxiv.org/pdf/1707.02485)

