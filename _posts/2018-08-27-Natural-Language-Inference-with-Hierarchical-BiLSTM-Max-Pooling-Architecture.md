---
layout: post
title: "Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture"
date: 2018-08-27 09:50:56
categories: arXiv_CL
tags: arXiv_CL Embedding Transfer_Learning Inference RNN
author: Aarne Talman, Anssi Yli-Jyr&#xe4;, J&#xf6;rg Tiedemann
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks have proven to be very effective for natural language inference tasks. We build on top of one such model, namely BiLSTM with max pooling, and show that adding a hierarchy of BiLSTM and max pooling layers yields state of the art results for the SNLI sentence encoding-based models and the SciTail dataset, as well as provides strong results for the MultiNLI dataset. We also show that our sentence embeddings can be utilized in a wide variety of transfer learning tasks, outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks. Furthermore, our model beats the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences.

##### Abstract (translated by Google)
事实证明，递归神经网络对于自然语言推理任务非常有效。我们建立在一个这样的模型之上，即具有最大池的BiLSTM，并且表明添加BiLSTM和最大池层的层次结构为基于SNLI语句编码的模型和SciTail数据集提供了最先进的结果，并且提供了MultiNLI数据集的强大结果。我们还表明，我们的句子嵌入可以用于各种各样的转移学习任务，优于10个中的7个InferSent，以及9个SentEval句子嵌入评估任务中的8个中的SkipThought。此外，我们的模型在最近发布的SentEval探测任务中有8个击败了InferSent模型，旨在评估句子嵌入捕获句子的一些重要语言属性的能力。

##### URL
[http://arxiv.org/abs/1808.08762](http://arxiv.org/abs/1808.08762)

##### PDF
[http://arxiv.org/pdf/1808.08762](http://arxiv.org/pdf/1808.08762)

