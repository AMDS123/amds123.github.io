---
layout: post
title: "Graph Reasoning Networks for Visual Question Answering"
date: 2019-07-23 10:59:54
categories: arXiv_CV
tags: arXiv_CV QA Attention Relation VQA
author: Dalu Guo, Chang Xu, Dacheng Tao
mathjax: true
---

* content
{:toc}

##### Abstract
The interaction between language and visual information has been emphasized in visual question answering (VQA) with the help of attention mechanism. However, the relationship between words in question has been underestimated, which makes it hard to answer questions that involve the relationship between multiple entities, such as comparison and counting. In this paper, we develop the graph reasoning networks to tackle this problem. Two kinds of graphs are investigated, namely inter-graph and intra-graph. The inter-graph transfers features of the detected objects to their related query words, enabling the output nodes to have both semantic and factual information. The intra-graph exchanges information between these output nodes from inter-graph to amplify implicit yet important relationship between objects. These two kinds of graphs cooperate with each other, and thus our resulting model can reason the relationship and dependence between objects, which leads to realization of multi-step reasoning. Experimental results on the GQA v1.1 dataset demonstrate the reasoning ability of our method to handle compositional questions about real-world images. We achieve state-of-the-art performance, boosting accuracy to 57.04%. On the VQA 2.0 dataset, we also receive a promising improvement on overall accuracy, especially on counting problem.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.09815](http://arxiv.org/abs/1907.09815)

##### PDF
[http://arxiv.org/pdf/1907.09815](http://arxiv.org/pdf/1907.09815)

