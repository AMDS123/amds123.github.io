---
layout: post
title: "A balanced memory network"
date: 2007-04-23 13:45:38
categories: arXiv_CV
tags: arXiv_CV Prediction
author: Yasser Roudi, Peter E. Latham
mathjax: true
---

* content
{:toc}

##### Abstract
A fundamental problem in neuroscience is understanding how working memory -- the ability to store information at intermediate timescales, like 10s of seconds -- is implemented in realistic neuronal networks. The most likely candidate mechanism is the attractor network, and a great deal of effort has gone toward investigating it theoretically. Yet, despite almost a quarter century of intense work, attractor networks are not fully understood. In particular, there are still two unanswered questions. First, how is it that attractor networks exhibit irregular firing, as is observed experimentally during working memory tasks? And second, how many memories can be stored under biologically realistic conditions? Here we answer both questions by studying an attractor neural network in which inhibition and excitation balance each other. Using mean field analysis, we derive a three-variable description of attractor networks. From this description it follows that irregular firing can exist only if the number of neurons involved in a memory is large. The same mean field analysis also shows that the number of memories that can be stored in a network scales with the number of excitatory connections, a result that has been suggested for simple models but never shown for realistic ones. Both of these predictions are verified using simulations with large networks of spiking neurons.

##### Abstract (translated by Google)
神经科学的一个根本问题是理解工作记忆如何在中间时间尺度存储信息的能力，比如几十秒钟，在现实的神经元网络中实现。最可能的候选机制是吸引子网络，并且在理论上进行了大量的努力。然而，尽管近四分之一世纪的工作紧张，吸引子网络还没有完全理解。特别是，还有两个没有答案的问题。首先，在工作记忆任务中如何观察到吸引网络如何表现出不规则的射击？第二，在生物现实条件下可以存储多少记忆？在这里，我们通过研究吸引子神经网络来回答这两个问题，其中抑制和激励相互平衡。使用平均场分析，我们得到吸引网络的三变量描述。从这个描述中可以看出，只有在涉及记忆的神经元数量很大的情况下，才能存在不规则的射击。相同的平均场分析也表明，可以存储在网络中的存储器的数量随着兴奋连接的数量而变化，这对于简单的模型已经提出了结果，但是对于实际的结果从未示出。这两种预测都是使用大型网络神经元模拟进行验证的。

##### URL
[https://arxiv.org/abs/0704.3005](https://arxiv.org/abs/0704.3005)

##### PDF
[https://arxiv.org/pdf/0704.3005](https://arxiv.org/pdf/0704.3005)

