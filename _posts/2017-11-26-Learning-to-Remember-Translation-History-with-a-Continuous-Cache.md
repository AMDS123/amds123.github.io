---
layout: post
title: "Learning to Remember Translation History with a Continuous Cache"
date: 2017-11-26 10:44:55
categories: arXiv_CL
tags: arXiv_CL NMT
author: Zhaopeng Tu, Yang Liu, Shuming Shi, Tong Zhang
---

* content
{:toc}

##### Abstract
Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost.

##### Abstract (translated by Google)
现有的神经机器翻译（NMT）模型通常单独翻译句子，错过了利用文档级信息的机会。在这项工作中，我们建议用一个非常轻量级的缓存式内存网络来扩充NMT模型，这个内存网络将最近隐藏的表示法存储为翻译历史。生成的词的概率分布根据从存储器检索到的翻译历史在线更新，赋予NMT模型随着时间动态适应的能力。在不同的主题和风格的多个领域的实验表明，提出的方法的有效性，对计算成本的影响可以忽略不计。

##### URL
[https://arxiv.org/abs/1711.09367](https://arxiv.org/abs/1711.09367)

