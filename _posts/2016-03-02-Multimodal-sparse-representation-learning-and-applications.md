---
layout: post
title: "Multimodal sparse representation learning and applications"
date: 2016-03-02 19:22:48
categories: arXiv_CV
tags: arXiv_CV Sentiment Sparse Sentiment_Classification Represenation_Learning Classification Detection Relation
author: Miriam Cha, Youngjune Gwon, H.T. Kung
mathjax: true
---

* content
{:toc}

##### Abstract
Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text).

##### Abstract (translated by Google)
无监督的方法已被证明是有效的单模态情况下的歧视性任务。在本文中，我们提出了一种多模式框架来学习稀疏表示，可以捕捉模态之间的语义相关性。该框架可以通过强制共享稀疏表示来建立较高级别的关系。具体而言，我们提出使用联合字典学习技术进行稀疏编码，并为简明，跨模态表示（如果缺少模态）以及跨模态表示的并集制定联合表示。鉴于网络上发布的多模式数据（如YouTube，维基百科和Twitter）加速增长，学习良好的多模态特性变得越来越重要。我们表明，我们的框架所支持的共享表示大大提高了单峰和多峰设置下的分类性能。我们进一步展示了建立在所提出的框架上的深层架构对于模态之间高度非线性相关的情况是有效的。我们的方法的有效性在图像去噪，TRECVID数据集（音频 - 视频）上的多媒体事件检测和检索，维基百科数据集（图像文本）上的类别分类以及PhotoTweet（图像文本）上的情感分类中被实验证明。

##### URL
[https://arxiv.org/abs/1511.06238](https://arxiv.org/abs/1511.06238)

##### PDF
[https://arxiv.org/pdf/1511.06238](https://arxiv.org/pdf/1511.06238)

