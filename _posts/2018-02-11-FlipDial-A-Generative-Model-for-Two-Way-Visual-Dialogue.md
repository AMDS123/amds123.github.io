---
layout: post
title: "FlipDial: A Generative Model for Two-Way Visual Dialogue"
date: 2018-02-11 19:40:16
categories: arXiv_CV
tags: arXiv_CV Caption CNN
author: Daniela Massiceti, N. Siddharth, Puneet Kumar Dokania, Philip H.S. Torr
mathjax: true
---

* content
{:toc}

##### Abstract
We present FlipDial, a generative model for visual dialogue that simultaneously plays the role of both participants in a visually-grounded dialogue. Given context in the form of an image and an associated caption summarising the contents of the image, FlipDial learns both to answer questions and put forward questions, capable of generating entire sequences of dialogue (question-answer pairs) which are diverse and relevant to the image. To do this, FlipDial relies on a simple but surprisingly powerful idea: it uses convolutional neural networks (CNNs) to encode entire dialogues directly, implicitly capturing dialogue context, and conditional VAEs to learn the generative model. FlipDial outperforms the state-of-the-art baseline in the sequential answering task (1VD) on the VisDial dataset by a significant margin of 12 points in Mean Rank. We are the first to extend this paradigm to full two-way visual dialogue (2VD), where our model is capable of generating visually-grounded both questions and answers in sequence, for which we propose a set of novel evaluation measures and metrics.

##### Abstract (translated by Google)
我们展示了FlipDial，这是一个视觉对话的生成模型，它同时扮演两个参与者在视觉对话中的角色。以图像形式的上下文以及概括图像内容的相关标题为例，FlipDial学习回答问题和提出问题，能够生成整个序列的对话（问答对），这些对话是多样的并与图片。为此，FlipDial依赖于一个简单但令人惊讶的强大想法：它使用卷积神经网络（CNN）直接编码整个对话，隐式捕获对话上下文，并使用条件VAE学习生成模型。 FlipDial在VisDial数据集上的顺序应答任务（1VD）中的平均等级中显着优于12分。我们是第一个将此范例扩展到完全双向视觉对话（2VD）的模型，其中我们的模型能够依次生成基于视觉的问题和答案，为此我们提出了一套新颖的评估测量和度量标准。

##### URL
[http://arxiv.org/abs/1802.03803](http://arxiv.org/abs/1802.03803)

##### PDF
[http://arxiv.org/pdf/1802.03803](http://arxiv.org/pdf/1802.03803)

