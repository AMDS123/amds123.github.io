---
layout: post
title: "A Learning-Based Visual Saliency Prediction Model for Stereoscopic 3D Video"
date: 2018-03-13 14:37:58
categories: arXiv_CV
tags: arXiv_CV Salient Segmentation Attention Face Tracking Prediction
author: Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, Panos Nasiopoulos
mathjax: true
---

* content
{:toc}

##### Abstract
Over the past decade, many computational saliency prediction models have been proposed for 2D images and videos. Considering that the human visual system has evolved in a natural 3D environment, it is only natural to want to design visual attention models for 3D content. Existing monocular saliency models are not able to accurately predict the attentive regions when applied to 3D image/video content, as they do not incorporate depth information. This paper explores stereoscopic video saliency prediction by exploiting both low-level attributes such as brightness, color, texture, orientation, motion, and depth, as well as high-level cues such as face, person, vehicle, animal, text, and horizon. Our model starts with a rough segmentation and quantifies several intuitive observations such as the effects of visual discomfort level, depth abruptness, motion acceleration, elements of surprise, size and compactness of the salient regions, and emphasizing only a few salient objects in a scene. A new fovea-based model of spatial distance between the image regions is adopted for considering local and global feature calculations. To efficiently fuse the conspicuity maps generated by our method to one single saliency map that is highly correlated with the eye-fixation data, a random forest based algorithm is utilized. The performance of the proposed saliency model is evaluated against the results of an eye-tracking experiment, which involved 24 subjects and an in-house database of 61 captured stereoscopic videos. Our stereo video database as well as the eye-tracking data are publicly available along with this paper. Experiment results show that the proposed saliency prediction method achieves competitive performance compared to the state-of-the-art approaches.

##### Abstract (translated by Google)
在过去的十年中，已经为二维图像和视频提出了许多计算显着性预测模型。考虑到人类视觉系统已经在自然的3D环境中发展，想要为3D内容设计视觉注意模型是很自然的。现有的单眼显着模型无法准确预测应用于3D图像/视频内容时的注意区域，因为它们不包含深度信息。本文探讨了利用低级属性（如亮度，颜色，纹理，方向，运动和深度）以及高级提示（如脸部，人物，车辆，动物，文本和视野）来进行立体视频显着性预测。我们的模型以粗略的分割开始，并量化几个直观的观察结果，如视觉不舒适程度，深度突然，运动加速度，惊喜元素，显着区域的大小和紧凑程度以及仅强调场景中的少数显着对象。采用新的基于中心凹的图像区域之间的空间距离模型来考虑局部和全局特征计算。为了将由我们的方法生成的明显性图高效地融合成与眼固定数据高度相关的单个显着图，使用基于随机森林的算法。所提出的显着性模型的性能根据包括24个对象和61个捕获的立体视频的内部数据库的眼睛追踪实验的结果来评估。我们的立体声视频数据库以及眼动数据随本文一起公开发布。实验结果表明，与最先进的方法相比，所提出的显着性预测方法实现了竞争性表现。

##### URL
[http://arxiv.org/abs/1803.04842](http://arxiv.org/abs/1803.04842)

##### PDF
[http://arxiv.org/pdf/1803.04842](http://arxiv.org/pdf/1803.04842)

