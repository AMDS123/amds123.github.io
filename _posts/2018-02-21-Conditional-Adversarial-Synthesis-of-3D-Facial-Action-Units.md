---
layout: post
title: "Conditional Adversarial Synthesis of 3D Facial Action Units"
date: 2018-02-21 04:31:51
categories: arXiv_CV
tags: arXiv_CV Adversarial Face Deep_Learning Quantitative
author: Zhilei Liu, Guoxian Song, Jianfei Cai, Tat-Jen Cham, Juyong Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Employing deep learning-based approaches for fine-grained facial expression analysis, such as those involving the estimation of Action Unit (AU) intensities, is difficult due to the lack of a large-scale dataset of real faces with sufficiently diverse AU labels for training. In this paper, we consider how AU-level facial image synthesis can be used to substantially augment such a dataset. We propose an AU synthesis framework that combines the well-known 3D Morphable Model (3DMM), which intrinsically disentangles expression parameters from other face attributes, with models that adversarially generate 3DMM expression parameters conditioned on given target AU labels, in contrast to the more conventional approach of generating facial images directly. In this way, we are able to synthesize new combinations of expression parameters and facial images from desired AU labels. Extensive quantitative and qualitative results on the benchmark DISFA dataset demonstrate the effectiveness of our method on 3DMM facial expression parameter synthesis and data augmentation for deep learning-based AU intensity estimation.

##### Abstract (translated by Google)
采用基于深度学习的方法进行精细粒度的面部表情分析，例如涉及对动作单元（AU）强度进行估计的面部表情分析是很困难的，因为缺乏真实面孔的大规模数据集，其中有足够多样的用于训练的AU标签。在本文中，我们考虑如何使用AU级面部图像合成来充分增强这样的数据集。我们提出了一个AU合成框架，它结合了众所周知的3D形状模型（3DMM），其内在地解开了来自其他人脸属性的表情参数，与对手生成3DMM表情参数的模型相对比，直接生成面部图像的方法。通过这种方式，我们能够合成来自所需AU标签的表情参数和面部图像的新组合。在基准DISFA数据集上的广泛的定量和定性结果证明了我们的方法在基于深度学习的AU强度估计的3DMM面部表情参数合成和数据增强方面的有效性。

##### URL
[http://arxiv.org/abs/1802.07421](http://arxiv.org/abs/1802.07421)

##### PDF
[http://arxiv.org/pdf/1802.07421](http://arxiv.org/pdf/1802.07421)

