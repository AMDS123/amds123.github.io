---
layout: post
title: "Backpropagation with N-D Vector-Valued Neurons Using Arbitrary Bilinear Products"
date: 2018-05-24 12:01:53
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Zhe-Cheng Fan, Tak-Shing T. Chan, Yi-Hsuan Yang, Jyh-Shing R. Jang
mathjax: true
---

* content
{:toc}

##### Abstract
Vector-valued neural learning has emerged as a promising direction in deep learning recently. Traditionally, training data for neural networks (NNs) are formulated as a vector of scalars; however, its performance may not be optimal since associations among adjacent scalars are not modeled. In this paper, we propose a new vector neural architecture called the Arbitrary BIlinear Product Neural Network (ABIPNN), which processes information as vectors in each neuron, and the feedforward projections are defined using arbitrary bilinear products. Such bilinear products can include circular convolution, seven-dimensional vector product, skew circular convolution, reversed- time circular convolution, or other new products not seen in previous work. As a proof-of-concept, we apply our proposed network to multispectral image denoising and singing voice sepa- ration. Experimental results show that ABIPNN gains substantial improvements when compared to conventional NNs, suggesting that associations are learned during training.

##### Abstract (translated by Google)
最近，向量值神经学习已经成为深度学习的一个有希望的方向。传统上，神经网络（NN）的训练数据被制定为标量向量;然而，由于相邻标量之间的关联没有建模，因此其性能可能不是最佳的。在本文中，我们提出了一种称为任意双线性乘积神经网络（ABIPNN）的新型矢量神经架构，它将信息处理为每个神经元中的矢量，前馈投影使用任意双线性乘积来定义。这种双线性产品可以包括循环卷积，七维矢量积，斜交循环卷积，反时间循环卷积或其他在以前的工作中没有看到的新产品。作为一个概念验证，我们将我们提出的网络应用于多光谱图像去噪和歌声分离。实验结果表明，与常规神经网络相比，ABIPNN获得了实质性改善，表明在训练过程中学会了关联。

##### URL
[http://arxiv.org/abs/1805.09621](http://arxiv.org/abs/1805.09621)

##### PDF
[http://arxiv.org/pdf/1805.09621](http://arxiv.org/pdf/1805.09621)

