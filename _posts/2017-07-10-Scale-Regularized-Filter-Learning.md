---
layout: post
title: "Scale-Regularized Filter Learning"
date: 2017-07-10 12:01:30
categories: arXiv_CV
tags: arXiv_CV Regularization CNN
author: Marco Loog, François Lauze
mathjax: true
---

* content
{:toc}

##### Abstract
We start out by demonstrating that an elementary learning task, corresponding to the training of a single linear neuron in a convolutional neural network, can be solved for feature spaces of very high dimensionality. In a second step, acknowledging that such high-dimensional learning tasks typically benefit from some form of regularization and arguing that the problem of scale has not been taken care of in a very satisfactory manner, we come to a combined resolution of both of these shortcomings by proposing a form of scale regularization. Moreover, using variational method, this regularization problem can also be solved rather efficiently and we demonstrate, on an artificial filter learning problem, the capabilities of our basic linear neuron. From a more general standpoint, we see this work as prime example of how learning and variational methods could, or even should work to their mutual benefit.

##### Abstract (translated by Google)
我们首先证明了一个基本的学习任务，对应卷积神经网络中单个线性神经元的训练，可以解决非常高维的特征空间。第二步，承认这样的高维学习任务通常受益于某种形式的正规化，并且认为规模问题没有得到很好的处理，我们将这两个缺点通过提出规模正规化的形式。此外，使用变分方法，这个正则化问题也可以被相当有效地解决，我们在人工滤波器学习问题上证明了我们的基本线性神经元的能力。从一个更普遍的观点来看，我们认为这项工作是学习和变化方法如何能够或者甚至应该为了共同利益而工作的最好例子。

##### URL
[https://arxiv.org/abs/1707.02813](https://arxiv.org/abs/1707.02813)

##### PDF
[https://arxiv.org/pdf/1707.02813](https://arxiv.org/pdf/1707.02813)

