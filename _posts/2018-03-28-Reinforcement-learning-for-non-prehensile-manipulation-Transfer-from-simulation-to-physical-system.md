---
layout: post
title: "Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system"
date: 2018-03-28 01:12:46
categories: arXiv_RO
tags: arXiv_RO Reinforcement_Learning
author: Kendall Lowrey, Svetoslav Kolev, Jeremy Dao, Aravind Rajeswaran, Emanuel Todorov
mathjax: true
---

* content
{:toc}

##### Abstract
Reinforcement learning has emerged as a promising methodology for training robot controllers. However, most results have been limited to simulation due to the need for a large number of samples and the lack of automated-yet-safe data collection methods. Model-based reinforcement learning methods provide an avenue to circumvent these challenges, but the traditional concern has been the mismatch between the simulator and the real world. Here, we show that control policies learned in simulation can successfully transfer to a physical system, composed of three Phantom robots pushing an object to various desired target positions. We use a modified form of the natural policy gradient algorithm for learning, applied to a carefully identified simulation model. The resulting policies, trained entirely in simulation, work well on the physical system without additional training. In addition, we show that training with an ensemble of models makes the learned policies more robust to modeling errors, thus compensating for difficulties in system identification.

##### Abstract (translated by Google)
强化学习已经成为训练机器人控制器的有前途的方法。然而，由于需要大量样本以及缺乏自动安全的数据收集方法，大多数结果仅限于模拟。基于模型的强化学习方法为避开这些挑战提供了一条途径，但传统的担忧是模拟器和现实世界之间的不匹配。在这里，我们展示在模拟中学习的控制策略可以成功转移到物理系统，由三个幻影机器人将物体推向各种期望的目标位置组成。我们使用修改后的自然策略梯度算法进行学习，应用于仔细识别的仿真模型。由此产生的完全在模拟中训练的策略在没有额外培训的情况下在物理系统上运行良好。此外，我们还表明，使用模型集合进行培训可以使学习策略更加稳健地模拟错误，从而弥补系统识别中的困难。

##### URL
[https://arxiv.org/abs/1803.10371](https://arxiv.org/abs/1803.10371)

##### PDF
[https://arxiv.org/pdf/1803.10371](https://arxiv.org/pdf/1803.10371)

