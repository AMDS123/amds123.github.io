---
layout: post
title: "Fast Directional Self-Attention Mechanism"
date: 2018-05-02 17:16:48
categories: arXiv_AI
tags: arXiv_AI Attention
author: Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a self-attention mechanism, dubbed "fast directional self-attention (Fast-DiSA)", which is a fast and light extension of "directional self-attention (DiSA)". The proposed Fast-DiSA performs as expressively as the original DiSA but only uses much less computation time and memory, in which 1) both token2token and source2token dependencies are modeled by a joint compatibility function designed for a hybrid of both dot-product and multi-dim ways; 2) both multi-head and multi-dim attention combined with bi-directional temporal information captured by multiple positional masks are in consideration without heavy time and memory consumption appearing in the DiSA. The experiment results show that the proposed Fast-DiSA can achieve state-of-the-art performance as fast and memory-friendly as CNNs. The code for Fast-DiSA is released at \url{this https URL}.

##### Abstract (translated by Google)
在本文中，我们提出了一种被称为“快速定向自我注意（Fast-DiSA）”的自我注意机制，它是“定向自我注意（DiSA）”的快速和轻度延伸。所提出的Fast-DiSA与原始DiSA一样具有表现力，但仅使用少得多的计算时间和内存，其中1）token2token和source2token依赖性由联合兼容性功能建模，该联合兼容性功能设计用于点积和多点的混合，昏暗的方式; 2）多头位和多位点注意力结合由多个位置掩模捕获的双向时间信息被考虑在内而没有在DiSA中出现大量时间和内存消耗。实验结果表明，所提出的Fast-DiSA可以实现与CNN一样快速和内存友好的最新性能。 Fast-DiSA的代码在\ url {this https URL}上发布。

##### URL
[https://arxiv.org/abs/1805.00912](https://arxiv.org/abs/1805.00912)

##### PDF
[https://arxiv.org/pdf/1805.00912](https://arxiv.org/pdf/1805.00912)

