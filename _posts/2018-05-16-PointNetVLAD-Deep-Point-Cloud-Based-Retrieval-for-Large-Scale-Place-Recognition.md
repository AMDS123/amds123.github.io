---
layout: post
title: "PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition"
date: 2018-05-16 08:47:33
categories: arXiv_CV
tags: arXiv_CV Inference Recognition
author: Mikaela Angelina Uy, Gim Hee Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Unlike its image based counterpart, point cloud based retrieval for place recognition has remained as an unexplored and unsolved problem. This is largely due to the difficulty in extracting local feature descriptors from a point cloud that can subsequently be encoded into a global descriptor for the retrieval task. In this paper, we propose the PointNetVLAD where we leverage on the recent success of deep networks to solve point cloud based retrieval for place recognition. Specifically, our PointNetVLAD is a combination/modification of the existing PointNet and NetVLAD, which allows end-to-end training and inference to extract the global descriptor from a given 3D point cloud. Furthermore, we propose the "lazy triplet and quadruplet" loss functions that can achieve more discriminative and generalizable global descriptors to tackle the retrieval task. We create benchmark datasets for point cloud based retrieval for place recognition, and the experimental results on these datasets show the feasibility of our PointNetVLAD. Our code and the link for the benchmark dataset downloads are available in our project website. <a href="http://github.com/mikacuy/pointnetvlad/">this http URL</a>

##### Abstract (translated by Google)
与其基于图像的副本不同，基于点云的地方识别检索仍然是一个尚未解决的问题。这主要是由于难以从点云中提取局部特征描述符，其随后可能被编码为用于检索任务的全局描述符。在本文中，我们提出了PointNetVLAD，我们利用深网络最近的成功来解决基于点云的地点识别检索问题。具体来说，我们的PointNetVLAD是对现有PointNet和NetV​​LAD的组合/修改，它允许端到端的训练和推断从给定的三维点云中提取全局描述符。此外，我们提出了“懒惰三元组和四元组”损失函数，可以实现更多的区分性和一般化的全局描述符来处理检索任务。我们为地点识别创建基于点云的检索的基准数据集，并且这些数据集上的实验结果显示了我们的PointNetVLAD的可行性。我们的代码和基准数据集下载链接可在我们的项目网站上找到。 <a href="http://github.com/mikacuy/pointnetvlad/">此http URL </a>

##### URL
[http://arxiv.org/abs/1804.03492](http://arxiv.org/abs/1804.03492)

##### PDF
[http://arxiv.org/pdf/1804.03492](http://arxiv.org/pdf/1804.03492)

