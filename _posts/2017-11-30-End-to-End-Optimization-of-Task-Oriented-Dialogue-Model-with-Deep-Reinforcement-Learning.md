---
layout: post
title: "End-to-End Optimization of Task-Oriented Dialogue Model with Deep Reinforcement Learning"
date: 2017-11-30 22:28:03
categories: arXiv_CL
tags: arXiv_CL Tracking Reinforcement_Learning
author: Bing Liu, Gokhan Tur, Dilek Hakkani-Tur, Pararth Shah, Larry Heck
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we present a neural network based task-oriented dialogue system that can be optimized end-to-end with deep reinforcement learning (RL). The system is able to track dialogue state, interface with knowledge bases, and incorporate query results into agent's responses to successfully complete task-oriented dialogues. Dialogue policy learning is conducted with a hybrid supervised and deep RL methods. We first train the dialogue agent in a supervised manner by learning directly from task-oriented dialogue corpora, and further optimize it with deep RL during its interaction with users. In the experiments on two different dialogue task domains, our model demonstrates robust performance in tracking dialogue state and producing reasonable system responses. We show that deep RL based optimization leads to significant improvement on task success rate and reduction in dialogue length comparing to supervised training model. We further show benefits of training task-oriented dialogue model end-to-end comparing to component-wise optimization with experiment results on dialogue simulations and human evaluations.

##### Abstract (translated by Google)
在本文中，我们提出了一个基于神经网络的任务导向对话系统，可以通过深度强化学习（RL）进行端到端的优化。系统能够跟踪对话状态，与知识库的接口，并将查询结果合并到代理的响应中，从而成功完成面向任务的对话。对话政策学习采用混合监督和深度RL方法进行。我们首先以监督的方式训练对话者，直接从任务导向的对话语料库中学习，并且在与用户交互的过程中用深度RL进一步优化。在两个不同对话任务域的实验中，我们的模型在跟踪对话状态和产生合理的系统响应方面表现出了强大的性能。我们发现基于深度RL的优化导致了任务成功率的显着提高，并且与监督训练模型相比对话长度的减少。我们进一步展示了以对话模拟和人类评估的实验结果为基础，端到端地训练面向任务的对话模型与分量优化相比的益处。

##### URL
[https://arxiv.org/abs/1711.10712](https://arxiv.org/abs/1711.10712)

##### PDF
[https://arxiv.org/pdf/1711.10712](https://arxiv.org/pdf/1711.10712)

