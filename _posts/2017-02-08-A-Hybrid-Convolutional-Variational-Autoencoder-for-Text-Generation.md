---
layout: post
title: "A Hybrid Convolutional Variational Autoencoder for Text Generation"
date: 2017-02-08 12:11:41
categories: arXiv_SD
tags: arXiv_SD Text_Generation CNN RNN Language_Model
author: Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.

##### Abstract (translated by Google)
在本文中，我们探讨了架构选择对学习文本生成的变分自动编码器（VAE）的影响。与先前介绍的编码器和解码器都是RNN的文本VAE模型相反，我们提出了一种新颖的混合架构，将完全前馈卷积和解卷积组件与循环语言模型混合。我们的架构展现出一些有吸引力的属性，例如更快的运行时间和收敛性，更好地处理长序列的能力，更重要的是，它有助于避免在文本数据上训练VAE模型所带来的一些主要困难。

##### URL
[https://arxiv.org/abs/1702.02390](https://arxiv.org/abs/1702.02390)

##### PDF
[https://arxiv.org/pdf/1702.02390](https://arxiv.org/pdf/1702.02390)

