---
layout: post
title: 'A GRU-Gated Attention Model for Neural Machine Translation'
date: 2017-12-06 03:09:44
categories: arXiv_CL
tags: arXiv_CL
author: Biao Zhang, Deyi Xiong, Jinsong Su
---

* content
{:toc}

##### Abstract
Neural machine translation (NMT) heavily relies on an attention network to produce a context vector for each target word prediction. In practice, we find that context vectors for different target words are quite similar to one another and therefore are insufficient in discriminatively predicting target words. The reason for this might be that context vectors produced by the vanilla attention network are just a weighted sum of source representations that are invariant to decoder states. In this paper, we propose a novel GRU-gated attention model (GAtt) for NMT which enhances the degree of discrimination of context vectors by enabling source representations to be sensitive to the partial translation generated by the decoder. GAtt uses a gated recurrent unit (GRU) to combine two types of information: treating a source annotation vector originally produced by the bidirectional encoder as the history state while the corresponding previous decoder state as the input to the GRU. The GRU-combined information forms a new source annotation vector. In this way, we can obtain translation-sensitive source representations which are then feed into the attention network to generate discriminative context vectors. We further propose a variant that regards a source annotation vector as the current input while the previous decoder state as the history. Experiments on NIST Chinese-English translation tasks show that both GAtt-based models achieve significant improvements over the vanilla attentionbased NMT. Further analyses on attention weights and context vectors demonstrate the effectiveness of GAtt in improving the discrimination power of representations and handling the challenging issue of over-translation.

##### Abstract (translated by Google)
神经机器翻译（NMT）在很大程度上依赖于关注网络来为每个目标词预测产生上下文向量。在实践中，我们发现不同目标词的语境向量彼此非常相似，因此在判别性地预测目标词时是不够的。其原因可能是由香草关注网络产生的上下文向量只是对解码器状态不变的源表示的加权和。在本文中，我们提出了一种NMT的GRU门控注意模型（GAtt），它通过使源表示对解码器产生的部分翻译敏感，提高了上下文向量的识别程度。 GAtt使用门控重发单元（GRU）来组合两种类型的信息：将由双向编码器最初产生的源注释向量视为历史状态，而将对应的先前解码器状态视为GRU的输入。 GRU组合信息形成新的源注释向量。通过这种方式，我们可以获得转换敏感的源表示，然后将其馈送到关注网络中以产生有区别的上下文向量。我们进一步提出了一个变体，将源注释向量视为当前输入，而将前一个解码器状态视为历史。 NIST中英文翻译任务的实验表明，基于GAtt的模型在基于香草注意的NMT方面取得了显着的改进。对注意权重和情境向量的进一步分析表明了GAtt在提高陈述的歧视能力和处理过度翻译的挑战性问题方面的有效性。

##### URL
[https://arxiv.org/abs/1704.08430](https://arxiv.org/abs/1704.08430)

