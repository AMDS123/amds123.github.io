---
layout: post
title: "Supervised Learning of Semantics-Preserving Hash via Deep Convolutional Neural Networks"
date: 2017-02-14 07:31:18
categories: arXiv_CV
tags: arXiv_CV Image_Caption CNN Classification
author: Huei-Fang Yang, Kevin Lin, Chu-Song Chen
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. We assume that the semantic labels are governed by several latent attributes with each attribute on or off, and classification relies on these attributes. Based on this assumption, our approach, dubbed supervised semantics-preserving deep hashing (SSDH), constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function defined over classification error and other desirable hash codes properties. With this design, SSDH has a nice characteristic that classification and retrieval are unified in a single learning model. Moreover, SSDH performs joint learning of image representations, hash codes, and classification in a point-wised manner, and thus is scalable to large-scale datasets. SSDH is simple and can be realized by a slight enhancement of an existing deep architecture for classification; yet it is effective and outperforms other hashing approaches on several benchmarks and large datasets. Compared with state-of-the-art approaches, SSDH achieves higher retrieval accuracy, while the classification performance is not sacrificed.

##### Abstract (translated by Google)
本文提出了一个简单而有效的监督深度哈希方法，从标记数据构造二进制哈希码进行大规模图像搜索。我们假设语义标签是由几个潜在的属性来控制的，每个属性都是打开或关闭的，分类依赖于这些属性。基于这个假设，我们的方法被称为监督语义保留深度散列（SSDH），将散列函数构建为深度网络中的潜在层，并且通过最小化定义在分类错误上的目标函数和其他期望的散列码来学习二进制代码属性。有了这个设计，SSDH有一个很好的特点，就是分类和检索统一在一个学习模型中。而且，SSDH以图像表示，散列码和分类的方式进行联合学习，因此可以扩展到大规模数据集。 SSDH很简单，可以通过略微增强现有的深层分类体系结构来实现;但它在几个基准和大型数据集上是有效的并且胜过其他散列方法。与先进的方法相比，SSDH实现了更高的检索准确率，而分类性能不会被牺牲。

##### URL
[https://arxiv.org/abs/1507.00101](https://arxiv.org/abs/1507.00101)

##### PDF
[https://arxiv.org/pdf/1507.00101](https://arxiv.org/pdf/1507.00101)

