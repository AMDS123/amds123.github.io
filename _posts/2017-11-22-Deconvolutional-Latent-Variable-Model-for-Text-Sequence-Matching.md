---
layout: post
title: "Deconvolutional Latent-Variable Model for Text Sequence Matching"
date: 2017-11-22 02:18:24
categories: arXiv_CL
tags: arXiv_CL CNN Optimization RNN
author: Dinghan Shen, Yizhe Zhang, Ricardo Henao, Qinliang Su, Lawrence Carin
mathjax: true
---

* content
{:toc}

##### Abstract
A latent-variable model is introduced for text matching, inferring sentence representations by jointly optimizing generative and discriminative objectives. To alleviate typical optimization challenges in latent-variable models for text, we employ deconvolutional networks as the sequence decoder (generator), providing learned latent codes with more semantic information and better generalization. Our model, trained in an unsupervised manner, yields stronger empirical predictive performance than a decoder based on Long Short-Term Memory (LSTM), with less parameters and considerably faster training. Further, we apply it to text sequence-matching problems. The proposed model significantly outperforms several strong sentence-encoding baselines, especially in the semi-supervised setting.

##### Abstract (translated by Google)
引入潜在变量模型进行文本匹配，通过联合优化生成和判别目标来推断句子表示。为了缓解文本潜变量模型的典型优化问题，我们采用反卷积网络作为序列解码器（发生器），为学习的潜在代码提供更多的语义信息和更好的泛化。我们的模型以无监督的方式进行训练，比基于长期短期记忆（LSTM）的解码器产生更强的经验预测性能，参数更少，训练速度更快。此外，我们将其应用于文本序列匹配问题。提出的模型显着优于几个强的句子编码基线，特别是在半监督的设置。

##### URL
[https://arxiv.org/abs/1709.07109](https://arxiv.org/abs/1709.07109)

##### PDF
[https://arxiv.org/pdf/1709.07109](https://arxiv.org/pdf/1709.07109)

