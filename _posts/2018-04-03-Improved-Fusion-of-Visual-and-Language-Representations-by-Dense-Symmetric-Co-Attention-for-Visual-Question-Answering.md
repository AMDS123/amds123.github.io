---
layout: post
title: "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering"
date: 2018-04-03 01:24:23
categories: arXiv_CV
tags: arXiv_CV QA Attention Prediction VQA
author: Duy-Kien Nguyen, Takayuki Okatani
mathjax: true
---

* content
{:toc}

##### Abstract
A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.

##### Abstract (translated by Google)
视觉问题解答（VQA）的关键解决方案存在于如何融合从输入图像和问题中提取的视觉和语言特征。我们展示了一种能够在两种模式之间实现密集的双向交互的注意机制，有助于提高答案预测的准确性。具体而言，我们提出了一个简单的架构，在视觉和语言表示之间完全对称，其中每个问题单词出现在图像区域上，每个图像区域出现在问题单词上。可以堆叠它以形成图像问题对之间的多步交互的层次结构。我们通过实验展示了所提出的体系结构在VQA和VQA 2.0上实现了最新的最新技术，尽管其体积很小。我们还提出了定性评估，展示了提议的注意机制如何能够对图像和问题产生合理的关注图，从而导致正确的答案预测。

##### URL
[https://arxiv.org/abs/1804.00775](https://arxiv.org/abs/1804.00775)

##### PDF
[https://arxiv.org/pdf/1804.00775](https://arxiv.org/pdf/1804.00775)

