---
layout: post
title: "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering"
date: 2018-04-03 01:24:23
categories: arXiv_CV
tags: arXiv_CV QA Attention Prediction VQA
author: Duy-Kien Nguyen, Takayuki Okatani
mathjax: true
---

* content
{:toc}

##### Abstract
A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.

##### Abstract (translated by Google)
视觉问题解答（VQA）的关键解决方案存在于如何融合从输入图像和问题中提取的视觉和语言特征。我们表明，一种能够在两种模态之间实现密集，双向相互作用的注意机制有助于提高答案预测的准确性。具体来说，我们提出了一种在视觉和语言表示之间完全对称的简单架构，其中每个问题词都出现在图像区域上，每个图像区域都出现在问题词上。它可以堆叠以形成图像 - 问题对之间的多步交互的层次结构。我们通过实验证明，尽管体积小，但所提出的架构在VQA和VQA 2.0上实现了最新的先进技术。我们还提出了定性评估，证明了所提出的注意机制如何能够在图像和问题上产生合理的注意力图，从而得出正确的答案预测。

##### URL
[https://arxiv.org/abs/1804.00775](https://arxiv.org/abs/1804.00775)

##### PDF
[https://arxiv.org/pdf/1804.00775](https://arxiv.org/pdf/1804.00775)

