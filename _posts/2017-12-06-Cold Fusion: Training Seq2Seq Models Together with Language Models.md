---
layout: post
title: 'Cold Fusion: Training Seq2Seq Models Together with Language Models'
date: 2017-12-06 03:33:01
categories: arXiv_CV
tags: arXiv_CV Image_Caption Speech_Recognition Caption Recognition
author: Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, Adam Coates
---

* content
{:toc}

##### Abstract
Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.

##### Abstract (translated by Google)
注重序列到序列（Seq2Seq）模型在涉及生成机器翻译，图像字幕和语音识别等自然语言句子的任务方面表现出色。通过利用无标签的数据（通常以语言模型的形式）来进一步提高性能。在这项工作中，我们提出冷融合方法，利用训练过程中的预训练语言模型，并显示其在语音识别任务的有效性。我们表明，冷聚变的Seq2Seq模型能够更好地利用语言信息享受i）更快的收敛和更好的泛化，和ii）几乎完成转移到一个新的领域，而少于10％的标记的训练数据。

##### URL
[https://arxiv.org/abs/1708.06426](https://arxiv.org/abs/1708.06426)

