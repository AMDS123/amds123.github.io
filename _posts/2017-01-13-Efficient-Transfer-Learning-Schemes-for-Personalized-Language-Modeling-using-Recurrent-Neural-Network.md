---
layout: post
title: "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network"
date: 2017-01-13 07:26:00
categories: arXiv_SD
tags: arXiv_SD Transfer_Learning Language_Model Quantitative
author: Seunghyun Yoon, Hyeongu Yun, Yuna Kim, Gyu-tae Park, Kyomin Jung
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.

##### Abstract (translated by Google)
在本文中，我们提出了一个高效的传递学习方法，用于训练一个个性化的语言模型，使用一个具有长期短期记忆架构的递归神经网络。利用我们提出的快速传递学习方案，将一般语言模型更新为具有少量用户数据和有限计算资源的个性化语言模型。这些方法对于移动设备环境特别有用，同时为了隐私目的防止数据转出设备。通过对戏剧中的对话数据的实验，验证了我们的转移学习方法已经成功地产生了个性化的语言模型，其定性和定量方面的输出更接近于个人语言风格。

##### URL
[https://arxiv.org/abs/1701.03578](https://arxiv.org/abs/1701.03578)

##### PDF
[https://arxiv.org/pdf/1701.03578](https://arxiv.org/pdf/1701.03578)

