---
layout: post
title: "M-BERT: Injecting Multimodal Information in the BERT Structure"
date: 2019-08-15 22:51:21
categories: arXiv_CL
tags: arXiv_CL Sentiment Inference
author: Wasifur Rahman, Md Kamrul Hasan, Amir Zadeh, Louis-Philippe Morency, Mohammed Ehsan Hoque
mathjax: true
---

* content
{:toc}

##### Abstract
Multimodal language analysis is an emerging research area in natural language processing that models language in a multimodal manner. It aims to understand language from the modalities of text, visual, and acoustic by modeling both intra-modal and cross-modal interactions. BERT (Bidirectional Encoder Representations from Transformers) provides strong contextual language representations after training on large-scale unlabeled corpora. Fine-tuning the vanilla BERT model has shown promising results in building state-of-the-art models for diverse NLP tasks like question answering and language inference. However, fine-tuning BERT in the presence of information from other modalities remains an open research problem. In this paper, we inject multimodal information within the input space of BERT network for modeling multimodal language. The proposed injection method allows BERT to reach a new state of the art of $84.38\%$ binary accuracy on CMU-MOSI dataset (multimodal sentiment analysis) with a gap of 5.98 percent to the previous state of the art and 1.02 percent to the text-only BERT.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.05787](https://arxiv.org/abs/1908.05787)

##### PDF
[https://arxiv.org/pdf/1908.05787](https://arxiv.org/pdf/1908.05787)

