---
layout: post
title: "CBAM: Convolutional Block Attention Module"
date: 2018-07-17 16:05:59
categories: arXiv_CV
tags: arXiv_CV Attention CNN Classification Detection
author: Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon
mathjax: true
---

* content
{:toc}

##### Abstract
We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.

##### Abstract (translated by Google)
我们提出了卷积块注意模块（CBAM），这是一种用于前馈卷积神经网络的简单而有效的注意模块。给定中间特征图，我们的模块沿着两个单独的维度（通道和空间）顺序地推断注意力图，然后将注意力图乘以输入特征图以进行自适应特征细化。由于CBAM是一个轻量级的通用模块，它可以无缝地集成到任何CNN架构中，开销可以忽略不计，并且可以与基本CNN一起进行端到端的训练。我们通过对ImageNet-1K，MS~COCO检测和VOC~2007检测数据集的大量实验来验证我们的CBAM。我们的实验表明，各种模型在分类和检测性能方面均有一定的改进，证明了CBAM的广泛适用性。代码和模型将公开提供。

##### URL
[http://arxiv.org/abs/1807.06521](http://arxiv.org/abs/1807.06521)

##### PDF
[http://arxiv.org/pdf/1807.06521](http://arxiv.org/pdf/1807.06521)

