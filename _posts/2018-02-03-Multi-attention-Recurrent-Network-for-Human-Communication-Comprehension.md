---
layout: post
title: "Multi-attention Recurrent Network for Human Communication Comprehension"
date: 2018-02-03 06:29:17
categories: arXiv_AI
tags: arXiv_AI Sentiment Attention Face Recognition
author: Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, Louis-Philippe Morency
mathjax: true
---

* content
{:toc}

##### Abstract
Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.

##### Abstract (translated by Google)
人脸对话是一种复杂的多模式信号。我们用语言（语言形式），手势（视觉形式）和语调变化（声学形式）来表达我们的意图。人类可以轻松地处理和理解面对面的交流，但是，理解这种交流方式仍然是人工智能（AI）的重大挑战。人工智能必须了解形成人际交往的各种形式及其相互作用。在本文中，我们提出了一种新的神经体系结构来理解人类通信，称为多重注意循环网络（MARN）。我们模型的主要优点是通过使用被称为多重注意块（MAB）的神经元发现模态之间的相互作用并将其存储在称为长短期混合存储器（LSTHM）的循环分量的混合存储器中。我们对六个公开可用的数据集进行多模态情感分析，说话人特征识别和情绪识别的广泛比较。 MARN在所有数据集上显示了最先进的性能。

##### URL
[http://arxiv.org/abs/1802.00923](http://arxiv.org/abs/1802.00923)

##### PDF
[http://arxiv.org/pdf/1802.00923](http://arxiv.org/pdf/1802.00923)

