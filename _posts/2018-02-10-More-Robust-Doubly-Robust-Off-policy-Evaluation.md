---
layout: post
title: "More Robust Doubly Robust Off-policy Evaluation"
date: 2018-02-10 01:32:03
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Mehrdad Farajtabar, Yinlam Chow, Mohammad Ghavamzadeh
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t.~the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.

##### Abstract (translated by Google)
我们研究强化学习（RL）中的非政策评估（OPE）问题，其目标是根据另一项政策产生的数据来评估政策的绩效。具体而言，我们关注由重要性抽样（IS）组件和性能模型组成的双稳健（DR）估计器，并同时利用IS的低（或零）偏差和模型的低方差。尽管模型的准确性对DR的整体性能有着巨大的影响，但在OPE中使用DR估计器的大部分工作都集中在改进IS部分，而不是如何学习模型。在本文中，我们提出了另一种DR估计器，称为更鲁棒的双稳健（MRDR），通过最小化DR估计量的方差来学习模型参数。我们首先提出一个用于学习RL中DR模型的公式。然后，我们推导了在情境匪和RL中DR估计量的方差的公式，使得它们的梯度可以从样本中估计出来，并且提出了有效地使方差最小化的方法。我们证明了MRDR估计量具有强一致性和渐近最优性。最后，我们评估MRDR在土匪和RL基准测试中的问题，并将其性能与现有方法进行比较。

##### URL
[http://arxiv.org/abs/1802.03493](http://arxiv.org/abs/1802.03493)

##### PDF
[http://arxiv.org/pdf/1802.03493](http://arxiv.org/pdf/1802.03493)

