---
layout: post
title: "A Flexible Approach to Automated RNN Architecture Generation"
date: 2017-12-20 04:20:40
categories: arXiv_CL
tags: arXiv_CL Knowledge Reinforcement_Learning RNN Language_Model
author: Martin Schrimpf, Stephen Merity, James Bradbury, Richard Socher
mathjax: true
---

* content
{:toc}

##### Abstract
The process of designing neural architectures requires expert knowledge and extensive trial and error. While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components. We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width. The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization. Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains. The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.

##### Abstract (translated by Google)
设计神经架构的过程需要专业知识和广泛的试验和错误。尽管自动化体系结构搜索可以简化这些要求，但由现有方法生成的递归神经网络（RNN）体系结构在灵活性和组件方面都受到限制。我们提出了一个领域特定的语言（DSL）用于自动化架构搜索，可以产生任意深度和宽度的新型RNN。 DSL足够灵活以定义标准体系结构，如门控重复单元和长期短期内存，并允许引入非标准RNN组件，如三角曲线和层规范化。使用两种不同的候选生成技术，具有排序功能的随机搜索和强化学习，我们探索了由RNN DSL为语言建模和机器翻译领域创建的新架构。由此产生的体系结构并不遵循人类的直觉，而是在其目标任务上表现良好，这表明可用的RNN体系结构的空间远大于以前的假设。

##### URL
[https://arxiv.org/abs/1712.07316](https://arxiv.org/abs/1712.07316)

##### PDF
[https://arxiv.org/pdf/1712.07316](https://arxiv.org/pdf/1712.07316)

