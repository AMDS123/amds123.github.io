---
layout: post
title: "How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation"
date: 2018-02-02 13:53:13
categories: arXiv_AI
tags: arXiv_AI Prediction
author: Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, Finale Doshi-Velez
mathjax: true
---

* content
{:toc}

##### Abstract
Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.

##### Abstract (translated by Google)
近些年来，人们对机器学习系统的兴趣激增，可以为他们的预测或决策提供人性化的理解。然而，究竟什么样的解释是真正的人类解释仍然不甚了解。这项工作提高了我们对在验证的具体情况下解释解释的理解。假设我们有一个预测X的机器学习系统，并且我们为这个预测X提供了基本原理。给定一个输入，一个解释和一个输出，是与输入和假定的基本原理一致的输出？通过一系列的用户研究，我们确定了复杂性的增加对人类验证原理所花费的时间有什么最大的影响，哪些似乎相对不敏感。

##### URL
[https://arxiv.org/abs/1802.00682](https://arxiv.org/abs/1802.00682)

##### PDF
[https://arxiv.org/pdf/1802.00682](https://arxiv.org/pdf/1802.00682)

