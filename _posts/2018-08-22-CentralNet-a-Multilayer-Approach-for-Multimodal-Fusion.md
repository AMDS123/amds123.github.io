---
layout: post
title: "CentralNet: a Multilayer Approach for Multimodal Fusion"
date: 2018-08-22 08:37:55
categories: arXiv_AI
tags: arXiv_AI Embedding CNN
author: Valentin Vielzeuf, Alexis Lechervy, St&#xe9;phane Pateux, Fr&#xe9;d&#xe9;ric Jurie
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a novel multimodal fusion approach, aiming to produce best possible decisions by integrating information coming from multiple media. While most of the past multimodal approaches either work by projecting the features of different modalities into the same space, or by coordinating the representations of each modality through the use of constraints, our approach borrows from both visions. More specifically, assuming each modality can be processed by a separated deep convolutional network, allowing to take decisions independently from each modality, we introduce a central network linking the modality specific networks. This central network not only provides a common feature embedding but also regularizes the modality specific networks through the use of multi-task learning. The proposed approach is validated on 4 different computer vision tasks on which it consistently improves the accuracy of existing multimodal fusion approaches.

##### Abstract (translated by Google)
本文提出了一种新颖的多模式融合方法，旨在通过整合来自多种媒体的信息来产生最佳决策。虽然过去的大多数多模式方法要么通过将不同模态的特征投射到同一空间中，要么通过使用约束来协调每种模态的表示，我们的方法借鉴了两种观点。更具体地说，假设每个模态可以由分离的深度卷积网络处理，允许独立于每种模态做出决策，我们引入了连接模态特定网络的中央网络。该中央网络不仅提供嵌入的共同特征，而且通过使用多任务学习来规范特定于模态的网络。所提出的方法在4种不同的计算机视觉任务上得到验证，在这些任务上它始终如一地提高了现有多模式融合方法的准确性。

##### URL
[http://arxiv.org/abs/1808.07275](http://arxiv.org/abs/1808.07275)

##### PDF
[http://arxiv.org/pdf/1808.07275](http://arxiv.org/pdf/1808.07275)

