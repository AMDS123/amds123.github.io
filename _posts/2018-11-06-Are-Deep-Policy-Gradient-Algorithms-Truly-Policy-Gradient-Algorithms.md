---
layout: post
title: "Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?"
date: 2018-11-06 18:54:21
categories: arXiv_RO
tags: arXiv_RO Optimization Prediction
author: Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry
mathjax: true
---

* content
{:toc}

##### Abstract
We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. We propose a fine-grained analysis of state-of-the-art methods based on key aspects of this framework: gradient estimation, value prediction, optimization landscapes, and trust region enforcement. We find that from this perspective, the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict. Our analysis suggests first steps towards solidifying the foundations of these algorithms, and in particular indicates that we may need to move beyond the current benchmark-centric evaluation methodology.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.02553](http://arxiv.org/abs/1811.02553)

##### PDF
[http://arxiv.org/pdf/1811.02553](http://arxiv.org/pdf/1811.02553)

