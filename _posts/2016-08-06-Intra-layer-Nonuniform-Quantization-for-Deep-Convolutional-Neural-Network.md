---
layout: post
title: "Intra-layer Nonuniform Quantization for Deep Convolutional Neural Network"
date: 2016-08-06 08:41:25
categories: arXiv_CV
tags: arXiv_CV Object_Detection Speech_Recognition CNN Classification Detection Recognition
author: Fangxuan Sun, Jun Lin, Zhongfeng Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Deep convolutional neural network (DCNN) has achieved remarkable performance on object detection and speech recognition in recent years. However, the excellent performance of a DCNN incurs high computational complexity and large memory requirement. In this paper, an equal distance nonuniform quantization (ENQ) scheme and a K-means clustering nonuniform quantization (KNQ) scheme are proposed to reduce the required memory storage when low complexity hardware or software implementations are considered. For the VGG-16 and the AlexNet, the proposed nonuniform quantization schemes reduce the number of required memory storage by approximately 50\% while achieving almost the same or even better classification accuracy compared to the state-of-the-art quantization method. Compared to the ENQ scheme, the proposed KNQ scheme provides a better tradeoff when higher accuracy is required.

##### Abstract (translated by Google)
深卷积神经网络（DCNN）近年来在物体检测和语音识别方面取得了显着的性能。然而，DCNN的优异性能引起了高计算复杂度和大内存需求。在本文中，当考虑低复杂度的硬件或软件实现时，提出了等距非均匀量化（ENQ）方案和K均值聚类非均匀量化（KNQ）方案以减少所需的存储器存储。对于VGG-16和AlexNet，与现有技术的量化方法相比，所提出的非均匀量化方案减少了大约50％的所需存储器存储的数量，同时达到几乎相同甚至更好的分类精度。与ENQ方案相比，当需要更高的精确度时，所提出的KNQ方案提供了更好的折衷。

##### URL
[https://arxiv.org/abs/1607.02720](https://arxiv.org/abs/1607.02720)

##### PDF
[https://arxiv.org/pdf/1607.02720](https://arxiv.org/pdf/1607.02720)

