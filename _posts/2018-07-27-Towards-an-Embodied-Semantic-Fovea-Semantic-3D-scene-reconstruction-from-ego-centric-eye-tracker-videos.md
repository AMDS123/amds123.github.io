---
layout: post
title: "Towards an Embodied Semantic Fovea: Semantic 3D scene reconstruction from ego-centric eye-tracker videos"
date: 2018-07-27 12:51:36
categories: arXiv_RO
tags: arXiv_RO Tracking SLAM
author: Mickey Li, Noyan Songur, Pavel Orlov, Stefan Leutenegger, A Aldo Faisal
mathjax: true
---

* content
{:toc}

##### Abstract
Incorporating the physical environment is essential for a complete understanding of human behavior in unconstrained every-day tasks. This is especially important in ego-centric tasks where obtaining 3 dimensional information is both limiting and challenging with the current 2D video analysis methods proving insufficient. Here we demonstrate a proof-of-concept system which provides real-time 3D mapping and semantic labeling of the local environment from an ego-centric RGB-D video-stream with 3D gaze point estimation from head mounted eye tracking glasses. We augment existing work in Semantic Simultaneous Localization And Mapping (Semantic SLAM) with collected gaze vectors. Our system can then find and track objects both inside and outside the user field-of-view in 3D from multiple perspectives with reasonable accuracy. We validate our concept by producing a semantic map from images of the NYUv2 dataset while simultaneously estimating gaze position and gaze classes from recorded gaze data of the dataset images.

##### Abstract (translated by Google)
融入物理环境对于在不受约束的日常任务中完全理解人类行为至关重要。这在以自我为中心的任务中尤其重要，其中获得三维信息既限制又具有挑战性，目前的2D视频分析方法证明不足。在这里，我们演示了一个概念验证系统，该系统从一个以自我为中心的RGB-D视频流提供本地环境的实时3D映射和语义标记，并通过头戴式眼睛跟踪眼镜进行3D注视点估计。我们用收集的凝视向量增加语义同时定位和映射（语义SLAM）中的现有工作。然后，我们的系统可以从多个视角以合理的精度在3D中查找和跟踪用户视野内外的对象。我们通过从NYUv2数据集的图像生成语义地图来验证我们的概念，同时从数据集图像的记录的凝视数据中估计凝视位置和凝视类别。

##### URL
[https://arxiv.org/abs/1807.10561](https://arxiv.org/abs/1807.10561)

##### PDF
[https://arxiv.org/pdf/1807.10561](https://arxiv.org/pdf/1807.10561)

