---
layout: post
title: "Building Deep Networks on Grassmann Manifolds"
date: 2017-11-22 07:57:48
categories: arXiv_CV
tags: arXiv_CV Deep_Learning Gradient_Descent Recognition
author: Zhiwu Huang, Jiqing Wu, Luc Van Gool
mathjax: true
---

* content
{:toc}

##### Abstract
Learning representations on Grassmann manifolds is popular in quite a few visual recognition tasks. In order to enable deep learning on Grassmann manifolds, this paper proposes a deep network architecture by generalizing the Euclidean network paradigm to Grassmann manifolds. In particular, we design full rank mapping layers to transform input Grassmannian data to more desirable ones, exploit re-orthonormalization layers to normalize the resulting matrices, study projection pooling layers to reduce the model complexity in the Grassmannian context, and devise projection mapping layers to respect Grassmannian geometry and meanwhile achieve Euclidean forms for regular output layers. To train the Grassmann networks, we exploit a stochastic gradient descent setting on manifolds of the connection weights, and study a matrix generalization of backpropagation to update the structured data. The evaluations on three visual recognition tasks show that our Grassmann networks have clear advantages over existing Grassmann learning methods, and achieve results comparable with state-of-the-art approaches.

##### Abstract (translated by Google)
Grassmann流形上的学习表示在很多视觉识别任务中很受欢迎。为了使Grassmann流形具有深度学习的能力，本文将欧氏网格范式推广到Grassmann流形，提出了深度网络结构。具体来说，我们设计了满秩映射层，将输入的Grassmann数据转换为更为理想的数据，利用重正交归一化层对归结矩阵进行归一化处理，研究投影池层以降低Grassmannian背景下的模型复杂度，尊重格拉斯曼几何，同时实现规则输出层的欧几里得形式。为了训练Grassmann网络，我们利用连接权重流形上的随机梯度下降设置，研究反向传播的矩阵泛化来更新结构化数据。对三个视觉识别任务的评估表明，我们的Grassmann网络比现有的Grassmann学习方法具有明显的优势，并且取得了与最先进的方法相媲美的结果。

##### URL
[https://arxiv.org/abs/1611.05742](https://arxiv.org/abs/1611.05742)

##### PDF
[https://arxiv.org/pdf/1611.05742](https://arxiv.org/pdf/1611.05742)

