---
layout: post
title: "Contextualized Word Representations for Reading Comprehension"
date: 2018-09-04 08:20:26
categories: arXiv_CL
tags: arXiv_CL Attention Language_Model
author: Shimi Salant, Jonathan Berant
mathjax: true
---

* content
{:toc}

##### Abstract
Reading a document and extracting an answer to a question about its content has attracted substantial attention recently. While most work has focused on the interaction between the question and the document, in this work we evaluate the importance of context when the question and document are processed independently. We take a standard neural architecture for this task, and show that by providing rich contextualized word representations from a large pre-trained language model as well as allowing the model to choose between context-dependent and context-independent word representations, we can obtain dramatic improvements and reach performance comparable to state-of-the-art on the competitive SQuAD dataset.

##### Abstract (translated by Google)
阅读文档并提取有关其内容的问题的答案最近引起了人们的极大关注。虽然大多数工作都侧重于问题与文档之间的相互作用，但在这项工作中，我们在独立处理问题和文档时评估上下文的重要性。我们采用标准的神经架构来完成这项任务，并通过提供来自大型预训练语言模型的丰富的语境化词语表示以及允许模型在依赖于上下文和与上下文无关的词语表示之间进行选择来表明，我们可以获得戏剧性的与竞争性SQuAD数据集上的最新技术相媲美的改进和达到性能。

##### URL
[http://arxiv.org/abs/1712.03609](http://arxiv.org/abs/1712.03609)

##### PDF
[http://arxiv.org/pdf/1712.03609](http://arxiv.org/pdf/1712.03609)

