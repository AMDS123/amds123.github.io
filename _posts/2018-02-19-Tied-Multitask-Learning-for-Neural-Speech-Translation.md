---
layout: post
title: "Tied Multitask Learning for Neural Speech Translation"
date: 2018-02-19 14:49:42
categories: arXiv_CL
tags: arXiv_CL Regularization Attention
author: Antonios Anastasopoulos, David Chiang
mathjax: true
---

* content
{:toc}

##### Abstract
We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.

##### Abstract (translated by Google)
我们探索多任务模型来进行语音的神经翻译，增加它们以反映两个直观的概念。首先，我们引入一个模型，其中第二个任务解码器从第一个任务的解码器接收信息，因为较高级别的中间表示应该提供有用的信息。其次，我们应用鼓励传递性和可逆性的正则化。我们表明，这些概念联合训练模型的应用提高了低资源语音转录和翻译任务的性能。当使用关注信息进行未分段输入的单词发现时，它还会带来更好的性能。

##### URL
[http://arxiv.org/abs/1802.06655](http://arxiv.org/abs/1802.06655)

##### PDF
[http://arxiv.org/pdf/1802.06655](http://arxiv.org/pdf/1802.06655)

