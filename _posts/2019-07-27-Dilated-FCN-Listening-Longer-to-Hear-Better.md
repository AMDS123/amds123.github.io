---
layout: post
title: "Dilated FCN: Listening Longer to Hear Better"
date: 2019-07-27 17:52:33
categories: arXiv_SD
tags: arXiv_SD CNN
author: Shuyu Gong, Zhewei Wang, Tao Sun, Yuanhang Zhang, Charles D. Smith, Li Xu, Jundong Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural network solutions have emerged as a new and powerful paradigm for speech enhancement (SE). The capabilities to capture long context and extract multi-scale patterns are crucial to design effective SE networks. Such capabilities, however, are often in conflict with the goal of maintaining compact networks to ensure good system generalization. In this paper, we explore dilation operations and apply them to fully convolutional networks (FCNs) to address this issue. Dilations equip the networks with greatly expanded receptive fields, without increasing the number of parameters. Different strategies to fuse multi-scale dilations, as well as to install the dilation modules are explored in this work. Using Noisy VCTK and AzBio sentences datasets, we demonstrate that the proposed dilation models significantly improve over the baseline FCN and outperform the state-of-the-art SE solutions.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.11956](http://arxiv.org/abs/1907.11956)

##### PDF
[http://arxiv.org/pdf/1907.11956](http://arxiv.org/pdf/1907.11956)

