---
layout: post
title: "FlipDial: A Generative Model for Two-Way Visual Dialogue"
date: 2018-04-03 13:59:08
categories: arXiv_CV
tags: arXiv_CV Caption CNN
author: Daniela Massiceti, N. Siddharth, Puneet K. Dokania, Philip H.S. Torr
mathjax: true
---

* content
{:toc}

##### Abstract
We present FlipDial, a generative model for visual dialogue that simultaneously plays the role of both participants in a visually-grounded dialogue. Given context in the form of an image and an associated caption summarising the contents of the image, FlipDial learns both to answer questions and put forward questions, capable of generating entire sequences of dialogue (question-answer pairs) which are diverse and relevant to the image. To do this, FlipDial relies on a simple but surprisingly powerful idea: it uses convolutional neural networks (CNNs) to encode entire dialogues directly, implicitly capturing dialogue context, and conditional VAEs to learn the generative model. FlipDial outperforms the state-of-the-art model in the sequential answering task (one-way visual dialogue) on the VisDial dataset by 5 points in Mean Rank using the generated answers. We are the first to extend this paradigm to full two-way visual dialogue, where our model is capable of generating both questions and answers in sequence based on a visual input, for which we propose a set of novel evaluation measures and metrics.

##### Abstract (translated by Google)
我们提出FlipDial，一种视觉对话的生成模型，同时在视觉基础对话中扮演两个参与者的角色。给定图像形式的上下文和总结图像内容的相关标题，FlipDial学习回答问题并提出问题，能够产生多种对话（问答对），这些对话是多样的，与图片。为此，FlipDial依赖于一个简单但令人惊讶的强大理念：它使用卷积神经网络（CNN）直接编码整个对话，隐式捕获对话上下文，以及条件VAE来学习生成模型。 FlipDial在VisDial数据集上的顺序回答任务（单向视觉对话）中使用生成的答案优于平均等级中的5个点，优于最先进的模型。我们是第一个将这种范式扩展到完全双向视觉对话的地方，我们的模型能够根据视觉输入顺序生成问题和答案，为此我们提出了一套新颖的评估指标和指标。

##### URL
[https://arxiv.org/abs/1802.03803](https://arxiv.org/abs/1802.03803)

##### PDF
[https://arxiv.org/pdf/1802.03803](https://arxiv.org/pdf/1802.03803)

