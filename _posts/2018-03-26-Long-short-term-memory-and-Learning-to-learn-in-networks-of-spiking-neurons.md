---
layout: post
title: "Long short-term memory and Learning-to-learn in networks of spiking neurons"
date: 2018-03-26 13:25:43
categories: arXiv_CV
tags: arXiv_CV RNN
author: Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, Wolfgang Maass
mathjax: true
---

* content
{:toc}

##### Abstract
Networks of spiking neurons (SNNs) are frequently studied as models for networks of neurons in the brain, but also as paradigm for novel energy efficient computing hardware. In principle they are especially suitable for computations in the temporal domain, such as speech processing, because their computations are carried out via events in time and space. But so far they have been lacking the capability to preserve information for longer time spans during a computation, until it is updated or needed - like a register of a digital computer. This function is provided to artificial neural networks through Long Short-Term Memory (LSTM) units. We show here that SNNs attain similar capabilities if one includes adapting neurons in the network. Adaptation denotes an increase of the firing threshold of a neuron after preceding firing. A substantial fraction of neurons in the neocortex of rodents and humans has been found to be adapting. It turns out that if adapting neurons are integrated in a suitable manner into the architecture of SNNs, the performance of these enhanced SNNs, which we call LSNNs, for computation in the temporal domain approaches that of artificial neural networks with LSTM-units. In addition, the computing and learning capabilities of LSNNs can be substantially enhanced through learning-to-learn (L2L) methods from machine learning, that have so far been applied primarily to LSTM networks and apparently never to SSNs. This preliminary report on arXiv will be replaced by a more detailed version in about a month.

##### Abstract (translated by Google)
尖峰神经元网络（SNNs）经常作为大脑中神经元网络的模型进行研究，但也是新型节能计算硬件的范例。原则上它们特别适用于时域中的计算，如语音处理，因为它们的计算是通过时间和空间的事件进行的。但是到目前为止，他们一直缺乏在计算过程中保存更长时间信息的能力，直到更新或需要 - 就像数字计算机的注册表一样。该功能通过长期短期记忆（LSTM）单元提供给人工神经网络。我们在这里表明，如果SNNs包括适应网络中的神经元，SNN就会获得类似的能力。适应表示在前次射击后神经元的发放阈值的增加。已经发现大部分啮齿动物和人类新皮层中的神经元正在适应。事实证明，如果适应神经元以适当的方式集成到SNN的体系结构中，那么这些增强的SNN（我们称之为LSNN）在时域中的计算的性能接近具有LSTM单元的人工神经网络。此外，通过机器学习的学习学习（L2L）方法，LSNN的计算和学习能力可以大大提高，迄今为止，这些方法主要应用于LSTM网络，显然永远不适用于SSN。关于arXiv的初步报告将在大约一个月内被更详细的版本所取代。

##### URL
[https://arxiv.org/abs/1803.09574](https://arxiv.org/abs/1803.09574)

##### PDF
[https://arxiv.org/pdf/1803.09574](https://arxiv.org/pdf/1803.09574)

