---
layout: post
title: 'Predicting Visual Features from Text for Image and Video Caption Retrieval'
date: 2017-12-06 02:32:56
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption CNN
author: Jianfeng Dong, Xirong Li, Cees G. M. Snoek
---

* content
{:toc}

##### Abstract
This paper strives to find amidst a set of sentences the one best describing the content of a given image or video. Different from existing works, which rely on a joint subspace for their image and video caption retrieval, we propose to do so in a visual space exclusively. Apart from this conceptual novelty, we contribute \emph{Word2VisualVec}, a deep neural network architecture that learns to predict a visual feature representation from textual input. Example captions are encoded into a textual embedding based on multi-scale sentence vectorization and further transferred into a deep visual feature of choice via a simple multi-layer perceptron. We further generalize Word2VisualVec for video caption retrieval, by predicting from text both 3-D convolutional neural network features as well as a visual-audio representation. Experiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset and the very recent NIST TrecVid challenge for video caption retrieval detail Word2VisualVec's properties, its benefit over textual embeddings, the potential for multimodal query composition and its state-of-the-art results.

##### Abstract (translated by Google)
本文力图在一组句子中找到最能描述给定图像或视频内容的句子。与现有的依靠联合子空间进行图像和视频字幕检索的作品不同，我们建议在视觉空间中独占。除了这个概念上的新颖之外，我们还贡献了一个深度的神经网络体系结构（Word2VisualVec），可以从文本输入中学习预测视觉特征表示。实例字幕被编码成基于多尺度句子矢量化的文本嵌入，并通过简单的多层感知器进一步转换成选择的深层视觉特征。我们进一步概括了Word2VisualVec视频字幕检索，通过预测文本三维卷积神经网络功能以及视觉表达。在Flickr8k，Flickr30k，Microsoft Video Description数据集和最近的NIST TrecVid对视频字幕检索的挑战方面进行的实验详细介绍了Word2VisualVec的属性，它在文本嵌入方面的优势，多模式查询构成的潜力以及最新的结果。

##### URL
[https://arxiv.org/abs/1709.01362](https://arxiv.org/abs/1709.01362)

