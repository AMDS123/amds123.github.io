---
layout: post
title: "DisCoRL: Continual Reinforcement Learning via Policy Distillation"
date: 2019-07-11 09:12:42
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Represenation_Learning
author: Ren&#xe9; Traor&#xe9;, Hugo Caselles-Dupr&#xe9;, Timoth&#xe9;e Lesort, Te Sun, Guanghang Cai, Natalia D&#xed;az-Rodr&#xed;guez, David Filliat
mathjax: true
---

* content
{:toc}

##### Abstract
In multi-task reinforcement learning there are two main challenges: at training time, the ability to learn different policies with a single model; at test time, inferring which of those policies applying without an external signal. In the case of continual reinforcement learning a third challenge arises: learning tasks sequentially without forgetting the previous ones. In this paper, we tackle these challenges by proposing DisCoRL, an approach combining state representation learning and policy distillation. We experiment on a sequence of three simulated 2D navigation tasks with a 3 wheel omni-directional robot. Moreover, we tested our approach's robustness by transferring the final policy into a real life setting. The policy can solve all tasks and automatically infer which one to run.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.05855](http://arxiv.org/abs/1907.05855)

##### PDF
[http://arxiv.org/pdf/1907.05855](http://arxiv.org/pdf/1907.05855)

