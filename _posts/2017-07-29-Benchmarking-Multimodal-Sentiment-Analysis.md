---
layout: post
title: "Benchmarking Multimodal Sentiment Analysis"
date: 2017-07-29 16:40:50
categories: arXiv_CL
tags: arXiv_CL Sentiment Face CNN Recognition
author: Erik Cambria, Devamanyu Hazarika, Soujanya Poria, Amir Hussain, R.B.V. Subramaanyam
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a framework for multimodal sentiment analysis and emotion recognition using convolutional neural network-based feature extraction from text and visual modalities. We obtain a performance improvement of 10% over the state of the art by combining visual, text and audio features. We also discuss some major issues frequently ignored in multimodal sentiment analysis research: the role of speaker-independent models, importance of the modalities and generalizability. The paper thus serve as a new benchmark for further research in multimodal sentiment analysis and also demonstrates the different facets of analysis to be considered while performing such tasks.

##### Abstract (translated by Google)
我们提出了一个基于卷积神经网络的文本和视觉形式特征提取的多模态情感分析和情感识别的框架。通过结合视觉，文本和音频功能，我们获得比现有技术高10​​％的性能提升。我们还讨论了多模态情感分析研究中经常被忽略的一些主要问题：独立于说话者的模型的作用，模态的重要性和一般性。因此，本文作为进一步研究多模态情绪分析的新基准，同时也展示了执行此类任务时需要考虑的不同分析方面。

##### URL
[https://arxiv.org/abs/1707.09538](https://arxiv.org/abs/1707.09538)

##### PDF
[https://arxiv.org/pdf/1707.09538](https://arxiv.org/pdf/1707.09538)

