---
layout: post
title: "Learning towards Minimum Hyperspherical Energy"
date: 2018-05-23 17:34:47
categories: arXiv_CV
tags: arXiv_CV Regularization
author: Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, Le Song
mathjax: true
---

* content
{:toc}

##### Abstract
Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as even as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our method, by showing the superior performance with MHE regularization.

##### Abstract (translated by Google)
神经网络是一类强大的非线性函数，可以在各种应用程序上进行端对端培训。虽然许多神经网络的过度参数化特性使得适应复杂函数的能力和处理具有挑战性任务的强大代表能力，但它也导致高度相关的神经元，这会伤害泛化能力并招致不必要的计算成本。因此，如何规范网络以避免不必要的表示冗余成为一个重要问题。为此，我们从物理学中一个众所周知的问题--Thomson问题中汲取灵感，在这个问题中，人们试图找到一种状态，尽可能地将N个电子分布在一个单位球上，并且具有最小势能。根据这种直觉，我们将冗余正则化问题简化为通用能量最小化，并提出了一个最小超球能量（MHE）目标作为神经网络的一般正则化。我们还提出了一些MHE的新颖变体，并从理论的角度提供了一些见解。最后，我们将具有MHE正则化的网络应用于几项具有挑战性的任务。通过展示MHE正则化的优越性能，大量实验证明了我们方法的有效性。

##### URL
[http://arxiv.org/abs/1805.09298](http://arxiv.org/abs/1805.09298)

##### PDF
[http://arxiv.org/pdf/1805.09298](http://arxiv.org/pdf/1805.09298)

