---
layout: post
title: "The Importance of Being Recurrent for Modeling Hierarchical Structure"
date: 2018-08-28 04:40:49
categories: arXiv_CL
tags: arXiv_CL Attention RNN Language_Model
author: Ke Tran, Arianna Bisazza, Christof Monz
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks such as language modeling (Linzen et al., 2016) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures---recurrent versus non-recurrent---with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose.

##### Abstract (translated by Google)
最近的研究表明，递归神经网络（RNN）可以在训练时隐式捕获和利用分层信息，以解决常见的自然语言处理任务，如语言建模（Linzen等，2016）和神经机器翻译（Shi等，2016） ）。相比之下，尽管在许多NLP任务中取得了成功，但使用非递归神经网络对结构化数据建模的能力却很少得到关注（Gehring等，2017; Vaswani等，2017）。在这项工作中，我们比较了两种架构 - 经常性与非经常性 - 关于它们对层次结构建模的能力，并发现重现性对于这个目的确实很重要。

##### URL
[http://arxiv.org/abs/1803.03585](http://arxiv.org/abs/1803.03585)

##### PDF
[http://arxiv.org/pdf/1803.03585](http://arxiv.org/pdf/1803.03585)

