---
layout: post
title: "Deep CT to MR Synthesis using Paired and Unpaired Data"
date: 2018-09-03 13:33:28
categories: arXiv_CV
tags: arXiv_CV Adversarial Sparse Segmentation GAN Quantitative
author: Cheng-Bin Jin, Hakil Kim, Wonmo Jung, Seongsu Joo, Ensik Park, Ahn Young Saem, In Ho Han, Jae Il Lee, Xuenan Cui
mathjax: true
---

* content
{:toc}

##### Abstract
MR imaging will play a very important role in radiotherapy treatment planning for segmentation of tumor volumes and organs. However, the use of MR-based radiotherapy is limited because of the high cost and the increased use of metal implants such as cardiac pacemakers and artificial joints in aging society. To improve the accuracy of CT-based radiotherapy planning, we propose a synthetic approach that translates a CT image into an MR image using paired and unpaired training data. In contrast to the current synthetic methods for medical images, which depend on sparse pairwise-aligned data or plentiful unpaired data, the proposed approach alleviates the rigid registration challenge of paired training and overcomes the context-misalignment problem of the unpaired training. A generative adversarial network was trained to transform 2D brain CT image slices into 2D brain MR image slices, combining adversarial loss, dual cycle-consistent loss, and voxel-wise loss. The experiments were analyzed using CT and MR images of 202 patients. Qualitative and quantitative comparisons against independent paired training and unpaired training methods demonstrate the superiority of our approach.

##### Abstract (translated by Google)
MR成像将在肿瘤体积和器官分割的放射治疗计划中发挥非常重要的作用。然而，由于高成本和金属植入物（例如心脏起搏器和人工关节在老龄化社会中）的使用增加，使用基于MR的放射疗法是有限的。为了提高基于CT的放射治疗计划的准确性，我们提出了一种合成方法，该方法使用成对和不成对的训练数据将CT图像转换成MR图像。与用于医学图像的当前合成方法（其依赖于稀疏成对比对数据或大量未配对数据）相比，所提出的方法减轻了配对训练的刚性配准挑战并且克服了未配对训练的上下文未对准问题。训练生成对抗网络，将2D脑CT图像切片转换为2D脑MR图像切片，结合对抗性损失，双周期一致性损失和体素消失。使用202名患者的CT和MR图像分析实验。针对独立配对培训和不成对培训方法的定性和定量比较证明了我们的方法的优越性。

##### URL
[http://arxiv.org/abs/1805.10790](http://arxiv.org/abs/1805.10790)

##### PDF
[http://arxiv.org/pdf/1805.10790](http://arxiv.org/pdf/1805.10790)

