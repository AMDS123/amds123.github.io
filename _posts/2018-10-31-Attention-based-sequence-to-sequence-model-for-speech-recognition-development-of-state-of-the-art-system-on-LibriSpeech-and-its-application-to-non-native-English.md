---
layout: post
title: "Attention-based sequence-to-sequence model for speech recognition: development of state-of-the-art system on LibriSpeech and its application to non-native English"
date: 2018-10-31 03:10:37
categories: arXiv_CL
tags: arXiv_CL Attention Speech_Recognition Recognition
author: Yan Yin, Ramon Prieto, Bin Wang, Jianwei Zhou, Yiwei Gu, Yang Liu, Hui Lin
mathjax: true
---

* content
{:toc}

##### Abstract
Recent research has shown that attention-based sequence-tosequence models such as Listen, Attend, and Spell (LAS) yield comparable results to state-of-the-art ASR systems on various tasks. In this paper, we describe the development of such a system and demonstrate its performance on two tasks: first we achieve a new state-of-the-art word error rate of 3.43% on the test clean subset of LibriSpeech English data; second on non-native English speech, including both read speech and spontaneous speech, we obtain very competitive results compared to a conventional system built with the most updated Kaldi recipe.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.13088](http://arxiv.org/abs/1810.13088)

##### PDF
[http://arxiv.org/pdf/1810.13088](http://arxiv.org/pdf/1810.13088)

