---
layout: post
title: "Wide Residual Networks"
date: 2017-06-14 06:06:48
categories: arXiv_CV
tags: arXiv_CV
author: Sergey Zagoruyko, Nikos Komodakis
mathjax: true
---

* content
{:toc}

##### Abstract
Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL

##### Abstract (translated by Google)
显示深度残留网络能够扩展到数千层，并且仍然具有改善的性能。然而，精度提高百分比的每一部分成本几乎是层数的两倍，因此训练非常深的残余网络具有减少特征重用的问题，这使得这些网络训练非常慢。针对这些问题，本文对ResNet模块的体系结构进行了详细的实验研究，在此基础上提出了一种降低深度，增加残余网络宽度的新颖架构。我们将所得到的网络结构称为广泛的残余网络（WRNs），并且表明这些网络远远优于它们常用的瘦和非常深的网络。例如，我们证明，即使是一个简单的16层深的残余网络，其精度和效率也要优于之前的所有深度残余网络，包括千层深网络，在CIFAR，SVHN上获得最新的最新成果，COCO，以及对ImageNet的重大改进。我们的代码和模型可在此https网址获得

##### URL
[https://arxiv.org/abs/1605.07146](https://arxiv.org/abs/1605.07146)

##### PDF
[https://arxiv.org/pdf/1605.07146](https://arxiv.org/pdf/1605.07146)

