---
layout: post
title: "Video Summarization with Attention-Based Encoder-Decoder Networks"
date: 2017-08-31 03:04:17
categories: arXiv_CV
tags: arXiv_CV Attention Summarization RNN
author: Zhong Ji, Kailin Xiong, Yanwei Pang, Xuelong Li
mathjax: true
---

* content
{:toc}

##### Abstract
This paper addresses the problem of supervised video summarization by formulating it as a sequence-to-sequence learning problem, where the input is a sequence of original video frames, the output is a keyshot sequence. Our key idea is to learn a deep summarization network with attention mechanism to mimic the way of selecting the keyshots of human. To this end, we propose a novel video summarization framework named Attentive encoder-decoder networks for Video Summarization (AVS), in which the encoder uses a Bidirectional Long Short-Term Memory (BiLSTM) to encode the contextual information among the input video frames. As for the decoder, two attention-based LSTM networks are explored by using additive and multiplicative objective functions, respectively. Extensive experiments are conducted on three video summarization benchmark datasets, i.e., SumMe, TVSum, and YouTube. The results demonstrate the superiority of the proposed AVS-based approaches against the state-of-the-art approaches, with remarkable improvements from 3% to 11% on the three datasets, respectively.

##### Abstract (translated by Google)
本文针对监督视频摘要的问题，将其视为序列到序列的学习问题，输入是原始视频帧序列，输出是keyhot序列。我们的主要思想是学习一个具有注意机制的深层次的总结网络来模拟人类选择的方式。为此，我们提出了一种新颖的视频摘要框架，称为视频摘要（Attentive）解码器网络（AVS），其中编码器使用双向长期短期存储器（BiLSTM）对输入视频帧之间的上下文信息进行编码。对于解码器，分别使用加性和乘性目标函数来研究两个基于注意力的LSTM网络。对三个视频摘要基准数据集（即SumMe，TVSum和YouTube）进行了大量实验。结果证明了所提出的基于AVS的方法与现有技术方法的优越性，在三个数据集上分别从3％到11％显着提高。

##### URL
[https://arxiv.org/abs/1708.09545](https://arxiv.org/abs/1708.09545)

##### PDF
[https://arxiv.org/pdf/1708.09545](https://arxiv.org/pdf/1708.09545)

