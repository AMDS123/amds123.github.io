---
layout: post
title: "Deep Recurrent Convolutional Neural Network: Improving Performance For Speech Recognition"
date: 2016-12-27 04:53:56
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition CNN Deep_Learning Language_Model Recognition
author: Zewang Zhang, Zheng Sun, Jiaqi Liu, Jingwen Chen, Zhao Huo, Xiao Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
A deep learning approach has been widely applied in sequence modeling problems. In terms of automatic speech recognition (ASR), its performance has significantly been improved by increasing large speech corpus and deeper neural network. Especially, recurrent neural network and deep convolutional neural network have been applied in ASR successfully. Given the arising problem of training speed, we build a novel deep recurrent convolutional network for acoustic modeling and then apply deep residual learning to it. Our experiments show that it has not only faster convergence speed but better recognition accuracy over traditional deep convolutional recurrent network. In the experiments, we compare the convergence speed of our novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks. With faster convergence speed, our novel deep recurrent convolutional networks can reach the comparable performance. We further show that applying deep residual learning can boost the convergence speed of our novel deep recurret convolutional networks. Finally, we evaluate all our experimental networks by phoneme error rate (PER) with our proposed bidirectional statistical n-gram language model. Our evaluation results show that our newly proposed deep recurrent convolutional network applied with deep residual learning can reach the best PER of 17.33\% with the fastest convergence speed on TIMIT database. The outstanding performance of our novel deep recurrent convolutional neural network with deep residual learning indicates that it can be potentially adopted in other sequential problems.

##### Abstract (translated by Google)
深度学习方法已被广泛应用于序列建模问题。在自动语音识别（ASR）方面，通过增加大的语音语料和更深的神经网络，其性能得到了显着的提高。特别是递归神经网络和深度卷积神经网络已成功应用于ASR。鉴于训练速度的问题，我们构建了一个新的深度回归卷积网络进行声学建模，然后对其进行深度残差学习。我们的实验表明，与传统的深度卷积循环网络相比，它不仅收敛速度更快，而且识别精度更高。在实验中，我们比较了新颖的深回归卷积网络和传统的深卷积回归网络的收敛速度。以更快的收敛速度，我们的新颖的深循环卷积网络可以达到相当的性能。我们进一步表明，应用深度残差学习可以提高我们的新型深度递归卷积网络的收敛速度。最后，我们用我们提出的双向统计n元语言模型通过音素错误率（PER）来评估我们所有的实验网络。我们的评估结果表明，我们新提出的应用了深度残差学习的深循环卷积网络能够达到17.33％的最佳PER值，TIMIT数据库的收敛速度最快。我们的具有深度残差学习的新型深回归卷积神经网络的出色表现表明，它可能在其他序列问题中可能被采用。

##### URL
[https://arxiv.org/abs/1611.07174](https://arxiv.org/abs/1611.07174)

##### PDF
[https://arxiv.org/pdf/1611.07174](https://arxiv.org/pdf/1611.07174)

