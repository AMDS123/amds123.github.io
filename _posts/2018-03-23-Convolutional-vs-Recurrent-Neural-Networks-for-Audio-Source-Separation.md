---
layout: post
title: "Convolutional vs. Recurrent Neural Networks for Audio Source Separation"
date: 2018-03-23 01:26:39
categories: arXiv_SD
tags: arXiv_SD CNN RNN
author: Shariq Mobin, Brian Cheung, Bruno Olshausen
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work has shown that recurrent neural networks can be trained to separate individual speakers in a sound mixture with high fidelity. Here we explore convolutional neural network models as an alternative and show that they achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize and compare the robustness and ability of these different approaches to generalize under three different test conditions: longer time sequences, the addition of intermittent noise, and different datasets not seen during training. For the last condition, we create a new dataset, RealTalkLibri, to test source separation in real-world environments. We show that the acoustics of the environment have significant impact on the structure of the waveform and the overall performance of neural network models, with the convolutional model showing superior ability to generalize to new environments. The code for our study is available at this https URL

##### Abstract (translated by Google)
最近的研究表明，可以训练递归神经网络以高保真度的混音方式将个别讲话者分开。在这里，我们探索卷积神经网络模型作为替代方案，并表明他们用较少数量级的参数实现了最新的结果。我们还刻画和比较了这三种不同测试条件下这些不同测试方法的鲁棒性和能力：更长的时间序列，间歇性噪声的添加以及训练期间未看到的不同数据集。对于最后一个条件，我们创建一个新的数据集RealTalkLibri，以测试真实环境中的源分离。我们表明，环境的声学对波形的结构和神经网络模型的整体性能具有显着的影响，卷积模型显示出优越的推广到新环境的能力。我们研究的代码可在此https网址获得

##### URL
[https://arxiv.org/abs/1803.08629](https://arxiv.org/abs/1803.08629)

##### PDF
[https://arxiv.org/pdf/1803.08629](https://arxiv.org/pdf/1803.08629)

