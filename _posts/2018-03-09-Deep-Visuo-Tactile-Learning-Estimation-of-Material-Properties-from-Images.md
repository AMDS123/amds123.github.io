---
layout: post
title: "Deep Visuo-Tactile Learning: Estimation of Material Properties from Images"
date: 2018-03-09 09:40:33
categories: arXiv_RO
tags: arXiv_RO Face Classification Recognition
author: Kuniyuki Takahashi, Jethro Tan
mathjax: true
---

* content
{:toc}

##### Abstract
Estimation of materials properties, such as softness or roughness from visual perception is an essential factor in deciding our way of interaction with our environment in e.g., object manipulation tasks or walking. In this research, we propose a method for deep visuo-tactile learning in which we train a encoder-decoder network with an intermediate layer in an unsupervised manner with images as input and tactile sequences as output. Materials properties are then represented in the intermediate layer as a continuous feature space and are estimated from image information. Unlike past studies utilizing tactile sensors focusing on classification for object recognition or recognizing material properties, our method does not require manually designing class labels or annotation, does not cause unknown objects to be classified into known discrete classes, and can be used without a tactile sensor after training. To collect data for training, we have attached a uSkin tactile sensor and a camera to the end-effector of a Sawyer robot to stroke surfaces of 21 different material surfaces. Our results after training show that features are indeed expressed continuously, and that our method is able to handle unknown objects in its feature space.

##### Abstract (translated by Google)
从视觉上评估材料属性（例如柔软度或粗糙度）是决定我们与我们的环境在例如对象操纵任务或行走中的交互方式的重要因素。在这项研究中，我们提出了一种深度视觉触觉学习方法，在这种方法中，我们以无监督的方式训练带有中间层的编码器 - 解码器网络，图像作为输入，触觉序列作为输出。材料属性然后在中间层中被表示为连续的特征空间并且从图像信息估计。与以往使用触觉传感器的研究不同，这些传感器专注于物体识别的分类或识别材料属性，我们的方法不需要手动设计类别标签或注释，不会导致将未知对象分类为已知的离散类别，并且可以在没有触觉传感器的情况下使用训练结束后。为了收集训练数据，我们将一个uSkin触觉传感器和一个摄像头连接到Sawyer机器人的末端执行器上，以抚摸21种不同材料表面的表面。我们在训练后的结果表明，特征确实是连续表达的，并且我们的方法能够处理其特征空间中的未知对象。

##### URL
[http://arxiv.org/abs/1803.03435](http://arxiv.org/abs/1803.03435)

##### PDF
[http://arxiv.org/pdf/1803.03435](http://arxiv.org/pdf/1803.03435)

