---
layout: post
title: "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"
date: 2017-10-05 13:04:51
categories: arXiv_CV
tags: arXiv_CV Segmentation Semantic_Segmentation Deep_Learning
author: Alex Kendall, Yarin Gal
mathjax: true
---

* content
{:toc}

##### Abstract
There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.

##### Abstract (translated by Google)
有两种主要的不确定性可以模拟。警戒不确定性捕捉观察中固有的噪音。另一方面，认知的不确定性也说明了模型中的不确定性 - 在给出足够数据的情况下可以解释的不确定性。传统上，在计算机视觉中对认知不确定性进行建模是困难的，但是现在可以使用新的贝叶斯深度学习工具。我们研究了视觉任务的贝叶斯深度学习模型中认识与任意不确定性建模的好处。为此，我们提出一个贝叶斯深度学习框架结合输入依赖任意不确定性与认知不确定性。我们在每个像素语义分割和深度回归任务框架下研究模型。此外，我们明确的不确定性公式导致这些任务的新损失函数，这可以被解释为学习衰减。这使得噪声数据的损失更加强劲，同时在分割和深度回归基准测试中提供了最新的最新结果。

##### URL
[https://arxiv.org/abs/1703.04977](https://arxiv.org/abs/1703.04977)

##### PDF
[https://arxiv.org/pdf/1703.04977](https://arxiv.org/pdf/1703.04977)

