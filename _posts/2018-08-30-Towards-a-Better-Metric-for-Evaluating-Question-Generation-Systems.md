---
layout: post
title: "Towards a Better Metric for Evaluating Question Generation Systems"
date: 2018-08-30 09:12:19
categories: arXiv_CL
tags: arXiv_CL Knowledge_Graph Knowledge Relation
author: Preksha Nema, Mitesh M. Khapra
mathjax: true
---

* content
{:toc}

##### Abstract
There has always been criticism for using $n$-gram based similarity metrics, such as BLEU, NIST, \textit{etc}, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, \textit{etc}. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on \textit{answerability} of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, \textit{etc}. In this work, we show that current automatic evaluation metrics based on $n$-gram similarity do not always correlate well with human judgments about \textit{answerability} of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture \textit{answerability} and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available at https://github.com/PrekshaNema25/Answerability-Metric.

##### Abstract (translated by Google)
一直有批评使用基于$ n $ -gram的相似性度量，例如BLEU，NIST，\ textit {etc}来评估NLG系统的性能。但是，这些指标仍然很受欢迎，最近被用于评估系统的性能，这些系统自动生成来自文档，知识图，图像，\ textit {等}的问题。鉴于对此类自动问题生成（AQG）系统的兴趣日益增加，客观地检查这些指标是否适合此任务非常重要。特别重要的是，通过优先考虑包含所有相关信息的问题，例如问题类型（Wh类型），实体，关系，验证用于评估AQG系统的这些度量是否集中于生成问题的\ textit {answerability}。 textit {等}。在这项工作中，我们表明，基于$ n $ -gram相似性的当前自动评估指标并不总是与人们对问题的\ textit {answerability}的判断相关联。为了缓解这个问题，作为迈向更好的AQG评估指标的第一步，我们引入了一个评分函数来捕获\ textit {answerability}，并表明当这个评分函数与现有指标集成时，它们与人类判断相关性更好。作为此项工作的一部分开发的脚本和数据可通过https://github.com/PrekshaNema25/Answerability-Metric公开获取。

##### URL
[http://arxiv.org/abs/1808.10192](http://arxiv.org/abs/1808.10192)

##### PDF
[http://arxiv.org/pdf/1808.10192](http://arxiv.org/pdf/1808.10192)

