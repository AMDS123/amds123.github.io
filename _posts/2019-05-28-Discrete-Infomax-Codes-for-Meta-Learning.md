---
layout: post
title: "Discrete Infomax Codes for Meta-Learning"
date: 2019-05-28 07:38:35
categories: arXiv_CV
tags: arXiv_CV Classification
author: Yoonho Lee, Wonjae Kim, Seungjin Choi
mathjax: true
---

* content
{:toc}

##### Abstract
Learning compact discrete representations of data is itself a key task in addition to facilitating subsequent processing. It is also relevant to meta-learning since a latent representation shared across relevant tasks enables a model to adapt to new tasks quickly. In this paper, we present a method for learning a stochastic encoder that yields discrete p-way codes of length d by maximizing the mutual information between representations and labels. We show that previous loss functions for deep metric learning are approximations to this information-theoretic objective function. Our model, Discrete InfoMax Codes (DIMCO), learns to produce a short representation of data that can be used to classify classes with few labeled examples. Our analysis shows that using shorter codes reduces overfitting in the context of few-shot classification. Experiments show that DIMCO requires less memory (i.e., code length) for performance similar to previous methods and that our method is particularly effective when the training dataset is small.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1905.11656](https://arxiv.org/abs/1905.11656)

##### PDF
[https://arxiv.org/pdf/1905.11656](https://arxiv.org/pdf/1905.11656)

