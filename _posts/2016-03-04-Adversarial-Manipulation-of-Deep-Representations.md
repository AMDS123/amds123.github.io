---
layout: post
title: "Adversarial Manipulation of Deep Representations"
date: 2016-03-04 20:21:24
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet
mathjax: true
---

* content
{:toc}

##### Abstract
We show that the representation of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels, while we concentrate on the internal layers of DNN representations. In this way our new class of adversarial images differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, one from a different class, bearing little if any apparent similarity to the input; they appear generic and consistent with the space of natural images. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.

##### Abstract (translated by Google)
我们表明，深度神经网络（DNN）中的图像的表示可以被操纵以模仿其他自然图像的那些，只有对原始图像的微小，不可察觉的扰动。先前的用于生成敌对图像的方法集中于设计用于产生错误类别标签的图像扰动，而我们专注于DNN表示的内部层。通过这种方式，我们新一类的对抗性图像在质量上与其他的不同。虽然对手在感知上与一幅图像相似，但其内部表现看起来与另一幅不同类别的图像非常相似，与其他图像几乎没有任何明显的相似性。它们似乎是通用的，并与自然图像的空间一致。这种现象引起了关于DNN表示的问题，以及自然图像本身的性质。

##### URL
[https://arxiv.org/abs/1511.05122](https://arxiv.org/abs/1511.05122)

##### PDF
[https://arxiv.org/pdf/1511.05122](https://arxiv.org/pdf/1511.05122)

