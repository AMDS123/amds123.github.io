---
layout: post
title: "DeepVO: A Deep Learning approach for Monocular Visual Odometry"
date: 2016-11-18 13:41:22
categories: arXiv_CV
tags: arXiv_CV Object_Detection Segmentation Tracking CNN Image_Classification Classification Deep_Learning Detection SLAM
author: Vikram Mohanty, Shubh Agrawal, Shaswat Datta, Arna Ghosh, Vishnu Dutt Sharma, Debashish Chakravarty
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Learning based techniques have been adopted with precision to solve a lot of standard computer vision problems, some of which are image classification, object detection and segmentation. Despite the widespread success of these approaches, they have not yet been exploited largely for solving the standard perception related problems encountered in autonomous navigation such as Visual Odometry (VO), Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM). This paper analyzes the problem of Monocular Visual Odometry using a Deep Learning-based framework, instead of the regular 'feature detection and tracking' pipeline approaches. Several experiments were performed to understand the influence of a known/unknown environment, a conventional trackable feature and pre-trained activations tuned for object classification on the network's ability to accurately estimate the motion trajectory of the camera (or the vehicle). Based on these observations, we propose a Convolutional Neural Network architecture, best suited for estimating the object's pose under known environment conditions, and displays promising results when it comes to inferring the actual scale using just a single camera in real-time.

##### Abstract (translated by Google)
基于深度学习的技术已经被精确地采用来解决许多标准的计算机视觉问题，其中一些是图像分类，目标检测和分割。尽管这些方法取得了广泛的成功，但它们还没有被广泛用于解决在自主导航中遇到的诸如视觉内测（VO），运动结构（SfM）和同时定位映射（SLAM）等标准感知问题。本文使用基于深度学习的框架来分析单目视觉测量的问题，而不是常规的“特征检测和跟踪”管道方法。为了理解已知的/未知的环境，传统的可跟踪特征和针对网络精确估计相机（或车辆）的运动轨迹的能力的对象分类而调整的预先训练的激活，进行了若干实验。基于这些观察结果，我们提出了一种卷积神经网络架构，最适合估计已知环境条件下的物体姿态，并且在实时使用单个相机来推断实际比例时显示出有希望的结果。

##### URL
[https://arxiv.org/abs/1611.06069](https://arxiv.org/abs/1611.06069)

##### PDF
[https://arxiv.org/pdf/1611.06069](https://arxiv.org/pdf/1611.06069)

