---
layout: post
title: "FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension"
date: 2018-02-04 04:56:45
categories: arXiv_AI
tags: arXiv_AI Adversarial Attention Embedding
author: Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, Weizhu Chen
mathjax: true
---

* content
{:toc}

##### Abstract
This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of "history of word" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the "history of word" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.

##### Abstract (translated by Google)
本文介绍了一种名为FusionNet的新型神经网络结构，从三个角度扩展了现有的关注方法。首先提出了“词语史”的一个新概念，即从最低的词层次嵌入到最高的语义层次表征的关注信息。其次，它引入了一个更好的利用“词的历史”概念的改进的注意力评分函数。第三，提出了一个充分认识的多层次关注机制来捕获一个文本中的完整信息（如问题），并在其对应的层次（如上下文或段落）中进行挖掘。我们将FusionNet应用于斯坦福大学问答数据集（SQUAD），并且在撰写本文时（2017年10月4日），在SQUAD官方排行榜上实现了单一和合奏模式的第一名。同时，我们用两个敌对的SQuAD数据集验证了FusionNet的泛化，并在两个数据集上建立了新的最新技术：在AddSent上，FusionNet将最佳F1度量从46.6％提高到51.4％。在AddOneSent上，FusionNet将最好的F1度量从56.0％提升到了60.7％。

##### URL
[http://arxiv.org/abs/1711.07341](http://arxiv.org/abs/1711.07341)

##### PDF
[http://arxiv.org/pdf/1711.07341](http://arxiv.org/pdf/1711.07341)

