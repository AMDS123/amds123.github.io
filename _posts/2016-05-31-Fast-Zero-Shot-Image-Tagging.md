---
layout: post
title: "Fast Zero-Shot Image Tagging"
date: 2016-05-31 18:39:48
categories: arXiv_CV
tags: arXiv_CV Relation
author: Yang Zhang, Boqing Gong, Mubarak Shah
mathjax: true
---

* content
{:toc}

##### Abstract
The well-known word analogy experiments show that the recent word vectors capture fine-grained linguistic regularities in words by linear vector offsets, but it is unclear how well the simple vector offsets can encode visual regularities over words. We study a particular image-word relevance relation in this paper. Our results show that the word vectors of relevant tags for a given image rank ahead of the irrelevant tags, along a principal direction in the word vector space. Inspired by this observation, we propose to solve image tagging by estimating the principal direction for an image. Particularly, we exploit linear mappings and nonlinear deep neural networks to approximate the principal direction from an input image. We arrive at a quite versatile tagging model. It runs fast given a test image, in constant time w.r.t.\ the training set size. It not only gives superior performance for the conventional tagging task on the NUS-WIDE dataset, but also outperforms competitive baselines on annotating images with previously unseen tags

##### Abstract (translated by Google)
众所周知的词类比实验表明，最近的单词向量通过线性向量偏移量来捕捉单词中的细粒度语言规则，但是不清楚简单向量偏移量能够如何将单词上的视觉规则编码。本文研究了一个特定的图像词相关关系。我们的结果显示，给定图像的相关标签的单词向量在不相关标签之前沿着单词向量空间中的主方向排列。受此观察的启发，我们提出通过估计图像的主方向来解决图像标记。特别是，我们利用线性映射和非线性深度神经网络来逼近输入图像的主方向。我们到达了一个相当全面的标签模型。它在给定测试图像的情况下运行得很快，在训练集大小的恒定时间内运行。它不仅为NUS-WIDE数据集上的常规标记任务提供了卓越的性能，而且还优于以前看不见的标签标注图像的竞争基线

##### URL
[https://arxiv.org/abs/1605.09759](https://arxiv.org/abs/1605.09759)

##### PDF
[https://arxiv.org/pdf/1605.09759](https://arxiv.org/pdf/1605.09759)

