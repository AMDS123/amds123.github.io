---
layout: post
title: "Control of Memory, Active Perception, and Action in Minecraft"
date: 2016-05-30 07:40:13
categories: arXiv_CV
tags: arXiv_CV Reinforcement_Learning
author: Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.

##### Abstract (translated by Google)
在本文中，我们在Minecraft（一个灵活的3D世界）中引入了一套新的强化学习（RL）任务。然后，我们使用这些任务来系统地比较和对比现有的深度强化学习（DRL）体系结构与我们新的基于内存的DRL体系结构。这些任务旨在以可控制的方式强调对RL方法带来挑战的问题，包括部分可观察性（由于第一人称视觉观察），延迟奖励，高维视觉观察以及需要使用主动感知正确的态度，以便在任务中表现良好。虽然这些任务在概念上很容易描述，但由于同时具有所有这些挑战，所以目前的DRL体系结构很难实现。此外，我们评估架构在未经培训的环境中的泛化性能。实验结果表明，我们的新体系结构比现有的DRL体系结构更好地推广到不可见的环境。

##### URL
[https://arxiv.org/abs/1605.09128](https://arxiv.org/abs/1605.09128)

##### PDF
[https://arxiv.org/pdf/1605.09128](https://arxiv.org/pdf/1605.09128)

