---
layout: post
title: "Representation Learning on Large and Small Data"
date: 2017-07-25 04:14:18
categories: arXiv_CV
tags: arXiv_CV Represenation_Learning Optimization Classification Deep_Learning
author: Chun-Nan Chou, Chuen-Kai Shie, Fu-Chieh Chang, Jocelyn Chang, Edward Y. Chang
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning owes its success to three key factors: scale of data, enhanced models to learn representations from data, and scale of computation. This book chapter presented the importance of the data-driven approach to learn good representations from both big data and small data. In terms of big data, it has been widely accepted in the research community that the more data the better for both representation and classification improvement. The question is then how to learn representations from big data, and how to perform representation learning when data is scarce. We addressed the first question by presenting CNN model enhancements in the aspects of representation, optimization, and generalization. To address the small data challenge, we showed transfer representation learning to be effective. Transfer representation learning transfers the learned representation from a source domain where abundant training data is available to a target domain where training data is scarce. Transfer representation learning gave the OM and melanoma diagnosis modules of our XPRIZE Tricorder device (which finished $2^{nd}$ out of $310$ competing teams) a significant boost in diagnosis accuracy.

##### Abstract (translated by Google)
深度学习的成功要归功于三个关键因素：数据规模，用数据学习表示的增强模型以及计算规模。本书介绍了数据驱动方法从大数据和小数据学习良好表示的重要性。在大数据方面，研究界普遍认为数据越多，表示和分类改进越好。问题是如何从大数据中学习表示，以及在数据稀缺时如何进行表示学习。我们通过在表示，优化和泛化方面提出CNN模型增强来解决第一个问题。为了解决小数据挑战，我们展示了转移表示学习的有效性。转移表示学习将学习表示从具有丰富训练数据的源域转移到训练数据稀缺的目标域。转移表示学习为我们的XPRIZE Tricorder设备（在$ 310 $竞争团队中完成$ 2 ^ {nd} $）的OM和黑色素瘤诊断模块显着提高了诊断的准确性。

##### URL
[https://arxiv.org/abs/1707.09873](https://arxiv.org/abs/1707.09873)

##### PDF
[https://arxiv.org/pdf/1707.09873](https://arxiv.org/pdf/1707.09873)

