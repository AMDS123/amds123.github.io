---
layout: post
title: "Intriguing Properties of Randomly Weighted Networks: Generalizing While Learning Next to Nothing"
date: 2018-02-02 20:53:31
categories: arXiv_AI
tags: arXiv_AI CNN
author: Amir Rosenfeld, John K. Tsotsos
mathjax: true
---

* content
{:toc}

##### Abstract
Training deep neural networks results in strong learned representations that show good generalization capabilities. In most cases, training involves iterative modification of all weights inside the network via back-propagation. In Extreme Learning Machines, it has been suggested to set the first layer of a network to fixed random values instead of learning it. In this paper, we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network, allowing only a small portion of the weights to be learned. As our experiments show, fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them. The implications of this intriguing property of deep neural networks are discussed and we suggest ways to harness it to create more robust representations.

##### Abstract (translated by Google)
训练深度神经网络导致强大的学习表现，表现出良好的泛化能力。在大多数情况下，训练涉及通过反向传播迭代修改网络内的所有权重。在极限学习机器中，有人建议将网络的第一层设置为固定的随机值而不是学习它。在本文中，我们建议采取这种方法更进一步，修复深层卷积神经网络的几乎所有层，只允许学习一小部分权重。正如我们的实验所表明的那样，即使固定网络的大部分参数也经常导致性能与学习所有这些参数的性能相当。讨论深度神经网络这个有趣的特性的含义，并且建议如何利用它来创建更强大的表示。

##### URL
[http://arxiv.org/abs/1802.00844](http://arxiv.org/abs/1802.00844)

##### PDF
[http://arxiv.org/pdf/1802.00844](http://arxiv.org/pdf/1802.00844)

