---
layout: post
title: "Gaussian Mixture Embeddings for Multiple Word Prototypes"
date: 2015-11-19 16:46:49
categories: arXiv_CL
tags: arXiv_CL Embedding Relation
author: Xinchi Chen, Xipeng Qiu, Jingxiang Jiang, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, word representation has been increasingly focused on for its excellent properties in representing the word semantics. Previous works mainly suffer from the problem of polysemy phenomenon. To address this problem, most of previous models represent words as multiple distributed vectors. However, it cannot reflect the rich relations between words by representing words as points in the embedded space. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to learn the Gaussian mixture embeddings for words based on skip-gram framework. Each word can be regarded as a gaussian mixture distribution in the embedded space, and each gaussian component represents a word sense. Since the number of senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense number of words during training. Experiments on four benchmarks show the effectiveness of our proposed model.

##### Abstract (translated by Google)
最近，单词表示已经越来越多地集中在表示单词语义的优良属性上。以往的作品主要存在多义现象的问题。为了解决这个问题，大多数以前的模型将单词表示为多个分布向量。然而，它不能通过在嵌入式空间中表现单词作为点来反映单词之间的丰富关系。在本文中，我们提出了基于跳 - 克框架的高斯混合跳跃（GMSG）模型来学习词的高斯混合嵌入。每个单词可以看作是嵌入式空间中的高斯混合分布，每个高斯分量表示一个单词意义。由于每个词的意义不同，我们进一步提出动态GMSG（D-GMSG）模型，通过自适应地增加训练期间词的意义数目。四个基准的实验显示了我们提出的模型的有效性。

##### URL
[https://arxiv.org/abs/1511.06246](https://arxiv.org/abs/1511.06246)

##### PDF
[https://arxiv.org/pdf/1511.06246](https://arxiv.org/pdf/1511.06246)

