---
layout: post
title: "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings"
date: 2017-04-30 19:18:22
categories: arXiv_CL
tags: arXiv_CL Embedding Transfer_Learning RNN Relation
author: John Wieting, Kevin Gimpel
mathjax: true
---

* content
{:toc}

##### Abstract
We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.

##### Abstract (translated by Google)
我们考虑学习通用性问题，使用句法嵌入句子，重新审视维亭等人的设置问题。 （2016B）。虽然他们发现LSTM经常性网络的表现不如字平均，但我们提出了几个发展，这些发展一起产生了相反的结论。这些包括对句子对而不是对短语的训练，对状态进行平均来表示序列，并积极规则化。这些改善了转移学习和监督设置的LSTMs。我们还介绍了一个新的经常性架构，门控循环平均网络，受平均和LSTM的启发，同时表现优异。我们分析我们的学习模型，找到特定词类和依赖关系的偏好证据。

##### URL
[https://arxiv.org/abs/1705.00364](https://arxiv.org/abs/1705.00364)

##### PDF
[https://arxiv.org/pdf/1705.00364](https://arxiv.org/pdf/1705.00364)

