---
layout: post
title: "Generalized Majorization-Minimization"
date: 2016-07-28 04:47:13
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Sobhan Naderi Parizi, Kun He, Stan Sclaroff, Pedro Felzenszwalb
mathjax: true
---

* content
{:toc}

##### Abstract
Non-convex optimization is ubiquitous in machine learning. The Majorization-Minimization (MM) procedure systematically optimizes non-convex functions through an iterative construction and optimization of upper bounds on the objective function. The bound at each iteration is required to \emph{touch} the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new framework for designing optimization algorithms, named Generalized Majorization-Minimization (G-MM). Compared to MM, G-MM is much more flexible. For instance, it can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization.

##### Abstract (translated by Google)
非凸优化在机器学习中无处不在。优化 - 最小化（MM）程序通过迭代构造和优化目标函数的上界，系统地优化非凸函数。在每次迭代中的边界都需要在上一个边界的优化器处触摸目标函数。我们证明这个感人的约束是不必要的，也是过度的限制。我们通过放宽这个约束来推广MM，并提出了一个新的设计优化算法的框架，称为通用优化最小化（G-MM）。与MM相比，G-MM更加灵活。例如，它可以在不改变目标函数的情况下将特定于应用的偏差结合到优化过程中。我们推导出了几种潜变量模型的G-MM算法，并证明了它们在优化非凸目标方面一贯优于MM模型。特别是，G-MM算法似乎对初始化不那么敏感。

##### URL
[https://arxiv.org/abs/1506.07613](https://arxiv.org/abs/1506.07613)

##### PDF
[https://arxiv.org/pdf/1506.07613](https://arxiv.org/pdf/1506.07613)

