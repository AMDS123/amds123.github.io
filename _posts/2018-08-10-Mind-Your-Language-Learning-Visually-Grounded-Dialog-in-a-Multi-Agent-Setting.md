---
layout: post
title: "Mind Your Language: Learning Visually Grounded Dialog in a Multi-Agent Setting"
date: 2018-08-10 22:09:43
categories: arXiv_AI
tags: arXiv_AI Quantitative
author: Akshat Agarwal, Swaminathan Gurumurthy, Vasu Sharma, Katia Sycara
mathjax: true
---

* content
{:toc}

##### Abstract
The task of visually grounded dialog involves learning goal-oriented cooperative dialog between autonomous agents who exchange information about a scene through several rounds of questions and answers. We posit that requiring agents to adhere to rules of human language while also maximizing information exchange is an ill-posed problem, and observe that humans do not stray from a common language, because they are social creatures and have to communicate with many people everyday, and it is far easier to stick to a common language even at the cost of some efficiency loss. Using this as inspiration, we propose and evaluate a multi-agent dialog framework where each agent interacts with, and learns from, multiple agents, and show that this results in more relevant and coherent dialog (as judged by human evaluators) without sacrificing task performance (as judged by quantitative metrics).

##### Abstract (translated by Google)
视觉上基于对话的任务涉及学习通过几轮问题和答案交换场景信息的自主代理之间的目标导向的合作对话。我们认为，要求代理人遵守人类语言规则，同时最大化信息交换是一个不适定的问题，并观察人类不偏离共同语言，因为他们是社会生物，每天必须与许多人沟通，即使以一些效率损失为代价，也更容易坚持使用共同语言。以此为灵感，我们提出并评估一个多代理对话框架，其中每个代理与多个代理进行交互并从中学习，并表明这会产生更相关和一致的对话（由人工评估员判断），而不会牺牲任务性能（按量化指标判断）。

##### URL
[http://arxiv.org/abs/1808.04359](http://arxiv.org/abs/1808.04359)

##### PDF
[http://arxiv.org/pdf/1808.04359](http://arxiv.org/pdf/1808.04359)

