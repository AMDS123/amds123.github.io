---
layout: post
title: "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling"
date: 2018-04-24 17:41:24
categories: arXiv_CV
tags: arXiv_CV Adversarial Face Reinforcement_Learning Caption
author: Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.

##### Abstract (translated by Google)
虽然在视觉字幕中取得了令人印象深刻的结果，但从照片流中产生抽象故事的任务仍然是一个小问题。与标题不同，故事具有更多表现力的语言风格，并包含许多不存在于图像中的虚构概念。因此它对行为克隆算法提出了挑战。此外，由于评估故事质量的自动指标的局限性，手工奖励的强化学习方法在获得整体性能提升方面也面临困难。因此，我们提出了一个敌对的REward Learning（AREL）框架，以从人类示威中学习隐式奖励功能，然后利用学习奖励功能优化政策搜索。虽然自动评估显示克隆专家行为的技术（SOTA）方法略有性能提升，但人类评估显示，我们的方法在产生比SOTA系统更多类人故事方面取得显着进步。

##### URL
[https://arxiv.org/abs/1804.09160](https://arxiv.org/abs/1804.09160)

##### PDF
[https://arxiv.org/pdf/1804.09160](https://arxiv.org/pdf/1804.09160)

