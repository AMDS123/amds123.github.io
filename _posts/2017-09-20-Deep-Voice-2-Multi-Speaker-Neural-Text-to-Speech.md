---
layout: post
title: "Deep Voice 2: Multi-Speaker Neural Text-to-Speech"
date: 2017-09-20 21:43:18
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Sercan Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, Yanqi Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.

##### Abstract (translated by Google)
我们引入了一种用低维可训练的扬声器嵌入来增强神经文本到语音（TTS）的技术，以从单个模型生成不同的语音。作为一个起点，我们展示了单声道神经TTS的两种最先进的方法：Deep Voice 1和Tacotron。我们引入了Deep Voice 2，它基于与Deep Voice 1类似的管道，但是使用更高性能的构建模块构建，并且在Deep Voice 1上演示了显着的音频质量改进。我们通过引入后处理神经声码器来改进Tacotron，展现出显着的音频质量改进。然后，我们演示了两个多扬声器TTS数据集上Deep Voice 2和Tacotron的多扬声器语音合成技术。我们表明，一个单一的神经TTS系统可以从每个扬声器不到半小时的数据中学习数百个独特的声音，同时实现高质量的音频合成和几乎完美地保存扬声器身份。

##### URL
[https://arxiv.org/abs/1705.08947](https://arxiv.org/abs/1705.08947)

##### PDF
[https://arxiv.org/pdf/1705.08947](https://arxiv.org/pdf/1705.08947)

