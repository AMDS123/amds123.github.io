---
layout: post
title: "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"
date: 2019-08-13 11:12:58
categories: arXiv_CL
tags: arXiv_CL Sentiment Attention Sentiment_Classification Inference Classification Language_Model
author: Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, Luo Si
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, the pre-trained language model, BERT (Devlin et al.(2018)Devlin, Chang, Lee, and Toutanova), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman (Elman(1990)), we extend BERT to a new model, StructBERT, by incorporating language structures into pretraining. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 84.5 (with Top 1 achievement on the Leaderboard at the time of paper submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.04577](http://arxiv.org/abs/1908.04577)

##### PDF
[http://arxiv.org/pdf/1908.04577](http://arxiv.org/pdf/1908.04577)

