---
layout: post
title:  'TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering'
date:   2017-12-05 18:47:33
categories: CV
tags: CV
author: Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim
---

* content
{:toc}

##### Abstract
Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.

##### Abstract (translated by Google)
视觉和语言理解已经成为人工智能研究的热门课题。在这一研究领域的许多工作中，视觉问答（VQA）是最成功的问题之一，其目标是学习一个模型，在区域层次的细节上理解视觉内容，发现它们与问题和自然语言形式的答案。尽管过去几年取得了快速进展，但VQA的大部分工作主要集中在图像上。在本文中，我们着重于将VQA扩展到视频领域，并通过三个重要方面对文献做出贡献。首先，我们提出了三个专门为视频VQA设计的新任务，这些任务需要视频的时空推理来正确回答问题。接下来，我们介绍一个名为TGIF-QA的视频VQA新的大规模数据集，它将现有的VQA工作扩展到我们的新任务。最后，我们提出了一个兼顾空间和时间关注的基于双重LSTM的方法，并通过实证评估显示了它与传统VQA技术的有效性。

##### URL
[http://arxiv.org/abs/1704.04497](http://arxiv.org/abs/1704.04497)

