---
layout: post
title: "Deep learning generalizes because the parameter-function map is biased towards simple functions"
date: 2018-05-22 11:51:36
categories: arXiv_AI
tags: arXiv_AI Regularization Deep_Learning
author: Guillermo Valle Pérez, Ard A. Louis, Chico Q. Camargo
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks generalize remarkably well without explicit regularization even in the strongly over-parametrized regime. This success suggests that some form of implicit regularization must be at work. By applying a modified version of the coding theorem from algorithmic information theory and by performing extensive empirical analysis of random neural networks, we argue that the parameter function map of deep neural networks is exponentially biased towards functions with lower descriptional complexity. We show explicitly for supervised learning of Boolean functions that the intrinsic simplicity bias of deep neural networks means that they generalize significantly better than an unbiased learning algorithm does. The superior generalization due to simplicity bias can be explained using PAC-Bayes theory, which yields useful generalization error bounds for learning Boolean functions with a wide range of complexities. Finally, we provide evidence that deeper neural networks trained on the CIFAR10 data set exhibit stronger simplicity bias than shallow networks do, which may help explain why deeper networks generalize better than shallow ones do.

##### Abstract (translated by Google)
即使在强烈的过度参数化机制下，深层神经网络也没有明确的正则化能够很好地推广。这一成功表明，某种形式的隐性正规化必须发挥作用。通过应用算法信息论的编码定理的修改版本和对随机神经网络进行广泛的经验分析，我们认为深层神经网络的参数函数图呈指数式地偏向于具有较低描述复杂度的函数。我们明确地展示了布尔函数的监督式学习，深度神经网络的内在简单偏差意味着它们比无偏差学习算法具有更好的泛化能力。可以使用PAC-Bayes理论来解释由简单偏差引起的优越泛化，该理论产生用于学习具有各种复杂性的布尔函数的有用的泛化误差界限。最后，我们提供的证据表明，在CIFAR10数据集上训练的更深层的神经网络比浅层网络表现出更强的简单偏差，这可能有助于解释为什么深层网络比浅层网络具有更好的泛化能力。

##### URL
[https://arxiv.org/abs/1805.08522](https://arxiv.org/abs/1805.08522)

##### PDF
[https://arxiv.org/pdf/1805.08522](https://arxiv.org/pdf/1805.08522)

