---
layout: post
title: "Embracing data abundance: BookTest Dataset for Reading Comprehension"
date: 2016-10-04 12:48:51
categories: arXiv_CL
tags: arXiv_CL Attention Face
author: Ondrej Bajgar, Rudolf Kadlec, Jan Kleindienst
mathjax: true
---

* content
{:toc}

##### Abstract
There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.

##### Abstract (translated by Google)
实际上有无限的自然语言数据可用。尽管如此，最近在文本理解方面的工作集中在相对于当前计算可能性较小的数据集上。本文正在为社区提供更大的数据，并向这个方向迈出了一步，它提出了书目测试（BookTest），这是一个与流行的儿童读物测试（CBT）类似的新数据集，但是超过了60倍。我们表明，对新数据的培训提高了我们注意 - 求和阅读器模型在原始CBT测试数据上的准确性，这比最近许多改进模型架构的尝试要大得多。在数据集的一个版本上，我们的集合甚至超过了Facebook提供的人类基线。然后，我们在自己的人类学研究中表明，还有进一步改进的空间。

##### URL
[https://arxiv.org/abs/1610.00956](https://arxiv.org/abs/1610.00956)

##### PDF
[https://arxiv.org/pdf/1610.00956](https://arxiv.org/pdf/1610.00956)

