---
layout: post
title: "Neural Machine Translation for Bilingually Scarce Scenarios: A Deep Multi-task Learning Approach"
date: 2018-05-11 03:36:32
categories: arXiv_CL
tags: arXiv_CL Knowledge Recognition
author: Poorya Zaremoodi, Gholamreza Haffari
mathjax: true
---

* content
{:toc}

##### Abstract
Neural machine translation requires large amounts of parallel training text to learn a reasonable-quality translation model. This is particularly inconvenient for language pairs for which enough parallel text is not available. In this paper, we use monolingual linguistic resources in the source side to address this challenging problem based on a multi-task learning approach. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition. This effectively injects semantic and/or syntactic knowledge into the translation model, which would otherwise require a large amount of training bitext. We empirically evaluate and show the effectiveness of our multi-task learning approach on three translation tasks: English-to-French, English-to-Farsi, and English-to-Vietnamese.

##### Abstract (translated by Google)
神经机器翻译需要大量的平行训练文本来学习合理质量的翻译模型。这对于没有足够的并行文本的语言对来说特别不方便。在本文中，我们使用源语言中的单语言语言资源来解决基于多任务学习方法的具有挑战性的问题。更具体地说，我们将机器翻译任务搭建在辅助任务上，包括语义分析，句法分析和命名实体识别。这有效地将语义和/或语法知识注入到翻译模型中，否则这将需要大量的训练双字。我们经验性地评估并展示了我们的多任务学习方法对三种翻译任务的有效性：英语到法语，英语到波斯语和英语到越南语。

##### URL
[http://arxiv.org/abs/1805.04237](http://arxiv.org/abs/1805.04237)

##### PDF
[http://arxiv.org/pdf/1805.04237](http://arxiv.org/pdf/1805.04237)

