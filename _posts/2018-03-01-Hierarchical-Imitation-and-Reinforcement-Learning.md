---
layout: post
title: "Hierarchical Imitation and Reinforcement Learning"
date: 2018-03-01 19:12:27
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Hoang M. Le, Nan Jiang, Alekh Agarwal, Miroslav Dud&#xed;k, Yisong Yue, Hal Daum&#xe9; III
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of learning policies over long time horizons. We present a framework that leverages and integrates two key concepts. First, we utilize hierarchical policy classes that enable planning over different time scales, i.e., the high level planner proposes a sequence of subgoals for the low level planner to achieve. Second, we utilize expert demonstrations within the hierarchical action space to dramatically reduce cost of exploration. Our framework is flexible and can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels of the hierarchy. Using long-horizon benchmarks, including Montezuma's Revenge, we empirically demonstrate that our approach can learn significantly faster compared to hierarchical RL, and can be significantly more label- and sample-efficient compared to flat IL. We also provide theoretical analysis of the labeling cost for certain instantiations of our framework.

##### Abstract (translated by Google)
我们研究长期以来学习政策的问题。我们提出了一个利用并集成两个关键概念的框架。首先，我们利用分级政策类别，可以在不同的时间范围内进行规划，即高级规划人员为低级规划人员提供一系列子目标。其次，我们利用分层行动空间内的专家演示来显着降低勘探成本。我们的框架是灵活的，可以在不同级别的层级中结合模仿学习（IL）和强化学习（RL）的不同组合。使用包括Montezuma's Revenge在内的长期基准测试，我们凭经验证明，与分级RL相比，我们的方法可以显着加快学习速度，并且与平面IL相比，可以显着提高标签和样本效率。我们还为我们框架的某些实例提供了标签成本的理论分析。

##### URL
[http://arxiv.org/abs/1803.00590](http://arxiv.org/abs/1803.00590)

##### PDF
[http://arxiv.org/pdf/1803.00590](http://arxiv.org/pdf/1803.00590)

