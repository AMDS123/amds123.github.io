---
layout: post
title: "Lensless Imaging with Compressive Ultrafast Sensing"
date: 2017-03-30 02:04:49
categories: arXiv_CV
tags: arXiv_CV Object_Detection Detection
author: Guy Satat, Matthew Tancik, Ramesh Raskar
mathjax: true
---

* content
{:toc}

##### Abstract
Lensless imaging is an important and challenging problem. One notable solution to lensless imaging is a single pixel camera which benefits from ideas central to compressive sampling. However, traditional single pixel cameras require many illumination patterns which result in a long acquisition process. Here we present a method for lensless imaging based on compressive ultrafast sensing. Each sensor acquisition is encoded with a different illumination pattern and produces a time series where time is a function of the photon's origin in the scene. Currently available hardware with picosecond time resolution enables time tagging photons as they arrive to an omnidirectional sensor. This allows lensless imaging with significantly fewer patterns compared to regular single pixel imaging. To that end, we develop a framework for designing lensless imaging systems that use ultrafast detectors. We provide an algorithm for ideal sensor placement and an algorithm for optimized active illumination patterns. We show that efficient lensless imaging is possible with ultrafast measurement and compressive sensing. This paves the way for novel imaging architectures and remote sensing in extreme situations where imaging with a lens is not possible.

##### Abstract (translated by Google)
无镜头成像是一个重要且具有挑战性的问题。无镜头成像的一个值得注意的解决方案是单像素相机，它受益于压缩采样的核心思想。然而，传统的单像素相机需要许多照明模式，这导致了长时间的采集过程。在这里我们介绍一种基于压缩超快速感应的无透镜成像方法。每个传感器采集都采用不同的照明模式进行编码，并生成一个时间序列，其中时间是场景中光子起点的函数。当前可用的皮秒时间分辨率的硬件可以在光子到达全向传感器时对其进行时间标记。与常规单像素成像相比，这使得无镜头成像的图案明显更少。为此，我们开发了一套使用超快探测器的无透镜成像系统设计框架。我们提供了理想的传感器布局算法和优化的主动照明模式算法。我们表明，高效率的无镜头成像是可能的超快测量和压缩感应。这为在极端情况下使用镜头成像是不可能的新型成像架构和遥感铺平了道路。

##### URL
[https://arxiv.org/abs/1610.05834](https://arxiv.org/abs/1610.05834)

##### PDF
[https://arxiv.org/pdf/1610.05834](https://arxiv.org/pdf/1610.05834)

