---
layout: post
title: "Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis"
date: 2018-09-15 08:36:28
categories: arXiv_CV
tags: arXiv_CV Review Survey Optimization Inference Deep_Learning
author: Tal Ben-Nun, Torsten Hoefler
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.

##### Abstract (translated by Google)
深度神经网络（DNN）正在成为现代计算应用中的重要工具。加速他们的培训是一项重大挑战，技术范围从分布式算法到低级电路设计。在本次调查中，我们从理论角度描述了问题，然后是并行化的方法。我们介绍了DNN体系结构的趋势以及对并行化策略的影响。然后，我们对DNN中不同类型的并发性进行检查和建模：从单个运算符，到网络推理和训练的并行性，再到分布式深度学习。我们讨论异步随机优化，分布式系统架构，通信方案和神经架构搜索。基于这些方法，我们推断出深度学习中并行性的潜在方向。

##### URL
[http://arxiv.org/abs/1802.09941](http://arxiv.org/abs/1802.09941)

##### PDF
[http://arxiv.org/pdf/1802.09941](http://arxiv.org/pdf/1802.09941)

