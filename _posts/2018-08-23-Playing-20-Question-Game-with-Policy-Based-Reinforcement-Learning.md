---
layout: post
title: "Playing 20 Question Game with Policy-Based Reinforcement Learning"
date: 2018-08-23 06:34:32
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning
author: Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, Zhan Chen
mathjax: true
---

* content
{:toc}

##### Abstract
The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.

##### Abstract (translated by Google)
20个问题（Q20）游戏是一个众所周知的游戏，鼓励演绎推理和创造力。在游戏中，回答者首先想到诸如名人或动物之类的物体。然后提问者试图通过询问20个问题来猜测对象。在Q20游戏系统中，用户被视为回答者，而系统本身充当提问者，需要良好的问题选择策略来找出正确的对象并赢得游戏。然而，由于游戏环境的复杂性和波动性，难以推导出问题选择的最优策略。在本文中，我们提出了一种新的基于策略的强化学习（RL）方法，该方法使提问者能够通过与用户的持续交互来学习问题选择的最优策略。为了便于培训，我们还建议使用奖励网络来估算更多信息奖励。与以前的方法相比，我们的RL方法对于嘈杂的答案是鲁棒的，并且不依赖于对象的知识库。实验结果表明，我们的RL方法明显优于基于熵的工程系统，并且在无噪声的仿真环境中具有竞争性。

##### URL
[http://arxiv.org/abs/1808.07645](http://arxiv.org/abs/1808.07645)

##### PDF
[http://arxiv.org/pdf/1808.07645](http://arxiv.org/pdf/1808.07645)

