---
layout: post
title: "Associative Memory by Recurrent Neural Networks with Delay Elements"
date: 2002-09-12 05:37:40
categories: arXiv_CV
tags: arXiv_CV RNN Relation
author: Seiji Miyoshi, Hiro-Fumi Yanai, Masato Okada
mathjax: true
---

* content
{:toc}

##### Abstract
The synapses of real neural systems seem to have delays. Therefore, it is worthwhile to analyze associative memory models with delayed synapses. Thus, a sequential associative memory model with delayed synapses is discussed, where a discrete synchronous updating rule and a correlation learning rule are employed. Its dynamic properties are analyzed by the statistical neurodynamics. In this paper, we first re-derive the Yanai-Kim theory, which involves macrodynamical equations for the dynamics of the network with serial delay elements. Since their theory needs a computational complexity of $O(L^4t)$ to obtain the macroscopic state at time step t where L is the length of delay, it is intractable to discuss the macroscopic properties for a large L limit. Thus, we derive steady state equations using the discrete Fourier transformation, where the computational complexity does not formally depend on L. We show that the storage capacity $\alpha_C$ is in proportion to the delay length L with a large L limit, and the proportion constant is 0.195, i.e., $\alpha_C = 0.195 L$. These results are supported by computer simulations.

##### Abstract (translated by Google)
真正的神经系统的突触似乎有延迟。因此，分析具有延迟突触的联想记忆模型是值得的。因此，讨论了具有延迟突触的顺序联想记忆模型，其中使用离散同步更新规则和相关学习规则。其动态特性由统计神经动力学分析。在本文中，我们首先重新推导了Yanai-Kim理论，该理论涉及具有串行延迟元件的网络动力学的宏观动力学方程。由于它们的理论需要$ O（L ^ 4t）$的计算复杂度来获得在时间步长t处的宏观状态，其中L是延迟的长度，所以讨论大的L极限的宏观性质是难以处理的。因此，我们使用离散傅里叶变换推导出稳态方程，其中计算复杂度不正式地依赖于L.我们表明，存储容量$ \ alpha_C $与具有大L限制的延迟长度L成比例，并且比例常数为0.195，即$ \ alpha_C = 0.195 L $。计算机模拟支持这些结果。

##### URL
[https://arxiv.org/abs/cond-mat/0209258](https://arxiv.org/abs/cond-mat/0209258)

##### PDF
[https://arxiv.org/pdf/cond-mat/0209258](https://arxiv.org/pdf/cond-mat/0209258)

