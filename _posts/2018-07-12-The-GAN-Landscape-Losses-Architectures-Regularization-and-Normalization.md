---
layout: post
title: "The GAN Landscape: Losses, Architectures, Regularization, and Normalization"
date: 2018-07-12 16:56:50
categories: arXiv_CV
tags: arXiv_CV Regularization Adversarial GAN
author: Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, Sylvain Gelly
mathjax: true
---

* content
{:toc}

##### Abstract
Generative Adversarial Networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant amount of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of "tricks". The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, and neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We reproduce the current state of the art and go beyond fairly exploring the GAN landscape. We discuss common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.

##### Abstract (translated by Google)
生成性对抗网络（GAN）是一类深度生成模型，旨在以无人监督的方式学习目标分布。虽然它们已成功应用于许多问题，但训练GAN是一项众所周知的挑战性任务，需要大量的超参数调整，神经架构工程以及非常重要的“技巧”。许多实际应用的成功加上缺乏量化GAN失效模式的措施导致了过多的建议损失，正则化和归一化方案以及神经架构。在这项工作中，我们从实际角度对GAN的当前状态进行了清醒的观察。我们重现当前的技术发展水平，并超越了公平地探索GAN的景观。我们讨论常见的陷阱和可重现性问题，在Github上开源我们的代码，并在TensorFlow Hub上提供预先训练的模型。

##### URL
[https://arxiv.org/abs/1807.04720](https://arxiv.org/abs/1807.04720)

##### PDF
[https://arxiv.org/pdf/1807.04720](https://arxiv.org/pdf/1807.04720)

