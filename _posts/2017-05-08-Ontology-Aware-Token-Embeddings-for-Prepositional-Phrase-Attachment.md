---
layout: post
title: "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment"
date: 2017-05-08 15:40:51
categories: arXiv_CL
tags: arXiv_CL Ontology Embedding
author: Pradeep Dasigi, Waleed Ammar, Chris Dyer, Eduard Hovy
mathjax: true
---

* content
{:toc}

##### Abstract
Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase(PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.

##### Abstract (translated by Google)
类型级别的词嵌入使用相同的一组参数来表示一个词的所有实例，而不管其上下文，忽略语言中固有的词汇歧义。相反，我们嵌入了WordNet中定义的语义概念（或概念集），并通过估计相关语义概念上的分布来表示特定上下文中的词标记。我们在模型中使用新的，上下文敏感的嵌入来预测介词短语（PP）附件，并共同学习概念嵌入和模型参数。我们显示使用上下文敏感的嵌入提高了5.4％的绝对点的PP附件模型的准确性，相当于减少了34.4％的错误。

##### URL
[https://arxiv.org/abs/1705.02925](https://arxiv.org/abs/1705.02925)

##### PDF
[https://arxiv.org/pdf/1705.02925](https://arxiv.org/pdf/1705.02925)

