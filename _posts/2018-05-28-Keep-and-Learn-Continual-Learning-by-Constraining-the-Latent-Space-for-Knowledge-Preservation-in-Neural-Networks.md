---
layout: post
title: "Keep and Learn: Continual Learning by Constraining the Latent Space for Knowledge Preservation in Neural Networks"
date: 2018-05-28 06:26:58
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: Hyo-Eun Kim, Seungwook Kim, Jaehwan Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Data is one of the most important factors in machine learning. However, even if we have high-quality data, there is a situation in which access to the data is restricted. For example, access to the medical data from outside is strictly limited due to the privacy issues. In this case, we have to learn a model sequentially only with the data accessible in the corresponding stage. In this work, we propose a new method for preserving learned knowledge by modeling the high-level feature space and the output space to be mutually informative, and constraining feature vectors to lie in the modeled space during training. The proposed method is easy to implement as it can be applied by simply adding a reconstruction loss to an objective function. We evaluate the proposed method on CIFAR-10/100 and a chest X-ray dataset, and show benefits in terms of knowledge preservation compared to previous approaches.

##### Abstract (translated by Google)
数据是机器学习中最重要的因素之一。但是，即使我们有高质量的数据，也存在数据访问受限的情况。例如，由于隐私问题，从外部访问医疗数据的行为受到严格限制。在这种情况下，我们必须按顺序学习一个模型，只能在相应阶段访问数据。在这项工作中，我们提出了一种保留学习知识的新方法，通过对高层特征空间和输出空间进行建模以提供相互信息，并将特征向量限制在训练期间的建模空间中。所提出的方法很容易实现，因为它可以通过简单地将重建损失添加到目标函数来应用。我们在CIFAR-10/100和胸部X射线数据集上评估了所提出的方法，并与以前的方法相比，显示了知识保存方面的益处。

##### URL
[http://arxiv.org/abs/1805.10784](http://arxiv.org/abs/1805.10784)

##### PDF
[http://arxiv.org/pdf/1805.10784](http://arxiv.org/pdf/1805.10784)

