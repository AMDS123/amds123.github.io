---
layout: post
title: "Piecewise Latent Variables for Neural Variational Text Processing"
date: 2017-09-23 13:33:55
categories: arXiv_CL
tags: arXiv_CL Inference
author: Iulian V. Serban, Alexander G. Ororbia II, Joelle Pineau, Aaron Courville
mathjax: true
---

* content
{:toc}

##### Abstract
Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.

##### Abstract (translated by Google)
神经变分推理的进展有助于学习具有连续潜变量的强大定向图形模型，如变分自编码器。希望这样的模型能够学习在现实世界的数据中，如自然语言文本中表现丰富的，多模式的潜在因素。然而，目前的模型往往假设潜在变量的简单先验 - 例如单峰高斯分布 - 它们不能有效地表示复杂的潜在因素。为了克服这个限制，我们提出了简单但高度灵活的分段不变分布。这个分布有能力代表一个潜在的目标分布模式的指数，同时保持数学上的易处理性。我们的研究结果表明，将这种新的潜在分布结合到不同的模型中，可以在自然语言处理任务（如文档建模和对话的自然语言生成）方面取得重大进展

##### URL
[https://arxiv.org/abs/1612.00377](https://arxiv.org/abs/1612.00377)

##### PDF
[https://arxiv.org/pdf/1612.00377](https://arxiv.org/pdf/1612.00377)

