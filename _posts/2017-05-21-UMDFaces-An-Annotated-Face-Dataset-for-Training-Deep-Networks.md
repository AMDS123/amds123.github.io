---
layout: post
title: "UMDFaces: An Annotated Face Dataset for Training Deep Networks"
date: 2017-05-21 08:00:42
categories: arXiv_CV
tags: arXiv_CV Face CNN Detection Face_Detection Recognition Face_Recognition
author: Ankan Bansal, Anirudh Nanduri, Carlos Castillo, Rajeev Ranjan, Rama Chellappa
mathjax: true
---

* content
{:toc}

##### Abstract
Recent progress in face detection (including keypoint detection), and recognition is mainly being driven by (i) deeper convolutional neural network architectures, and (ii) larger datasets. However, most of the large datasets are maintained by private companies and are not publicly available. The academic computer vision community needs larger and more varied datasets to make further progress. In this paper we introduce a new face dataset, called UMDFaces, which has 367,888 annotated faces of 8,277 subjects. We also introduce a new face recognition evaluation protocol which will help advance the state-of-the-art in this area. We discuss how a large dataset can be collected and annotated using human annotators and deep networks. We provide human curated bounding boxes for faces. We also provide estimated pose (roll, pitch and yaw), locations of twenty-one key-points and gender information generated by a pre-trained neural network. In addition, the quality of keypoint annotations has been verified by humans for about 115,000 images. Finally, we compare the quality of the dataset with other publicly available face datasets at similar scales.

##### Abstract (translated by Google)
人脸检测（包括关键点检测）和识别的最新进展主要由（i）更深的卷积神经网络结构和（ii）更大的数据集驱动。然而，大部分大型数据集都是由私营公司维护的，并没有公开提供。学术计算机视觉社区需要更大，更多样化的数据集以取得进一步的进展。在本文中，我们将介绍一个名为UMDFaces的新人脸数据集，其中包含8287个主题的367888个注释面。我们还推出了一个新的人脸识别评估协议，有助于推动这一领域的最新进展。我们讨论如何使用人类注释器和深度网络来收集和注释大型数据集。我们为人脸提供人类策划的边界框。我们还提供估计姿势（滚动，俯仰和偏航），二十一个关键点的位置以及由预先训练的神经网络生成的性别信息。另外，关键点注释的质量已经被人类验证了约115,000个图像。最后，我们将数据集的质量与其他公开可用的类似尺度的人脸数据集进行比较。

##### URL
[https://arxiv.org/abs/1611.01484](https://arxiv.org/abs/1611.01484)

##### PDF
[https://arxiv.org/pdf/1611.01484](https://arxiv.org/pdf/1611.01484)

