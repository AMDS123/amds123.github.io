---
layout: post
title: "Paraphrasing Complex Network: Network Compression via Factor Transfer"
date: 2018-02-14 07:46:15
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: Jangho Kim, SeoungUK Park, Nojun Kwak
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks (DNN) have recently shown promising performances in various areas. Although DNNs are very powerful, a large number of network parameters requires substantial storage and memory bandwidth which hinders them from being applied to actual embedded systems. Many researchers have sought ways of model compression to reduce the size of a network with minimal performance degradation. Among them, a method called knowledge transfer is to train the student network with a stronger teacher network. In this paper, we propose a method to overcome the limitations of conventional knowledge transfer methods and improve the performance of a student network. An auto-encoder is used in an unsupervised manner to extract compact factors which are defined as compressed feature maps of the teacher network. When using the factors to train the student network, we observed that the performance of the student network becomes better than the ones with other conventional knowledge transfer methods because factors contain paraphrased compact information of the teacher network that is easy for the student network to understand.

##### Abstract (translated by Google)
最近，深度神经网络（DNN）在各个领域显示出良好的表现。尽管DNN非常强大，但大量的网络参数需要大量的存储和内存带宽，这阻碍了它们被应用于实际的嵌入式系统。许多研究人员已经寻求模型压缩的方法，以最小的性能降低来减小网络的大小。其中，一种称为知识转移的方法是用更强大的教师网络来培训学生网络。在本文中，我们提出了一种方法来克服传统知识转移方法的局限性并提高学生网络的性能。以无监督的方式使用自动编码器来提取被定义为教师网络的压缩特征图的紧凑因子。当使用这些因素来训练学生网络时，我们观察到学生网络的表现比其他传统知识转移方法的表现更好，因为因素包含了容易让学生网络理解的教师网络的转述紧凑信息。

##### URL
[http://arxiv.org/abs/1802.04977](http://arxiv.org/abs/1802.04977)

##### PDF
[http://arxiv.org/pdf/1802.04977](http://arxiv.org/pdf/1802.04977)

