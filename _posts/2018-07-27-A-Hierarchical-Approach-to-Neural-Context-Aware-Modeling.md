---
layout: post
title: "A Hierarchical Approach to Neural Context-Aware Modeling"
date: 2018-07-27 11:10:03
categories: arXiv_CL
tags: arXiv_CL Salient Classification Language_Model Detection
author: Patrick Huber, Jan Niehues, Alex Waibel
mathjax: true
---

* content
{:toc}

##### Abstract
We present a new recurrent neural network topology to enhance state-of-the-art machine learning systems by incorporating a broader context. Our approach overcomes recent limitations with extended narratives through a multi-layered computational approach to generate an abstract context representation. Therefore, the developed system captures the narrative on word-level, sentence-level, and context-level. Through the hierarchical set-up, our proposed model summarizes the most salient information on each level and creates an abstract representation of the extended context. We subsequently use this representation to enhance neural language processing systems on the task of semantic error detection. To show the potential of the newly introduced topology, we compare the approach against a context-agnostic set-up including a standard neural language model and a supervised binary classification network. The performance measures on the error detection task show the advantage of the hierarchical context-aware topologies, improving the baseline by 12.75% relative for unsupervised models and 20.37% relative for supervised models.

##### Abstract (translated by Google)
我们提出了一种新的递归神经网络拓扑结构，通过结合更广泛的背景来增强最先进的机器学习系统。我们的方法通过多层计算方法克服了最近的扩展叙述限制，以生成抽象的上下文表示。因此，开发的系统捕获了关于词级，句子级和上下文级的叙述。通过分层设置，我们提出的模型总结了每个级别上最显着的信息，并创建了扩展上下文的抽象表示。我们随后使用该表示来增强神经语言处理系统的语义错误检测任务。为了展示新引入的拓扑结构的潜力，我们将该方法与包括标准神经语言模型和监督二进制分类网络在内的上下文无关设置进行了比较。错误检测任务的性能测量显示了分层上下文感知拓扑的优势，相对于无监督模型，基线改善了12.75％，相对于监督模型，基线改善了20.37％。

##### URL
[http://arxiv.org/abs/1807.11582](http://arxiv.org/abs/1807.11582)

##### PDF
[http://arxiv.org/pdf/1807.11582](http://arxiv.org/pdf/1807.11582)

