---
layout: post
title: "Comparative Analysis of Neural QA models on SQuAD"
date: 2018-06-18 22:29:51
categories: arXiv_AI
tags: arXiv_AI QA Prediction Quantitative
author: Soumya Wadhwa, Khyathi Raghavi Chandu, Eric Nyberg
mathjax: true
---

* content
{:toc}

##### Abstract
The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks. Different components in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.

##### Abstract (translated by Google)
在过去的几十年中，问答测验的任务在测试机器理解自然语言的能力方面取得了突出的成绩。与信息检索任务相比，机器阅读的大型数据集导致了神经模型的发展，以迎合更深层次的语言理解。这些神经架构中的不同组件旨在解决不同的挑战。作为跨多个领域实现泛化的第一步，我们试图通过定量和定性分析斯坦福问题应答数据集（SQRT）的结果来了解和比较现有的端到端神经模型的特性，他们每个人。我们观察到预测误差反映了某些模型特定的偏差，我们在本文中将进一步讨论这些偏差。

##### URL
[http://arxiv.org/abs/1806.06972](http://arxiv.org/abs/1806.06972)

##### PDF
[http://arxiv.org/pdf/1806.06972](http://arxiv.org/pdf/1806.06972)

