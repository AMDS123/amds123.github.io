---
layout: post
title: "On the Power of Over-parametrization in Neural Networks with Quadratic Activation"
date: 2018-03-03 17:37:57
categories: arXiv_AI
tags: arXiv_AI Face
author: Simon S. Du, Jason D. Lee
mathjax: true
---

* content
{:toc}

##### Abstract
We provide new theoretical insights on why over-parametrization is effective in learning neural networks. For a $k$ hidden node shallow network with quadratic activation and $n$ training data points, we show as long as $ k \ge \sqrt{2n}$, over-parametrization enables local search algorithms to find a \emph{globally} optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when $k\ge \sqrt{2n}$, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks.

##### Abstract (translated by Google)
我们提供了关于为什么过参数化在学习神经网络方面有效的新理论见解。对于具有二次激活和$ n $训练数据点的$ k $隐藏节点浅层网络，只要$ k \ ge \ sqrt {2n} $显示，过参数化就可以使局部搜索算法找到\ emph {全局一般平滑和凸损失函数的最优解。此外，尽管参数的数量可能会超过样本大小，但使用Rademacher复杂度理论，我们可以看到重量衰减，如果数据是从诸如高斯的正态分布中采样的，则该解决方案也可以很好地推广。为了证明当$ k \ ge \ sqrt {2n} $时，损失函数具有良好的景观特性，我们采用了平滑分析的思路，这可能有其他应用来研究神经网络的损失表面。

##### URL
[http://arxiv.org/abs/1803.01206](http://arxiv.org/abs/1803.01206)

##### PDF
[http://arxiv.org/pdf/1803.01206](http://arxiv.org/pdf/1803.01206)

