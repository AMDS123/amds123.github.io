---
layout: post
title: "Lift-the-Flap: Context Reasoning Using Object-Centered Graphs"
date: 2019-02-01 03:37:17
categories: arXiv_AI
tags: arXiv_AI Segmentation Reinforcement_Learning Semantic_Segmentation Inference Classification Recognition
author: Mengmi Zhang, Jiashi Feng, Karla Montejo, Joseph Kwon, Joo Hwee Lim, Gabriel Kreiman
mathjax: true
---

* content
{:toc}

##### Abstract
Children benefit from lift-the-flap books by taking on an active role in guessing what is behind the flap based on the context. In this paper, we introduce lift-the-flap games for computational models. The task is to reason about the scene context and infer what the target behind the flap is in a natural image. Context reasoning is critical in many computer vision applications, such as object recognition and semantic segmentation. To tackle this problem, we propose an object-centered graph representing the scene configuration of the image where each node corresponds to a group of objects belonging to the same category. To infer the target's class label, we introduce an object-centered graph network model consisting of two sub-networks. The classification sub-network takes the complete graph as input and outputs a classification vector assigning the probability for each class. The reinforcement learning sub-network exploits the class label dependencies and learns the joint probability among objects in order to generate multiple reasonable answers for the missing target. To evaluate our model's performance, we carry out human behavioral experiments for lift-the-flap games as a benchmark. Our model makes reasonable inferences compared to humans, and significantly outperforms all the null models. We also demonstrate the usefulness of our object-centered graph network model in context-aware object recognition and target priming in visual search.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.00163](http://arxiv.org/abs/1902.00163)

##### PDF
[http://arxiv.org/pdf/1902.00163](http://arxiv.org/pdf/1902.00163)

