---
layout: post
title: "ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases"
date: 2018-07-20 16:57:30
categories: arXiv_AI
tags: arXiv_AI Adversarial Image_Classification Classification Deep_Learning Prediction
author: Pierre Stock, Moustapha Cisse
mathjax: true
---

* content
{:toc}

##### Abstract
ConvNets and Imagenet have driven the recent success of deep learning for image classification. However, the marked slowdown in performance improvement combined with the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases question the reliability of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. The contribution of this study is threefold. We first experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are vastly underestimated. Next, we show that explanations can mitigate the impact of misclassified adversarial examples from the perspective of the end-user. We finally introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a valuable tool both for improving our understanding of ConvNets' predictions and for designing more reliable models.

##### Abstract (translated by Google)
ConvNets和Imagenet推动了最近深度学习图像分类的成功。然而，性能改进的显着减缓加上神经网络对对抗性实例缺乏稳健性以及它们表现出不希望的偏差的倾向，都会质疑这些方法的可靠性。这项工作通过使用人类学科研究和解释从最终用户的角度研究这些问题。这项研究的贡献有三个方面。我们首先通过实验证明，在Imagenet上测量的ConvNets的准确性和稳健性被大大低估了。接下来，我们展示解释可以从最终用户的角度减轻错误分类的对抗性示例的影响。我们最终介绍了一种新工具，用于揭示模型学到的不良偏差。这些贡献还表明，解释是一个有价值的工具，既可以提高我们对ConvNets预测的理解，也可以设计更可靠的模型。

##### URL
[http://arxiv.org/abs/1711.11443](http://arxiv.org/abs/1711.11443)

##### PDF
[http://arxiv.org/pdf/1711.11443](http://arxiv.org/pdf/1711.11443)

