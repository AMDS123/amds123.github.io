---
layout: post
title: "Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets"
date: 2017-09-13 09:43:02
categories: arXiv_CL
tags: arXiv_CL Sentiment Embedding RNN
author: Jeremy Barnes, Roman Klinger, Sabine Schulte im Walde
mathjax: true
---

* content
{:toc}

##### Abstract
There has been a good amount of progress in sentiment analysis over the past 10 years, including the proposal of new methods and the creation of benchmark datasets. In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task. Accordingly, it is hard to understand how well a certain model generalizes across different tasks and datasets. In this paper, we contribute to this situation by comparing several models on six different benchmarks, which belong to different domains and additionally have different levels of granularity (binary, 3-class, 4-class and 5-class). We show that Bi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are particularly good at fine-grained sentiment tasks (i. e., with more than two classes). Incorporating sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the SenTube datasets.

##### Abstract (translated by Google)
在过去的十年中，情绪分析有了很大的进展，包括提出新的方法和建立基准数据集。然而，在一些论文中，只有一个或两个数据集才有比较模型的趋势，或者是因为时间限制，或者是因为模型是针对特定任务量身定做的。因此，很难理解某个模型在不同的任务和数据集上的推广情况。在本文中，我们通过比较六个不同领域的不同级别（二进制，三级，四级和五级）不同级别的模型，对这种情况做出贡献。我们显示Bi-LSTM在数据集中表现良好，并且LSTM和Bi-LSTM在细粒度的情感任务（即，具有两个以上的类）上特别好。将情绪信息与训练期间的词嵌入相结合，对于训练数据在词汇上类似的数据集给出良好的结果。通过我们的实验，我们有助于更好地理解不同模型体系结构在不同数据集上的性能。因此，我们在SenTube数据集上检测出新颖的最新结果。

##### URL
[https://arxiv.org/abs/1709.04219](https://arxiv.org/abs/1709.04219)

##### PDF
[https://arxiv.org/pdf/1709.04219](https://arxiv.org/pdf/1709.04219)

