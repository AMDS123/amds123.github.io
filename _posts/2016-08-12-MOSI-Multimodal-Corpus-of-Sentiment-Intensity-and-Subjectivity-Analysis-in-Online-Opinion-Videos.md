---
layout: post
title: "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos"
date: 2016-08-12 02:39:40
categories: arXiv_CL
tags: arXiv_CL Sentiment Review Attention
author: Amir Zadeh, Rowan Zellers, Eli Pincus, Louis-Philippe Morency
mathjax: true
---

* content
{:toc}

##### Abstract
People are sharing their opinions, stories and reviews through online video sharing websites every day. Studying sentiment and subjectivity in these opinion videos is experiencing a growing attention from academia and industry. While sentiment analysis has been successful for text, it is an understudied research question for videos and multimedia content. The biggest setbacks for studies in this direction are lack of a proper dataset, methodology, baselines and statistical analysis of how information from different modality sources relate to each other. This paper introduces to the scientific community the first opinion-level annotated corpus of sentiment and subjectivity analysis in online videos called Multimodal Opinion-level Sentiment Intensity dataset (MOSI). The dataset is rigorously annotated with labels for subjectivity, sentiment intensity, per-frame and per-opinion annotated visual features, and per-milliseconds annotated audio features. Furthermore, we present baselines for future studies in this direction as well as a new multimodal fusion approach that jointly models spoken words and visual gestures.

##### Abstract (translated by Google)
人们每天都通过在线视频分享网站分享他们的意见，故事和评论。在这些观点视频中学习情感和主观性受到学术界和工业界的关注。尽管情感分析对于文本而言是成功的，但是对于视频和多媒体内容来说，这是一个不为人知的研究问题。在这个方向研究的最大挫折是缺乏一个适当的数据集，方法，基线和统计分析来自不同形式来源的信息是如何相互关联的。本文首先向科学界介绍了多媒体意见层次情感强度数据集（MOSI）在线视频中的第一个意见层注释情感主观性分析语料库。数据集使用主观性，情感强度，每帧和每个观点注释的视觉特征以及每毫秒注释音频特征的标签进行严格标注。此外，我们为这个方向的未来研究提供基线，以及一个新的多模态融合方法，共同模拟说话的语言和视觉的手势。

##### URL
[https://arxiv.org/abs/1606.06259](https://arxiv.org/abs/1606.06259)

##### PDF
[https://arxiv.org/pdf/1606.06259](https://arxiv.org/pdf/1606.06259)

