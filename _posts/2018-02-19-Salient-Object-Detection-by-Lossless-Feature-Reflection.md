---
layout: post
title: "Salient Object Detection by Lossless Feature Reflection"
date: 2018-02-19 05:59:08
categories: arXiv_CV
tags: arXiv_CV Salient Object_Detection CNN Prediction Detection
author: Pingping Zhang, Wei Liu, Huchuan Lu, Chunhua Shen
mathjax: true
---

* content
{:toc}

##### Abstract
Salient object detection, which aims to identify and locate the most salient pixels or regions in images, has been attracting more and more interest due to its various real-world applications. However, this vision task is quite challenging, especially under complex image scenes. Inspired by the intrinsic reflection of natural images, in this paper we propose a novel feature learning framework for large-scale salient object detection. Specifically, we design a symmetrical fully convolutional network (SFCN) to learn complementary saliency features under the guidance of lossless feature reflection. The location information, together with contextual and semantic information, of salient objects are jointly utilized to supervise the proposed network for more accurate saliency predictions. In addition, to overcome the blurry boundary problem, we propose a new structural loss function to learn clear object boundaries and spatially consistent saliency. The coarse prediction results are effectively refined by these structural information for performance improvements. Extensive experiments on seven saliency detection datasets demonstrate that our approach achieves consistently superior performance and outperforms the very recent state-of-the-art methods.

##### Abstract (translated by Google)
显着物体检测，旨在识别和定位图像中最显着的像素或区域，由于其各种实际应用而引起越来越多的兴趣。然而，这个视觉任务是非常具有挑战性的，特别是在复杂的图像场景下。受到自然图像内在反映的启发，本文提出了一种新颖的大规模显着物体检测特征学习框架。具体而言，我们设计了一个对称完全卷积网络（SFCN），以在无损特征反射的指导下学习互补显着特征。显着对象的位置信息连同上下文和语义信息被共同利用来监督所提出的网络以用于更准确的显着性预测。另外，为了克服模糊的边界问题，我们提出了一种新的结构损失函数来学习清晰的物体边界和空间一致的显着性。粗略的预测结果可以通过这些结构信息进行有效改进，以提高性能。对七个显着性检测数据集进行的大量实验表明，我们的方法实现了始终如一的卓越性能，并且胜过了最新的最先进的方法。

##### URL
[http://arxiv.org/abs/1802.06527](http://arxiv.org/abs/1802.06527)

##### PDF
[http://arxiv.org/pdf/1802.06527](http://arxiv.org/pdf/1802.06527)

