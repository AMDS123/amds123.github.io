---
layout: post
title: "Word2Bits - Quantized Word Vectors"
date: 2018-03-15 09:21:34
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Maximilian Lam
mathjax: true
---

* content
{:toc}

##### Abstract
Word vectors require significant amounts of memory and storage, posing issues to resource limited devices like mobile phones and GPUs. We show that high quality quantized word vectors using 1-2 bits per parameter can be learned by introducing a quantization function into Word2Vec. We furthermore show that training with the quantization function acts as a regularizer. We train word vectors on English Wikipedia (2017) and evaluate them on standard word similarity and analogy tasks and on question answering (SQuAD). Our quantized word vectors not only take 8-16x less space than full precision (32 bit) word vectors but also outperform them on word similarity tasks and question answering.

##### Abstract (translated by Google)
Word向量需要大量的内存和存储空间，这对资源有限的设备（如手机和GPU）造成了问题。我们表明，通过在Word2Vec中引入量化函数，可以学习使用每个参数1-2位的高质量量化字向量。我们还表明，用量化函数进行训练就像一个正规化器。我们在英语维基百科（2017）上训练单词向量，并对标准单词相似性和类比任务以及问题回答（SQUAD）进行评估。我们的量化词向量不仅比全精度（32位）的单词向量少8-16倍的空间，而且在单词相似性任务和问题回答方面也优于它们。

##### URL
[https://arxiv.org/abs/1803.05651](https://arxiv.org/abs/1803.05651)

##### PDF
[https://arxiv.org/pdf/1803.05651](https://arxiv.org/pdf/1803.05651)

