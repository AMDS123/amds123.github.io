---
layout: post
title: "Adversarial Contrastive Estimation"
date: 2018-05-09 04:06:30
categories: arXiv_AI
tags: arXiv_AI Adversarial Knowledge_Graph Knowledge Embedding
author: Avishek Bose, Huan Ling, Yanshuai Cao
mathjax: true
---

* content
{:toc}

##### Abstract
Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.

##### Abstract (translated by Google)
通过对比正面和负面样本进行学习是许多方法采用的一般策略。用于词语嵌入的噪声对比估计（NCE）和用于知识图表的翻译嵌入是使用该方法的NLP中的示例。在这项工作中，我们将对比学习视为所有这些方法的抽象，并将负面采样器扩展为包含对手学习的采样器的混合分布。由此产生的自适应采样器找到了更难以消除的例子，这迫使主模型学习更好的数据表示。我们评估我们关于学习单词嵌入，顺序嵌入和知识图嵌入的建议，并观察更快的收敛性和改进的多指标结果。

##### URL
[http://arxiv.org/abs/1805.03642](http://arxiv.org/abs/1805.03642)

##### PDF
[http://arxiv.org/pdf/1805.03642](http://arxiv.org/pdf/1805.03642)

