---
layout: post
title: "Adaptive Gradient Refinement for Adversarial Perturbation Generation"
date: 2019-02-01 10:47:21
categories: arXiv_CV
tags: arXiv_CV Adversarial Image_Classification Classification Prediction
author: Yatie Xiao, Chi-Man Pun, Xia Du, Jizhe Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks have achieved remarkable success in computer vision, natural language processing, and audio tasks. However, in classification domains, researches proved that Deep neural models are easily fooled and make different or wrong classification prediction, which may cause server results. Many attack methods generate adversarial perturbation with large-scale pixel modification and low cosine-similarity between origin and corresponding adversarial examples, to address these issues, we propose an adversarial method with adaptive adjusting perturbation strength and update gradient in direction to generate attacks, it generate perturbation tensors by adjusting its strength adaptively and update gradient in direction which can escape local minimal or maximal by combining with previous calculate history gradient. In this paper, we evaluate several traditional perturbations creating methods in image classification with ours. Experimental results show that our approach works well and outperform recent techniques in the change of misclassifying image classification, and excellent efficiency in fooling deep network models.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.01220](http://arxiv.org/abs/1902.01220)

##### PDF
[http://arxiv.org/pdf/1902.01220](http://arxiv.org/pdf/1902.01220)

