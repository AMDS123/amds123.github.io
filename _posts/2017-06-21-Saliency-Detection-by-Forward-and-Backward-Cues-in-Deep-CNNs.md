---
layout: post
title: "Saliency Detection by Forward and Backward Cues in Deep-CNNs"
date: 2017-06-21 09:04:55
categories: arXiv_CV
tags: arXiv_CV Salient Knowledge Attention Weakly_Supervised CNN Detection Relation
author: Nevrez Imamoglu, Chi Zhang, Wataru Shimoda, Yuming Fang, Boxin Shi
mathjax: true
---

* content
{:toc}

##### Abstract
As prior knowledge of objects or object features helps us make relations for similar objects on attentional tasks, pre-trained deep convolutional neural networks (CNNs) can be used to detect salient objects on images regardless of the object class is in the network knowledge or not. In this paper, we propose a top-down saliency model using CNN, a weakly supervised CNN model trained for 1000 object labelling task from RGB images. The model detects attentive regions based on their objectness scores predicted by selected features from CNNs. To estimate the salient objects effectively, we combine both forward and backward features, while demonstrating that partially-guided backpropagation will provide sufficient information for selecting the features from forward run of CNN model. Finally, these top-down cues are enhanced with a state-of-the-art bottom-up model as complementing the overall saliency. As the proposed model is an effective integration of forward and backward cues through objectness without any supervision or regression to ground truth data, it gives promising results compared to state-of-the-art models in two different datasets.

##### Abstract (translated by Google)
由于对象或对象特征的先验知识可以帮助我们在注意任务上建立类似对象的关系，所以可以使用预先训练的深度卷积神经网络（CNN）来检测图像上的显着对象，而不管对象类别是否在网络知识中。在本文中，我们提出了一个自顶向下的显着性模型使用CNN，一个弱监督CNN模型训练了1000个对象标记任务从RGB图像。该模型根据从CNN选定的特征预测的对象分数检测细心区域。为了有效地估计显着对象，我们结合了前向和后向特征，同时证明了部分向导反向传播将为选择CNN模型的前向特征提供足够的信息。最后，这些自顶向下的线索被加强了，作为补充整体显着性的最先进的自下而上的模式。由于所提出的模型是通过对象而有效地整合前向和后向线索，而不对任何地面实况数据进行任何监督或回归，因此与两个不同数据集中的最新模型相比，它提供了有希望的结果。

##### URL
[https://arxiv.org/abs/1703.00152](https://arxiv.org/abs/1703.00152)

##### PDF
[https://arxiv.org/pdf/1703.00152](https://arxiv.org/pdf/1703.00152)

