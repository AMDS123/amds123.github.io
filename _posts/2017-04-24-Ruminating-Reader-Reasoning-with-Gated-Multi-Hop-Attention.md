---
layout: post
title: "Ruminating Reader: Reasoning with Gated Multi-Hop Attention"
date: 2017-04-24 18:49:38
categories: arXiv_CL
tags: arXiv_CL Attention
author: Yichen Gong, Samuel R. Bowman
mathjax: true
---

* content
{:toc}

##### Abstract
To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems.

##### Abstract (translated by Google)
为了回答机器理解（MC）任务中的问题，模型需要建立问题与上下文之间的交互。针对单程模型无法反思和纠正的问题，提出反刍读者。反刍读卡器向双向注意流模型（BiDAF）注入流模型（BiDAF）添加了第二关注点和新的信息融合组件。我们提出了新颖的层结构，构建了一个查询意识的上下文向量表示和融合编码表示与BiDAF模型顶部的中间表示。我们表明，多跳注意机制可以应用于双向注意力结构。在SQUAD的实验中，我们发现读者的表现优于BiDAF的基准，并且匹配或超过了所有其他已发表系统的表现。

##### URL
[https://arxiv.org/abs/1704.07415](https://arxiv.org/abs/1704.07415)

##### PDF
[https://arxiv.org/pdf/1704.07415](https://arxiv.org/pdf/1704.07415)

