---
layout: post
title: "Learning Image Representations by Completing Damaged Jigsaw Puzzles"
date: 2018-02-06 10:42:28
categories: arXiv_CV
tags: arXiv_CV Image_Caption Segmentation CNN Transfer_Learning Semantic_Segmentation Represenation_Learning Classification
author: Dahun Kim, Donghyeon Cho, Donggeun Yoo, In So Kweon
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we explore methods of complicating self-supervised tasks for representation learning. That is, we do severe damage to data and encourage a network to recover them. First, we complicate each of three powerful self-supervised task candidates: jigsaw puzzle, inpainting, and colorization. In addition, we introduce a novel complicated self-supervised task called "Completing damaged jigsaw puzzles" which is puzzles with one piece missing and the other pieces without color. We train a convolutional neural network not only to solve the puzzles, but also generate the missing content and colorize the puzzles. The recovery of the aforementioned damage pushes the network to obtain robust and general-purpose representations. We demonstrate that complicating the self-supervised tasks improves their original versions and that our final task learns more robust and transferable representations compared to the previous methods, as well as the simple combination of our candidate tasks. Our approach achieves state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.

##### Abstract (translated by Google)
在本文中，我们探讨复杂的自我监督任务表示学习的方法。也就是说，我们对数据造成了严重的损害，并鼓励网络来恢复它们。首先，我们将三个强大的自我监督的任务候选人拼图：拼图，修补和彩色化。此外，我们还引入了一个新的复杂的自我监督的任务，称为“完成损坏的拼图”，这是一个失踪的拼图，其他没有颜色的拼图。我们训练一个卷积神经网络，不仅要解决谜题，还要生成缺失的内容，使谜题着色。上述损害的恢复推动网络获得强健和通用的表示。我们证明，使自我监督的任务复杂化改善了他们的原始版本，并且与先前的方法相比，我们的最终任务学习了更鲁棒和可转移的表示，以及我们的候选任务的简单组合。我们的方法在PASCAL分类和语义分割的转换学习方面达到了最先进的性能。

##### URL
[http://arxiv.org/abs/1802.01880](http://arxiv.org/abs/1802.01880)

##### PDF
[http://arxiv.org/pdf/1802.01880](http://arxiv.org/pdf/1802.01880)

