---
layout: post
title: "This Time with Feeling: Learning Expressive Musical Performance"
date: 2018-08-10 21:53:51
categories: arXiv_SD
tags: arXiv_SD RNN
author: Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, Karen Simonyan
mathjax: true
---

* content
{:toc}

##### Abstract
Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct $\it performance$ generation: jointly predicting the notes $\it and$ $\it also$ their expressive timing and dynamics. We consider the significance and qualities of the data set needed for this. Having identified both a problem domain and characteristics of an appropriate data set, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.

##### Abstract (translated by Google)
音乐生成一般侧重于创建乐谱或解释它们。我们讨论了这两个问题之间的差异，并提出，事实上，在直接$ \ it性能$ generation的工作空间工作可能是有价值的：联合预测笔记$ \ it和$ $ \它们也是他们的表达时间和动力学。我们考虑了这需要的数据集的重要性和质量。在确定了问题域和适当数据集的特征后，我们展示了一种基于LSTM的循环网络模型，该模型主观上在此任务上表现良好。关键的是，我们提供生成的示例。我们还包括来自专业作曲家和音乐家的反馈，其中包括一些这样的例子。

##### URL
[http://arxiv.org/abs/1808.03715](http://arxiv.org/abs/1808.03715)

##### PDF
[http://arxiv.org/pdf/1808.03715](http://arxiv.org/pdf/1808.03715)

