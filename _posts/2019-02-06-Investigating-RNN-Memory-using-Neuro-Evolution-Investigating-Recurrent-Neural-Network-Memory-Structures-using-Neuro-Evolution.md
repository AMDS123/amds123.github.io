---
layout: post
title: "Investigating RNN Memory using Neuro-Evolution: Investigating Recurrent Neural Network Memory Structures using Neuro-Evolution"
date: 2019-02-06 20:33:12
categories: arXiv_AI
tags: arXiv_AI RNN Prediction
author: Alexander Ororbia, Ahmed Ahmed Elsaid, Travis Desell
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a new algorithm, Evolutionary eXploration of Augmenting Memory Models (EXAMM), which is capable of evolving recurrent neural networks (RNNs) using a wide variety of memory structures, such as Delta-RNN, GRU, LSTM, MGU and UGRNN cells. EXAMM evolved RNNs to perform prediction of large-scale, real world time series data from the aviation and power industries. These data sets consist of very long time series (thousands of readings), each with a large number of potentially correlated and dependent parameters. Four different parameters were selected for prediction and EXAMM runs were performed using each memory cell type alone, each cell type with feed forward nodes, and with all possible memory cell types. Evolved RNN performance was measured using repeated k-fold cross validation, resulting in 1210 EXAMM runs which evolved 2,420,000 RNNs in 12,100 CPU hours on a high performance computing cluster. Generalization of the evolved RNNs was examined statistically, providing interesting findings that can help refine the RNN memory cell design as well as inform future neuro-evolution algorithms development.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.02390](http://arxiv.org/abs/1902.02390)

##### PDF
[http://arxiv.org/pdf/1902.02390](http://arxiv.org/pdf/1902.02390)

