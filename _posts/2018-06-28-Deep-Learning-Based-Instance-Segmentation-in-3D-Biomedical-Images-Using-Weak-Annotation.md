---
layout: post
title: "Deep Learning Based Instance Segmentation in 3D Biomedical Images Using Weak Annotation"
date: 2018-06-28 18:22:52
categories: arXiv_CV
tags: arXiv_CV Segmentation Face Deep_Learning
author: Zhuo Zhao, Lin Yang, Hao Zheng, Ian H. Guldner, Siyuan Zhang, Danny Z. Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Instance segmentation in 3D images is a fundamental task in biomedical image analysis. While deep learning models often work well for 2D instance segmentation, 3D instance segmentation still faces critical challenges, such as insufficient training data due to various annotation difficulties in 3D biomedical images. Common 3D annotation methods (e.g., full voxel annotation) incur high workloads and costs for labeling enough instances for training deep learning 3D instance segmentation models. In this paper, we propose a new weak annotation approach for training a fast deep learning 3D instance segmentation model without using full voxel mask annotation. Our approach needs only 3D bounding boxes for all instances and full voxel annotation for a small fraction of the instances, and uses a novel two-stage 3D instance segmentation model utilizing these two kinds of annotation, respectively. We evaluate our approach on several biomedical image datasets, and the experimental results show that (1) with full annotated boxes and a small amount of masks, our approach can achieve similar performance as the best known methods using full annotation, and (2) with similar annotation time, our approach outperforms the best known methods that use full annotation.

##### Abstract (translated by Google)
3D图像中的实例分割是生物医学图像分析中的基本任务。虽然深度学习模型通常适用于2D实例分割，但3D实例分割仍然面临关键挑战，例如由于3D生物医学图像中的各种注释困难导致的训练数据不足。常见的三维注释方法（例如全体素注释）会产生较高的工作量和成本，用于标记足够的实例来训练深度学习三维实例分割模型。在本文中，我们提出了一种新的弱注释方法来训练快速深度学习3D实例分割模型，而不使用全体素掩模注释。我们的方法仅需要针对所有实例的3D边界框和针对一小部分实例的完整体素注释，并且分别使用利用这两种注释的新颖的两阶段3D实例分割模型。我们在几个生物医学图像数据集上评估我们的方法，实验结果表明（1）使用完整注释框和少量掩模，我们的方法可以实现与使用完整注释的最着名方法类似的性能，以及（2）与类似的注释时间，我们的方法优于使用完整注释的最着名的方法。

##### URL
[http://arxiv.org/abs/1806.11137](http://arxiv.org/abs/1806.11137)

##### PDF
[http://arxiv.org/pdf/1806.11137](http://arxiv.org/pdf/1806.11137)

