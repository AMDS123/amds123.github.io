---
layout: post
title: "Using NLU in Context for Question Answering: Improving on Facebook's bAbI Tasks"
date: 2017-09-21 01:19:52
categories: arXiv_CL
tags: arXiv_CL Knowledge Face Tracking Deep_Learning
author: John S. Ball
mathjax: true
---

* content
{:toc}

##### Abstract
For the next step in human to machine interaction, Artificial Intelligence (AI) should interact predominantly using natural language because, if it worked, it would be the fastest way to communicate. Facebook's toy tasks (bAbI) provide a useful benchmark to compare implementations for conversational AI. While the published experiments so far have been based on exploiting the distributional hypothesis with machine learning, our model exploits natural language understanding (NLU) with the decomposition of language based on Role and Reference Grammar (RRG) and the brain-based Patom theory. Our combinatorial system for conversational AI based on linguistics has many advantages: passing bAbI task tests without parsing or statistics while increasing scalability. Our model validates both the training and test data to find 'garbage' input and output (GIGO). It is not rules-based, nor does it use parts of speech, but instead relies on meaning. While Deep Learning is difficult to debug and fix, every step in our model can be understood and changed like any non-statistical computer program. Deep Learning's lack of explicable reasoning has raised opposition to AI, partly due to fear of the unknown. To support the goals of AI, we propose extended tasks to use human-level statements with tense, aspect and voice, and embedded clauses with junctures: and answers to be natural language generation (NLG) instead of keywords. While machine learning permits invalid training data to produce incorrect test responses, our system cannot because the context tracking would need to be intentionally broken. We believe no existing learning systems can currently solve these extended natural language tests. There appears to be a knowledge gap between NLP researchers and linguists, but ongoing competitive results such as these promise to narrow that gap.

##### Abstract (translated by Google)
对于人机交互的下一步，人工智能（AI）应该主要使用自然语言进行交互，因为如果它工作的话，这将是最快速的沟通方式。 Facebook的玩具任务（bAbI）提供了一个有用的基准来比较会话式AI的实现。虽然到目前为止发表的实验都是基于利用机器学习的分布假设，但是我们的模型利用基于角色和参考语法（RRG）和基于脑的Patom理论的语言分解来利用自然语言理解（NLU）。我们的基于语言学的会话式AI组合系统具有许多优点：在不增加分析或统计的情况下通过bAbI任务测试，同时提高可扩展性。我们的模型验证训练和测试数据，以找到“垃圾”输入和输出（GIGO）。它不是以规则为基础的，也不是使用词性，而是依赖于意义。虽然深度学习很难调试和修复，但我们模型中的每一步都可以像任何非统计计算机程序一样被理解和改变。深度学习缺乏可解释的推理，已经引起人们对AI的反对，部分原因是对未知的恐惧。为了支持人工智能的目标，我们提出了扩展任务，使用时态，方面和语音的人类语句，嵌入式语句和关联语句，并且应用自然语言生成（NLG）而不是关键字。虽然机器学习允许无效的训练数据产生不正确的测试响应，但我们的系统不能因为上下文跟踪需要被故意破坏。我们相信目前没有现有的学习系统可以解决这些扩展的自然语言测试NLP研究人员和语言学家之间似乎存在知识差距，但持续的竞争结果（如这些）有望缩小这一差距。

##### URL
[https://arxiv.org/abs/1709.04558](https://arxiv.org/abs/1709.04558)

##### PDF
[https://arxiv.org/pdf/1709.04558](https://arxiv.org/pdf/1709.04558)

