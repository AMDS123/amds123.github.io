---
layout: post
title: "Crowdsourcing Ground Truth for Medical Relation Extraction"
date: 2017-10-03 15:04:43
categories: arXiv_SD
tags: arXiv_SD Relation_Extraction Relation
author: Anca Dumitrache, Lora Aroyo, Chris Welty
mathjax: true
---

* content
{:toc}

##### Abstract
Cognitive computing systems require human labeled data for evaluation, and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, that reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the $cause$ and $treat$ relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task.

##### Abstract (translated by Google)
认知计算系统需要人工标记的数据进行评估，通常需要进行培训。收集这些数据所使用的标准做法最大限度地减少了注释者之间的分歧，并且我们发现这导致数据不能解释语言中固有的模糊性。我们提出了CrowdTruth方法，通过众包来收集地面真相，重新考虑人们在机器学习中的作用，基于观察者之间的不一致为文本中的歧义现象提供了有用的信号。我们报告使用这种方法为$ cause $和$ treat $关系的医疗关系提取建立一个注释数据集，以及这个数据是如何在一个有监督的训练实验中进行的。我们证明，通过建模歧义，标签数据收集的人群工作者可以（1）达到这个任务的领域专家的质量水平，同时降低成本，和（2）提供比远程监督规模更好的训练数据。我们进一步提出并验证了精确度，召回率和F-度量的新的加权度量，这个度量在这个任务中解释了人力和机器性能的模糊性。

##### URL
[https://arxiv.org/abs/1701.02185](https://arxiv.org/abs/1701.02185)

##### PDF
[https://arxiv.org/pdf/1701.02185](https://arxiv.org/pdf/1701.02185)

