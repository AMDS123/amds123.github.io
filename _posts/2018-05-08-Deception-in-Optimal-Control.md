---
layout: post
title: "Deception in Optimal Control"
date: 2018-05-08 15:08:40
categories: arXiv_AI
tags: arXiv_AI Adversarial Knowledge
author: Melkior Ornik, Ufuk Topcu
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we consider an adversarial scenario where one agent seeks to achieve an objective and its adversary seeks to learn the agent's intentions and prevent the agent from achieving its objective. The agent has an incentive to try to deceive the adversary about its intentions, while at the same time working to achieve its objective. The primary contribution of this paper is to introduce a mathematically rigorous framework for the notion of deception within the context of optimal control. The central notion introduced in the paper is that of a belief-induced reward: a reward dependent not only on the agent's state and action, but also adversary's beliefs. Design of an optimal deceptive strategy then becomes a question of optimal control design on the product of the agent's state space and the adversary's belief space. The proposed framework allows for deception to be defined in an arbitrary control system endowed with a reward function, as well as with additional specifications limiting the agent's control policy. In addition to defining deception, we discuss design of optimally deceptive strategies under uncertainties in agent's knowledge about the adversary's learning process. In the latter part of the paper, we focus on a setting where the agent's behavior is governed by a Markov decision process, and show that the design of optimally deceptive strategies under lack of knowledge about the adversary naturally reduces to previously discussed problems in control design on partially observable or uncertain Markov decision processes. Finally, we present two examples of deceptive strategies: a "cops and robbers" scenario and an example where an agent may use camouflage while moving. We show that optimally deceptive strategies in such examples follow the intuitive idea of how to deceive an adversary in the above settings.

##### Abstract (translated by Google)
在本文中，我们考虑一个敌对的情景，其中一个代理寻求实现一个目标，而其敌手试图学习代理的意图并阻止代理实现其目标。代理人有动机试图欺骗对手的意图，同时努力实现其目标。本文的主要贡献是在最优控制范围内引入欺骗概念的数学严格框架。本文中引入的中心概念是信仰诱导奖励：奖励不仅取决于行为人的状态和行为，还取决于敌方的信念。最优欺骗策略的设计则成为代理人状态空间与敌手信仰空间的最优控制设计问题。所提出的框架允许在具有奖励功能的任意控制系统中定义欺骗，以及限制代理人控制策略的附加规范。除了定义欺骗之外，我们还讨论了代理人对敌人学习过程的了解的不确定性下最佳欺骗策略的设计。在本文的后半部分，我们将重点放在一个环境中，其中的行为是由马尔科夫决策过程控制的，并且表明在缺乏对对手知识的情况下设计最佳欺骗策略自然会降低到之前讨论的控制设计问题关于部分可观察或不确定的马尔可夫决策过程。最后，我们举两个欺骗策略的例子：一个“警察和强盗”的场景和一个代理人在移动时可能使用伪装​​的例子。我们表明，在这些例子中，最佳欺骗策略遵循如何在上述设置中欺骗对手的直观想法。

##### URL
[https://arxiv.org/abs/1805.03090](https://arxiv.org/abs/1805.03090)

##### PDF
[https://arxiv.org/pdf/1805.03090](https://arxiv.org/pdf/1805.03090)

