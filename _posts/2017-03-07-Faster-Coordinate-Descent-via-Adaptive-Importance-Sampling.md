---
layout: post
title: "Faster Coordinate Descent via Adaptive Importance Sampling"
date: 2017-03-07 18:36:55
categories: arXiv_CV
tags: arXiv_CV Optimization
author: Dmytro Perekrestenko, Volkan Cevher, Martin Jaggi
mathjax: true
---

* content
{:toc}

##### Abstract
Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.

##### Abstract (translated by Google)
坐标下降法采用决策变量的随机部分更新来解决大规模凸优化问题。在这项工作中，我们为随机选择更新引入了新的自适应规则。通过自适应，我们的意思是我们的选择规则是基于双残差或原始 - 双缺口估计，并且可以在每次迭代中改变。我们从理论上刻画了我们的选择规则的性能，并展示了对最新技术的改进，并将我们的理论和算法扩展到一般的凸面目标。铰链损失支持向量机和Lasso的数值证据证实了这一实践遵循了理论。

##### URL
[https://arxiv.org/abs/1703.02518](https://arxiv.org/abs/1703.02518)

##### PDF
[https://arxiv.org/pdf/1703.02518](https://arxiv.org/pdf/1703.02518)

