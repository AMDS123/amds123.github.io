---
layout: post
title: "Hybrid VAE: Improving Deep Generative Models using Partial Observations"
date: 2017-11-30 18:37:37
categories: arXiv_CV
tags: arXiv_CV Regularization
author: Sergey Tulyakov, Andrew Fitzgibbon, Sebastian Nowozin
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural network models trained on large labeled datasets are the state-of-the-art in a large variety of computer vision tasks. In many applications, however, labeled data is expensive to obtain or requires a time consuming manual annotation process. In contrast, unlabeled data is often abundant and available in large quantities. We present a principled framework to capitalize on unlabeled data by training deep generative models on both labeled and unlabeled data. We show that such a combination is beneficial because the unlabeled data acts as a data-driven form of regularization, allowing generative models trained on few labeled samples to reach the performance of fully-supervised generative models trained on much larger datasets. We call our method Hybrid VAE (H-VAE) as it contains both the generative and the discriminative parts. We validate H-VAE on three large-scale datasets of different modalities: two face datasets: (MultiPIE, CelebA) and a hand pose dataset (NYU Hand Pose). Our qualitative visualizations further support improvements achieved by using partial observations.

##### Abstract (translated by Google)
在大型标记数据集上训练的深度神经网络模型是各种计算机视觉任务中的最新技术。然而，在许多应用中，标记的数据是昂贵的，或者需要耗时的手动注释过程。相比之下，无标签的数据往往是丰富的，可以大量使用。我们提出了一个原则性的框架，通过在标记和未标记的数据上训练深度生成模型来利用未标记的数据。我们表明，这样的组合是有益的，因为无标签数据作为正规化的数据驱动形式，允许在少数标签样本上训练的生成模型达到在大得多的数据集上训练的完全监督生成模型的性能。我们将其称为Hybrid VAE（H-VAE）方法，因为它包含生成部分和判别部分。我们在三种不同形式的大规模数据集上验证H-VAE：两个人脸数据集（MultiPIE，CelebA）和一个手势数据集（NYU Hand Pose）。我们的定性可视化进一步支持使用部分观测获得的改进。

##### URL
[https://arxiv.org/abs/1711.11566](https://arxiv.org/abs/1711.11566)

##### PDF
[https://arxiv.org/pdf/1711.11566](https://arxiv.org/pdf/1711.11566)

