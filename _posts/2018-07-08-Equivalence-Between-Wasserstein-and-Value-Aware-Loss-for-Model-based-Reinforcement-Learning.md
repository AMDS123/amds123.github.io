---
layout: post
title: "Equivalence Between Wasserstein and Value-Aware Loss for Model-based Reinforcement Learning"
date: 2018-07-08 12:53:13
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Kavosh Asadi, Evan Cater, Dipendra Misra, Michael L. Littman
mathjax: true
---

* content
{:toc}

##### Abstract
Learning a generative model is a key component of model-based reinforcement learning. Though learning a good model in the tabular setting is a simple task, learning a useful model in the approximate setting is challenging. In this context, an important question is the loss function used for model learning as varying the loss function can have a remarkable impact on effectiveness of planning. Recently Farahmand et al. (2017) proposed a value-aware model learning (VAML) objective that captures the structure of value function during model learning. Using tools from Asadi et al. (2018), we show that minimizing the VAML objective is in fact equivalent to minimizing the Wasserstein metric. This equivalence improves our understanding of value-aware models, and also creates a theoretical foundation for applications of Wasserstein in model-based reinforcement~learning.

##### Abstract (translated by Google)
学习生成模型是基于模型的强化学习的关键组成部分。虽然在表格设置中学习一个好的模型是一项简单的任务，但在近似设置中学习一个有用的模型是具有挑战性的。在这种情况下，一个重要的问题是用于模型学习的损失函数，因为改变损失函数会对规划的有效性产生显着影响。最近Farahmand等人。 （2017）提出了一种价值感知模型学习（VAML）目标，它捕捉模型学习过程中价值功能的结构。使用Asadi等人的工具。 （2018），我们证明最小化VAML目标实际上等同于最小化Wasserstein度量。这种等价性提高了我们对价值感知模型的理解，也为Wasserstein在基于模型的强化学习中的应用创造了理论基础。

##### URL
[http://arxiv.org/abs/1806.01265](http://arxiv.org/abs/1806.01265)

##### PDF
[http://arxiv.org/pdf/1806.01265](http://arxiv.org/pdf/1806.01265)

