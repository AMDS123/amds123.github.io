---
layout: post
title: "Learning Inductive Biases with Simple Neural Networks"
date: 2018-02-08 08:25:51
categories: arXiv_CL
tags: arXiv_CL Knowledge Inference Recognition
author: Reuben Feinman, Brenden M. Lake
mathjax: true
---

* content
{:toc}

##### Abstract
People use rich prior knowledge about the world in order to efficiently learn new concepts. These priors - also known as "inductive biases" - pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data. A recent study found that deep neural networks optimized for object recognition develop the shape bias (Ritter et al., 2017), an inductive bias possessed by children that plays an important role in early word learning. However, these networks use unrealistically large quantities of training data, and the conditions required for these biases to develop are not well understood. Moreover, it is unclear how the learning dynamics of these networks relate to developmental processes in childhood. We investigate the development and influence of the shape bias in neural networks using controlled datasets of abstract patterns and synthetic images, allowing us to systematically vary the quantity and form of the experience provided to the learning algorithms. We find that simple neural networks develop a shape bias after seeing as few as 3 examples of 4 object categories. The development of these biases predicts the onset of vocabulary acceleration in our networks, consistent with the developmental process in children.

##### Abstract (translated by Google)
人们利用丰富的关于世界的先验知识来有效地学习新的概念。这些先验 - 也被称为“归纳偏见” - 与学习者考虑的内部模型的空间有关，它们帮助学习者进行超出观察数据的推论。最近的一项研究发现，针对物体识别进行优化的深度神经网络会形成形状偏差（Ritter et al。，2017），这是儿童在早期词汇学习中起重要作用的归纳偏见。然而，这些网络使用不切实际的大量的训练数据，而这些偏差发展所需的条件并不完全清楚。此外，这些网络的学习动力如何与儿童发育过程相关尚不清楚。我们使用抽象模式和合成图像的受控数据集来调查神经网络中形状偏差的发展和影响，使我们能够系统地改变提供给学习算法的经验的数量和形式。我们发现，简单的神经网络在看到4个对象类别的3个例子之后就会形成一个形状偏差。这些偏见的发展预示着我们网络中词汇加速的开始，与儿童的发展过程一致。

##### URL
[http://arxiv.org/abs/1802.02745](http://arxiv.org/abs/1802.02745)

##### PDF
[http://arxiv.org/pdf/1802.02745](http://arxiv.org/pdf/1802.02745)

