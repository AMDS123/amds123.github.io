---
layout: post
title: "Linguistic Input Features Improve Neural Machine Translation"
date: 2016-06-27 23:11:51
categories: arXiv_CL
tags: arXiv_CL Attention Embedding
author: Rico Sennrich, Barry Haddow
mathjax: true
---

* content
{:toc}

##### Abstract
Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to English<->German, and English->Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An open-source implementation of our neural MT system is available, as are sample files and configurations.

##### Abstract (translated by Google)
神经机器翻译最近取得了令人印象深刻的成果，而对外部语言信息的使用很少。在本文中，我们表明神经MT模型的强大的学习能力不会使语言功能冗余;他们可以很容易地结合起来，进一步提高性能。我们将编码器的嵌入层推广到注意编码器 - 解码器体系结构中，以支持包含任意特征以及基线字特征。我们在英语德语和英语 - >罗马尼亚语神经机器翻译系统中添加了形态特征，词性标记和句法依赖标签作为输入特征。在WMT16训练和测试集的实验中，我们发现语言输入特征根据三个度量标准提高模型质量：困惑，BLEU和CHRF3。我们的神经MT系统的开源实现也是可用的，样本文件和配置也是如此。

##### URL
[https://arxiv.org/abs/1606.02892](https://arxiv.org/abs/1606.02892)

##### PDF
[https://arxiv.org/pdf/1606.02892](https://arxiv.org/pdf/1606.02892)

