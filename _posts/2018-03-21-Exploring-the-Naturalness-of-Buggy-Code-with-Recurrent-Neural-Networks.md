---
layout: post
title: "Exploring the Naturalness of Buggy Code with Recurrent Neural Networks"
date: 2018-03-21 16:14:22
categories: arXiv_CL
tags: arXiv_CL RNN Classification Language_Model
author: Jack Lanchantin, Ji Gao
mathjax: true
---

* content
{:toc}

##### Abstract
Statistical language models are powerful tools which have been used for many tasks within natural language processing. Recently, they have been used for other sequential data such as source code.(Ray et al., 2015) showed that it is possible train an n-gram source code language mode, and use it to predict buggy lines in code by determining "unnatural" lines via entropy with respect to the language model. In this work, we propose using a more advanced language modeling technique, Long Short-term Memory recurrent neural networks, to model source code and classify buggy lines based on entropy. We show that our method slightly outperforms an n-gram model in the buggy line classification task using AUC.

##### Abstract (translated by Google)
统计语言模型是强大的工具，已被用于自然语言处理中的许多任务。最近，它们已被用于其他连续数据，如源代码（Ray et al。，2015），它表明可以训练n-gram源代码语言模式，并用它来预测代码中的buggy行，不自然“的线条通过熵语言模型。在这项工作中，我们建议使用更先进的语言建模技术 - 长期短期记忆递归神经网络来对源代码进行建模并基于熵对行车线进行分类。我们证明我们的方法在使用AUC的车线分类任务中略胜过n-gram模型。

##### URL
[https://arxiv.org/abs/1803.08793](https://arxiv.org/abs/1803.08793)

##### PDF
[https://arxiv.org/pdf/1803.08793](https://arxiv.org/pdf/1803.08793)

