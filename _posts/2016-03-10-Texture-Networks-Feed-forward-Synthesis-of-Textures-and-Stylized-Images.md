---
layout: post
title: "Texture Networks: Feed-forward Synthesis of Textures and Stylized Images"
date: 2016-03-10 20:45:40
categories: arXiv_CV
tags: arXiv_CV CNN Optimization
author: Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky
mathjax: true
---

* content
{:toc}

##### Abstract
Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.

##### Abstract (translated by Google)
Gatys等人最近证明了深度网络可以从一个纹理例子中生成漂亮的纹理和风格化的图像。但是，他们的方法需要一个缓慢且耗费内存的优化过程。我们在这里提出一种将计算负担转移到学习阶段的替代方法。给定一个纹理的例子，我们的方法训练紧凑的前馈卷积网络，以生成任意大小的相同纹理的多个样本，并将艺术风格从给定图像转移到任何其他图像。由此产生的网络质量非常轻，可以产生与Gatys等相当的质量，但速度却快了上百倍。更一般地说，我们的方法突出了用复杂和表达性损失函数训练的生成前馈模型的强大性和灵活性。

##### URL
[https://arxiv.org/abs/1603.03417](https://arxiv.org/abs/1603.03417)

##### PDF
[https://arxiv.org/pdf/1603.03417](https://arxiv.org/pdf/1603.03417)

