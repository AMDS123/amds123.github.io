---
layout: post
title: "The Influence of Feature Representation of Text on the Performance of Document Classification"
date: 2017-07-05 11:09:31
categories: arXiv_CL
tags: arXiv_CL Classification Language_Model
author: Sanda Martinčić-Ipšić, Tanja Miličić, Ljupčo Todorovski
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we perform a comparative analysis of three models for feature representation of text documents in the context of document classification. In particular, we consider the most often used family of models bag-of-words, recently proposed continuous space models word2vec and doc2vec, and the model based on the representation of text documents as language networks. While the bag-of-word models have been extensively used for the document classification task, the performance of the other two models for the same task have not been well understood. This is especially true for the network-based model that have been rarely considered for representation of text documents for classification. In this study, we measure the performance of the document classifiers trained using the method of random forests for features generated the three models and their variants. The results of the empirical comparison show that the commonly used bag-of-words model has performance comparable to the one obtained by the emerging continuous-space model of doc2vec. In particular, the low-dimensional variants of doc2vec generating up to 75 features are among the top-performing document representation models. The results finally point out that doc2vec shows a superior performance in the tasks of classifying large documents.

##### Abstract (translated by Google)
在本文中，我们对文档分类中文本文档的特征表示进行了三种模型的比较分析。特别是，我们考虑最常用的模型族，最近提出的连续空间模型word2vec和doc2vec，以及基于文本文档表示的语言网络模型。虽然bag-of-word模型已经被广泛用于文档分类任务，但其他两种模型在同一个任务中的表现还没有得到很好的理解。对于很少考虑用于分类的文本文档表示的基于网络的模型尤其如此。在这项研究中，我们测量了使用随机森林方法训练的文档分类器对三个模型及其变体生成的特征的性能。实证比较的结果表明，常用的词袋模型具有与doc2vec新兴的连续空间模型相当的性能。特别是，生成多达75个特征的doc2vec的低维变体是表现最佳的文档表示模型。结果最后指出，doc2vec在大文档分类任务中表现出了优越的性能。

##### URL
[https://arxiv.org/abs/1707.01321](https://arxiv.org/abs/1707.01321)

##### PDF
[https://arxiv.org/pdf/1707.01321](https://arxiv.org/pdf/1707.01321)

