---
layout: post
title: "SNN: Stacked Neural Networks"
date: 2016-05-27 06:02:48
categories: arXiv_CV
tags: arXiv_CV Transfer_Learning
author: Milad Mohammadi, Subhasis Das
mathjax: true
---

* content
{:toc}

##### Abstract
It has been proven that transfer learning provides an easy way to achieve state-of-the-art accuracies on several vision tasks by training a simple classifier on top of features obtained from pre-trained neural networks. The goal of this work is to generate better features for transfer learning from multiple publicly available pre-trained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning. A second aim of this work is to make network fine- tuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called "joint fine-tuning" that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task.

##### Abstract (translated by Google)
已经证明，转移学习提供了一种简单的方法，通过在预先训练的神经网络获得的特征之上训练一个简单的分类器，在几个视觉任务上达到最新的精确度。这项工作的目标是产生更好的功能，从多个公开可用的预训练神经网络转移学习。为此，我们提出了一种称为堆叠式神经网络的新型架构，它利用了快速训练时间的转移学习，同时更准确。我们表明，使用堆叠的NN架构可以比仅使用一个预先训练的网络进行传输学习的最新技术的精度提高8％。这项工作的第二个目的是使网络微调保持基础网络的普遍性，看不见的任务。为此，我们提出了一种称为“联合微调”的新技术，它能够提供精确度相当于在两个数据集上单独调谐同一个网络的精度。我们还表明，与单一任务调整的网络相比，联合的精细化网络更好地推广了看不见的任务。

##### URL
[https://arxiv.org/abs/1605.08512](https://arxiv.org/abs/1605.08512)

##### PDF
[https://arxiv.org/pdf/1605.08512](https://arxiv.org/pdf/1605.08512)

