---
layout: post
title: "A scalable convolutional neural network for task-specified scenarios via knowledge distillation"
date: 2017-01-06 13:57:47
categories: arXiv_CV
tags: arXiv_CV Knowledge CNN
author: Mengnan Shi, Fei Qin, Qixiang Ye, Zhenjun Han, Jianbin Jiao
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we explore the redundancy in convolutional neural network, which scales with the complexity of vision tasks. Considering that many front-end visual systems are interested in only a limited range of visual targets, the removing of task-specified network redundancy can promote a wide range of potential applications. We propose a task-specified knowledge distillation algorithm to derive a simplified model with pre-set computation cost and minimized accuracy loss, which suits the resource constraint front-end systems well. Experiments on the MNIST and CIFAR10 datasets demonstrate the feasibility of the proposed approach as well as the existence of task-specified redundancy.

##### Abstract (translated by Google)
在本文中，我们探讨卷积神经网络中的冗余度，它随视觉任务的复杂程度而变化。考虑到许多前端视觉系统仅对有限范围的视觉目标感兴趣，去除任务指定的网络冗余可以促进广泛的潜在应用。我们提出了一个任务指定的知识蒸馏算法，以得到一个预先设定的计算成本和最小化精度损失的简化模型，很好地适合于资源约束的前端系统。在MNIST和CIFAR10数据集上的实验证明了所提出的方法的可行性以及存在任务指定的冗余。

##### URL
[https://arxiv.org/abs/1609.05695](https://arxiv.org/abs/1609.05695)

##### PDF
[https://arxiv.org/pdf/1609.05695](https://arxiv.org/pdf/1609.05695)

