---
layout: post
title: "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia"
date: 2017-03-15 19:58:44
categories: arXiv_CL
tags: arXiv_CL Knowledge Classification
author: Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, David Berthelot
mathjax: true
---

* content
{:toc}

##### Abstract
We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.

##### Abstract (translated by Google)
我们介绍WikiReading，一个大型的自然语言理解任务和公开的数据集，有1800万个实例。其任务是通过阅读相应维基百科文章的文本来预测来自结构化知识库维基数据的文本值。该任务包含丰富多样的挑战性分类和提取子任务，使其非常适合深度神经网络（DNN）等端到端模型。我们比较了各种最先进的基于DNN的体系结构，用于文档分类，信息提取和问题回答。我们发现支持丰富的答案空间的模型，如单词或字符序列，表现最好。我们的表现最好的模型，一个单词序列序列模型和一个机制，以复制词汇的词汇，获得了71.8％的准确性。

##### URL
[https://arxiv.org/abs/1608.03542](https://arxiv.org/abs/1608.03542)

##### PDF
[https://arxiv.org/pdf/1608.03542](https://arxiv.org/pdf/1608.03542)

