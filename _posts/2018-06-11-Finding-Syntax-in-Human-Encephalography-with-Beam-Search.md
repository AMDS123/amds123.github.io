---
layout: post
title: "Finding Syntax in Human Encephalography with Beam Search"
date: 2018-06-11 17:51:23
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: John Hale, Chris Dyer, Adhiguna Kuncoro, Jonathan R. Brennan
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural network grammars (RNNGs) are generative models of (tree,string) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.

##### Abstract (translated by Google)
递归神经网络语法（RNNG）是依赖于神经网络来评估派生选择的（树，串）对的生成模型。使用波束搜索与它们解析会产生各种增量复杂性度量，例如单词惊讶和解析器动作计数。当用作对自然文本的人类电生理反应的回归因子时，它们得到两种幅度效应：早期峰值和P600样后期峰值。相反，非句法神经语言模型不会产生可靠的效果。模型比较将早期峰值归属于RNNG内的句法组成。这种结果模式推荐RNNG +波束搜索组合作为在正常人类语言理解期间发生的句法处理的机制模型。

##### URL
[http://arxiv.org/abs/1806.04127](http://arxiv.org/abs/1806.04127)

##### PDF
[http://arxiv.org/pdf/1806.04127](http://arxiv.org/pdf/1806.04127)

