---
layout: post
title: "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets"
date: 2017-09-27 13:31:41
categories: arXiv_CL
tags: arXiv_CL Sentiment Sentiment_Classification Classification Prediction
author: Rotem Dror, Gili Baumer, Marina Bogomolov, Roi Reichart
mathjax: true
---

* content
{:toc}

##### Abstract
With the ever-growing amounts of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper, we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.

##### Abstract (translated by Google)
随着大量来自各种语言，领域和流派的文本数据的不断增加，在多个数据集上评估NLP算法已成为标准，以确保跨异构设置的一致性能。然而，这种多重比较对NLP中的传统统计分析方法提出了重大挑战，并且可能导致错误的结论。在本文中，我们提出了一个可重复性分析框架，用于NLP任务之间多重比较的统计分析。我们讨论了这个框架在NLP文献当前在统计上不合理的实践中的理论优势，并展示了它在四个应用程序上的经验价值：多领域依赖解析，多语言POS标记，跨领域情感分类和词语相似性预测。

##### URL
[https://arxiv.org/abs/1709.09500](https://arxiv.org/abs/1709.09500)

##### PDF
[https://arxiv.org/pdf/1709.09500](https://arxiv.org/pdf/1709.09500)

