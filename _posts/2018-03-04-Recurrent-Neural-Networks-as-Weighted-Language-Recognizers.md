---
layout: post
title: "Recurrent Neural Networks as Weighted Language Recognizers"
date: 2018-03-04 18:27:59
categories: arXiv_CL
tags: arXiv_CL RNN
author: Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, Kevin Knight
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.

##### Abstract (translated by Google)
我们研究了简单递归神经网络（RNNs）作为识别加权语言的形式模型的各种问题的计算复杂性。我们专注于单层，ReLU激活，具有softmax的合理权重RNN，这些常用于自然语言处理应用。我们证明，这样的RNNs的大多数问题都是不可判定的，包括一致性，等价性，最小化和最高权重字符串的确定。然而，对于一致的RNN来说，最后一个问题变得可以确定，尽管解的长度可以超过所有可计算的范围。如果另外字符串被限制为多项式长度，则问题变为NP完全且APX难。总之，这表明近似和启发式算法在这些RNN的实际应用中是必需的。

##### URL
[http://arxiv.org/abs/1711.05408](http://arxiv.org/abs/1711.05408)

##### PDF
[http://arxiv.org/pdf/1711.05408](http://arxiv.org/pdf/1711.05408)

