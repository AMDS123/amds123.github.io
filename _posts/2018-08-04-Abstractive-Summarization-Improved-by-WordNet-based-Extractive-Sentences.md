---
layout: post
title: "Abstractive Summarization Improved by WordNet-based Extractive Sentences"
date: 2018-08-04 05:03:43
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Niantao Xie, Sujian Li, Huiling Ren, Qibin Zhai
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, the seq2seq abstractive summarization models have achieved good results on the CNN/Daily Mail dataset. Still, how to improve abstractive methods with extractive methods is a good research direction, since extractive methods have their potentials of exploiting various efficient features for extracting important sentences in one text. In this paper, in order to improve the semantic relevance of abstractive summaries, we adopt the WordNet based sentence ranking algorithm to extract the sentences which are most semantically to one text. Then, we design a dual attentional seq2seq framework to generate summaries with consideration of the extracted information. At the same time, we combine pointer-generator and coverage mechanisms to solve the problems of out-of-vocabulary (OOV) words and duplicate words which exist in the abstractive models. Experiments on the CNN/Daily Mail dataset show that our models achieve competitive performance with the state-of-the-art ROUGE scores. Human evaluations also show that the summaries generated by our models have high semantic relevance to the original text.

##### Abstract (translated by Google)
最近，seq2seq抽象摘要模型在CNN / Daily Mail数据集上取得了很好的效果。如何用提取方法改进抽象方法是一个很好的研究方向，因为提取方法有可能利用各种有效的特征来提取一个文本中的重要句子。在本文中，为了提高抽象概要的语义相关性，我们采用基于WordNet的句子排序算法来提取语义最多的句子到一个文本。然后，我们设计了一个双重关注的seq2seq框架，以便在考虑提取的信息的情况下生成摘要。同时，我们结合指针生成器和覆盖机制来解决抽象模型中存在的词外（OOV）词和重复词的问题。在CNN /每日邮报数据集上的实验表明，我们的模型通过最先进的ROUGE分数获得了竞争性的表现。人工评估还表明，我们的模型生成的摘要与原始文本具有高度语义相关性。

##### URL
[http://arxiv.org/abs/1808.01426](http://arxiv.org/abs/1808.01426)

##### PDF
[http://arxiv.org/pdf/1808.01426](http://arxiv.org/pdf/1808.01426)

