---
layout: post
title: "Stochastic Function Norm Regularization of Deep Networks"
date: 2016-12-07 14:14:30
categories: arXiv_CV
tags: arXiv_CV Regularization
author: Amal Rannen Triki, Matthew B. Blaschko
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks have had an enormous impact on image analysis. State-of-the-art training methods, based on weight decay and DropOut, result in impressive performance when a very large training set is available. However, they tend to have large problems overfitting to small data sets. Indeed, the available regularization methods deal with the complexity of the network function only indirectly. In this paper, we study the feasibility of directly using the $L_2$ function norm for regularization. Two methods to integrate this new regularization in the stochastic backpropagation are proposed. Moreover, the convergence of these new algorithms is studied. We finally show that they outperform the state-of-the-art methods in the low sample regime on benchmark datasets (MNIST and CIFAR10). The obtained results demonstrate very clear improvement, especially in the context of small sample regimes with data laying in a low dimensional manifold. Source code of the method can be found at \url{this https URL}.

##### Abstract (translated by Google)
深度神经网络对图像分析产生了巨大的影响。基于重量衰减和DropOut的最先进的训练方法在有大量训练集时可以获得令人印象深刻的性能。但是，他们往往有大量的问题过度拟合小数据集。事实上，现有的正则化方法只能间接处理网络功能的复杂性。在本文中，我们研究直接使用$ L_2 $函数规范进行正则化的可行性。提出了将这种新的正则化方法整合到随机反向传播中的两种方法。此外，还研究了这些新算法的收敛性。我们最终表明它们在基准数据集（MNIST和CIFAR10）的低样本制度下表现优于最先进的方法。所获得的结果显示出非常明显的改善，特别是在数据处于低维流形的小样本情况下。该方法的源代码可以在\ url {this https URL}中找到。

##### URL
[https://arxiv.org/abs/1605.09085](https://arxiv.org/abs/1605.09085)

##### PDF
[https://arxiv.org/pdf/1605.09085](https://arxiv.org/pdf/1605.09085)

