---
layout: post
title: "Scalable Recollections for Continual Lifelong Learning"
date: 2018-02-17 01:10:31
categories: arXiv_AI
tags: arXiv_AI Deep_Learning
author: Matthew Riemer, Tim Klinger, Michele Franceschini, Djallel Bouneffouf
mathjax: true
---

* content
{:toc}

##### Abstract
Given the recent success of Deep Learning applied to a variety of single tasks, it is natural to consider more human-realistic settings. Perhaps the most difficult of these settings is that of continual lifelong learning, where the model must learn online over a continuous stream of non-stationary data. A continual lifelong learning system must have three primary capabilities to succeed: it must learn and adapt over time, it must not forget what it has learned, and it must be efficient in both training time and memory. Recent techniques have focused their efforts largely on the first two capabilities while the third capability remains largely unexplored. In this paper, we consider the problem of efficient and effective storage of experiences over very large time-frames. In particular we consider the case where typical experiences are n bits and memories are limited to k bits for k &lt;&lt; n. We present a novel scalable architecture and training algorithm in this challenging domain and provide an extensive evaluation of its performance. Our results show that we can achieve considerable gains on top of state-of-the-art methods such as GEM.

##### Abstract (translated by Google)
鉴于Deep Learning最近成功应用于各种单一任务，因此考虑更人性化的设置是很自然的。也许这些设置中最困难的是持续终身学习，其中模型必须通过连续的非静态数据在线学习。一个持续的终身学习系统必须具备三个主要的成功能力：必须随着时间的推移学习和适应，不能忘记自己学到了什么，而且在培训时间和记忆方面都必须高效。最近的技术主要集中在前两种能力上，而第三种能力在很大程度上仍未被探索。在本文中，我们考虑了在非常大的时间范围内高效而有效地存储体验的问题。具体而言，我们考虑这样的情况，其中典型的体验是n位，并且存储器被限制为k位， ñ。我们在这个具有挑战性的领域提出了一种新颖的可扩展架构和训练算法，并提供了对其性能的广泛评估。我们的研究结果表明，我们可以在诸如创业板等最先进的方法之上取得可观的收益。

##### URL
[http://arxiv.org/abs/1711.06761](http://arxiv.org/abs/1711.06761)

##### PDF
[http://arxiv.org/pdf/1711.06761](http://arxiv.org/pdf/1711.06761)

