---
layout: post
title: "A New Hybrid-parameter Recurrent Neural Networks for Online Handwritten Chinese Character Recognition"
date: 2017-11-08 03:04:41
categories: arXiv_CV
tags: arXiv_CV RNN Recognition
author: Haiqing Ren, Weiqiang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
The recurrent neural network (RNN) is appropriate for dealing with temporal sequences. In this paper, we present a deep RNN with new features and apply it for online handwritten Chinese character recognition. Compared with the existing RNN models, three innovations are involved in the proposed system. First, a new hidden layer function for RNN is proposed for learning temporal information better. we call it Memory Pool Unit (MPU). The proposed MPU has a simple architecture. Second, a new RNN architecture with hybrid parameter is presented, in order to increasing the expression capacity of RNN. The proposed hybrid-parameter RNN has parameter changes when calculating the iteration at temporal dimension. Third, we make a adaptation that all the outputs of each layer are stacked as the output of network. Stacked hidden layer states combine all the hidden layer states for increasing the expression capacity. Experiments are carried out on the IAHCC-UCAS2016 dataset and the CASIA-OLHWDB1.1 dataset. The experimental results show that the hybrid-parameter RNN obtain a better recognition performance with higher efficiency (fewer parameters and faster speed). And the proposed Memory Pool Unit is proved to be a simple hidden layer function and obtains a competitive recognition results.

##### Abstract (translated by Google)
递归神经网络（RNN）适用于处理时间序列。在本文中，我们提出了一个新的特征，并将其应用于在线手写汉字识别。与现有的RNN模型相比，该系统涉及到三个创新。首先提出了一种新的RNN隐层功能，用于更好地学习时态信息。我们称之为内存池单元（MPU）。所提出的MPU具有简单的结构。其次，提出了一种新的混合参数RNN结构，以提高RNN的表达能力。所提出的混合参数RNN在计算时间维度上的迭代时具有参数变化。第三，我们将每层的所有输出作为网络的输出进行调整。堆叠的隐藏层状态结合了所有的隐藏层状态以提高表达能力。在IAHCC-UCAS2016数据集和CASIA-OLHWDB1.1数据集上进行实验。实验结果表明，混合参数RNN具有更高的效率（参数更少，速度更快），能够获得更好的识别性能。提出的内存池单元被证明是一个简单的隐层功能，并获得了一个有竞争力的识别结果。

##### URL
[https://arxiv.org/abs/1711.02809](https://arxiv.org/abs/1711.02809)

##### PDF
[https://arxiv.org/pdf/1711.02809](https://arxiv.org/pdf/1711.02809)

