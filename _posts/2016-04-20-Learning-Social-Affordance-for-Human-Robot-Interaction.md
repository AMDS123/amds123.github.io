---
layout: post
title: "Learning Social Affordance for Human-Robot Interaction"
date: 2016-04-20 21:02:02
categories: arXiv_CV
tags: arXiv_CV Weakly_Supervised Relation
author: Tianmin Shu, M. S. Ryoo, Song-Chun Zhu
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we present an approach for robot learning of social affordance from human activity videos. We consider the problem in the context of human-robot interaction: Our approach learns structural representations of human-human (and human-object-human) interactions, describing how body-parts of each agent move with respect to each other and what spatial relations they should maintain to complete each sub-event (i.e., sub-goal). This enables the robot to infer its own movement in reaction to the human body motion, allowing it to naturally replicate such interactions. We introduce the representation of social affordance and propose a generative model for its weakly supervised learning from human demonstration videos. Our approach discovers critical steps (i.e., latent sub-events) in an interaction and the typical motion associated with them, learning what body-parts should be involved and how. The experimental results demonstrate that our Markov Chain Monte Carlo (MCMC) based learning algorithm automatically discovers semantically meaningful interactive affordance from RGB-D videos, which allows us to generate appropriate full body motion for an agent.

##### Abstract (translated by Google)
在本文中，我们提出了一个从人类活动视频机器人学习社会可供性的方法。我们在人机交互的背景下考虑这个问题：我们的方法学习人 - 人（和人 - 物 - 人）相互作用的结构表征，描述每个主体的身体部位如何相对移动，以及什么空间关系他们应该保持完成每个子事件（即子目标）。这使得机器人能够根据人体运动推断其自身的运动，使其自然地复制这种相互作用。我们介绍了社会可供性的表示，并提出了一个从人类示范视频中弱监督学习的生成模型。我们的方法发现交互中的关键步骤（即潜在的子事件）和与之相关的典型动作，学习什么身体部位应该涉及以及如何。实验结果表明，基于马尔可夫链蒙特卡罗（MCMC）的学习算法自动发现了RGB-D视频中语义上有意义的交互式可供性，这使我们能够为代理生成适当的全身运动。

##### URL
[https://arxiv.org/abs/1604.03692](https://arxiv.org/abs/1604.03692)

##### PDF
[https://arxiv.org/pdf/1604.03692](https://arxiv.org/pdf/1604.03692)

