---
layout: post
title: "A General Theory for Training Learning Machine"
date: 2017-04-23 05:48:18
categories: arXiv_CV
tags: arXiv_CV Knowledge Inference Classification Deep_Learning Recognition
author: Hong Zhao
mathjax: true
---

* content
{:toc}

##### Abstract
Though the deep learning is pushing the machine learning to a new stage, basic theories of machine learning are still limited. The principle of learning, the role of the a prior knowledge, the role of neuron bias, and the basis for choosing neural transfer function and cost function, etc., are still far from clear. In this paper, we present a general theoretical framework for machine learning. We classify the prior knowledge into common and problem-dependent parts, and consider that the aim of learning is to maximally incorporate them. The principle we suggested for maximizing the former is the design risk minimization principle, while the neural transfer function, the cost function, as well as pretreatment of samples, are endowed with the role for maximizing the latter. The role of the neuron bias is explained from a different angle. We develop a Monte Carlo algorithm to establish the input-output responses, and we control the input-output sensitivity of a learning machine by controlling that of individual neurons. Applications of function approaching and smoothing, pattern recognition and classification, are provided to illustrate how to train general learning machines based on our theory and algorithm. Our method may in addition induce new applications, such as the transductive inference.

##### Abstract (translated by Google)
虽然深度学习正将机器学习推向一个新的阶段，但机器学习的基础理论仍然有限。学习的原理，先验知识的作用，神经元偏倚的作用以及选择神经传递函数和代价函数的依据等，目前还很不清楚。在本文中，我们提出了一个机器学习的一般理论框架。我们将先前的知识分为常见的和问题相关的部分，并认为学习的目的是最大限度地将它们结合起来。我们提出的使得前者最大化的原则是设计风险最小化原则，而神经传递函数，成本函数以及样品的预处理被赋予最大化后者的作用。神经元偏倚的作用是从另一个角度来解释的。我们开发了蒙特卡洛算法来建立输入输出响应，并且通过控制单个神经元的控制来控制学习机器的输入 - 输出灵敏度。提供了函数逼近和平滑，模式识别和分类的应用，以说明如何根据我们的理论和算法来训练通用学习机。我们的方法可能会引起新的应用，如推导推理。

##### URL
[https://arxiv.org/abs/1704.06885](https://arxiv.org/abs/1704.06885)

##### PDF
[https://arxiv.org/pdf/1704.06885](https://arxiv.org/pdf/1704.06885)

