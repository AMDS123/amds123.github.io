---
layout: post
title: "Growing Better Graphs With Latent-Variable Probabilistic Graph Grammars"
date: 2018-06-11 11:36:43
categories: arXiv_AI
tags: arXiv_AI
author: Xinyi Wang, Salvador Aguinaga, Tim Weninger, David Chiang
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work in graph models has found that probabilistic hyperedge replacement grammars (HRGs) can be extracted from graphs and used to generate new random graphs with graph properties and substructures close to the original. In this paper, we show how to add latent variables to the model, trained using Expectation-Maximization, to generate still better graphs, that is, ones that generalize better to the test data. We evaluate the new method by separating training and test graphs, building the model on the former and measuring the likelihood of the latter, as a more stringent test of how well the model can generalize to new graphs. On this metric, we find that our latent-variable HRGs consistently outperform several existing graph models and provide interesting insights into the building blocks of real world networks.

##### Abstract (translated by Google)
最近在图模型中的工作发现概率超立体替换语法（HRGs）可以从图中提取出来，并用于生成新的随机图，其图形属性和子结构接近原始图。在本文中，我们展示了如何将潜在变量添加到使用期望最大化进行训练的模型中，以生成更好的图形，即更好地推广到测试数据的图形。我们通过分离训练和测试图来评估新方法，对前者建立模型并测量后者的可能性，作为模型如何更好地推广到新图的更严格测试。在这个度量标准中，我们发现我们的潜变量HRG始终超越了几个现有的图形模型，并提供了有关真实世界网络构建模块的有趣见解。

##### URL
[http://arxiv.org/abs/1806.07955](http://arxiv.org/abs/1806.07955)

##### PDF
[http://arxiv.org/pdf/1806.07955](http://arxiv.org/pdf/1806.07955)

