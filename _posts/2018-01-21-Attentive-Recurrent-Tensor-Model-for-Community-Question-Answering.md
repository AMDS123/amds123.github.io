---
layout: post
title: "Attentive Recurrent Tensor Model for Community Question Answering"
date: 2018-01-21 09:01:46
categories: arXiv_CL
tags: arXiv_CL Regularization QA Attention Optimization
author: Gaurav Bhatt, Shivam Sharma, Balasubramanian Raman
mathjax: true
---

* content
{:toc}

##### Abstract
A major challenge to the problem of community question answering is the lexical and semantic gap between the sentence representations. Some solutions to minimize this gap includes the introduction of extra parameters to deep models or augmenting the external handcrafted features. In this paper, we propose a novel attentive recurrent tensor network for solving the lexical and semantic gap in community question answering. We introduce token-level and phrase-level attention strategy that maps input sequences to the output using trainable parameters. Further, we use the tensor parameters to introduce a 3-way interaction between question, answer and external features in vector space. We introduce simplified tensor matrices with L2 regularization that results in smooth optimization during training. The proposed model achieves state-of-the-art performance on the task of answer sentence selection (TrecQA and WikiQA datasets) while outperforming the current state-of-the-art on the tasks of best answer selection (Yahoo! L4) and answer triggering task (WikiQA).

##### Abstract (translated by Google)
对社区问题回答问题的一个主要挑战是句子表征之间的词汇和语义鸿沟。减少这种差距的一些解决方案包括向深层模型引入额外的参数或增加外部手工特征。在本文中，我们提出了一个新颖的注意循环张量网络来解决社区问答中的词汇和语义鸿沟。我们引入令牌级和短语级的注意策略，使用可训练参数将输入序列映射到输出。此外，我们使用张量参数来引入向量空间中的问题，答案和外部特征之间的三向交互。我们引入简化的二维正则化张量矩阵，导致训练过程中的平滑优化。所提出的模型在回答句子选择（TrecQA和WikiQA数据集）的任务方面达到了最先进的性能，同时在最佳答案选择（Yahoo！L4）的任务上胜出了当前的最新技术水平，触发任务（WikiQA）。

##### URL
[https://arxiv.org/abs/1801.06792](https://arxiv.org/abs/1801.06792)

##### PDF
[https://arxiv.org/pdf/1801.06792](https://arxiv.org/pdf/1801.06792)

