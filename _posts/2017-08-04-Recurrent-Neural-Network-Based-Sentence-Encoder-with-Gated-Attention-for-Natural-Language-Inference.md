---
layout: post
title: "Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference"
date: 2017-08-04 01:55:18
categories: arXiv_CL
tags: arXiv_CL Attention Inference
author: Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, Diana Inkpen
mathjax: true
---

* content
{:toc}

##### Abstract
The RepEval 2017 Shared Task aims to evaluate natural language understanding models for sentence representation, in which a sentence is represented as a fixed-length vector with neural networks and the quality of the representation is tested with a natural language inference task. This paper describes our system (alpha) that is ranked among the top in the Shared Task, on both the in-domain test set (obtaining a 74.9% accuracy) and on the cross-domain test set (also attaining a 74.9% accuracy), demonstrating that the model generalizes well to the cross-domain data. Our model is equipped with intra-sentence gated-attention composition which helps achieve a better performance. In addition to submitting our model to the Shared Task, we have also tested it on the Stanford Natural Language Inference (SNLI) dataset. We obtain an accuracy of 85.5%, which is the best reported result on SNLI when cross-sentence attention is not allowed, the same condition enforced in RepEval 2017.

##### Abstract (translated by Google)
RepEval 2017共享任务旨在评估用于句子表示的自然语言理解模型，其中句子表示为具有神经网络的固定长度矢量，并且用自然语言推理任务来测试表示的质量。本文描述了我们的系统（alpha）在共享任务中排名最高，在域内测试集（获得74.9％的准确率）和跨域测试集（也达到74.9％的准确度）证明了该模型对跨域数据的推广。我们的模型配备了句内gated-attention组合，有助于获得更好的表现。除了将模型提交给共享任务外，我们还在斯坦福自然语言推理（SNLI）数据集上进行了测试。我们获得了85.5％的准确性，这是在不允许跨语句注意的情况下在SNLI上最好的报告结果，在2017年RepEval中执行相同的条件。

##### URL
[https://arxiv.org/abs/1708.01353](https://arxiv.org/abs/1708.01353)

##### PDF
[https://arxiv.org/pdf/1708.01353](https://arxiv.org/pdf/1708.01353)

