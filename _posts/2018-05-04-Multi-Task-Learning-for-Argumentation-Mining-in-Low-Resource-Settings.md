---
layout: post
title: "Multi-Task Learning for Argumentation Mining in Low-Resource Settings"
date: 2018-05-04 12:25:09
categories: arXiv_CL
tags: arXiv_CL
author: Claudia Schulz, Steffen Eger, Johannes Daxenberger, Tobias Kahse, Iryna Gurevych
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate whether and where multi-task learning (MTL) can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks.

##### Abstract (translated by Google)
我们调查多任务学习（MTL）能否提高与论证挖掘（AM）相关的NLP问题（特别是论证成分识别）的性能。我们的研究结果表明，当少量训练数据可用于主要任务时，MTL表现得特别好（并且比单任务学习更好），这是AM中的一种常见场景。我们的研究结果挑战了先前的假设，即跨AM数据集的概念化是不同的，并且MTL对于语义或更高级别的任务来说是困难的。

##### URL
[http://arxiv.org/abs/1804.04083](http://arxiv.org/abs/1804.04083)

##### PDF
[http://arxiv.org/pdf/1804.04083](http://arxiv.org/pdf/1804.04083)

