---
layout: post
title: "VideoLSTM Convolves, Attends and Flows for Action Recognition"
date: 2016-07-06 20:00:20
categories: arXiv_CV
tags: arXiv_CV Attention Action_Recognition CNN RNN Classification Relation Recognition
author: Zhenyang Li, Efstratios Gavves, Mihir Jain, Cees G. M. Snoek
mathjax: true
---

* content
{:toc}

##### Abstract
We present a new architecture for end-to-end sequence learning of actions in video, we call VideoLSTM. Rather than adapting the video to the peculiarities of established recurrent or convolutional architectures, we adapt the architecture to fit the requirements of the video medium. Starting from the soft-Attention LSTM, VideoLSTM makes three novel contributions. First, video has a spatial layout. To exploit the spatial correlation we hardwire convolutions in the soft-Attention LSTM architecture. Second, motion not only informs us about the action content, but also guides better the attention towards the relevant spatio-temporal locations. We introduce motion-based attention. And finally, we demonstrate how the attention from VideoLSTM can be used for action localization by relying on just the action class label. Experiments and comparisons on challenging datasets for action classification and localization support our claims.

##### Abstract (translated by Google)
我们提出了一个新的架构，用于端到端的视频动作序列学习，我们称之为VideoLSTM。我们不是将视频适应已建立的经常性或卷积体系结构的特性，而是根据视频媒体的要求来调整体系结构。从软件注意LSTM开始，VideoLSTM做出三个小说贡献。首先，视频有一个空间布局。为了利用空间相关性，我们硬连接软注意LSTM体系结构中的卷积。其次，动议不仅告诉我们行动的内容，而且引导我们更好地关注相关的时空位置。我们引入基于运动的注意力。最后，我们演示了如何使用VideoLSTM的注意力，通过依赖动作类标签来实现动作本地化。对具有挑战性的数据集进行操作分类和本地化的实验和比较支持我们的说法。

##### URL
[https://arxiv.org/abs/1607.01794](https://arxiv.org/abs/1607.01794)

##### PDF
[https://arxiv.org/pdf/1607.01794](https://arxiv.org/pdf/1607.01794)

