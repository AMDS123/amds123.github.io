---
layout: post
title: "Joint Event Detection and Description in Continuous Video Streams"
date: 2018-04-13 23:03:39
categories: arXiv_CV
tags: arXiv_CV Video_Caption Segmentation Caption CNN Detection Relation
author: Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko
mathjax: true
---

* content
{:toc}

##### Abstract
Dense video captioning is a fine-grained video understanding task that involves two sub-problems: localizing distinct events in a long video stream, and generating captions for the localized events. We propose the Joint Event Detection and Description Network (JEDDi-Net), which solves the dense video captioning task in an end-to-end fashion. Our model continuously encodes the input video stream with three-dimensional convolutional layers, proposes variable-length temporal events based on pooled features, and generates their captions. Unlike existing approaches, our event proposal generation and language captioning networks are trained jointly and end-to-end, allowing for improved temporal segmentation. In order to explicitly model temporal relationships between visual events and their captions in a single video, we also propose a two-level hierarchical captioning module that keeps track of context. On the large-scale ActivityNet Captions dataset, JEDDi-Net demonstrates improved results as measured by standard metrics. We also present the first dense captioning results on the TACoS-MultiLevel dataset.

##### Abstract (translated by Google)
密集视频字幕是一种细粒度的视频理解任务，涉及两个子问题：在长视频流中本地化不同的事件，以及为本地化事件生成字幕。我们提出了联合事件检测和描述网络（JEDDi-Net），它以端到端的方式解决了密集的视频字幕任务。我们的模型使用三维卷积层连续编码输入视频流，基于合并特征提出可变长度时间事件，并生成其字幕。与现有方法不同，我们的事件提议生成和语言字幕网络是联合和端到端的训练，允许改进时间分割。为了在单个视频中明确地模拟视觉事件与其字幕之间的时间关系，我们还提出了一个跟踪上下文的两级分层字幕模块。在大型ActivityNet Captions数据集中，JEDDi-Net通过标准指标衡量了改进的结果。我们还在TACoS-MultiLevel数据集上提供了第一个密集字幕结果。

##### URL
[https://arxiv.org/abs/1802.10250](https://arxiv.org/abs/1802.10250)

##### PDF
[https://arxiv.org/pdf/1802.10250](https://arxiv.org/pdf/1802.10250)

