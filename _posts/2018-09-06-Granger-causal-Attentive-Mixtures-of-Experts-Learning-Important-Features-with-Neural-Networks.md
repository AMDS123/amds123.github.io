---
layout: post
title: "Granger-causal Attentive Mixtures of Experts: Learning Important Features with Neural Networks"
date: 2018-09-06 12:49:54
categories: arXiv_AI
tags: arXiv_AI Knowledge Prediction
author: Patrick Schwab, Djordje Miladinovic, Walter Karlen
mathjax: true
---

* content
{:toc}

##### Abstract
Knowledge of the importance of input features towards decisions made by machine-learning models is essential to increase our understanding of both the models and the underlying data. Here, we present a new approach to estimating feature importance with neural networks based on the idea of distributing the features of interest among experts in an attentive mixture of experts (AME). AMEs use attentive gating networks trained with a Granger-causal objective to learn to jointly produce accurate predictions as well as estimates of feature importance in a single model. Our experiments on an established benchmark and two real-world datasets show (i) that the feature importance estimates provided by AMEs compare favourably to those provided by state-of-the-art methods, (ii) that AMEs are significantly faster than existing methods, and (iii) that the associations discovered by AMEs are consistent with those reported by domain experts.

##### Abstract (translated by Google)
了解输入特征对机器学习模型决策的重要性，对于增加我们对模型和基础数据的理解至关重要。在这里，我们提出了一种新的方法，用神经网络估计特征重要性，基于在专家的专家组（AME）中分配专家感兴趣的特征的想法。 AME使用由Granger因果目标训练的专心门控网络来学习联合生成准确预测以及在单个模型中估计特征重要性。我们在已建立的基准和两个真实数据集上的实验表明：（i）AME提供的特征重要性估计与最先进方法提供的特征相比有利，（ii）AME明显快于现有方法（iii）AME发现的关联与领域专家报告的关联一致。

##### URL
[http://arxiv.org/abs/1802.02195](http://arxiv.org/abs/1802.02195)

##### PDF
[http://arxiv.org/pdf/1802.02195](http://arxiv.org/pdf/1802.02195)

