---
layout: post
title: "Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting"
date: 2018-02-08 16:25:29
categories: arXiv_CV
tags: arXiv_CV
author: Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M. Lopez, Andrew D. Bagdanov
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to other state-of-the-art in lifelong learning without forgetting.

##### Abstract (translated by Google)
在本文中，我们提出了一种避免顺序任务学习场景中的灾难性遗忘的方法。我们的技术是基于一个网络重新参数化，大致对角化网络参数的Fisher信息矩阵。这种重新参数化采用参数空间的因式分解旋转的形式，当与弹性重量合并（其假定对角Fisher信息矩阵）结合使用时，导致连续任务的终身学习的显着更好的性能。 MNIST，CIFAR-100，CUB-200和Stanford-40数据集上的实验结果表明，我们显着提高了标准弹性体重固结的结果，并且与其他最新的终身技术相比，我们获得了有竞争力的结果学习不忘记。

##### URL
[http://arxiv.org/abs/1802.02950](http://arxiv.org/abs/1802.02950)

##### PDF
[http://arxiv.org/pdf/1802.02950](http://arxiv.org/pdf/1802.02950)

