---
layout: post
title: "Neural Lattice Language Models"
date: 2018-03-13 23:13:58
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model Prediction
author: Jacob Buckman, Graham Neubig
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions - including polysemy and existence of multi-word lexical items - into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline.

##### Abstract (translated by Google)
在这项工作中，我们提出了一种新的语言建模范例，它能够以多种粒度执行信息流的预测和调节：神经网格语言模型。这些模型通过句子构建可能路径的网格，并在该网格上边缘化以计算序列概率或优化参数。这种方法使我们可以将语言直觉（包括多义词和多词词汇项的存在）无缝地融入到我们的语言模型中。多语言建模任务的实验表明，使用多义嵌入的英语神经网格语言模型相对于单词级别的基线能够将混淆度提高9.95％，并且处理多字符标记的中国模型能够通过相对于角色等级基准的20.94％。

##### URL
[https://arxiv.org/abs/1803.05071](https://arxiv.org/abs/1803.05071)

##### PDF
[https://arxiv.org/pdf/1803.05071](https://arxiv.org/pdf/1803.05071)

