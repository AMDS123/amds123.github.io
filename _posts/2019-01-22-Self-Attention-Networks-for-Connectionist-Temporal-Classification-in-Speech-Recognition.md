---
layout: post
title: "Self-Attention Networks for Connectionist Temporal Classification in Speech Recognition"
date: 2019-01-22 21:37:07
categories: arXiv_CL
tags: arXiv_CL Attention Speech_Recognition Classification Recognition
author: Julian Salazar, Katrin Kirchhoff, Zhiheng Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Self-attention has demonstrated great success in sequence-to-sequence tasks in natural language processing, with preliminary work applying it to end-to-end encoder-decoder approaches in speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free strategy for monotonic sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for speech recognition. On the Wall Street Journal and LibriSpeech datasets, SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, attaining 4.7% CER in 1 day and 2.8% CER in 1 week respectively, using the same architecture and one GPU. We motivate the architecture for speech, evaluate position and downsampling approaches, and explore how the label alphabet affects attention head and performance outcomes.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1901.10055](http://arxiv.org/abs/1901.10055)

##### PDF
[http://arxiv.org/pdf/1901.10055](http://arxiv.org/pdf/1901.10055)

