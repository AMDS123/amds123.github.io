---
layout: post
title: "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank"
date: 2017-09-22 01:53:39
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan, Bo Yuan
mathjax: true
---

* content
{:toc}

##### Abstract
Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks.

##### Abstract (translated by Google)
最近提出了低位移排列（LDR）矩阵或所谓的结构化矩阵来压缩大规模神经网络。实证结果表明，具有LDR矩阵加权矩阵的神经网络，称为LDR神经网络，在保持高精度的同时，可以显着降低空间和计算复杂度。我们在深度学习中正式研究LDR矩阵。首先证明了位移算子具有温和条件的LDR神经网络的全局逼近性质。然后，我们证明LDR神经网络的误差范围与单层和多层结构的一般神经网络一样有效。最后，提出了一般LDR神经网络的反向传播训练算法。

##### URL
[https://arxiv.org/abs/1703.00144](https://arxiv.org/abs/1703.00144)

##### PDF
[https://arxiv.org/pdf/1703.00144](https://arxiv.org/pdf/1703.00144)

