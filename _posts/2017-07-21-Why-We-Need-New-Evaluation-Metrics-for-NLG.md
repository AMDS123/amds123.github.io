---
layout: post
title: "Why We Need New Evaluation Metrics for NLG"
date: 2017-07-21 12:47:03
categories: arXiv_CL
tags: arXiv_CL
author: Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, Verena Rieser
mathjax: true
---

* content
{:toc}

##### Abstract
The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.

##### Abstract (translated by Google)
NLG评估的大部分依赖于自动度量，比如BLEU。在本文中，我们激发了对新颖，独立于系统和数据的自动评估方法的需求：我们调查了广泛的度量标准，其中包括最先进的基于单词的和基于新型语法的评估方法，它们只是弱地反映了由数据驱动的端到端NLG产生的对系统输出的人为判断。我们还显示度量性能是数据和系统特定的。尽管如此，我们的结果还表明，自动度量在系统级可靠地执行，并且可以通过查找系统性能不佳的情况来支持系统开发。

##### URL
[https://arxiv.org/abs/1707.06875](https://arxiv.org/abs/1707.06875)

##### PDF
[https://arxiv.org/pdf/1707.06875](https://arxiv.org/pdf/1707.06875)

