---
layout: post
title: "Deconvolutional Paragraph Representation Learning"
date: 2017-09-22 15:20:27
categories: arXiv_CL
tags: arXiv_CL Text_Classification Summarization CNN Represenation_Learning RNN Classification Quantitative
author: Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, Lawrence Carin
mathjax: true
---

* content
{:toc}

##### Abstract
Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.

##### Abstract (translated by Google)
从长文本序列学习潜在表示是许多自然语言处理应用程序的重要的第一步。递归神经网络（RNN）已经成为这一具有挑战性任务的基石。然而，基于RNN的解码（重构）期间句子的质量随着文本的长度而减小。我们提出了一个序列到序列的，纯粹的卷积和去卷积自编码框架，它没有上述问题，同时也是计算效率高的。所提出的方法简单，易于实现，并可以作为许多应用程序的构建块。我们经验地表明，与RNN相比，我们的框架在重建和纠正长段落方面更好。对半监督文本分类和摘要任务的定量评估表明了更好地利用长的无标签文本数据的潜力。

##### URL
[https://arxiv.org/abs/1708.04729](https://arxiv.org/abs/1708.04729)

##### PDF
[https://arxiv.org/pdf/1708.04729](https://arxiv.org/pdf/1708.04729)

