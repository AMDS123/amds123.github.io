---
layout: post
title: "AutoScaler: Scale-Attention Networks for Visual Correspondence"
date: 2016-11-17 20:01:05
categories: arXiv_CV
tags: arXiv_CV Attention
author: Shenlong Wang, Linjie Luo, Ning Zhang, Jia Li
mathjax: true
---

* content
{:toc}

##### Abstract
Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our network consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive receptive field sizes over different scales of the input. The entire network is trained end-to-end in a siamese framework for visual correspondence tasks. Our method achieves favorable results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB-2011. We also show that our method can generalize to improve hand-crafted descriptors (e.g Daisy) on general visual correspondence tasks. Finally, our attention network can generate visually interpretable scale attention maps.

##### Abstract (translated by Google)
寻找局部特征之间的视觉对应是许多计算机视觉问题的关键。虽然定义具有较大上下文尺度的特征通常意味着更大的区分度，但也可能导致特征的空间精度较低。我们建议AutoScaler，一个规模关注网络，明确优化这种视觉对应任务的权衡。我们的网络由一个权重共享特征网络组成，用于计算多尺度特征地图和一个关注网络，以便在尺度空间中进行最优组合。这使得我们的网络在输入的不同尺度上具有自适应的接收字段大小。整个网络在可视通信任务的暹罗框架中进行端对端培训。我们的方法与包括Sintel，KITTI和CUB-2011在内的具有挑战性的光流和语义匹配基准的最新方法相比，取得了令人满意的结果。我们还表明，我们的方法可以泛化，以改善一般视觉对应任务的手工描述符（例如菊花）。最后，我们的关注网络可以生成视觉可解释的关注地图。

##### URL
[https://arxiv.org/abs/1611.05837](https://arxiv.org/abs/1611.05837)

##### PDF
[https://arxiv.org/pdf/1611.05837](https://arxiv.org/pdf/1611.05837)

