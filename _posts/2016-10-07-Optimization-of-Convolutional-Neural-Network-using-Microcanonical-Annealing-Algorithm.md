---
layout: post
title: "Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm"
date: 2016-10-07 14:39:50
categories: arXiv_CV
tags: arXiv_CV CNN Optimization Classification Deep_Learning Recognition
author: Vina Ayumi, L.M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni Arymurthy
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural network (CNN) is one of the most prominent architectures and algorithm in Deep Learning. It shows a remarkable improvement in the recognition and classification of objects. This method has also been proven to be very effective in a variety of computer vision and machine learning problems. As in other deep learning, however, training the CNN is interesting yet challenging. Recently, some metaheuristic algorithms have been used to optimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing and Harmony Search. In this paper, another type of metaheuristic algorithms with different strategy has been proposed, i.e. Microcanonical Annealing to optimize Convolutional Neural Network. The performance of the proposed method is tested using the MNIST and CIFAR-10 datasets. Although experiment results of MNIST dataset indicate the increase in computation time (1.02x - 1.38x), nevertheless this proposed method can considerably enhance the performance of the original CNN (up to 4.60\%). On the CIFAR10 dataset, currently, state of the art is 96.53\% using fractional pooling, while this proposed method achieves 99.14\%.

##### Abstract (translated by Google)
卷积神经网络（CNN）是深度学习中最突出的体系结构和算法之一。对物体的识别和分类显示出了显着的提高。这种方法在各种计算机视觉和机器学习问题中也被证明是非常有效的。然而，像其他深入的学习一样，培训CNN是有趣而又富有挑战性的。最近，一些启发式算法已经被用于使用遗传算法，粒子群优化，模拟退火和和声搜索来优化CNN。本文提出了另一种具有不同策略的启发式算法，即微正则退火算法来优化卷积神经网络。所提出的方法的性能使用MNIST和CIFAR-10数据集进行测试。尽管MNIST数据集的实验结果表明计算时间（1.02x  -  1.38x）的增加，但这种方法可以大大提高原CNN的性能（达到4.60％）。在CIFAR10数据集上，目前使用分数汇集的技术水平是96.53％，而这个方法达到了99.14％。

##### URL
[https://arxiv.org/abs/1610.02306](https://arxiv.org/abs/1610.02306)

##### PDF
[https://arxiv.org/pdf/1610.02306](https://arxiv.org/pdf/1610.02306)

