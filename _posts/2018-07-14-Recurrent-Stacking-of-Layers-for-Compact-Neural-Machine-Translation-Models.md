---
layout: post
title: "Recurrent Stacking of Layers for Compact Neural Machine Translation Models"
date: 2018-07-14 08:18:45
categories: arXiv_CL
tags: arXiv_CL NMT
author: Raj Dabre, Atsushi Fujita
mathjax: true
---

* content
{:toc}

##### Abstract
In Neural Machine Translation (NMT), the most common practice is to stack a number of recurrent or feed-forward layers in the encoder and the decoder. As a result, the addition of each new layer improves the translation quality significantly. However, this also leads to a significant increase in the number of parameters. In this paper we propose to share parameters across all the layers thereby leading to a recurrently stacked NMT model. We empirically show that the translation quality of a model that recurrently stacks a single layer 6 times is comparable to the translation quality of a model that stacks 6 separate layers. We also show that using back-translated parallel corpora as additional data leads to further significant improvements in translation quality.

##### Abstract (translated by Google)
在神经机器翻译（NMT）中，最常见的做法是在编码器和解码器中堆叠多个循环或前馈层。结果，每个新层的添加显着提高了翻译质量。然而，这也导致参数数量的显着增加。在本文中，我们建议在所有层之间共享参数，从而导致循环堆叠的NMT模型。我们凭经验表明，反复堆叠单层6次的模型的翻译质量与堆叠6个独立层的模型的翻译质量相当。我们还表明，使用反向翻译的并行语料库作为附加数据可以进一步显着提高翻译质量。

##### URL
[http://arxiv.org/abs/1807.05353](http://arxiv.org/abs/1807.05353)

##### PDF
[http://arxiv.org/pdf/1807.05353](http://arxiv.org/pdf/1807.05353)

