---
layout: post
title: "Proximal Alternating Direction Network: A Globally Converged Deep Unrolling Framework"
date: 2017-11-21 07:21:53
categories: arXiv_CV
tags: arXiv_CV Optimization Deep_Learning
author: Risheng Liu, Xin Fan, Shichao Cheng, Xiangyu Wang, Zhongxuan Luo
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning models have gained great success in many real-world applications. However, most existing networks are typically designed in heuristic manners, thus lack of rigorous mathematical principles and derivations. Several recent studies build deep structures by unrolling a particular optimization model that involves task information. Unfortunately, due to the dynamic nature of network parameters, their resultant deep propagation networks do \emph{not} possess the nice convergence property as the original optimization scheme does. This paper provides a novel proximal unrolling framework to establish deep models by integrating experimentally verified network architectures and rich cues of the tasks. More importantly, we \emph{prove in theory} that 1) the propagation generated by our unrolled deep model globally converges to a critical-point of a given variational energy, and 2) the proposed framework is still able to learn priors from training data to generate a convergent propagation even when task information is only partially available. Indeed, these theoretical results are the best we can ask for, unless stronger assumptions are enforced. Extensive experiments on various real-world applications verify the theoretical convergence and demonstrate the effectiveness of designed deep models.

##### Abstract (translated by Google)
深度学习模式在许多实际应用中取得了巨大的成功。然而，大多数现有网络通常以启发式方式设计，因此缺乏严格的数学原理和派生。最近的几项研究通过展开涉及任务信息的特定优化模型来建立深层结构。不幸的是，由于网络参数的动态性质，它们的深层传播网络不像原始优化方案那样具有良好的收敛性。本文提供了一个新的近端展开框架，通过集成实验验证的网络架构和丰富的任务线索来建立深度模型。更重要的是，我们从理论上证明：1）由展开的深层模型产生的传播全局收敛到给定变分能量的临界点; 2）所提出的框架仍然能够从训练数据中学习先验即使在任务信息仅部分可用时也能产生收敛传播。事实上，除非有更强的假设执行，否则这些理论结果是我们可以要求的最好结果。对各种现实世界的应用进行大量的实验验证了理论上的收敛性，并证明了设计的深层模型的有效性。

##### URL
[https://arxiv.org/abs/1711.07653](https://arxiv.org/abs/1711.07653)

##### PDF
[https://arxiv.org/pdf/1711.07653](https://arxiv.org/pdf/1711.07653)

