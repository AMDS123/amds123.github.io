---
layout: post
title: "weight-importance sparse training in keyword spotting"
date: 2018-07-02 09:34:34
categories: arXiv_CL
tags: arXiv_CL Sparse Speech_Recognition Recognition
author: Sihao Xue, Zhenyi Ying, Fan Mo, Min Wang, Jue Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Large size models are implemented in recently ASR system to deal with complex speech recognition problems. The num- ber of parameters in these models makes them hard to deploy, especially on some resource-short devices such as car tablet. Besides this, at most of time, ASR system is used to deal with real-time problem such as keyword spotting (KWS). It is contradictory to the fact that large model requires long com- putation time. To deal with this problem, we apply some sparse algo- rithms to reduces number of parameters in some widely used models, Deep Neural Network (DNN) KWS, which requires real short computation time. We can prune more than 90 % even 95% of parameters in the model with tiny effect decline. And the sparse model performs better than baseline models which has same order number of parameters. Besides this, sparse algorithm can lead us to find rational model size au- tomatically for certain problem without concerning choosing an original model size.

##### Abstract (translated by Google)
最近的ASR系统实现了大尺寸模型，以处理复杂的语音识别问题。这些模型中的参数数量使其难以部署，尤其是在一些资源短的设备（如平板电脑）上。除此之外，在大多数情况下，ASR系统用于处理关键字定位（KWS）等实时问题。与大型模型需要较长的计算时间这一事实相矛盾。为了解决这个问题，我们应用一些稀疏算法来减少一些广泛使用的模型中的参数数量，即深度神经网络（DNN）KWS，这需要真正的短计算时间。我们可以在模型中修剪90％以上甚至95％的参数，效果微小下降。并且稀疏模型比具有相同订单数量的参数的基线模型执行得更好。除此之外，稀疏算法可以使我们在不考虑选择原始模型大小的情况下自动为某些问题找到合理的模型大小。

##### URL
[http://arxiv.org/abs/1807.00560](http://arxiv.org/abs/1807.00560)

##### PDF
[http://arxiv.org/pdf/1807.00560](http://arxiv.org/pdf/1807.00560)

