---
layout: post
title: "Hierarchical Attention Network for Action Recognition in Videos"
date: 2016-07-21 18:16:39
categories: arXiv_CV
tags: arXiv_CV Attention Action_Recognition CNN Recognition
author: Yilin Wang, Suhang Wang, Jiliang Tang, Neil O'Hare, Yi Chang, Baoxin Li
mathjax: true
---

* content
{:toc}

##### Abstract
Understanding human actions in wild videos is an important task with a broad range of applications. In this paper we propose a novel approach named Hierarchical Attention Network (HAN), which enables to incorporate static spatial information, short-term motion information and long-term video temporal structures for complex human action understanding. Compared to recent convolutional neural network based approaches, HAN has following advantages (1) HAN can efficiently capture video temporal structures in a longer range; (2) HAN is able to reveal temporal transitions between frame chunks with different time steps, i.e. it explicitly models the temporal transitions between frames as well as video segments and (3) with a multiple step spatial temporal attention mechanism, HAN automatically learns important regions in video frames and temporal segments in the video. The proposed model is trained and evaluated on the standard video action benchmarks, i.e., UCF-101 and HMDB-51, and it significantly outperforms the state-of-the arts

##### Abstract (translated by Google)
理解野外视频中的人类行为是一个具有广泛应用的重要任务。在本文中，我们提出了一种名为Hierarchical Attention Network（HAN）的新方法，它可以将静态空间信息，短期运动信息和长期视频时间结构合并为复杂的人类行为理解。与最近的基于卷积神经网络的方法相比，HAN具有以下优点：（1）HAN能够更有效地捕捉更长距离的视频时间结构; （2）HAN能够揭示不同时间步长的帧间的时间转换，也就是明确地建立帧间和视频片段之间的时间转换，（3）具有多步空间时间关注机制，HAN自动学习重要区域视频中的视频帧和时间片段。所提出的模型在标准的视频行动基准上进行了训练和评估，即UCF-101和HMDB-51，它明显优于现有技术

##### URL
[https://arxiv.org/abs/1607.06416](https://arxiv.org/abs/1607.06416)

##### PDF
[https://arxiv.org/pdf/1607.06416](https://arxiv.org/pdf/1607.06416)

