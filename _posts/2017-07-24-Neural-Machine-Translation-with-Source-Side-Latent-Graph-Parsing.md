---
layout: post
title: "Neural Machine Translation with Source-Side Latent Graph Parsing"
date: 2017-07-24 14:52:06
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Kazuma Hashimoto, Yoshimasa Tsuruoka
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective. In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our model can be further improved by pre-training it with a small amount of treebank annotations. Our final ensemble model significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1702.02265](https://arxiv.org/abs/1702.02265)

##### PDF
[https://arxiv.org/pdf/1702.02265](https://arxiv.org/pdf/1702.02265)

