---
layout: post
title: "Adversarial Examples: Attacks and Defenses for Deep Learning"
date: 2017-12-19 18:44:07
categories: arXiv_CV
tags: arXiv_CV Review Adversarial Attention Deep_Learning
author: Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, Xiaolin Li
mathjax: true
---

* content
{:toc}

##### Abstract
With rapid progress and great successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called \textit{adversarial examples}. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical scenarios. Therefore, the attacks and defenses on adversarial examples draw great attention. 
 In this paper, we review recent findings on adversarial examples against deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications and countermeasures for adversarial examples are investigated. We further elaborate on adversarial examples and explore the challenges and the potential solutions.

##### Abstract (translated by Google)
随着各种应用的迅速发展和巨大的成功，深度学习正在被应用于许多安全苛刻的环境中。然而，最近发现深度神经网络容易受到设计良好的输入样本的影响，称为\ textit {adversarial examples}。敌对的例子是人类无法察觉的，但在测试/部署阶段很容易欺骗深层的神经网络。敌对的例子的脆弱性成为在安全关键的情况下应用深度神经网络的主要风险之一。因此，对敌对案例的攻击和防范备受关注。
 在本文中，我们回顾了近期有关对付深度神经网络的敌对案例的发现，总结了产生敌对案例的方法，并提出了这些方法的分类。根据分类法，对敌对案例的应用和对策进行了调查。我们进一步阐述对抗性的例子，探索挑战和潜在的解决方案。

##### URL
[http://arxiv.org/abs/1712.07107](http://arxiv.org/abs/1712.07107)

##### PDF
[http://arxiv.org/pdf/1712.07107](http://arxiv.org/pdf/1712.07107)

