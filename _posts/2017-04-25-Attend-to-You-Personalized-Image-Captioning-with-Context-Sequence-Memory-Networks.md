---
layout: post
title: "Attend to You: Personalized Image Captioning with Context Sequence Memory Networks"
date: 2017-04-25 23:30:43
categories: arXiv_CV
tags: arXiv_CV Image_Caption Knowledge Caption Prediction Quantitative Memory_Networks
author: Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim
mathjax: true
---

* content
{:toc}

##### Abstract
We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.

##### Abstract (translated by Google)
我们解决了图像字幕的个性化问题，这在以前的研究中尚未讨论过。对于查询图像，我们的目标是生成描述性句子，考虑先前知识，例如先前文档中用户的活动词汇。作为个性化图像字幕的应用，我们在新收集的Instagram数据集上处理两个后期自动化任务：主题标签预测和后期生成，包括来自6.3K用户的1.1M帖子。我们提出了一种名为Context Sequence Memory Network（CSMN）的新颖字幕模型。其对先前存储器网络模型的独特更新包括（i）利用存储器作为多种类型的上下文信息的存储库，（ii）将先前生成的单词附加到存储器中以捕获长期信息而不遭受消失的梯度问题，并且（iii） ）采用CNN存储器结构共同表示附近的有序存储器插槽，以便更好地理解上下文。通过Amazon Mechanical Turk的定量评估和用户研究，我们展示了CSMN的三个新颖特征的有效性及其对最先进字幕模型的个性化图像字幕的性能增强。

##### URL
[https://arxiv.org/abs/1704.06485](https://arxiv.org/abs/1704.06485)

##### PDF
[https://arxiv.org/pdf/1704.06485](https://arxiv.org/pdf/1704.06485)

