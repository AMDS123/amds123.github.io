---
layout: post
title: "Analyzing Linear Dynamical Systems: From Modeling to Coding and Learning"
date: 2017-02-27 02:42:26
categories: arXiv_CV
tags: arXiv_CV Sparse Segmentation Video_Classification Classification Recognition
author: Wenbing Huang, Fuchun Sun, Lele Cao, Mehrtash Harandi
mathjax: true
---

* content
{:toc}

##### Abstract
Encoding time-series with Linear Dynamical Systems (LDSs) leads to rich models with applications ranging from dynamical texture recognition to video segmentation to name a few. In this paper, we propose to represent LDSs with infinite-dimensional subspaces and derive an analytic solution to obtain stable LDSs. We then devise efficient algorithms to perform sparse coding and dictionary learning on the space of infinite-dimensional subspaces. In particular, two solutions are developed to sparsely encode an LDS. In the first method, we map the subspaces into a Reproducing Kernel Hilbert Space (RKHS) and achieve our goal through kernel sparse coding. As for the second solution, we propose to embed the infinite-dimensional subspaces into the space of symmetric matrices and formulate the sparse coding accordingly in the induced space. For dictionary learning, we encode time-series by introducing a novel concept, namely the two-fold LDSs. We then make use of the two-fold LDSs to derive an analytical form for updating atoms of an LDS dictionary, i.e., each atom is an LDS itself. Compared to several baselines and state-of-the-art methods, the proposed methods yield higher accuracies in various classification tasks including video classification and tactile recognition.

##### Abstract (translated by Google)
利用线性动态系统（LDS）对时间序列进行编码产生丰富的模型，其应用范围从动态纹理识别到视频分割等等。在本文中，我们提出用无限维子空间来表示LDS，并得到一个分析解来获得稳定的LDS。然后设计高效的算法对无穷维子空间进行稀疏编码和字典学习。特别是开发了两种解决方案来对LDS进行稀疏编码。在第一种方法中，我们将子空间映射到再生核Hilbert空间（RKHS），并通过内核稀疏编码实现我们的目标。对于第二种解决方案，我们建议将无限维子空间嵌入对称矩阵的空间，并在诱导空间中相应地制定稀疏编码。对于词典学习，我们通过引入一个新颖的概念来编码时间序列，即双重LDS。然后，我们利用双重LDS来导出用于更新LDS字典的原子的分析形式，即，每个原子是LDS本身。相比于几个基线和最先进的方法，所提出的方法在包括视频分类和触觉识别的各种分类任务中产生更高的精度。

##### URL
[https://arxiv.org/abs/1608.01059](https://arxiv.org/abs/1608.01059)

##### PDF
[https://arxiv.org/pdf/1608.01059](https://arxiv.org/pdf/1608.01059)

