---
layout: post
title: "Single-Pass, Adaptive Natural Language Filtering: Measuring Value in User Generated Comments on Large-Scale, Social Media News Forums"
date: 2017-01-12 04:55:36
categories: arXiv_SD
tags: arXiv_SD Face
author: Manuel Amunategui
mathjax: true
---

* content
{:toc}

##### Abstract
There are large amounts of insight and social discovery potential in mining crowd-sourced comments left on popular news forums like Reddit.com, Tumblr.com, Facebook.com and Hacker News. Unfortunately, due the overwhelming amount of participation with its varying quality of commentary, extracting value out of such data isn't always obvious nor timely. By designing efficient, single-pass and adaptive natural language filters to quickly prune spam, noise, copy-cats, marketing diversions, and out-of-context posts, we can remove over a third of entries and return the comments with a higher probability of relatedness to the original article in question. The approach presented here uses an adaptive, two-step filtering process. It first leverages the original article posted in the thread as a starting corpus to parse comments by matching intersecting words and term-ratio balance per sentence then grows the corpus by adding new words harvested from high-matching comments to increase filtering accuracy over time.

##### Abstract (translated by Google)
在Reddit，Tumblr.com，Facebook.com和黑客新闻等热门新闻论坛上，挖掘众包的评论中有大量的洞察力和社会发现潜力。不幸的是，由于评论的质量参差不齐，参与量极大，从这些数据中提取价值并不总是显而易见，也不及时。通过设计高效的单通道和自适应自然语言过滤器来快速修剪垃圾邮件，噪音，复制猫，市场营销转移和不在上下文的帖子，我们可以删除超过三分之一的条目，并以较高的概率返回评论与有关原始文章的相关性。这里介绍的方法使用自适应，两步过滤过程。它首先利用线程中发布的原始文章作为起始语料库，通过匹配相交词和每个句子的词比平衡来解析评论，然后通过添加从高匹配评论中收集的新词增加语料库以增加随着时间的过滤精度。

##### URL
[https://arxiv.org/abs/1701.03231](https://arxiv.org/abs/1701.03231)

##### PDF
[https://arxiv.org/pdf/1701.03231](https://arxiv.org/pdf/1701.03231)

