---
layout: post
title: "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"
date: 2018-05-22 13:06:37
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, Nazli Goharian
mathjax: true
---

* content
{:toc}

##### Abstract
Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.

##### Abstract (translated by Google)
神经抽象概括模型已经在总结相对较短的文档中产生了有希望的结果。我们提出了第一种抽象概括单一，长形式文档（例如研究论文）的模型。我们的方法包括一个新的分层编码器，模拟文档的话语结构，以及一个专注的话语感知解码器来生成摘要。两个大型科学论文数据集的实证结果表明，我们的模型显着优于最先进的模型。

##### URL
[http://arxiv.org/abs/1804.05685](http://arxiv.org/abs/1804.05685)

##### PDF
[http://arxiv.org/pdf/1804.05685](http://arxiv.org/pdf/1804.05685)

