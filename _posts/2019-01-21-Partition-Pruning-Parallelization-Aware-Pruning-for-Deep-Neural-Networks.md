---
layout: post
title: "Partition Pruning: Parallelization-Aware Pruning for Deep Neural Networks"
date: 2019-01-21 19:34:21
categories: arXiv_CV
tags: arXiv_CV Inference
author: Sina Shahhosseini, Ahmad Albaqsami, Masoomeh Jasemi, Shaahin Hessabi, Nader Bagherzadeh
mathjax: true
---

* content
{:toc}

##### Abstract
Parameters of recent neural networks require a huge amount of memory. These parameters are used by neural networks to perform machine learning tasks when processing inputs. To speed up inference, we develop Partition Pruning, an innovative scheme to reduce the parameters used while taking into consideration parallelization. We evaluated the performance and energy consumption of parallel inference of partitioned models, which showed a 7.72x speed up of performance and a 2.73x reduction in the energy used for computing pruned layers of TinyVGG16 in comparison to running the unpruned model on a single accelerator. In addition, our method showed a limited reduction some numbers in accuracy while partitioning fully connected layers.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1901.11391](http://arxiv.org/abs/1901.11391)

##### PDF
[http://arxiv.org/pdf/1901.11391](http://arxiv.org/pdf/1901.11391)

