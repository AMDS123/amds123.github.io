---
layout: post
title: "The Role of Context Types and Dimensionality in Learning Word Embeddings"
date: 2017-07-19 15:32:54
categories: arXiv_SD
tags: arXiv_SD Embedding
author: Oren Melamud, David McClosky, Siddharth Patwardhan, Mohit Bansal
mathjax: true
---

* content
{:toc}

##### Abstract
We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.

##### Abstract (translated by Google)
我们首先对如何使用不同类型的上下文来学习跳转词嵌入影响广泛的内在和外在NLP任务的性能提供了第一个广泛的评估。我们的研究结果表明，尽管内在任务倾向于对特定类型的上下文和更高的维度表现出明显的偏好，但为了找到我们考虑的大多数外部任务的最佳设置，还需要进行更仔细的调整。此外，对于这些外在任务，我们发现，一旦从增加嵌入维度中获得的收益大部分已经耗尽，则使用不同上下文类型学习的单词嵌入的简单级联可以产生进一步的性能增益。作为一个额外的贡献，我们提出了一个跳跃模型的新变种，从替代词的加权上下文中学习词嵌入。

##### URL
[https://arxiv.org/abs/1601.00893](https://arxiv.org/abs/1601.00893)

##### PDF
[https://arxiv.org/pdf/1601.00893](https://arxiv.org/pdf/1601.00893)

