---
layout: post
title: "Combining Language and Vision with a Multimodal Skip-gram Model"
date: 2015-03-12 09:47:33
categories: arXiv_CL
tags: arXiv_CL
author: Angeliki Lazaridou, Nghia The Pham, Marco Baroni
mathjax: true
---

* content
{:toc}

##### Abstract
We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.

##### Abstract (translated by Google)
我们扩展了Mikolov等人的SKIP-GRAM模型。 （2013a）通过考虑视觉信息。像SKIP-GRAM一样，我们的多模式模型（MMSKIP-GRAM）通过学习预测文本语料库中的语言上下文来构建基于矢量的词表示。然而，对于一组有限的单词，模型也暴露在它们表示的对象（从自然图像中提取）的视觉表示中，并且必须共同预测语言和视觉特征。 MMSKIP-GRAM模型在各种语义基准上取得了良好的性能。而且，由于它们将视觉信息传播到所有的单词，所以我们用它们来改善零点设置中的图像标记和检索，其中在模型训练期间从未看到测试概念。最后，MMSKIP-GRAM模型发现了抽象词的有趣视觉特性，为具体的意义理论的实现铺平了道路。

##### URL
[https://arxiv.org/abs/1501.02598](https://arxiv.org/abs/1501.02598)

##### PDF
[https://arxiv.org/pdf/1501.02598](https://arxiv.org/pdf/1501.02598)

