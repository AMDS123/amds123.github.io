---
layout: post
title: "Time Perception Machine: Temporal Point Processes for the When, Where and What of Activity Prediction"
date: 2018-08-14 06:36:27
categories: arXiv_CV
tags: arXiv_CV Prediction Relation
author: Yatao Zhong, Bicheng Xu, Guang-Tong Zhou, Luke Bornn, Greg Mori
mathjax: true
---

* content
{:toc}

##### Abstract
Numerous powerful point process models have been developed to understand temporal patterns in sequential data from fields such as health-care, electronic commerce, social networks, and natural disaster forecasting. In this paper, we develop novel models for learning the temporal distribution of human activities in streaming data (e.g., videos and person trajectories). We propose an integrated framework of neural networks and temporal point processes for predicting when the next activity will happen. Because point processes are limited to taking event frames as input, we propose a simple yet effective mechanism to extract features at frames of interest while also preserving the rich information in the remaining frames. We evaluate our model on two challenging datasets. The results show that our model outperforms traditional statistical point process approaches significantly, demonstrating its effectiveness in capturing the underlying temporal dynamics as well as the correlation within sequential activities. Furthermore, we also extend our model to a joint estimation framework for predicting the timing, spatial location, and category of the activity simultaneously, to answer the when, where, and what of activity prediction.

##### Abstract (translated by Google)
已经开发了许多强大的点过程模型来理解来自诸如医疗保健，电子商务，社交网络和自然灾害预测等领域的顺序数据中的时间模式。在本文中，我们开发了用于学习流数据（例如，视频和人物轨迹）中的人类活动的时间分布的新模型。我们提出了一个神经网络和时间点过程的集成框架，用于预测下一个活动何时发生。由于点过程仅限于将事件帧作为输入，因此我们提出了一种简单而有效的机制来提取感兴趣帧的特征，同时还保留剩余帧中的丰富信息。我们在两个挑战性数据集上评估我们的模型。结果表明，我们的模型显着优于传统的统计点过程方法，证明了它在捕获潜在时间动态以及顺序活动中的相关性方面的有效性。此外，我们还将模型扩展到联合估算框架，以同时预测活动的时间，空间位置和类别，以回答活动预测的时间，地点和内容。

##### URL
[http://arxiv.org/abs/1808.04063](http://arxiv.org/abs/1808.04063)

##### PDF
[http://arxiv.org/pdf/1808.04063](http://arxiv.org/pdf/1808.04063)

