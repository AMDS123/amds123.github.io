---
layout: post
title: "HUME: Human UCCA-Based Evaluation of Machine Translation"
date: 2016-09-27 13:39:42
categories: arXiv_CL
tags: arXiv_CL Relation
author: Alexandra Birch, Omri Abend, Ondrej Bojar, Barry Haddow
mathjax: true
---

* content
{:toc}

##### Abstract
Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.

##### Abstract (translated by Google)
机器翻译的人类评价通常使用句级测量，比如相对排序或充足度量表。然而，这些没有提供对可能的错误的洞察力，并且不能很好地用句子长度来衡量。我们主张采用基于语义的评估，即捕获MT输出中保留的组件意义，从而提供更精细的翻译质量分析，并实现基于语义的MT的构建和调整。我们提出了一个新的人类语义评估测量，基于人类基于UCCA的MT评估（HUME），基于UCCA语义表示方案。与之前的方法相比，HUME涵盖了更广泛的语义现象，并且不依赖潜在的乱码MT输出的语义标注。我们使用四种语言对进行实验，证明了HUME的广泛适用性，并报告了良好的内部注释符合率以及与人类适当性分数的相关性。

##### URL
[https://arxiv.org/abs/1607.00030](https://arxiv.org/abs/1607.00030)

##### PDF
[https://arxiv.org/pdf/1607.00030](https://arxiv.org/pdf/1607.00030)

