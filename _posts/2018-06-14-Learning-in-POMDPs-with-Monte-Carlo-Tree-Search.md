---
layout: post
title: "Learning in POMDPs with Monte Carlo Tree Search"
date: 2018-06-14 16:17:44
categories: arXiv_AI
tags: arXiv_AI
author: Sammie Katt, Frans A. Oliehoek, Christopher Amato
mathjax: true
---

* content
{:toc}

##### Abstract
The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.

##### Abstract (translated by Google)
POMDP是结果和信息不确定性下推理的强大框架，但构建准确的POMDP模型很困难。贝叶斯自适应部分可观察马尔可夫决策过程（BA-POMDP）扩展POMDP以允许在执行期间学习模型。 BA-POMDPs是贝叶斯RL方法，原则上允许在开采和勘探之间进行最佳平衡。不幸的是，BA-POMDP目前不能解决任何非平凡的领域。在本文中，我们将蒙特卡罗树搜索方法POMCP扩展到BA-POMDPs，并表明由此产生的方法，我们称之为BA-POMCP，能够解决以前的解决方法无法解决的问题。此外，我们介绍几种利用BA-POMDP结构的技术，以提高BA-POMCP的效率以及它们的收敛性。

##### URL
[https://arxiv.org/abs/1806.05631](https://arxiv.org/abs/1806.05631)

##### PDF
[https://arxiv.org/pdf/1806.05631](https://arxiv.org/pdf/1806.05631)

