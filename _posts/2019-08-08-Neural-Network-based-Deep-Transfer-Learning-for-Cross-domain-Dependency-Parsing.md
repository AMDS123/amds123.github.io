---
layout: post
title: "Neural Network based Deep Transfer Learning for Cross-domain Dependency Parsing"
date: 2019-08-08 01:16:34
categories: arXiv_CL
tags: arXiv_CL Attention Transfer_Learning
author: Zhentao Xia, Likai Wang, Weiguang Qu, Junsheng Zhou, Yanhui Gu
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we describe the details of the neural dependency parser sub-mitted by our team to the NLPCC 2019 Shared Task of Semi-supervised do-main adaptation subtask on Cross-domain Dependency Parsing. Our system is based on the stack-pointer networks(STACKPTR). Considering the im-portance of context, we utilize self-attention mechanism for the representa-tion vectors to capture the meaning of words. In addition, to adapt three dif-ferent domains, we utilize neural network based deep transfer learning which transfers the pre-trained partial network in the source domain to be a part of deep neural network in the three target domains (product comments, product blogs and web fiction) respectively. Results on the three target domains demonstrate that our model performs competitively.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.02895](http://arxiv.org/abs/1908.02895)

##### PDF
[http://arxiv.org/pdf/1908.02895](http://arxiv.org/pdf/1908.02895)

