---
layout: post
title: "DropNeuron: Simplifying the Structure of Deep Neural Networks"
date: 2016-07-03 09:39:30
categories: arXiv_CV
tags: arXiv_CV Sparse CNN Deep_Learning
author: Wei Pan, Hao Dong, Yike Guo
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning using multi-layer neural networks (NNs) architecture manifests superb power in modern machine learning systems. The trained Deep Neural Networks (DNNs) are typically large. The question we would like to address is whether it is possible to simplify the NN during training process to achieve a reasonable performance within an acceptable computational time. We presented a novel approach of optimising a deep neural network through regularisation of net- work architecture. We proposed regularisers which support a simple mechanism of dropping neurons during a network training process. The method supports the construction of a simpler deep neural networks with compatible performance with its simplified version. As a proof of concept, we evaluate the proposed method with examples including sparse linear regression, deep autoencoder and convolutional neural network. The valuations demonstrate excellent performance. The code for this work can be found in this http URL

##### Abstract (translated by Google)
使用多层神经网络（NN）架构的深度学习在现代机器学习系统中体现出超强的力量。训练有素的深度神经网络（DNN）通常很大。我们想要解决的问题是在训练过程中是否有可能简化NN以在可接受的计算时间内达到合理的性能。我们提出了一种通过网络结构正则化来优化深度神经网络的新方法。我们提出了支持在网络训练过程中丢弃神经元的简单机制的regulariser。该方法支持使用其简化版本来构建具有兼容性能的更简单的深度神经网络。作为概念证明，我们用稀疏线性回归，深度自动编码器和卷积神经网络等方法来评估所提出的方法。估值表现出色。这个工作的代码可以在这个http URL中找到

##### URL
[https://arxiv.org/abs/1606.07326](https://arxiv.org/abs/1606.07326)

##### PDF
[https://arxiv.org/pdf/1606.07326](https://arxiv.org/pdf/1606.07326)

