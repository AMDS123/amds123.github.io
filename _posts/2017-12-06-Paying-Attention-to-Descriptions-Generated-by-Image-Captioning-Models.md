---
layout: post
title: 'Paying Attention to Descriptions Generated by Image Captioning Models'
date: 2017-12-06 02:40:01
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption
author: Hamed R. Tavakoli, Rakshith Shetty, Ali Borji, Jorma Laaksonen
---

* content
{:toc}

##### Abstract
To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.

##### Abstract (translated by Google)
为了弥补人类和机器在图像理解和描述上的差距，我们需要进一步了解人们如何描述感知场景。在本文中，我们研究了场景描述结构中自下而上的基于显着性的视觉注意与对象引用的一致性。我们研究人写描述和机器生成描述的属性。然后，我们提出一个显着提升的图像字幕模型，以便从语言模型中的低级线索中获益。我们知道：（1）人类早于描述中较不显着的物体就提及更多显着的物体，（2）字幕模型表现得越好，与人类描述的注意协议越好;（3）提出的显着性提升模型，与其基线形式相比，在MS COCO数据库上并没有显着提高，这表明明确的自下而上的提升并不能帮助当任务得到良好的学习和调整数据，（4）然而，观察到更好的泛化显着提高了看不见的数据模型。

##### URL
[https://arxiv.org/abs/1704.07434](https://arxiv.org/abs/1704.07434)

