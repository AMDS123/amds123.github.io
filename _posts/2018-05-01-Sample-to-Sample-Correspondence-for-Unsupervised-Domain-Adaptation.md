---
layout: post
title: "Sample-to-Sample Correspondence for Unsupervised Domain Adaptation"
date: 2018-05-01 14:12:57
categories: arXiv_CV
tags: arXiv_CV Regularization Sentiment Sentiment_Classification Image_Classification Optimization Classification
author: Debasmit Das, C.S. George Lee
mathjax: true
---

* content
{:toc}

##### Abstract
The assumption that training and testing samples are generated from the same distribution does not always hold for real-world machine-learning applications. The procedure of tackling this discrepancy between the training (source) and testing (target) domains is known as domain adaptation. We propose an unsupervised version of domain adaptation that considers the presence of only unlabelled data in the target domain. Our approach centers on finding correspondences between samples of each domain. The correspondences are obtained by treating the source and target samples as graphs and using a convex criterion to match them. The criteria used are first-order and second-order similarities between the graphs as well as a class-based regularization. We have also developed a computationally efficient routine for the convex optimization, thus allowing the proposed method to be used widely. To verify the effectiveness of the proposed method, computer simulations were conducted on synthetic, image classification and sentiment classification datasets. Results validated that the proposed local sample-to-sample matching method out-performs traditional moment-matching methods and is competitive with respect to current local domain-adaptation methods.

##### Abstract (translated by Google)
假设训练和测试样本是从相同的分布产生的，并不总是适用于真实世界的机器学习应用。解决训练（源）和测试（目标）域之间差异的过程称为域适应。我们提出了一个无监督的域适应版本，它考虑目标域中仅存在未标记的数据。我们的方法集中于找到每个域样本之间的对应关系。对应关系是通过将源样本和目标样本作为图形处理并使用凸标准来匹配它们而获得的。使用的标准是图之间的一阶和二阶相似度以及基于类的正则化。我们还为凸优化开发了一个计算效率高的程序，因此使得所提出的方法得到了广泛的应用。为了验证所提出的方法的有效性，对合成图像分类和情感分类数据集进行了计算机模拟。结果验证了所提出的局部样本间匹配方法优于传统的时间匹配方法，并且与当前局部域适应方法相比具有竞争性。

##### URL
[https://arxiv.org/abs/1805.00355](https://arxiv.org/abs/1805.00355)

##### PDF
[https://arxiv.org/pdf/1805.00355](https://arxiv.org/pdf/1805.00355)

