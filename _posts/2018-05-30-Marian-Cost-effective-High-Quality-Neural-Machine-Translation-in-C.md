---
layout: post
title: "Marian: Cost-effective High-Quality Neural Machine Translation in C++"
date: 2018-05-30 17:23:24
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Marcin Junczys-Dowmunt, Kenneth Heafield, Hieu Hoang, Roman Grundkiewicz, Anthony Aue
mathjax: true
---

* content
{:toc}

##### Abstract
This paper describes the submissions of the "Marian" team to the WNMT 2018 shared task. We investigate combinations of teacher-student training, low-precision matrix products, auto-tuning and other methods to optimize the Transformer model on GPU and CPU. By further integrating these methods with the new averaging attention networks, a recently introduced faster Transformer variant, we create a number of high-quality, high-performance models on the GPU and CPU, dominating the Pareto frontier for this shared task.

##### Abstract (translated by Google)
本文介绍了“玛丽安”团队对WNMT 2018共享任务的意见。我们调查师生培训，低精度矩阵产品，自动调整和其他方法的组合，以优化GPU和CPU上的Transformer模型。通过将这些方法与新近推出的平均关注网络（最近推出的更快的Transformer变体）进一步整合，我们在GPU和CPU上创建了许多高质量，高性能的模型，在这个共享任务中支配帕累托边界。

##### URL
[http://arxiv.org/abs/1805.12096](http://arxiv.org/abs/1805.12096)

##### PDF
[http://arxiv.org/pdf/1805.12096](http://arxiv.org/pdf/1805.12096)

