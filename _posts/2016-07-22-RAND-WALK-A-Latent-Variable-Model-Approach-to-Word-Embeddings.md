---
layout: post
title: "RAND-WALK: A Latent Variable Model Approach to Word Embeddings"
date: 2016-07-22 20:09:25
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model
author: Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of~\citet{mnih2007three}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by~\citet{mikolov2013efficient} and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.

##### Abstract (translated by Google)
语义词嵌入通过矢量来表示单词的意义，并且通过不同的方法来创建。许多人对共现统计使用非线性操作，并且拥有手动调整的超参数和重新加权方法。本文提出了一个新的生成模型，一个动态的〜\ citet {mnih2007three}对数线性主题模型。方法的新颖性在于使用之前的计算封闭式表达式来进行单词统计。这为PMI，word2vec和GloVe等非线性模型以及一些超参数选择提供了理论依据。这也有助于解释为什么低维语义嵌入包含线性代数结构，可以解决单词类比问题，正如下面的文章所显示的那样。为生成模型假设提供了实验支持，其中最重要的是潜在词向量相当均匀地分散在空间中。

##### URL
[https://arxiv.org/abs/1502.03520](https://arxiv.org/abs/1502.03520)

##### PDF
[https://arxiv.org/pdf/1502.03520](https://arxiv.org/pdf/1502.03520)

