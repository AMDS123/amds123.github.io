---
layout: post
title: "Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking"
date: 2016-01-12 10:06:52
categories: arXiv_SD
tags: arXiv_SD GAN
author: Krzysztof Wołk, Danijel Koržinek
mathjax: true
---

* content
{:toc}

##### Abstract
Re-speaking is a mechanism for obtaining high quality subtitles for use in live broadcast and other public events. Because it relies on humans performing the actual re-speaking, the task of estimating the quality of the results is non-trivial. Most organisations rely on humans to perform the actual quality assessment, but purely automatic methods have been developed for other similar problems, like Machine Translation. This paper will try to compare several of these methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will then be matched to the human-derived NER metric, commonly used in re-speaking.

##### Abstract (translated by Google)
重讲是获得高品质字幕供现场直播和其他公共活动使用的机制。因为它依赖于人类进行实际的重新讲话，估计结果质量的任务是非平凡的。大多数组织都依靠人来进行实际的质量评估，但是针对其他类似的问题，如机器翻译，已经开发了纯粹的自动方法。本文将尝试比较几种方法：BLEU，EBLEU，NIST，METEOR，METEOR-PL，TER和RIBES。然后将这些匹配到通常用于重新说话的人类派生的NER度量。

##### URL
[https://arxiv.org/abs/1601.02789](https://arxiv.org/abs/1601.02789)

##### PDF
[https://arxiv.org/pdf/1601.02789](https://arxiv.org/pdf/1601.02789)

