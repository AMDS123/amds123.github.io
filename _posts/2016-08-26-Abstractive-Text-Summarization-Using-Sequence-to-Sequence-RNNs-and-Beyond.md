---
layout: post
title: "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond"
date: 2016-08-26 16:13:13
categories: arXiv_SD
tags: arXiv_SD Attention Summarization RNN
author: Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, Bing Xiang
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.

##### Abstract (translated by Google)
在这项工作中，我们使用Attentional Encoder-Decoder Recurrent Neural Networks对抽象文本摘要建模，并且表明它们在两个不同的语料库上达到了最新的性能。我们提出了几个新颖的模型，用于解决摘要中的关键问题，这些模型没有被基本架构充分建模，如关键词建模，获取句子结构的层次结构，以及在培训时间发布罕见或不可见的词语。我们的工作表明，我们提出的许多模型有助于进一步提高性能。我们还提出了一个由多句话总结组成的新数据集，并为进一步的研究建立了性能基准。

##### URL
[https://arxiv.org/abs/1602.06023](https://arxiv.org/abs/1602.06023)

##### PDF
[https://arxiv.org/pdf/1602.06023](https://arxiv.org/pdf/1602.06023)

