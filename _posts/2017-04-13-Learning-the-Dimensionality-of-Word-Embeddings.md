---
layout: post
title: "Learning the Dimensionality of Word Embeddings"
date: 2017-04-13 17:44:37
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model Quantitative
author: Eric Nalisnick, Sachin Ravi
mathjax: true
---

* content
{:toc}

##### Abstract
We describe a method for learning word embeddings with data-dependent dimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic Dimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of Mikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is made dynamic by employing techniques used by Cote & Larochelle (2016) to define an RBM with an infinite number of hidden units. We show qualitatively and quantitatively that SD-SG and SD-CBOW are competitive with their fixed-dimension counterparts while providing a distribution over embedding dimensionalities, which offers a window into how semantics distribute across dimensions.

##### Abstract (translated by Google)
我们描述了一种用数据依赖性维度来学习单词嵌入的方法。我们的随机维度Skip-Gram（SD-SG）和随机维度连续词袋（SD-CBOW）是Mikolov等人（2013）着名的“word2vec”模型的非参数类比。通过采用Cote＆Larochelle（2016年）使用的技术来定义矢量维度，以定义具有无限数量隐藏单元的RBM。我们通过定性和定量的方式表明，SD-SG和SD-CBOW与其固定维度的对等体相比具有竞争性，同时提供了嵌入维度上的分布，为跨维度的语义分布提供了一个窗口。

##### URL
[https://arxiv.org/abs/1511.05392](https://arxiv.org/abs/1511.05392)

##### PDF
[https://arxiv.org/pdf/1511.05392](https://arxiv.org/pdf/1511.05392)

