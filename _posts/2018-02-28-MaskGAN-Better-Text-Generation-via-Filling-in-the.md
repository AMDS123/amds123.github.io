---
layout: post
title: "MaskGAN: Better Text Generation via Filling in the______"
date: 2018-02-28 16:26:04
categories: arXiv_AI
tags: arXiv_AI Adversarial GAN Summarization Text_Generation Language_Model Quantitative
author: William Fedus, Ian Goodfellow, Andrew M. Dai
mathjax: true
---

* content
{:toc}

##### Abstract
Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.

##### Abstract (translated by Google)
神经文本生成模型通常是自回归语言模型或seq2seq模型。这些模型通过依次对单词进行采样来生成文本，每个单词以前一个单词为条件，并且是几种机器翻译和汇总基准的最新技术。这些基准通常由验证困惑定义，尽管这不是对生成文本质量的直接测量。另外，这些模型通常是通过最大可能性和教师强迫训练的。这些方法非常适合优化混淆，但是可能导致样本质量差，因为生成文本需要调整在训练时间从未观察到的单词序列。我们建议使用生成敌对​​网络（GAN）来改进样本质量，该网络明确训练发生器生成高质量样本，并在图像生成中取得了很大成功。 GAN最初设计用于输出可区分的值，因此离散的语言生成对他们来说是具有挑战性的。我们声称，验证困惑本身并不代表模型生成的文本的质量。我们引入了一个演员 - 评论者条件GAN，它填补了周围环境条件的缺失文本。我们用定性和定量的方式表明，与最大似然训练模型相比，这会产生更真实的条件和无条件文本样本。

##### URL
[http://arxiv.org/abs/1801.07736](http://arxiv.org/abs/1801.07736)

##### PDF
[http://arxiv.org/pdf/1801.07736](http://arxiv.org/pdf/1801.07736)

