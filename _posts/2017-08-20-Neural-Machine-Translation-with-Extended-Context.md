---
layout: post
title: "Neural Machine Translation with Extended Context"
date: 2017-08-20 09:31:49
categories: arXiv_CL
tags: arXiv_CL Attention
author: Jörg Tiedemann, Yves Scherrer
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.

##### Abstract (translated by Google)
我们调查扩展上下文在基于注意的神经机器翻译中的使用。我们以翻译的电影字幕为基础进行实验，并讨论增加单个翻译单元之外的片段的效果。我们研究了扩展源语言的使用以及双语扩展。这些模型学习区分来自不同部分的信息，并且在翻译质量方面令人惊讶地稳健。在这个试点研究中，我们观察到至少在一些选定的案例中有意思的交叉句子注意模式，可以提高翻译的文本连贯性。

##### URL
[https://arxiv.org/abs/1708.05943](https://arxiv.org/abs/1708.05943)

##### PDF
[https://arxiv.org/pdf/1708.05943](https://arxiv.org/pdf/1708.05943)

