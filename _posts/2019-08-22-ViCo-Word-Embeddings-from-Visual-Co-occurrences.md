---
layout: post
title: "ViCo: Word Embeddings from Visual Co-occurrences"
date: 2019-08-22 17:58:52
categories: arXiv_CV
tags: arXiv_CV Embedding
author: Tanmay Gupta, Alexander Schwing, Derek Hoiem
mathjax: true
---

* content
{:toc}

##### Abstract
We propose to learn word embeddings from visual co-occurrences. Two words co-occur visually if both words apply to the same image or image region. Specifically, we extract four types of visual co-occurrences between object and attribute words from large-scale, textually-annotated visual databases like VisualGenome and ImageNet. We then train a multi-task log-bilinear model that compactly encodes word "meanings" represented by each co-occurrence type into a single visual word-vector. Through unsupervised clustering, supervised partitioning, and a zero-shot-like generalization analysis we show that our word embeddings complement text-only embeddings like GloVe by better representing similarities and differences between visual concepts that are difficult to obtain from text corpora alone. We further evaluate our embeddings on five downstream applications, four of which are vision-language tasks. Augmenting GloVe with our embeddings yields gains on all tasks. We also find that random embeddings perform comparably to learned embeddings on all supervised vision-language tasks, contrary to conventional wisdom.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.08527](https://arxiv.org/abs/1908.08527)

##### PDF
[https://arxiv.org/pdf/1908.08527](https://arxiv.org/pdf/1908.08527)

