---
layout: post
title: "Deep Depth From Focus"
date: 2017-11-24 14:44:56
categories: arXiv_CV
tags: arXiv_CV
author: Caner Hazirbas, Laura Leal-Taixé, Daniel Cremers
mathjax: true
---

* content
{:toc}

##### Abstract
Depth from Focus (DFF) is one of the classical ill-posed inverse problems in computer vision. Most approaches recover the depth at each pixel based on the focal setting which exhibits maximal sharpness. Yet, it is not obvious how to reliably estimate the sharpness level, particularly in low-textured areas. In this paper, we propose `Deep Depth From Focus (DDFF)' as the first end-to-end learning approach to this problem. Towards this goal, we create a novel real-scene indoor benchmark composed of 4D light-field images obtained from a plenoptic camera and ground truth depth obtained from a registered RGB-D sensor. Compared to existing benchmarks our dataset is 25 times larger, enabling the use of machine learning for this inverse problem. We compare our results with state-of-the-art DFF methods and we also analyze the effect of several key deep architectural components. These experiments show that DDFFNet achieves state-of-the-art performance in all scenes, reducing depth error by more than 75% wrt classic DFF methods.

##### Abstract (translated by Google)
焦点深度（DFF）是计算机视觉中经典的病态反问题之一。大多数方法基于展现最大清晰度的焦点设置来恢复每个像素的深度。然而，如何可靠地估计清晰度水平并不明显，特别是在低质感区域。在本文中，我们提出“深度聚焦（DDFF）”作为第一个端到端的学习方法来解决这个问题。为了实现这个目标，我们创建了一个新的实景室内基准，它由从全光照相机获得的4D光场图像和从登记的RGB-D传感器获得的地面真实深度组成。与现有的基准相比，我们的数据集大25倍，使机器学习可以用于反演问题。我们将我们的结果与最先进的DFF方法进行比较，我们还分析了几个关键的深层建筑组件的影响。这些实验表明，DDFFNet在所有场景中达到了最先进的性能，与传统的DFF方法相比，深度误差减少了75％以上。

##### URL
[https://arxiv.org/abs/1704.01085](https://arxiv.org/abs/1704.01085)

##### PDF
[https://arxiv.org/pdf/1704.01085](https://arxiv.org/pdf/1704.01085)

