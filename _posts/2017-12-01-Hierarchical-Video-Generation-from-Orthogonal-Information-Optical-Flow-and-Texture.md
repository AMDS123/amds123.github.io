---
layout: post
title: "Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture"
date: 2017-12-01 08:23:09
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Classification
author: Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada
mathjax: true
---

* content
{:toc}

##### Abstract
Learning to represent and generate videos from unlabeled data is a very challenging problem. To generate realistic videos, it is important not only to ensure that the appearance of each frame is real, but also to ensure the plausibility of a video motion and consistency of a video appearance in the time direction. The process of video generation should be divided according to these intrinsic difficulties. In this study, we focus on the motion and appearance information as two important orthogonal components of a video, and propose Flow-and-Texture-Generative Adversarial Networks (FTGAN) consisting of FlowGAN and TextureGAN. In order to avoid a huge annotation cost, we have to explore a way to learn from unlabeled data. Thus, we employ optical flow as motion information to generate videos. FlowGAN generates optical flow, which contains only the edge and motion of the videos to be begerated. On the other hand, TextureGAN specializes in giving a texture to optical flow generated by FlowGAN. This hierarchical approach brings more realistic videos with plausible motion and appearance consistency. Our experiments show that our model generates more plausible motion videos and also achieves significantly improved performance for unsupervised action classification in comparison to previous GAN works. In addition, because our model generates videos from two independent information, our model can generate new combinations of motion and attribute that are not seen in training data, such as a video in which a person is doing sit-up in a baseball ground.

##### Abstract (translated by Google)
学习从未标记的数据中表示和生成视频是一个非常具有挑战性的问题。为了生成逼真的视频，重要的是不仅要确保每个帧的外观是真实的，而且要确保视频运动的合理性和视频外观在时间方向上的一致性。视频生成的过程应该根据这些固有的困难来划分。在这项研究中，我们把运动和外观信息作为一个视频的两个重要的正交分量，提出了由FlowGAN和TextureGAN组成的Flow-and-Texture-Generative Adversarial Networks（FTGAN）。为了避免巨大的注释成本，我们必须探索一种从无标签数据中学习的方法。因此，我们采用光流作为运动信息来生成视频。 FlowGAN生成光流，其中只包含视频的边缘和运动。另一方面，TextureGAN专门为FlowGAN生成的光流赋予纹理。这种分层次的方法带来了更真实的视频与合理的运动和外观一致性。我们的实验表明，与以前的GAN作品相比，我们的模型生成更多合理的运动视频，并且实现了无监督动作分类的显着改进的性能。另外，由于我们的模型通过两个独立的信息生成视频，因此我们的模型可以生成在训练数据中看不到的运动和属性的新组合，例如人们在棒球场上进行仰卧起坐的视频。

##### URL
[https://arxiv.org/abs/1711.09618](https://arxiv.org/abs/1711.09618)

##### PDF
[https://arxiv.org/pdf/1711.09618](https://arxiv.org/pdf/1711.09618)

