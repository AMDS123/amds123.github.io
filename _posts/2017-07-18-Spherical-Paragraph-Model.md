---
layout: post
title: "Spherical Paragraph Model"
date: 2017-07-18 14:19:50
categories: arXiv_CL
tags: arXiv_CL Sentiment Embedding Represenation_Learning Classification Relation
author: Ruqing Zhang, Jiafeng Guo, Yanyan Lan, Jun Xu, Xueqi Cheng
mathjax: true
---

* content
{:toc}

##### Abstract
Representing texts as fixed-length vectors is central to many language processing tasks. Most traditional methods build text representations based on the simple Bag-of-Words (BoW) representation, which loses the rich semantic relations between words. Recent advances in natural language processing have shown that semantically meaningful representations of words can be efficiently acquired by distributed models, making it possible to build text representations based on a better foundation called the Bag-of-Word-Embedding (BoWE) representation. However, existing text representation methods using BoWE often lack sound probabilistic foundations or cannot well capture the semantic relatedness encoded in word vectors. To address these problems, we introduce the Spherical Paragraph Model (SPM), a probabilistic generative model based on BoWE, for text representation. SPM has good probabilistic interpretability and can fully leverage the rich semantics of words, the word co-occurrence information as well as the corpus-wide information to help the representation learning of texts. Experimental results on topical classification and sentiment analysis demonstrate that SPM can achieve new state-of-the-art performances on several benchmark datasets.

##### Abstract (translated by Google)
将文本表示为固定长度的矢量是许多语言处理任务的核心。大多数传统的方法基于简单的词袋（BoW）表示来建立文本表示，这丢失了单词之间丰富的语义关系。自然语言处理方面的最新进展表明，可以通过分布式模型有效地获取单词的语义上有意义的表示，使得可以基于名为Bag-of-Word-Embedding（BoWE）表示的更好基础来建立文本表示。然而，现有的使用BoWE的文本表示方法往往缺乏完善的概率论基础，或者无法很好地捕捉到单词向量中编码的语义相关性。为了解决这些问题，我们引入了基于BoWE的概率生成模型（Spheric Paragraph Model，SPM）来进行文本表示。 SPM具有良好的概率解释能力，能够充分利用词汇的丰富语义，词汇共现信息以及全集的信息来帮助文本的表示学习。在局部分类和情感分析上的实验结果表明SPM可以在几个基准数据集上实现新的最新的性能表现。

##### URL
[https://arxiv.org/abs/1707.05635](https://arxiv.org/abs/1707.05635)

##### PDF
[https://arxiv.org/pdf/1707.05635](https://arxiv.org/pdf/1707.05635)

