---
layout: post
title:  'Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge'
date:   2017-12-05 19:45:32
categories: arXiv_CL
arXiv_CL
author: Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg
---

* content
{:toc}

##### Abstract
While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in topic modeling, they often require detailed assumptions and careful specification of hyperparameters. Such model complexity issues only compound when trying to generalize generative models to incorporate human input. We introduce Correlation Explanation (CorEx), an alternative approach to topic modeling that does not assume an underlying generative model, and instead learns maximally informative topics through an information-theoretic framework. This framework naturally generalizes to hierarchical and semi-supervised extensions with no additional modeling assumptions. In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and representation to be promoted with minimal human intervention. Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semi-supervised variants of LDA.

##### Abstract (translated by Google)
虽然生成模型如潜在狄利克雷分配（LDA）在主题建模方面已经证明是成功的，但他们通常需要详细的假设和仔细的超参数说明。这种模型复杂性问题只是在试图推广生成模型以结合人类输入时才会复杂化。我们引入相关性解释（CorEx），这是一种不采用潜在生成模式的主题建模的替代方法，而是通过信息论框架学习最大量的信息性主题。这个框架自然地推广到分层和半监督的扩展，而没有附加的建模假设。特别是，可以通过锚词灵活地将单词级领域知识整合到CorEx中，从而以最小的人为干预促进话题分离和表示。在各种数据集，度量和实验中，我们证明CorEx生成的质量与LDA的无监督和半监督变体产生的质量相当。

##### URL
[http://arxiv.org/abs/1611.10277](http://arxiv.org/abs/1611.10277)

