---
layout: post
title: "A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation"
date: 2017-04-09 11:54:01
categories: arXiv_CL
tags: arXiv_CL Embedding RNN
author: Hong Jin Kang, Tao Chen, Muthu Kumar Chandrasekaran, Min-Yen Kan
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD - where the word senses of a word in a source language e come from a separate target translation language f - can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome.

##### Abstract (translated by Google)
词嵌入现在在自然语言处理中是无处不在的词表示形式。英语中单义词词义消歧（WSD）已经有单词嵌入的应用，但是很少进行比较。本文试图通过考察流行的嵌入式单语英语水务工作来弥合这一差距。我们简化的方法可以在不需要昂贵的再培训的情况下实现可比较的最先进的性能。跨语言WSD  - 源语言中单词的词义来自单独的目标翻译语言f  - 也可以帮助语言学习;例如，当为学习者提供目标词汇的翻译时。因此，我们也将语言嵌入应用于中文的跨语言WSD的新任务，并为进一步的基准测试提供了一个公共数据集。我们还尝试了使用LSTM网络的词嵌入，并且惊奇地发现基本的LSTM网络不能很好地工作。我们讨论这个结果的后果。

##### URL
[https://arxiv.org/abs/1611.02956](https://arxiv.org/abs/1611.02956)

##### PDF
[https://arxiv.org/pdf/1611.02956](https://arxiv.org/pdf/1611.02956)

