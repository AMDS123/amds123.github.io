---
layout: post
title: "Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering"
date: 2017-12-12 08:34:43
categories: arXiv_CV
tags: arXiv_CV QA Attention Embedding Detection VQA
author: Pan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang, Xiaogang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, the Visual Question Answering (VQA) task has gained increasing attention in artificial intelligence. Existing VQA methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering. The free-form region based and the detection-based visual attention mechanisms are mostly investigated, with the former ones attending free-form image regions and the latter ones attending pre-specified detection-box regions. We argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the VQA problem. In this paper, we propose a novel deep neural network for VQA that integrates both attention mechanisms. Our proposed framework effectively fuses features from free-form image regions, detection boxes, and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering. The proposed method is extensively evaluated on two publicly available datasets, COCO-QA and VQA, and outperforms state-of-the-art approaches. Source code is available at this https URL

##### Abstract (translated by Google)
最近，视觉问题回答（VQA）任务在人工智能中得到越来越多的关注。现有的VQA方法主要采用视觉注意机制将输入问题与相应的图像区域相关联，以便有效地回答问题。基于自由形态区域和基于检测的视觉注意机制大多被研究，前者参与自由形式图像区域，后者参与预先指定的检测框区域。我们认为这两种注意机制能够提供补充信息，并且应该有效地整合以更好地解决VQA问题。在本文中，我们提出了一种新的VQA深度神经网络，它集成了两种注意机制。我们提出的框架通过多模态乘法特征嵌入方案有效地融合了自由形式图像区域，检测框和问题表示的特征，以共同参与与问题相关的自由形式图像区域和检测框，以便更准确地回答问题。所提出的方法在两个公开可用的数据集COCO-QA和VQA上进行了广泛的评估，并且优于最先进的方法。源代码可在此https URL中找到

##### URL
[https://arxiv.org/abs/1711.06794](https://arxiv.org/abs/1711.06794)

##### PDF
[https://arxiv.org/pdf/1711.06794](https://arxiv.org/pdf/1711.06794)

