---
layout: post
title: "Residual CNDS"
date: 2016-08-07 10:34:02
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Classification
author: Hussein A. Al-Barazanchi, Hussam Qassim, Abhishek Verma
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional Neural networks nowadays are of tremendous importance for any image classification system. One of the most investigated methods to increase the accuracy of CNN is by increasing the depth of CNN. Increasing the depth by stacking more layers also increases the difficulty of training besides making it computationally expensive. Some research found that adding auxiliary forks after intermediate layers increases the accuracy. Specifying which intermediate layer shoud have the fork just addressed recently. Where a simple rule were used to detect the position of intermediate layers that needs the auxiliary supervision fork. This technique known as convolutional neural networks with deep supervision (CNDS). This technique enhanced the accuracy of classification over the straight forward CNN used on the MIT places dataset and ImageNet. In the other side, Residual Learning is another technique emerged recently to ease the training of very deep CNN. Residual Learning framwork changed the learning of layers from unreferenced functions to learning residual function with regard to the layer's input. Residual Learning achieved state of arts results on ImageNet 2015 and COCO competitions. In this paper, we study the effect of adding residual connections to CNDS network. Our experiments results show increasing of accuracy over using CNDS only.

##### Abstract (translated by Google)
当今的卷积神经网络对于任何图像分类系统都是非常重要的。提高CNN准确度的最被调查的方法之一是增加CNN的深度。通过堆叠更多层来增加深度也增加了训练的难度，除了使其在计算上昂贵。一些研究发现，在中间层之后添加辅助叉可以提高准确性。指定哪个中间层最近才有了fork。用简单的规则来检测需要辅助监视叉的中间层的位置。这种技术被称为具有深度监督的卷积神经网络（CNDS）。这种技术增强了对MIT位置数据集和ImageNet上使用的直接CNN进行分类的准确性。另一方面，残留学习是最近出现的另一种技术，可以缓解CNN的深度训练。残差学习框架将图层的学习从未被引用的功能转变为对图层输入的学习剩余功能。剩余学习在ImageNet 2015和COCO比赛中取得了艺术成果。在本文中，我们研究了添加剩余连接到CNDS网络的效果。我们的实验结果显示仅使用CNDS的精确度增加。

##### URL
[https://arxiv.org/abs/1608.02201](https://arxiv.org/abs/1608.02201)

##### PDF
[https://arxiv.org/pdf/1608.02201](https://arxiv.org/pdf/1608.02201)

