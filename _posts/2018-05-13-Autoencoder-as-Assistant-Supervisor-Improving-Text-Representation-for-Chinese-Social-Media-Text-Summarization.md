---
layout: post
title: "Autoencoder as Assistant Supervisor: Improving Text Representation for Chinese Social Media Text Summarization"
date: 2018-05-13 12:23:44
categories: arXiv_CL
tags: arXiv_CL Summarization
author: Shuming Ma, Xu Sun, Junyang Lin, Houfeng Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.

##### Abstract (translated by Google)
目前大多数抽象文本摘要模型都基于序列到序列模型（Seq2Seq）。社交媒体的来源内容冗长而嘈杂，因此Seq2Seq很难学习准确的语义表达。与源内容相比，注释摘要简短而且写得很好。此外，它与源内容具有相同的含义。在这项工作中，我们监督对源内容的表示与摘要的学习。在实现中，我们将一个自动编码器作为Seq2Seq的助理监督员。继以前的工作之后，我们在流行的中国社交媒体数据集上评估我们的模型。实验结果表明，我们的模型在基准数据集上实现了最先进的性能。

##### URL
[https://arxiv.org/abs/1805.04869](https://arxiv.org/abs/1805.04869)

##### PDF
[https://arxiv.org/pdf/1805.04869](https://arxiv.org/pdf/1805.04869)

