---
layout: post
title: "Long short-term memory networks in memristor crossbars"
date: 2018-05-30 04:25:32
categories: arXiv_CV
tags: arXiv_CV Inference RNN Classification Memory_Networks
author: Can Li, Zhongrui Wang, Mingyi Rao, Daniel Belkin, Wenhao Song, Hao Jiang, Peng Yan, Yunning Li, Peng Lin, Miao Hu, Ning Ge, John Paul Strachan, Mark Barnell, Qing Wu, R. Stanley Williams, J. Joshua Yang, Qiangfei Xia
mathjax: true
---

* content
{:toc}

##### Abstract
Recent breakthroughs in recurrent deep neural networks with long short-term memory (LSTM) units has led to major advances in artificial intelligence. State-of-the-art LSTM models with significantly increased complexity and a large number of parameters, however, have a bottleneck in computing power resulting from limited memory capacity and data communication bandwidth. Here we demonstrate experimentally that LSTM can be implemented with a memristor crossbar, which has a small circuit footprint to store a large number of parameters and in-memory computing capability that circumvents the 'von Neumann bottleneck'. We illustrate the capability of our system by solving real-world problems in regression and classification, which shows that memristor LSTM is a promising low-power and low-latency hardware platform for edge inference.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1805.11801](https://arxiv.org/abs/1805.11801)

##### PDF
[https://arxiv.org/pdf/1805.11801](https://arxiv.org/pdf/1805.11801)

