---
layout: post
title: "Long short-term memory networks in memristor crossbars"
date: 2018-05-30 04:25:32
categories: arXiv_CV
tags: arXiv_CV Inference RNN Classification Memory_Networks
author: Can Li, Zhongrui Wang, Mingyi Rao, Daniel Belkin, Wenhao Song, Hao Jiang, Peng Yan, Yunning Li, Peng Lin, Miao Hu, Ning Ge, John Paul Strachan, Mark Barnell, Qing Wu, R. Stanley Williams, J. Joshua Yang, Qiangfei Xia
mathjax: true
---

* content
{:toc}

##### Abstract
Recent breakthroughs in recurrent deep neural networks with long short-term memory (LSTM) units has led to major advances in artificial intelligence. State-of-the-art LSTM models with significantly increased complexity and a large number of parameters, however, have a bottleneck in computing power resulting from limited memory capacity and data communication bandwidth. Here we demonstrate experimentally that LSTM can be implemented with a memristor crossbar, which has a small circuit footprint to store a large number of parameters and in-memory computing capability that circumvents the 'von Neumann bottleneck'. We illustrate the capability of our system by solving real-world problems in regression and classification, which shows that memristor LSTM is a promising low-power and low-latency hardware platform for edge inference.

##### Abstract (translated by Google)
最近在具有长期短期记忆（LSTM）单元的经常性深度神经网络方面的突破导致了人工智能的重大进步。但是，具有显着增加的复杂性和大量参数的最先进的LSTM模型由于有限的存储容量和数据通信带宽而具有计算能力的瓶颈。在这里，我们通过实验证明LSTM可以通过忆阻器交叉开关实现，该交叉开关具有很小的电路占位面积来存储大量的参数和内存计算能力，以避免'冯诺依曼瓶颈'。我们通过解决回归和分类中的实际问题来说明我们的系统的能力，这表明忆阻器LSTM是一种有前途的低功耗和低延迟的边缘推断硬件平台。

##### URL
[https://arxiv.org/abs/1805.11801](https://arxiv.org/abs/1805.11801)

##### PDF
[https://arxiv.org/pdf/1805.11801](https://arxiv.org/pdf/1805.11801)

