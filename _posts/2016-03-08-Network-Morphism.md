---
layout: post
title: "Network Morphism"
date: 2016-03-08 16:36:00
categories: arXiv_CV
tags: arXiv_CV Knowledge CNN
author: Tao Wei, Changhu Wang, Yong Rui, Chang Wen Chen
mathjax: true
---

* content
{:toc}

##### Abstract
We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as \emph{network morphism} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.

##### Abstract (translated by Google)
我们在本文中提出了一个系统的研究，如何将训练有素的神经网络变形为一个新的网络，使其网络功能可以完全保留。在这项研究中，我们将其定义为\ emph {network morphism}。在父网络变形之后，子网络有望从其母网络继承知识，并有可能继续发展成为一个功能更强大的网络，并缩短了训练时间。这种网络态射的第一个要求是能够处理不同类型的网络，包括深度，宽度，内核大小甚至子网的变化。为了满足这个要求，我们首先引入网络态射方程，然后为经典卷积神经网络和卷积神经网络开发所有这些变体类型的变形算法。这种网络态射的第二个要求是它能够处理网络中的非线性。我们提出了一系列参数激活函数来促进任何连续的非线性激活神经元的变形。基准数据集和典型神经网络的实验结果证明了所提出的网络态射方案的有效性。

##### URL
[https://arxiv.org/abs/1603.01670](https://arxiv.org/abs/1603.01670)

##### PDF
[https://arxiv.org/pdf/1603.01670](https://arxiv.org/pdf/1603.01670)

