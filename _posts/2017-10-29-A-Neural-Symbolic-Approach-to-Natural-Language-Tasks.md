---
layout: post
title: "A Neural-Symbolic Approach to Natural Language Tasks"
date: 2017-10-29 09:18:51
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption RNN Deep_Learning
author: Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, Dapeng Wu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning (DL) has in recent years been widely used in natural language processing (NLP) applications due to its superior performance. However, while natural languages are rich in grammatical structure, DL has not been able to explicitly represent and enforce such structures. This paper proposes a new architecture to bridge this gap by exploiting tensor product representations (TPR), a structured neural-symbolic framework developed in cognitive science over the past 20 years, with the aim of integrating DL with explicit language structures and rules. We call it the Tensor Product Generation Network (TPGN), and apply it to 1) image captioning, 2) classification of the part of speech of a word, and 3) identification of the phrase structure of a sentence. The key ideas of TPGN are: 1) unsupervised learning of role-unbinding vectors of words via a TPR-based deep neural network, and 2) integration of TPR with typical DL architectures including Long Short-Term Memory (LSTM) models. The novelty of our approach lies in its ability to generate a sentence and extract partial grammatical structure of the sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. Experimental results demonstrate the effectiveness of the proposed approach.

##### Abstract (translated by Google)
深度学习（DL）由于其优越的性能，近年来已被广泛用于自然语言处理（NLP）应用。然而，虽然自然语言在语法结构上很丰富，但是DL没有能够明确表示和强制执行这样的结构。本文提出了一种新的架构，通过利用张量积表示（TPR）这一结构化的神经 - 符号框架，在认知科学中发展了20年，旨在将DL与明确的语言结构和规则结合起来。我们称之为张量产品生成网（TPGN），并将其应用于1）图像字幕，2）一个词的词性分类，以及3）一个句子的词组结构的识别。 TPGN的主要思想是：1）通过基于TPR的深度神经网络无监督地学习单词的角色解绑向量; 2）将TPR与典型的DL体系结构（包括长期短期记忆（LSTM）模型）相集成。我们的方法的新颖之处在于它能够生成一个句子，并通过使用角色非绑定向量来提取句子的部分语法结构，这是以无监督的方式获得的。实验结果证明了该方法的有效性。

##### URL
[https://arxiv.org/abs/1710.11475](https://arxiv.org/abs/1710.11475)

##### PDF
[https://arxiv.org/pdf/1710.11475](https://arxiv.org/pdf/1710.11475)

