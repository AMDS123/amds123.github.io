---
layout: post
title: "A Neural-Symbolic Approach to Natural Language Tasks"
date: 2017-10-29 09:18:51
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption RNN Classification Deep_Learning
author: Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, Dapeng Wu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning (DL) has in recent years been widely used in natural language processing (NLP) applications due to its superior performance. However, while natural languages are rich in grammatical structure, DL has not been able to explicitly represent and enforce such structures. This paper proposes a new architecture to bridge this gap by exploiting tensor product representations (TPR), a structured neural-symbolic framework developed in cognitive science over the past 20 years, with the aim of integrating DL with explicit language structures and rules. We call it the Tensor Product Generation Network (TPGN), and apply it to 1) image captioning, 2) classification of the part of speech of a word, and 3) identification of the phrase structure of a sentence. The key ideas of TPGN are: 1) unsupervised learning of role-unbinding vectors of words via a TPR-based deep neural network, and 2) integration of TPR with typical DL architectures including Long Short-Term Memory (LSTM) models. The novelty of our approach lies in its ability to generate a sentence and extract partial grammatical structure of the sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. Experimental results demonstrate the effectiveness of the proposed approach.

##### Abstract (translated by Google)
近年来，深度学习（DL）由于其优越的性能而被广泛用于自然语言处理（NLP）应用中。然而，虽然自然语言具有丰富的语法结构，但DL无法明确地表示和实施这样的结构。本文提出了一种新的架构，通过利用张量产品表示（TPR）来弥补这一差距，这是一种在过去20年中在认知科学中发展起来的结构化神经 - 象征框架，目的是将DL与明确的语言结构和规则相结合。我们将其称为张量产品生成网络（TPGN），并将其应用于1）图像字幕，2）单词的词性分类，以及3）句子的短语结构的识别。 TPGN的关键思想是：1）通过基于TPR的深度神经网络对角色解除单词向量的无监督学习，以及2）TPR与包括长短期记忆（LSTM）模型的典型DL架构的集成。我们的方法的新颖性在于它能够通过使用以无人监督的方式获得的角色解除向量来生成句子并提取句子的部分语法结构。实验结果证明了该方法的有效性。

##### URL
[https://arxiv.org/abs/1710.11475](https://arxiv.org/abs/1710.11475)

##### PDF
[https://arxiv.org/pdf/1710.11475](https://arxiv.org/pdf/1710.11475)

