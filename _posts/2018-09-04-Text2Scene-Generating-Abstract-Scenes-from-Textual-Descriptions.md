---
layout: post
title: "Text2Scene: Generating Abstract Scenes from Textual Descriptions"
date: 2018-09-04 17:31:13
categories: arXiv_CV
tags: arXiv_CV Regularization
author: Fuwen Tan, Song Feng, Vicente Ordonez
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose an end-to-end model that learns to interpret natural language describing a scene to generate an abstract pictorial representation. The pictorial representations generated by our model comprise the spatial distribution and attributes of the objects in the described scene. Our model uses a sequence-to-sequence network with a double attentive mechanism and introduces a regularization strategy. These scene representations can be sampled from our model similarly as in language-generation models. We show that the proposed model, initially designed to handle the generation of cartoon-like pictorial representations in the Abstract Scenes Dataset, can also handle, under minimal modifications, the generation of semantic layouts corresponding to real images in the COCO dataset. Human evaluations using a visual entailment task show that pictorial representations generated with our full model can entail at least one out of three input visual descriptions 94% of the times, and at least two out of three 62% of the times for each image.

##### Abstract (translated by Google)
在本文中，我们提出了一种端到端模型，该模型学习解释描述场景的自然语言以生成抽象的图形表示。由我们的模型生成的图形表示包括所描述场景中的对象的空间分布和属性。我们的模型使用具有双重注意机制的序列到序列网络，并引入正则化策略。这些场景表示可以从我们的模型中采样，类似于语言生成模型。我们表明，最初设计用于处理抽象场景数据集中卡通式图形表示的生成的所提出的模型还可以在最小的修改下处理与COCO数据集中的真实图像相对应的语义布局的生成。使用视觉蕴涵任务的人工评估表明，使用我们的完整模型生成的图形表示可以在94％的时间内至少有三分之一的输入视觉描述，并且每个图像的62％的时间中至少有三分之二。

##### URL
[http://arxiv.org/abs/1809.01110](http://arxiv.org/abs/1809.01110)

##### PDF
[http://arxiv.org/pdf/1809.01110](http://arxiv.org/pdf/1809.01110)

