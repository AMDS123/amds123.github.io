---
layout: post
title: "The Curious Robot: Learning Visual Representations via Physical Interactions"
date: 2016-07-26 03:30:44
categories: arXiv_CV
tags: arXiv_CV Image_Classification Represenation_Learning Classification Quantitative
author: Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, Abhinav Gupta
mathjax: true
---

* content
{:toc}

##### Abstract
What is the right supervisory signal to train visual representations? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets. However, in case of biological agents, visual representation learning does not require millions of semantic labels. We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). For example, babies push objects, poke them, put them in their mouth and throw them to learn representations. Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and observes objects in a tabletop environment. It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Quantitatively, we evaluate our learned ConvNet on image classification tasks and show improvements compared to learning without external data. Finally, on the task of instance retrieval, our network outperforms the ImageNet network on recall@1 by 3%

##### Abstract (translated by Google)
培训视觉表现的正确的监督信号是什么？目前的计算机视觉方法使用数据集（如ImageNet）的类别标签来训练ConvNets。然而，在生物制剂的情况下，视觉表示学习不需要数百万个语义标签。我们认为，生物制剂使用与世界的物理交互来学习视觉表示，而不像当前使用被动观察（从网络下载的图像和视频）的视觉系统。例如，婴儿推物体，戳他们，把他们放在嘴里，扔他们学习交涉。为了实现这一目标，我们在百特平台上构建了第一个系统之一，该平台在桌面环境中推动，探测，抓取和观察物体。它使用四种不同类型的物理交互来收集超过130K的数据点，每个数据点提供对共享ConvNet架构的监督，使我们能够学习视觉表示。我们通过观察神经元激活并在这个学习的表示上执行最近的邻居检索来显示学习表示的质量。在数量上，我们评估我们学习的ConvNet的图像分类任务，并显示相比，没有外部数据学习的改善。最后，在实例检索的任务中，我们的网络在召回@ 1上的性能优于ImageNet网络的3％

##### URL
[https://arxiv.org/abs/1604.01360](https://arxiv.org/abs/1604.01360)

##### PDF
[https://arxiv.org/pdf/1604.01360](https://arxiv.org/pdf/1604.01360)

