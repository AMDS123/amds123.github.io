---
layout: post
title: "One Model to Learn Both: Zero Pronoun Prediction and Translation"
date: 2019-09-01 10:07:20
categories: arXiv_CL
tags: arXiv_CL Prediction
author: Longyue Wang, Zhaopeng Tu, Xing Wang, Shuming Shi
mathjax: true
---

* content
{:toc}

##### Abstract
Zero pronouns (ZPs) are frequently omitted in pro-drop languages, but should be recalled in non-pro-drop languages. This discourse phenomenon poses a significant challenge for machine translation (MT) when translating texts from pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware ZP translation approach for neural MT models. Specifically, we jointly learn to predict and translate ZPs in an end-to-end manner, allowing both components to interact with each other. In addition, we employ hierarchical neural networks to exploit discourse-level context, which is beneficial for ZP prediction and thus translation. Experimental results on both Chinese-English and Japanese-English data show that our approach significantly and accumulatively improves both translation performance and ZP prediction accuracy over not only baseline but also previous works using external ZP prediction models. Extensive analyses confirm that the performance improvement comes from the alleviation of different kinds of errors especially caused by subjective ZPs.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1909.00369](http://arxiv.org/abs/1909.00369)

##### PDF
[http://arxiv.org/pdf/1909.00369](http://arxiv.org/pdf/1909.00369)

