---
layout: post
title: "A Teacher-Student Framework for Zero-Resource Neural Machine Translation"
date: 2017-05-02 01:14:06
categories: arXiv_CL
tags: arXiv_CL NMT
author: Yun Chen, Yang Liu, Yong Cheng, Victor O.K. Li
mathjax: true
---

* content
{:toc}

##### Abstract
While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model ("student") without parallel corpora available, guided by an existing pivot-to-target NMT model ("teacher") on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.

##### Abstract (translated by Google)
虽然端到端神经机器翻译（NMT）近来取得了令人瞩目的进展，但对于低资源语言对和领域仍然存在数据稀缺的问题。在本文中，我们提出一种零资源NMT的方法，假设并行句子具有以第三语言生成句子的接近概率。基于这个假设，我们的方法能够训练源到目标的NMT模型（“学生”），没有可用的平行语料库，由源 - 枢轴上的现有的枢转到目标NMT模型（“教师”）平行语料库。实验结果表明，所提出的方法在基线枢轴模型上显着提高了不同语言对的+3.0 BLEU点。

##### URL
[https://arxiv.org/abs/1705.00753](https://arxiv.org/abs/1705.00753)

##### PDF
[https://arxiv.org/pdf/1705.00753](https://arxiv.org/pdf/1705.00753)

