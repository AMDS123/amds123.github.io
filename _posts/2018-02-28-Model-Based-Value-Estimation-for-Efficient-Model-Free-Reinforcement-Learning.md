---
layout: post
title: "Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning"
date: 2018-02-28 21:43:37
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, Sergey Levine
mathjax: true
---

* content
{:toc}

##### Abstract
Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.

##### Abstract (translated by Google)
最近的无模型强化学习算法已经提出，将学习动力学模型作为附加数据的来源，旨在降低样本的复杂性。这样的方法有望结合想象数据和模型不确定性的概念来加速对连续控制任务的学习。不幸的是，他们依赖限制动态模型使用的启发式方法。我们提出了基于模型的价值扩张，它通过只允许想象力固定深度来控制模型中的不确定性。通过在无模型强化学习算法中实现更广泛的学习动力学模型的使用，我们改进了估计值，从而降低了学习的样本复杂度。

##### URL
[http://arxiv.org/abs/1803.00101](http://arxiv.org/abs/1803.00101)

##### PDF
[http://arxiv.org/pdf/1803.00101](http://arxiv.org/pdf/1803.00101)

