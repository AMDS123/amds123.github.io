---
layout: post
title: "Unsupervised Neural Text Simplification"
date: 2018-10-18 07:43:12
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention Quantitative
author: Sai Surya, Abhijit Mishra, Anirban Laha, Parag Jain, Karthik Sankaranarayanan
mathjax: true
---

* content
{:toc}

##### Abstract
The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. The core framework is comprised of a shared encoder and a pair of attentional-decoders that gains knowledge of both text simplification and complexification through discriminator-based-losses, back-translation and denoising. The framework is trained using unlabeled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on a public test data shows the efficacy of our model to perform simplification at both lexical and syntactic levels, competitive to existing supervised methods. We open source our implementation for academic use.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.07931](http://arxiv.org/abs/1810.07931)

##### PDF
[http://arxiv.org/pdf/1810.07931](http://arxiv.org/pdf/1810.07931)

