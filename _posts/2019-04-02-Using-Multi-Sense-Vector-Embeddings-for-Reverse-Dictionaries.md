---
layout: post
title: "Using Multi-Sense Vector Embeddings for Reverse Dictionaries"
date: 2019-04-02 14:17:19
categories: arXiv_CL
tags: arXiv_CL Attention Embedding Language_Model
author: Michael A. Hedderich, Andrew Yates, Dietrich Klakow, Gerard de Melo
mathjax: true
---

* content
{:toc}

##### Abstract
Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.01451](http://arxiv.org/abs/1904.01451)

##### PDF
[http://arxiv.org/pdf/1904.01451](http://arxiv.org/pdf/1904.01451)

