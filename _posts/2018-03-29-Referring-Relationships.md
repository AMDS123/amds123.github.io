---
layout: post
title: "Referring Relationships"
date: 2018-03-29 05:37:25
categories: arXiv_CV
tags: arXiv_CV Attention Relation
author: Ranjay Krishna, Ines Chami, Michael Bernstein, Li Fei-Fei
mathjax: true
---

* content
{:toc}

##### Abstract
Images are not simply sets of objects: each image represents a web of interconnected relationships. These relationships between entities carry semantic meaning and help a viewer differentiate between instances of an entity. For example, in an image of a soccer match, there may be multiple persons present, but each participates in different relationships: one is kicking the ball, and the other is guarding the goal. In this paper, we formulate the task of utilizing these "referring relationships" to disambiguate between entities of the same category. We introduce an iterative model that localizes the two entities in the referring relationship, conditioned on one another. We formulate the cyclic condition between the entities in a relationship by modelling predicates that connect the entities as shifts in attention from one entity to another. We demonstrate that our model can not only outperform existing approaches on three datasets --- CLEVR, VRD and Visual Genome --- but also that it produces visually meaningful predicate shifts, as an instance of interpretable neural networks. Finally, we show that by modelling predicates as attention shifts, we can even localize entities in the absence of their category, allowing our model to find completely unseen categories.

##### Abstract (translated by Google)
图像不是简单的对象集合：每个图像代表一个互相关联的网络。实体之间的这些关系具有语义意义，并​​帮助观察者区分实体的实例。例如，在足球比赛的图像中，可能有多人在场，但每个人都参与不同的关系：一个是踢球，另一个是守门。在本文中，我们制定了利用这些“引用关系”来消除同一类别实体之间的歧义的任务。我们引入了一种迭代模型，它将引用关系中的两个实体本地化，并相互制约。我们通过建模谓词来建立关系中实体之间的循环条件，这些谓词将实体连接起来，将注意力从一个实体转移到另一个实体。我们证明，我们的模型不仅可以胜过三种数据集上的现有方法--- CLEVR，VRD和Visual Genome ---而且它还可以产生视觉上有意义的谓词变换，作为可解释神经网络的一个实例。最后，我们表明，通过将谓词建模为注意力转移，我们甚至可以在没有其类别的情况下本地化实体，从而使我们的模型找到完全看不见的类别。

##### URL
[http://arxiv.org/abs/1803.10362](http://arxiv.org/abs/1803.10362)

##### PDF
[http://arxiv.org/pdf/1803.10362](http://arxiv.org/pdf/1803.10362)

