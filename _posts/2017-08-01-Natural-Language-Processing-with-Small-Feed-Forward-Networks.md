---
layout: post
title: "Natural Language Processing with Small Feed-Forward Networks"
date: 2017-08-01 09:13:44
categories: arXiv_CL
tags: arXiv_CL
author: Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, Slav Petrov
mathjax: true
---

* content
{:toc}

##### Abstract
We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.

##### Abstract (translated by Google)
我们表明，小而浅的前馈神经网络可以在一系列非结构化和结构化的语言处理任务上达到接近最先进的结果，同时在内存和计算要求方面要比深回归模型便宜得多。受移动电话等资源受限环境的驱动，我们展示了获取这种小型神经网络模型的简单技术，并在决定如何分配小内存预算时研究不同的折衷。

##### URL
[https://arxiv.org/abs/1708.00214](https://arxiv.org/abs/1708.00214)

##### PDF
[https://arxiv.org/pdf/1708.00214](https://arxiv.org/pdf/1708.00214)

