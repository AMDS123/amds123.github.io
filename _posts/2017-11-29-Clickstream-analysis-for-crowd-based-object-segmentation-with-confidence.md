---
layout: post
title: "Clickstream analysis for crowd-based object segmentation with confidence"
date: 2017-11-29 13:15:26
categories: arXiv_CV
tags: arXiv_CV Knowledge Segmentation
author: Eric Heim, Alexander Seitel, Jonas Andrulis, Fabian Isensee, Christian Stock, Tobias Ross, Lena Maier-Hein
mathjax: true
---

* content
{:toc}

##### Abstract
With the rapidly increasing interest in machine learning based solutions for automatic image annotation, the availability of reference annotations for algorithm training is one of the major bottlenecks in the field. Crowdsourcing has evolved as a valuable option for low-cost and large-scale data annotation; however, quality control remains a major issue which needs to be addressed. To our knowledge, we are the first to analyze the annotation process to improve crowd-sourced image segmentation. Our method involves training a regressor to estimate the quality of a segmentation from the annotator's clickstream data. The quality estimation can be used to identify spam and weight individual annotations by their (estimated) quality when merging multiple segmentations of one image. Using a total of 29,000 crowd annotations performed on publicly available data of different object classes, we show that (1) our method is highly accurate in estimating the segmentation quality based on clickstream data, (2) outperforms state-of-the-art methods for merging multiple annotations. As the regressor does not need to be trained on the object class that it is applied to it can be regarded as a low-cost option for quality control and confidence analysis in the context of crowd-based image annotation.

##### Abstract (translated by Google)
随着对基于机器学习的自动图像标注解决方案的兴趣日益增加，算法训练的参考标注的可用性成为该领域的主要瓶颈之一。众包已经演变为低成本和大规模数据注释的有价值的选择;但是，质量控制仍然是一个需要解决的重大问题。就我们所知，我们是第一个分析注释过程以改善众包的图像分割。我们的方法涉及训练回归者从注释者的点击流数据中估计分割的质量。质量估计可用于在合并一个图像的多个分段时通过其（估计）质量来识别垃圾邮件和加权单个注释。 （1）我们的方法在估计基于点击流数据的分割质量方面是高度精确的，（2）优于最先进的方法合并多个注释。由于回归器不需要对其应用的对象类进行训练，因此可以将其视为在基于人群的图像注释的情况下用于质量控制和置信度分析的低成本选项。

##### URL
[https://arxiv.org/abs/1611.08527](https://arxiv.org/abs/1611.08527)

##### PDF
[https://arxiv.org/pdf/1611.08527](https://arxiv.org/pdf/1611.08527)

