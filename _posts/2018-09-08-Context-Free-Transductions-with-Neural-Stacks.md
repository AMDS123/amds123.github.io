---
layout: post
title: "Context-Free Transductions with Neural Stacks"
date: 2018-09-08 17:04:53
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Yiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz, Simon Mendelsohn
mathjax: true
---

* content
{:toc}

##### Abstract
This paper analyzes the behavior of stack-augmented recurrent neural network (RNN) models. Due to the architectural similarity between stack RNNs and pushdown transducers, we train stack RNN models on a number of tasks, including string reversal, context-free language modelling, and cumulative XOR evaluation. Examining the behavior of our networks, we show that stack-augmented RNNs can discover intuitive stack-based strategies for solving our tasks. However, stack RNNs are more difficult to train than classical architectures such as LSTMs. Rather than employ stack-based strategies, more complex networks often find approximate solutions by using the stack as unstructured memory.

##### Abstract (translated by Google)
本文分析了叠加增强型递归神经网络（RNN）模型的行为。由于堆栈RNN和下推传感器之间的架构相似性，我们在许多任务上训练堆栈RNN模型，包括字符串反转，无上下文语言建模和累积XOR评估。通过检查我们网络的行为，我们发现堆栈扩充的RNN可以发现直观的基于堆栈的策略来解决我们的任务。然而，堆栈RNN比诸如LSTM的传统架构更难训练。不是采用基于堆栈的策略，更复杂的网络通常通过将堆栈用作非结构化存储器来找到近似解决方案。

##### URL
[http://arxiv.org/abs/1809.02836](http://arxiv.org/abs/1809.02836)

##### PDF
[http://arxiv.org/pdf/1809.02836](http://arxiv.org/pdf/1809.02836)

