---
layout: post
title: "Can Neural Networks Understand Logical Entailment?"
date: 2018-02-23 14:04:30
categories: arXiv_AI
tags: arXiv_AI CNN RNN Prediction
author: Richard Evans, David Saxton, David Amos, Pushmeet Kohli, Edward Grefenstette
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a "convolution over possible worlds". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.

##### Abstract (translated by Google)
我们引入了一个新的逻辑蕴涵数据集，目的是测量模型捕获和利用逻辑表达式的结构来对付蕴含预测任务的能力。我们使用这个任务来比较一系列在序列处理文献中无处不在的体系结构，以及一个新的模型类--- PossibleWorldNets ---它将蕴含计算为“对可能世界的卷积”。结果表明，卷积网络相对于LSTM RNNs存在着这类问题的错误归纳偏差，由于树形结构神经网络利用逻辑语法的能力得到增强，所以其性能优于LSTM RNNs，而PossibleWorldNets优于所有基准。

##### URL
[http://arxiv.org/abs/1802.08535](http://arxiv.org/abs/1802.08535)

##### PDF
[http://arxiv.org/pdf/1802.08535](http://arxiv.org/pdf/1802.08535)

