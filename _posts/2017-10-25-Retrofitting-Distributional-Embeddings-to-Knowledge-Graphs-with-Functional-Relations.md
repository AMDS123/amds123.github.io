---
layout: post
title: "Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations"
date: 2017-10-25 18:13:26
categories: arXiv_CL
tags: arXiv_CL Knowledge_Graph Knowledge Embedding Relation
author: Benjamin J. Lengerich, Andrew L. Maas, Christopher Potts
mathjax: true
---

* content
{:toc}

##### Abstract
Knowledge graphs are a versatile framework to encode richly structured data relationships, but it not always apparent how to combine these with existing entity representations. Methods for retrofitting pre-trained entity representations to the structure of a knowledge graph typically assume that entities are embedded in a connected space and that relations imply similarity. However, useful knowledge graphs often contain diverse entities and relations (with potentially disjoint underlying corpora) which do not accord with these assumptions. To overcome these limitations, we present Functional Retrofitting, a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations. Our framework can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion. We present both linear and neural instantiations of the framework. Functional Retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs (in which relations do imply similarity). Finally, we demonstrate the utility of the framework by predicting new drug--disease treatment pairs in a large, complex health knowledge graph.

##### Abstract (translated by Google)
知识图是编码丰富结构化数据关系的多功能框架，但是如何将这些与现有的实体表示法结合起来并不总是显而易见的。用于将预先训练的实体表示改造成知识图的结构的方法通常假定实体嵌入在连接的空间中，并且该关系意味着相似性。然而，有用的知识图通常包含不同的实体和关系（与潜在不相交的基础语料库），这不符合这些假设。为了克服这些限制，我们提出功能改进，一个框架，通过显式建模配对关系来推广当前的改造方法。我们的框架可以直接包含以前为知识图完成而开发的各种成对惩罚函数。我们提出框架的线性和神经实例。功能改进显着地优于现有的复杂知识图的改进方法，并且在简单的图（其中关系确实意味着相似性）上失去了准确性。最后，我们通过在一个庞大的，复杂的健康知识图中预测新的药物 - 疾病治疗对来证明该框架的实用性。

##### URL
[https://arxiv.org/abs/1708.00112](https://arxiv.org/abs/1708.00112)

##### PDF
[https://arxiv.org/pdf/1708.00112](https://arxiv.org/pdf/1708.00112)

