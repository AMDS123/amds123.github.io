---
layout: post
title: "Deep Learning for Saliency Prediction in Natural Video"
date: 2016-04-27 10:34:21
categories: arXiv_CV
tags: arXiv_CV Salient Attention Deep_Learning Prediction Detection
author: Souad Chaabouni, Jenny Benois-Pineau, Ofer Hadar, Chokri Ben Amar
mathjax: true
---

* content
{:toc}

##### Abstract
The purpose of this paper is the detection of salient areas in natural video by using the new deep learning techniques. Salient patches in video frames are predicted first. Then the predicted visual fixation maps are built upon them. We design the deep architecture on the basis of CaffeNet implemented with Caffe toolkit. We show that changing the way of data selection for optimisation of network parameters, we can save computation cost up to 12 times. We extend deep learning approaches for saliency prediction in still images with RGB values to specificity of video using the sensitivity of the human visual system to residual motion. Furthermore, we complete primary colour pixel values by contrast features proposed in classical visual attention prediction models. The experiments are conducted on two publicly available datasets. The first is IRCCYN video database containing 31 videos with an overall amount of 7300 frames and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided 2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the accuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of saliency of patches show the improvement up to 2% with regard to RGB use only. The resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of predicted saliency maps with visual fixation maps shows the increase up to 16% on a sample of video clips from this dataset.

##### Abstract (translated by Google)
本文的目的是利用新的深度学习技术来检测自然视频中的显着区域。首先预测视频帧中的显着色块。然后预测的视觉注视图建立在他们身上。我们在Caffe工具包实现的CaffeNet的基础上设计深层架构。我们表明，改变数据选择的方式来优化网络参数，我们可以节省高达12倍的计算成本。我们使用人类视觉系统对残余运动的敏感性，将具有RGB值的静止图像中的显着性预测的深度学习方法扩展到视频的特异性。此外，我们通过在经典的视觉关注预测模型中提出的对比特征来完成原色像素值。实验在两个公开可用的数据集上进行。首先是IRCCYN视频数据库，包含31个视频总数为7300帧，37个视角的视频。第二个是HOLLYWOOD2提供了2517个电影剪辑，19个科目的注视。在IRCYYN数据集上，得到的准确率为89.51％。在HOLLYWOOD2数据集上，贴片显着性的预测结果显示，仅在RGB使用方面，提高了2％。得到的精度为76％，6％。在预测显着性图与视觉固定图的比较中的AUC度量显示来自该数据集的视频剪辑样本的增加高达16％。

##### URL
[https://arxiv.org/abs/1604.08010](https://arxiv.org/abs/1604.08010)

##### PDF
[https://arxiv.org/pdf/1604.08010](https://arxiv.org/pdf/1604.08010)

