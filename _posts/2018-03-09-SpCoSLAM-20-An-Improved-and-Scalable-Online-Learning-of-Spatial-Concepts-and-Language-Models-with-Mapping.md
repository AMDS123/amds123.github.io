---
layout: post
title: "SpCoSLAM 2.0: An Improved and Scalable Online Learning of Spatial Concepts and Language Models with Mapping"
date: 2018-03-09 12:06:04
categories: arXiv_AI
tags: arXiv_AI Language_Model SLAM
author: Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi, Tetsunari Inamura
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we propose a novel online learning algorithm, SpCoSLAM 2.0 for spatial concepts and lexical acquisition with higher accuracy and scalability. In previous work, we proposed SpCoSLAM as an online learning algorithm based on the Rao--Blackwellized particle filter. However, this conventional algorithm had problems such as the decrease of the estimation accuracy due to the influence of the early stages of learning as well as the increase of the computational complexity with the increase of the training data. Therefore, we first develop an improved algorithm by introducing new techniques such as rejuvenation. Next, we develop a scalable algorithm to reduce the calculation time while maintaining a higher accuracy than the conventional algorithm. In the experiment, we evaluate and compare the estimation accuracy and calculation time of the proposed algorithm, conventional online algorithm, and batch learning. The experimental results demonstrate that the proposed algorithm not only exceeds the accuracy of the conventional algorithm but also capable of achieving an accuracy comparable to that of batch learning. In addition, the proposed algorithm showed that the calculation time does not depend on the amount of training data and becomes constant for each step with the scalable algorithm.

##### Abstract (translated by Google)
在本文中，我们提出了一种新的在线学习算法SpCoSLAM 2.0，用于空间概念和词法采集，具有更高的准确性和可扩展性。在以前的工作中，我们提出了SpCoSLAM作为基于Rao  -  Blackwellized粒子滤波器的在线学习算法。然而，这种传统的算法存在诸如由于学习的早期阶段的影响而导致估计精度降低以及随着训练数据的增加而计算复杂度增加的问题。因此，我们首先通过引入新技术（例如复兴）来开发改进的算法。接下来，我们开发了一种可扩展的算法，以减少计算时间，同时保持比传统算法更高的精度。在实验中，我们评估和比较了所提出算法，常规在线算法和批量学习的估计精度和计算时间。实验结果表明，该算法不仅能够超越传统算法的精度，而且能够达到与批量学习相当的精度。另外，所提出的算法表明，计算时间不依赖于训练数据量，并且对于可伸缩算法的每个步骤而言变得恒定。

##### URL
[http://arxiv.org/abs/1803.03481](http://arxiv.org/abs/1803.03481)

##### PDF
[http://arxiv.org/pdf/1803.03481](http://arxiv.org/pdf/1803.03481)

