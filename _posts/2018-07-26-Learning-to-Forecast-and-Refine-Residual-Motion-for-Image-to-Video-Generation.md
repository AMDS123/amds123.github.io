---
layout: post
title: "Learning to Forecast and Refine Residual Motion for Image-to-Video Generation"
date: 2018-07-26 04:42:58
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, Dimitris Metaxas
mathjax: true
---

* content
{:toc}

##### Abstract
We consider the problem of image-to-video translation, where an input image is translated into an output video containing motions of a single object. Recent methods for such problems typically train transformation networks to generate future frames conditioned on the structure sequence. Parallel work has shown that short high-quality motions can be generated by spatiotemporal generative networks that leverage temporal knowledge from the training data. We combine the benefits of both approaches and propose a two-stage generation framework where videos are generated from structures and then refined by temporal signals. To model motions more efficiently, we train networks to learn residual motion between the current and future frames, which avoids learning motion-irrelevant details. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state-of-the-art methods on both tasks demonstrate the effectiveness of our approach.

##### Abstract (translated by Google)
我们考虑图像到视频转换的问题，其中输入图像被转换成包含单个对象的运动的输出视频。用于此类问题的最新方法通常训练变换网络以生成以结构序列为条件的未来帧。并行工作表明，利用训练数据中的时间知识，时空生成网络可以产生短的高质量运动。我们结合了两种方法的优点，并提出了一个两阶段生成框架，其中视频由结构生成，然后通过时间信号进行细化。为了更有效地模拟运动，我们训练网络以学习当前帧和未来帧之间的残余运动，这避免学习与运动无关的细节。我们对两个图像到视频翻译任务进行了大量实验：面部表情重定向和人体姿势预测。在这两项任务中，最先进的方法取得了卓越的成果，证明了我们的方法的有效性。

##### URL
[http://arxiv.org/abs/1807.09951](http://arxiv.org/abs/1807.09951)

##### PDF
[http://arxiv.org/pdf/1807.09951](http://arxiv.org/pdf/1807.09951)

