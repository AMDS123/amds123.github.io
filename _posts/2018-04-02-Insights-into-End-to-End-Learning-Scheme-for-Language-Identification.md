---
layout: post
title: "Insights into End-to-End Learning Scheme for Language Identification"
date: 2018-04-02 03:19:44
categories: arXiv_SD
tags: arXiv_SD
author: Weicheng Cai, Zexin Cai, Wenbo Liu, Xiaoqi Wang, Ming Li
mathjax: true
---

* content
{:toc}

##### Abstract
A novel interpretable end-to-end learning scheme for language identification is proposed. It is in line with the classical GMM i-vector methods both theoretically and practically. In the end-to-end pipeline, a general encoding layer is employed on top of the front-end CNN, so that it can encode the variable-length input sequence into an utterance level vector automatically. After comparing with the state-of-the-art GMM i-vector methods, we give insights into CNN, and reveal its role and effect in the whole pipeline. We further introduce a general encoding layer, illustrating the reason why they might be appropriate for language identification. We elaborate on several typical encoding layers, including a temporal average pooling layer, a recurrent encoding layer and a novel learnable dictionary encoding layer. We conducted experiment on NIST LRE07 closed-set task, and the results show that our proposed end-to-end systems achieve state-of-the-art performance.

##### Abstract (translated by Google)
提出了一种用于语言识别的新型可解释的端到端学习方案。它在理论和实践上都符合经典的GMM i矢量方法。在端到端流水线中，在前端CNN之上采用通用编码层，以便它可以将可变长度输入序列自动编码为话语水平矢量。通过与最先进的GMM i矢量方法进行比较，我们对CNN提供了深入的见解，并揭示了它在整个管道中的作用和影响。我们进一步介绍一个通用编码层，说明它们可能适合语言识别的原因。我们详细介绍了几个典型的编码层，包括时间平均池层，反复编码层和新颖的可学习字典编码层。我们对NIST LRE07闭环任务进行了实验，结果表明我们提出的端到端系统达到了最先进的性能。

##### URL
[http://arxiv.org/abs/1804.00381](http://arxiv.org/abs/1804.00381)

##### PDF
[http://arxiv.org/pdf/1804.00381](http://arxiv.org/pdf/1804.00381)

