---
layout: post
title: "A unified strategy for implementing curiosity and empowerment driven reinforcement learning"
date: 2018-06-18 05:58:04
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Ildefons Magrans de Abril, Ryota Kanai
mathjax: true
---

* content
{:toc}

##### Abstract
Although there are many approaches to implement intrinsically motivated artificial agents, the combined usage of multiple intrinsic drives remains still a relatively unexplored research area. Specifically, we hypothesize that a mechanism capable of quantifying and controlling the evolution of the information flow between the agent and the environment could be the fundamental component for implementing a higher degree of autonomy into artificial intelligent agents. This paper propose a unified strategy for implementing two semantically orthogonal intrinsic motivations: curiosity and empowerment. Curiosity reward informs the agent about the relevance of a recent agent action, whereas empowerment is implemented as the opposite information flow from the agent to the environment that quantifies the agent's potential of controlling its own future. We show that an additional homeostatic drive is derived from the curiosity reward, which generalizes and enhances the information gain of a classical curious/heterostatic reinforcement learning agent. We show how a shared internal model by curiosity and empowerment facilitates a more efficient training of the empowerment function. Finally, we discuss future directions for further leveraging the interplay between these two intrinsic rewards.

##### Abstract (translated by Google)
虽然有很多方法可以实现内在驱动的人工制剂，但多种内在驱动的组合使用仍然是一个相对未开发的研究领域。具体而言，我们假设能够量化和控制代理与环境之间信息流演变的机制可能是实现更高程度的自主智能代理的基本组成部分。本文提出了一个统一的策略来实现两个语义正交的内在动机：好奇心和授权。好奇心奖励通知代理人关于最近代理人行为的相关性，而赋权是通过从代理人到环境的相反信息流实现的，从而量化代理人控制其未来的潜力。我们表明，一个额外的自我稳定驱动源于好奇心奖励，它推广和增强了古典好奇/异形强化学习代理的信息获取。我们展示了如何通过好奇心和授权共享内部模型，以便更有效地培养授权功能。最后，我们讨论未来的方向，以进一步利用这两种内在奖励之间的相互作用。

##### URL
[http://arxiv.org/abs/1806.06505](http://arxiv.org/abs/1806.06505)

##### PDF
[http://arxiv.org/pdf/1806.06505](http://arxiv.org/pdf/1806.06505)

