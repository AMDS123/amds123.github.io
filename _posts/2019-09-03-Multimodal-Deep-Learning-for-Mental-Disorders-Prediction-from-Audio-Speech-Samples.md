---
layout: post
title: "Multimodal Deep Learning for Mental Disorders Prediction from Audio Speech Samples"
date: 2019-09-03 11:15:19
categories: arXiv_SD
tags: arXiv_SD Salient Embedding Transfer_Learning Represenation_Learning Deep_Learning Prediction
author: Habibeh Naderi, Behrouz Haji Soleimani, Sheri Rempel, Stan Matwin, Rudolf Uher
mathjax: true
---

* content
{:toc}

##### Abstract
Key features of mental illnesses are reflected in speech. Our research focuses on designing a multimodal deep learning structure that automatically extracts salient features from recorded speech samples for predicting various mental disorders including depression, bipolar, and schizophrenia. We adopt a variety of pre-trained models to extract embeddings from both audio and text segments. We use several state-of-the-art embedding techniques including BERT, FastText, and Doc2VecC for the text representation learning and WaveNet and VGG-ish models for audio encoding. We also leverage huge auxiliary emotion-labeled text and audio corpora to train emotion-specific embeddings and use transfer learning in order to address the problem of insufficient annotated multimodal data available. All these embeddings are then combined into a joint representation in a multimodal fusion layer and finally a recurrent neural network is used to predict the mental disorder. Our results show that mental disorders can be predicted with acceptable accuracy through multimodal analysis of clinical interviews.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1909.01067](https://arxiv.org/abs/1909.01067)

##### PDF
[https://arxiv.org/pdf/1909.01067](https://arxiv.org/pdf/1909.01067)

