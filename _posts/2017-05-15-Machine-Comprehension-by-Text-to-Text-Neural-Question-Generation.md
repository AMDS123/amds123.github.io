---
layout: post
title: "Machine Comprehension by Text-to-Text Neural Question Generation"
date: 2017-05-15 14:47:05
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro Sordoni, Philip Bachman, Sandeep Subramanian, Saizheng Zhang, Adam Trischler
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.

##### Abstract (translated by Google)
我们提出一个循环的神经模型，从文件生成自然语言的问题，以答案为条件。我们展示了如何使用监督和强化学习的组合来训练模型。在教师强制进行标准最大似然训练后，我们使用策略梯度技术对模型进行微调，以最大化衡量质量问题的多个奖励。最值得注意的是，这些奖励之一就是问答系统的表现。我们将问题产生作为提高问题回答系统性能的一种手段。我们的模型在最近的问答数据集SQUAD上进行了训练和评估。

##### URL
[https://arxiv.org/abs/1705.02012](https://arxiv.org/abs/1705.02012)

##### PDF
[https://arxiv.org/pdf/1705.02012](https://arxiv.org/pdf/1705.02012)

