---
layout: post
title: "Seeing Neural Networks Through a Box of Toys: The Toybox Dataset of Visual Object Transformations"
date: 2018-06-15 16:17:02
categories: arXiv_CV
tags: arXiv_CV CNN Recognition
author: Xiaohan Wang, Tengyu Ma, James Ainooson, Seunghwan Cha, Xiaotian Wang, Azhar Molla, Maithilee Kunda
mathjax: true
---

* content
{:toc}

##### Abstract
Deep convolutional neural networks (CNNs) have enjoyed tremendous success in computer vision in the past several years, particularly for visual object recognition.However, how CNNs work remains poorly understood, and the training of deep CNNs is still considered more art than science. To better characterize deep CNNs and the training process, we introduce a new video dataset called Toybox. Images in Toybox come from first-person, wearable camera recordings of common household objects and toys being manually manipulated to undergo structured transformations like rotations and translations. We also present results from initial experiments using deep CNNs that begin to examine how different distributions of training data can affect visual object recognition performance, and how visual object concepts are represented within a trained network.

##### Abstract (translated by Google)
深卷积神经网络（CNNs）在过去几年中在计算机视觉领域取得了巨大的成功，特别是对于视觉对象识别。但是，CNN的工作方式仍然没有得到很好的理解，深度CNN的训练仍然被认为比科学更具艺术性。为了更好地表征深层CNN和训练过程，我们引入了一个名为Toybox的新视频数据集。 Toybox中的图像来自第一人称可穿戴摄像机的普通家用物品和玩具，它们被手动操作以进行旋转和平移等结构化转换。我们还展示了使用深度CNN进行初始实验的结果，这些实验开始研究不同的训练数据分布如何影响视觉对象识别性能，以及视觉对象概念如何在训练网络中呈现。

##### URL
[http://arxiv.org/abs/1806.06034](http://arxiv.org/abs/1806.06034)

##### PDF
[http://arxiv.org/pdf/1806.06034](http://arxiv.org/pdf/1806.06034)

