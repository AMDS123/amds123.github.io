---
layout: post
title: "Neural Associative Memory for Dual-Sequence Modeling"
date: 2016-06-14 07:59:18
categories: arXiv_CL
tags: arXiv_CL Regularization RNN
author: Dirk Weissenborn
mathjax: true
---

* content
{:toc}

##### Abstract
Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.

##### Abstract (translated by Google)
许多重要的NLP问题可以作为双序列或序列 - 序列建模任务来提出。在构建端到端神经架构方面的最新进展在解决这些任务方面已经非常成功。在这项工作中，我们提出了一种基于联想记忆的双序列建模新架构。我们推导出AM-RNN，这是一种增加通用递归神经网络（RNN）的经常性联想记忆（AM）。该体系结构扩展到一次在两个AM上运行的双AM-RNN。我们的模型在文本蕴含方面取得了非常有竞争力的结果。定性分析表明，使用双AM-RNN可以有效弥补源与目标序列之间的长距离依赖性。然而，自动编码的初始实验表明，当学习解决序列到序列的任务时，系统没有利用这些好处，这表明需要额外的监督或正则化。

##### URL
[https://arxiv.org/abs/1606.03864](https://arxiv.org/abs/1606.03864)

##### PDF
[https://arxiv.org/pdf/1606.03864](https://arxiv.org/pdf/1606.03864)

