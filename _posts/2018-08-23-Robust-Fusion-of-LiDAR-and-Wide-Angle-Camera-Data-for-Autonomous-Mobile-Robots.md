---
layout: post
title: "Robust Fusion of LiDAR and Wide-Angle Camera Data for Autonomous Mobile Robots"
date: 2018-08-23 04:05:50
categories: arXiv_CV
tags: arXiv_CV Detection
author: Varuna De Silva, Jamie Roche, Ahmet Kondoz
mathjax: true
---

* content
{:toc}

##### Abstract
Autonomous robots that assist humans in day to day living tasks are becoming increasingly popular. Autonomous mobile robots operate by sensing and perceiving their surrounding environment to make accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of autonomous vehicles. These heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor for free space detection. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression-based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a uncertainty aware free space detection algorithm

##### Abstract (translated by Google)
帮助人类完成日常生活任务的自主机器人正变得越来越流行。自主移动机器人通过感测和感知周围环境来进行操作，以做出准确的驾驶决策。利用诸如LiDAR，雷达，超声传感器和照相机的若干不同传感器的组合来感测自动车辆的周围环境。这些异构传感器同时捕获环境的各种物理属性。需要积极地利用这种多模态和传感冗余，以通过传感器数据融合来可靠和一致地感知环境。然而，这些多模传感器数据流在许多方面彼此不同，例如时间和空间分辨率，数据格式和几何对准。对于随后的感知算法来利用由多模态感测提供的分集，数据流需要在空间上，几何上和时间上彼此对准。在本文中，我们解决了融合光检测和测距（LiDAR）扫描仪和广角单眼图像传感器的输出以进行自由空间检测的问题。 LiDAR扫描仪和图像传感器的输出具有不同的空间分辨率，需要彼此对齐。几何模型用于在空间上对齐两个传感器输出，接着是基于高斯过程（GP）回归的分辨率匹配算法，以可量化的不确定性内插缺失数据。结果表明，所提出的传感器数据融合框架显着有助于随后的感知步骤，如不确定性感知自由空间检测算法的性能改进所示。

##### URL
[http://arxiv.org/abs/1710.06230](http://arxiv.org/abs/1710.06230)

##### PDF
[http://arxiv.org/pdf/1710.06230](http://arxiv.org/pdf/1710.06230)

