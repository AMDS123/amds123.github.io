---
layout: post
title: "Language Models of Spoken Dutch"
date: 2017-09-12 09:27:12
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition Language_Model Recognition
author: Lyan Verwimp, Joris Pelemans, Marieke Lycke, Hugo Van hamme, Patrick Wambacq
mathjax: true
---

* content
{:toc}

##### Abstract
In Flanders, all TV shows are subtitled. However, the process of subtitling is a very time-consuming one and can be sped up by providing the output of a speech recognizer run on the audio of the TV show, prior to the subtitling. Naturally, this speech recognition will perform much better if the employed language model is adapted to the register and the topic of the program. We present several language models trained on subtitles of television shows provided by the Flemish public-service broadcaster VRT. This data was gathered in the context of the project STON which has as purpose to facilitate the process of subtitling TV shows. One model is trained on all available data (46M word tokens), but we also trained models on a specific type of TV show or domain/topic. Language models of spoken language are quite rare due to the lack of training data. The size of this corpus is relatively large for a corpus of spoken language (compare with e.g. CGN which has 9M words), but still rather small for a language model. Thus, in practice it is advised to interpolate these models with a large background language model trained on written language. The models can be freely downloaded on this http URL

##### Abstract (translated by Google)
在法兰德斯，所有的电视节目都有字幕。然而，字幕的处理是非常耗时的，并且可以通过在字幕之前提供在电视节目的音频上运行的语音识别器的输出来加速。当然，如果所采用的语言模型适合于程序的注册和主题，这种语音识别将会更好。我们提供了由佛兰芒公共服务广播公司VRT提供的电视节目字幕培训的几种语言模型。这些数据是在STON项目的背景下收集的，目的是促进电视节目字幕的过程。一个模型是针对所有可用数据（46M字词标记）进行训练的，但是我们还对特定类型的电视节目或域/话题进行了训练。由于缺乏训练数据，口语的语言模型相当罕见。对于口语语料库来说，这个语料库的大小相对较大（与例如具有9M个词语的CGN相比），但对于语言模型而言仍然相当小。因此，在实践中，建议使用以书面语言训练的大型背景语言模型来插入这些模型。这些模型可以在这个http URL上自由下载

##### URL
[https://arxiv.org/abs/1709.03759](https://arxiv.org/abs/1709.03759)

##### PDF
[https://arxiv.org/pdf/1709.03759](https://arxiv.org/pdf/1709.03759)

