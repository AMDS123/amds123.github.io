---
layout: post
title: "Dynamic Network Surgery for Efficient DNNs"
date: 2016-11-10 00:17:25
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Yiwen Guo, Anbang Yao, Yurong Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\bm{108}\times$ and $\bm{17.7}\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at this https URL

##### Abstract (translated by Google)
深度学习已成为提高机器智能的无处不在的技术。但是，现有的深层模型大都结构复杂，难以在有限的计算能力的移动平台上部署。在本文中，我们提出了一种新的网络压缩方法，称为动态网络外科手术，可以通过动态连接修剪显着降低网络复杂度。与之前以贪婪的方式完成这个任务的方法不同，我们在整个过程中正确地加入了连接拼接，避免了不正确的修剪，使之成为一个连续的网络维护。实验证明了我们方法的有效性。在没有任何精度损失的情况下，我们的方法可以有效地将LeNet-5和AlexNet中的参数数量分别以$ \ bm {108} \ times $和$ \ bm {17.7} \ times $进行压缩，证明它优于最近的修剪方法相当大的利润。代码和一些模型可在此https网址上获得

##### URL
[https://arxiv.org/abs/1608.04493](https://arxiv.org/abs/1608.04493)

##### PDF
[https://arxiv.org/pdf/1608.04493](https://arxiv.org/pdf/1608.04493)

