---
layout: post
title: "Encouraging LSTMs to Anticipate Actions Very Early"
date: 2017-08-14 01:59:04
categories: arXiv_CV
tags: arXiv_CV RNN Prediction
author: Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, Lars Andersson
mathjax: true
---

* content
{:toc}

##### Abstract
In contrast to the widely studied problem of recognizing an action given a complete sequence, action anticipation aims to identify the action from only partially available videos. As such, it is therefore key to the success of computer vision applications requiring to react as early as possible, such as autonomous navigation. In this paper, we propose a new action anticipation method that achieves high prediction accuracy even in the presence of a very small percentage of a video sequence. To this end, we develop a multi-stage LSTM architecture that leverages context-aware and action-aware features, and introduce a novel loss function that encourages the model to predict the correct class as early as possible. Our experiments on standard benchmark datasets evidence the benefits of our approach; We outperform the state-of-the-art action anticipation methods for early prediction by a relative increase in accuracy of 22.0% on JHMDB-21, 14.0% on UT-Interaction and 49.9% on UCF-101.

##### Abstract (translated by Google)
与被广泛研究的以完整序列来识别动作的问题形成对比的是，动作预期旨在从仅部分可用的视频中识别动作。因此，它是计算机视觉应用成功的关键，这些应用需要尽早作出反应，例如自主导航。在本文中，我们提出了一种新的动作预测方法，即使在视频序列的比例非常小的情况下也能实现高预测精度。为此，我们开发了一个多阶段LSTM架构，利用上下文感知和动作感知功能，并引入了一种新的损失函数，鼓励模型尽早预测正确的类别。我们对标准基准数据集的实验证明了我们方法的好处;我们比JHMDB-21的22.0％，UT-Interaction的14.0％和UCF-101的49.9％的准确性相对提高，超越了现有的早期预测行动预测方法。

##### URL
[https://arxiv.org/abs/1703.07023](https://arxiv.org/abs/1703.07023)

##### PDF
[https://arxiv.org/pdf/1703.07023](https://arxiv.org/pdf/1703.07023)

