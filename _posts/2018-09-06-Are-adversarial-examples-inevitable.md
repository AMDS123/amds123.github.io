---
layout: post
title: "Are adversarial examples inevitable?"
date: 2018-09-06 17:26:58
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, Tom Goldstein
mathjax: true
---

* content
{:toc}

##### Abstract
A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? 
 This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks. We show that, for certain classes of problems, adversarial examples are inescapable. Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier's robustness against adversarial examples.

##### Abstract (translated by Google)
已经提出了各种各样的防御来加强神经网络以抵抗对抗性攻击。然而，出现了一种模式，其中大多数对抗防御很快被新的攻击打破。鉴于在产生强有力的防御方面缺乏成功，我们被引出了一个基本问题：对抗性攻击是否不可避免？
 本文从理论的角度分析了对抗性的例子，并确定了分类器对对抗性攻击的敏感性的基本界限。我们表明，对于某些类型的问题，对抗性的例子是不可避免的。通过实验，我们探索理论保证对现实世界问题的影响，并讨论维度和图像复杂性等因素如何限制分类器对抗对抗性示例的鲁棒性。

##### URL
[http://arxiv.org/abs/1809.02104](http://arxiv.org/abs/1809.02104)

##### PDF
[http://arxiv.org/pdf/1809.02104](http://arxiv.org/pdf/1809.02104)

