---
layout: post
title: "Adversarial vulnerability for any classifier"
date: 2018-02-23 15:46:05
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Alhussein Fawzi, Hamza Fawzi, Omar Fawzi
mathjax: true
---

* content
{:toc}

##### Abstract
Despite achieving impressive and often superhuman performance on multiple benchmarks, state-of-the-art deep networks remain highly vulnerable to perturbations: adding small, imperceptible, adversarial perturbations can lead to very high error rates. Provided the data distribution is defined using a generative model mapping latent vectors to datapoints in the distribution, we prove that no classifier can be robust to adversarial perturbations when the latent space is sufficiently large and the generative model sufficiently smooth. Under the same conditions, we prove the existence of adversarial perturbations that transfer well across different models with small risk. We conclude the paper with experiments validating the theoretical bounds.

##### Abstract (translated by Google)
尽管在多个基准测试中取得了令人印象深刻的超人表现，但最先进的深度网络仍然非常容易受到扰动的影响：增加小的，难以察觉的敌对扰动可能导致非常高的错误率。假设数据分布是使用将潜在向量映射到分布中的数据点的生成模型定义的，我们证明当潜在空间足够大并且生成模型足够平滑时，没有分类器可以对抗对抗扰动。在相同的条件下，我们证明存在对抗性扰动，可以在小风险的不同模型间传递。我们通过验证理论界限的实验来总结本文。

##### URL
[http://arxiv.org/abs/1802.08686](http://arxiv.org/abs/1802.08686)

##### PDF
[http://arxiv.org/pdf/1802.08686](http://arxiv.org/pdf/1802.08686)

