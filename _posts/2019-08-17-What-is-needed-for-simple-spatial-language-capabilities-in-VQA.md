---
layout: post
title: "What is needed for simple spatial language capabilities in VQA?"
date: 2019-08-17 20:12:39
categories: arXiv_AI
tags: arXiv_AI QA RNN Relation VQA
author: Alexander Kuhnle, Ann Copestake
mathjax: true
---

* content
{:toc}

##### Abstract
Visual question answering (VQA) comprises a variety of language capabilities. The diagnostic benchmark dataset CLEVR has fueled progress by helping to better assess and distinguish models in basic abilities like counting, comparing and spatial reasoning in vitro. Following this approach, we focus on spatial language capabilities and investigate the question: what are the key ingredients to handle simple visual-spatial relations? We look at the SAN, RelNet, FiLM and MC models and evaluate their learning behavior on diagnostic data which is solely focused on spatial relations. Via comparative analysis and targeted model modification we identify what really is required to substantially improve upon the CNN-LSTM baseline.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.06336](http://arxiv.org/abs/1908.06336)

##### PDF
[http://arxiv.org/pdf/1908.06336](http://arxiv.org/pdf/1908.06336)

