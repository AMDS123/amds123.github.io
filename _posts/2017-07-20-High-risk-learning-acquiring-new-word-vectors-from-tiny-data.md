---
layout: post
title: "High-risk learning: acquiring new word vectors from tiny data"
date: 2017-07-20 15:02:14
categories: arXiv_CL
tags: arXiv_CL Knowledge Language_Model
author: Aurelie Herbelot, Marco Baroni
mathjax: true
---

* content
{:toc}

##### Abstract
Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn 'a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.

##### Abstract (translated by Google)
已知分布式语义模型与小数据纠缠在一起。人们普遍认为，为了学习一个词的“良好载体”，一个模型必须有足够的使用范例。这与人类只能从少数事件中猜测词的含义的事实相矛盾。在本文中，我们展示了一个像Word2Vec这样的神经语言模型只需要对其标准体系结构进行微小的修改就可以从以前学过的语义空间中使用背景知识从微小的数据中学习新的术语。我们测试我们的词义定义模型和一个涉及2-6个句子的上下文的随机任务，显示出在定义任务上性能比最先进的模型大大增加。

##### URL
[https://arxiv.org/abs/1707.06556](https://arxiv.org/abs/1707.06556)

##### PDF
[https://arxiv.org/pdf/1707.06556](https://arxiv.org/pdf/1707.06556)

