---
layout: post
title: "Training behavior of deep neural network in frequency domain"
date: 2018-07-03 15:50:41
categories: arXiv_AI
tags: arXiv_AI Optimization Deep_Learning
author: Zhi-Qin J. Xu, Yaoyu Zhang, Yanyang Xiao
mathjax: true
---

* content
{:toc}

##### Abstract
Why deep neural networks (DNNs) capable of overfitting often generalize well in practice is a mystery in deep learning. Existing works indicate that this observation holds for both complicated real datasets and simple datasets of one-dimensional (1-d) functions. In this work, for general low-frequency dominant 1-d functions, we find that a DNN with common settings first quickly captures the dominant low-frequency components, and then relatively slowly captures high-frequency ones. We call this phenomenon Frequency Principle (F-Principle). F-Principle can be observed over various DNN setups of different activation functions, layer structures and training algorithms in our experiments. F-Principle can be used to understand (i) the behavior of DNN training in the information plane and (ii) why DNNs often generalize well albeit its ability of overfitting. This F-Principle potentially can provide insights into understanding the general principle underlying DNN optimization and generalization for real datasets.

##### Abstract (translated by Google)
为什么能够过度拟合的深度神经网络（DNN）经常在实践中得到很好的推广，这在深度学习中是一个谜。现有工作表明，该观察结果适用于复杂的真实数据集和一维（1-d）函数的简单数据集。在这项工作中，对于一般的低频主导1-d功能，我们发现具有共同设置的DNN首先快速捕获主导的低频分量，然后相对缓慢地捕获高频分量。我们把这种现象称为频率原理（F-Principle）。在我们的实验中，可以通过不同激活函数，层结构和训练算法的各种DNN设置观察F原理。 F原理可用于理解（i）DNN训练在信息平面中的行为，以及（ii）为什么DNN经常很好地概括，尽管它具有过度拟合的能力。这个F原理可能提供深入了解DNN优化的基本原理和真实数据集的推广。

##### URL
[http://arxiv.org/abs/1807.01251](http://arxiv.org/abs/1807.01251)

##### PDF
[http://arxiv.org/pdf/1807.01251](http://arxiv.org/pdf/1807.01251)

