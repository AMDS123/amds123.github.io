---
layout: post
title: "Feedforward Sequential Memory Neural Networks without Recurrent Feedback"
date: 2015-10-09 15:04:11
categories: arXiv_CV
tags: arXiv_CV RNN Language_Model Memory_Networks
author: ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.

##### Abstract (translated by Google)
我们引入了一种称为前馈顺序存储器网络（FSMN）的记忆神经网络的新结构，它可以在不使用递归反馈的情况下学习长期依赖性。所提出的FSMN是在隐藏层中配备可学习的顺序存储器块的标准前馈神经网络。在这项工作中，我们已经将FSMN应用于几种语言建模（LM）任务。实验结果表明，FSMN中的内存块可以学习历史悠久的有效表示。实验表明，基于FSMN的语言模型不仅可以显着地胜过基于前向神经网络（FNN）的LM，而且可以显着地胜任流行的递归神经网络（RNN）LM。

##### URL
[https://arxiv.org/abs/1510.02693](https://arxiv.org/abs/1510.02693)

##### PDF
[https://arxiv.org/pdf/1510.02693](https://arxiv.org/pdf/1510.02693)

