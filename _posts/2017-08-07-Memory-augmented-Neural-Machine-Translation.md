---
layout: post
title: "Memory-augmented Neural Machine Translation"
date: 2017-08-07 06:47:23
categories: arXiv_CL
tags: arXiv_CL NMT
author: Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, Andrew Abel
mathjax: true
---

* content
{:toc}

##### Abstract
Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by $9.0$ and $2.7$ BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.

##### Abstract (translated by Google)
神经机器翻译（NMT）近来取得了显着的成就，然而人们也普遍认识到，这种方法在处理不经常的单词和单词对方面有局限性。本文提出了一种新型的记忆增强型NMT（NM-NMT）架构，它存储关于如何将单词（通常不经常遇到的单词）翻译成记忆的知识，然后利用它们来辅助神经模型。我们使用这种记忆机制将从传统统计机器翻译系统学到的知识与NMT系统学习到的规则结合起来，并提出一个基于这个框架的词外（OOV）单词的解决方案。我们对两个中英文翻译任务的实验表明，M-NMT架构分别在两个任务上的表现优于NMT基线$ 9.0 $和$ 2.7 $ BLEU。此外，我们发现这种架构比竞争方法导致更有效的OOV治疗。

##### URL
[https://arxiv.org/abs/1708.02005](https://arxiv.org/abs/1708.02005)

