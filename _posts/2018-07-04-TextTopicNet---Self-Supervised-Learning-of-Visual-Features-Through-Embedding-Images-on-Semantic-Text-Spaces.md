---
layout: post
title: "TextTopicNet - Self-Supervised Learning of Visual Features Through Embedding Images on Semantic Text Spaces"
date: 2018-07-04 21:44:09
categories: arXiv_CV
tags: arXiv_CV Object_Detection Embedding Image_Classification Classification Deep_Learning Detection
author: Yash Patel, Lluis Gomez, Raul Gomez, Mar&#xe7;al Rusi&#xf1;ol, Dimosthenis Karatzas, C.V. Jawahar
mathjax: true
---

* content
{:toc}

##### Abstract
The immense success of deep learning based methods in computer vision heavily relies on large scale training datasets. These richly annotated datasets help the network learn discriminative visual features. Collecting and annotating such datasets requires a tremendous amount of human effort and annotations are limited to popular set of classes. As an alternative, learning visual features by designing auxiliary tasks which make use of freely available self-supervision has become increasingly popular in the computer vision community. 
 In this paper, we put forward an idea to take advantage of multi-modal context to provide self-supervision for the training of computer vision algorithms. We show that adequate visual features can be learned efficiently by training a CNN to predict the semantic textual context in which a particular image is more probable to appear as an illustration. More specifically we use popular text embedding techniques to provide the self-supervision for the training of deep CNN. 
 Our experiments demonstrate state-of-the-art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or naturally-supervised approaches.

##### Abstract (translated by Google)
基于深度学习的计算机视觉方法的巨大成功在很大程度上依赖于大规模的训练数据集。这些带有丰富注释的数据集有助于网络学习辨别视觉特征。收集和注释这些数据集需要大量的人力，并且注释仅限于流行的类集。作为替代方案，通过设计利用可自由使用的自我监督的辅助任务来学习视觉特征已经在计算机视觉社区中变得越来越流行。
 在本文中，我们提出了利用多模态上下文为计算机视觉算法的训练提供自我监督的想法。我们展示了通过训练CNN来预测语义文本上下文可以有效地学习足够的视觉特征，其中特定图像更可能以图示的形式出现。更具体地说，我们使用流行的文本嵌入技术来为深度CNN的训练提供自我监督。
 我们的实验证明，与最近的自我监督或自然监督方法相比，图像分类，物体检测和多模态检索具有最先进的性能。

##### URL
[http://arxiv.org/abs/1807.02110](http://arxiv.org/abs/1807.02110)

##### PDF
[http://arxiv.org/pdf/1807.02110](http://arxiv.org/pdf/1807.02110)

