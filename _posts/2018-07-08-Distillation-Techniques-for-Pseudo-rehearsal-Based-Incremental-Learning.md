---
layout: post
title: "Distillation Techniques for Pseudo-rehearsal Based Incremental Learning"
date: 2018-07-08 11:01:00
categories: arXiv_AI
tags: arXiv_AI Adversarial Knowledge GAN
author: Haseeb Shah, Khurram Javed, Faisal Shafait
mathjax: true
---

* content
{:toc}

##### Abstract
The ability to learn from incrementally arriving data is essential for any life-long learning system. However, standard deep neural networks forget the knowledge about the old tasks, a phenomenon called catastrophic forgetting, when trained on incrementally arriving data. We discuss the biases in current Generative Adversarial Networks (GAN) based approaches that learn the classifier by knowledge distillation from previously trained classifiers. These biases cause the trained classifier to perform poorly. We propose an approach to remove these biases by distilling knowledge from the classifier of AC-GAN. Experiments on MNIST and CIFAR10 show that this method is comparable to current state of the art rehearsal based approaches. The code for this paper is available at this $\href{https://github.com/haseebs/Pseudo-rehearsal-Incremental-Learning}{link}$.

##### Abstract (translated by Google)
从渐进式数据中学习的能力对任何终身学习系统都至关重要。然而，标准的深度神经网络在逐渐到达的数据训练时忘记了关于旧任务的知识，这种现象称为灾难性遗忘。我们讨论了当前基于生成性对抗网络（GAN）的方法中的偏差，这些方法通过先前训练的分类器的知识蒸馏来学习分类器。这些偏差导致训练有素的分类器表现不佳。我们提出了一种通过从AC-GAN的分类器中提取知识来消除这些偏差的方法。对MNIST和CIFAR10的实验表明，该方法与现有技术的基于排练的方法相当。本文的代码可在此$ \ href {https://github.com/haseebs/Pseudo-rehearsal-Incremental-Learning} {link} $中找到。

##### URL
[http://arxiv.org/abs/1807.02799](http://arxiv.org/abs/1807.02799)

##### PDF
[http://arxiv.org/pdf/1807.02799](http://arxiv.org/pdf/1807.02799)

