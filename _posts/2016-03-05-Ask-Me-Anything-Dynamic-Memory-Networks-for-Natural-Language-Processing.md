---
layout: post
title: "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"
date: 2016-03-05 20:18:55
categories: arXiv_CV
tags: arXiv_CV Sentiment QA Dynamic_Memory_Network Attention Face Text_Classification Classification Memory_Networks
author: Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher
mathjax: true
---

* content
{:toc}

##### Abstract
Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.

##### Abstract (translated by Google)
自然语言处理中的大部分任务都可以通过语言输入转换成问题回答（QA）问题。我们引入了动态记忆网络（DMN），它是一个处理输入序列和问题，形成情节记忆并产生相关答案的神经网络结构。问题触发了一个迭代的注意过程，它允许模型将注意力放在输入和以前迭代的结果上。然后，这些结果在层次递归序列模型中进行推理以生成答案。可以对DMN进行端对端培训，并获得关于几种类型的任务和数据集的最新结果：问答（Facebook的bAbI数据集），情感分析的文本分类（Stanford Sentiment Treebank）和序列建模词性标注（WSJ-PTB）。这些不同任务的训练完全依赖于训练的词向量表示和输入问答三元组。

##### URL
[https://arxiv.org/abs/1506.07285](https://arxiv.org/abs/1506.07285)

##### PDF
[https://arxiv.org/pdf/1506.07285](https://arxiv.org/pdf/1506.07285)

