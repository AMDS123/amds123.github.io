---
layout: post
title: "Emotion Recognition in Speech using Cross-Modal Transfer in the Wild"
date: 2018-08-16 16:10:23
categories: arXiv_CV
tags: arXiv_CV Face Embedding Classification Relation Recognition
author: Samuel Albanie, Arsha Nagrani, Andrea Vedaldi, Andrew Zisserman
mathjax: true
---

* content
{:toc}

##### Abstract
Obtaining large, human labelled speech datasets to train models for emotion recognition is a notoriously challenging task, hindered by annotation cost and label ambiguity. In this work, we consider the task of learning embeddings for speech classification without access to any form of labelled audio. We base our approach on a simple hypothesis: that the emotional content of speech correlates with the facial expression of the speaker. By exploiting this relationship, we show that annotations of expression can be transferred from the visual domain (faces) to the speech domain (voices) through cross-modal distillation. We make the following contributions: (i) we develop a strong teacher network for facial emotion recognition that achieves the state of the art on a standard benchmark; (ii) we use the teacher to train a student, tabula rasa, to learn representations (embeddings) for speech emotion recognition without access to labelled audio data; and (iii) we show that the speech emotion embedding can be used for speech emotion recognition on external benchmark datasets. Code, models and data are available.

##### Abstract (translated by Google)
获得大的，人类标记的语音数据集来训练情绪识别模型是一项众所周知的挑战性任务，受到注释成本和标签模糊性的阻碍。在这项工作中，我们考虑学习嵌入语音分类的任务，而无需访问任何形式的标记音频。我们的方法基于一个简单的假设：语言的情感内容与说话者的面部表情相关。通过利用这种关系，我们表明表达的注释可以通过跨模态蒸馏从视觉域（面）转移到语音域（声音）。我们做出以下贡献：（i）我们开发了一个强大的面部情感识别教师网络，以达到标准基准的最新技术水平; （ii）我们使用教师培训学生，tabula rasa，学习语音情感识别的表示（嵌入），而无需访问标记的音频数据; （iii）我们表明语音情感嵌入可以用于外部基准数据集上的语音情感识别。代码，模型和数据可用。

##### URL
[http://arxiv.org/abs/1808.05561](http://arxiv.org/abs/1808.05561)

##### PDF
[http://arxiv.org/pdf/1808.05561](http://arxiv.org/pdf/1808.05561)

