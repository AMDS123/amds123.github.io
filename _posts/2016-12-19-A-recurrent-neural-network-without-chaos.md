---
layout: post
title: "A recurrent neural network without chaos"
date: 2016-12-19 14:59:14
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Thomas Laurent, James von Brecht
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.

##### Abstract (translated by Google)
我们引入了一个非常简单的门控递归神经网络（RNN），在单词级语言建模任务中实现了与众所周知的门控体系结构（如LSTM和GRU）相媲美的性能。我们证明了我们的模型具有简单，可预测和非混沌的动力学。这与更加标准的门控架构形成鲜明对比，后者的基础动力系统表现出混乱的行为。

##### URL
[https://arxiv.org/abs/1612.06212](https://arxiv.org/abs/1612.06212)

##### PDF
[https://arxiv.org/pdf/1612.06212](https://arxiv.org/pdf/1612.06212)

