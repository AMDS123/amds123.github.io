---
layout: post
title: "Satisficing in Time-Sensitive Bandit Learning"
date: 2018-03-07 19:41:44
categories: arXiv_AI
tags: arXiv_AI Relation
author: Daniel Russo, Benjamin Van Roy
mathjax: true
---

* content
{:toc}

##### Abstract
Much of the recent literature on bandit learning focuses on algorithms that aim to converge on an optimal action. One shortcoming is that this orientation does not account for time sensitivity, which can play a crucial role when learning an optimal action requires much more information than near-optimal ones. Indeed, popular approaches such as upper-confidence-bound methods and Thompson sampling can fare poorly in such situations. We consider instead learning a satisficing action, which is near-optimal while requiring less information, and propose satisficing Thompson sampling, an algorithm that serves this purpose. We establish a general bound on expected discounted regret and study the application of satisficing Thompson sampling to linear and infinite-armed bandits, demonstrating arbitrarily large benefits over Thompson sampling. We also discuss the relation between the notion of satisficing and the theory of rate distortion, which offers guidance on the selection of satisficing actions.

##### Abstract (translated by Google)
最近关于强盗学习的文献大多集中在旨在收敛于最优行动的算法上。一个缺点是，这种定位并没有考虑到时间敏感性，在学习最佳动作需要比接近最优信息更多的信息时，这可能起到至关重要的作用。事实上，在这种情况下，流行的方法，如置信度较高的方法和汤普森抽样可能效果不佳。我们考虑学习一种满意的行为，它在接近最优的同时需要较少的信息，并且提出满足Thompson抽样的算法，这是一种符合此目的的算法。我们建立了一个关于期望折现遗憾的一般约束，并研究了对线性和无限武装的土匪的满意汤普森抽样的应用，证明了汤普森抽样的任意大的好处。我们还讨论了满足概念和率失真理论之间的关系，它为选择满意行为提供了指导。

##### URL
[http://arxiv.org/abs/1803.02855](http://arxiv.org/abs/1803.02855)

##### PDF
[http://arxiv.org/pdf/1803.02855](http://arxiv.org/pdf/1803.02855)

