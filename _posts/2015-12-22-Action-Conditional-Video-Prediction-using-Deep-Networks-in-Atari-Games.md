---
layout: post
title: "Action-Conditional Video Prediction using Deep Networks in Atari Games"
date: 2015-12-22 04:26:54
categories: arXiv_CV
tags: arXiv_CV Knowledge Reinforcement_Learning CNN RNN Prediction
author: Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh
mathjax: true
---

* content
{:toc}

##### Abstract
Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.

##### Abstract (translated by Google)
受基于视觉强化学习（RL）问题的驱动，尤其是来自最近基准Aracade学习环境（ALE）的Atari游戏，我们考虑时空预测问题，未来（图像）帧依赖于控制变量或行为作为前面的帧。虽然不是由自然场景组成的，但Atari游戏中的框架尺寸较高，可以涉及数十个对象，一个或多个对象被动作直接控制，其他许多对象间接受到影响，可以涉及对象的入口和出口，并可能涉及深部分可观测性。我们提出并评估了两种基于卷积神经网络和递归神经网络的深度神经网络体系结构，其中包括编码，动作条件转换和解码层。实验结果表明，所提出的体系结构能够生成视觉逼真的帧，这对于在一些游戏中控制约100步的动作条件期货也是有用的。据我们所知，本文是第一个对由控制输入调节的高维视频进行长期预测的评估和评估。

##### URL
[https://arxiv.org/abs/1507.08750](https://arxiv.org/abs/1507.08750)

##### PDF
[https://arxiv.org/pdf/1507.08750](https://arxiv.org/pdf/1507.08750)

