---
layout: post
title: "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods"
date: 2017-11-01 04:07:05
categories: arXiv_CV
tags: arXiv_CV Adversarial Survey Detection
author: Nicholas Carlini, David Wagner
mathjax: true
---

* content
{:toc}

##### Abstract
Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.

##### Abstract (translated by Google)
已知神经网络容易受到敌对的例子：接近于自然输入但分类错误的输入。为了更好地理解对抗案例的空间，我们调查了最近提出的10个为检测而设计的建议，并比较它们的功效。我们表明，所有可以通过构建新的损失函数来击败。我们得出这样的结论：敌对的例子比以前所认识的要难得多，而且被认为是对抗性例子固有的性质其实不然。最后，我们提出几个评估未来提出的防御措施的简单指导方针。

##### URL
[https://arxiv.org/abs/1705.07263](https://arxiv.org/abs/1705.07263)

##### PDF
[https://arxiv.org/pdf/1705.07263](https://arxiv.org/pdf/1705.07263)

