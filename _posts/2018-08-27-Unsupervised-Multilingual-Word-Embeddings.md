---
layout: post
title: "Unsupervised Multilingual Word Embeddings"
date: 2018-08-27 17:22:15
categories: arXiv_CL
tags: arXiv_CL Embedding Relation
author: Xilun Chen, Claire Cardie
mathjax: true
---

* content
{:toc}

##### Abstract
Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision, which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however, merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources.

##### Abstract (translated by Google)
多语言词嵌入（MWE）在单个分布向量空间中表示来自多种语言的单词。无监督的MWE（UMWE）方法在没有跨语言监督的情况下获得多语言嵌入，这是一种优于传统监督方法的重要优势，并为低资源语言开辟了许多新的可能性。然而，用于学习UMWE的现有技术仅依赖于许多独立训练的无监督双语词嵌入（UBWE）来获得多语言嵌入。这些方法无法利用许多语言之间存在的相互依赖性。为了解决这个缺点，我们提出了一个完全无监督的学习MWE的框架，它直接利用了所有语言对之间的关​​系。我们的模型在多语言单词翻译和跨语言单词相似性的实验中基本上优于以前的方法。此外，我们的模型甚至胜过用跨语言资源培训的监督方法。

##### URL
[http://arxiv.org/abs/1808.08933](http://arxiv.org/abs/1808.08933)

##### PDF
[http://arxiv.org/pdf/1808.08933](http://arxiv.org/pdf/1808.08933)

