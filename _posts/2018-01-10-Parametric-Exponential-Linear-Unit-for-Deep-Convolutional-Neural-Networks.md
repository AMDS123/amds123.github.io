---
layout: post
title: "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks"
date: 2018-01-10 15:18:48
categories: arXiv_CV
tags: arXiv_CV CNN Recognition
author: Ludovic Trottier, Philippe Gigu&#xe8;re, Brahim Chaib-draa
mathjax: true
---

* content
{:toc}

##### Abstract
Object recognition is an important task for improving the ability of visual systems to perform complex scene understanding. Recently, the Exponential Linear Unit (ELU) has been proposed as a key component for managing bias shift in Convolutional Neural Networks (CNNs), but defines a parameter that must be set by hand. In this paper, we propose learning a parameterization of ELU in order to learn the proper activation shape at each layer in the CNNs. Our results on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN, Overfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU (PELU) has better performances than the non-parametric ELU. We have observed as much as a 7.28% relative error improvement on ImageNet with the NiN network, with only 0.0003% parameter increase. Our visual examination of the non-linear behaviors adopted by Vgg using PELU shows that the network took advantage of the added flexibility by learning different activations at different layers.

##### Abstract (translated by Google)
目标识别是提高视觉系统进行复杂场景理解能力的重要任务。近来，指数线性单元（ELU）已经被提出作为管理卷积神经网络（CNN）中的偏移位移的关键组件，但是定义了一个必须由手工设置的参数。在本文中，我们建议学习ELU的参数化，以便在CNN中的每一层学习适当的激活形状。我们在MNIST，CIFAR-10/100和使用NiN，Overfeat，All-CNN和ResNet网络的ImageNet数据集上的结果表明，我们提出的参数ELU（PELU）比非参数ELU具有更好的性能。在NiN网络上，我们观察到ImageNet的相对误差提高了7.28％，参数仅增加了0.0003％。我们对使用PELU的Vgg采用的非线性行为进行视觉检查，结果表明，网络利用增加的灵活性，通过学习不同层次上的不同激活。

##### URL
[http://arxiv.org/abs/1605.09332](http://arxiv.org/abs/1605.09332)

##### PDF
[http://arxiv.org/pdf/1605.09332](http://arxiv.org/pdf/1605.09332)

