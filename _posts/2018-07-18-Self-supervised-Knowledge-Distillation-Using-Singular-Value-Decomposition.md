---
layout: post
title: "Self-supervised Knowledge Distillation Using Singular Value Decomposition"
date: 2018-07-18 08:52:05
categories: arXiv_CV
tags: arXiv_CV Knowledge Classification
author: Seung Hyun Lee, Dae Ha Kim, Byung Cheol Song
mathjax: true
---

* content
{:toc}

##### Abstract
To solve deep neural network (DNN)'s huge training dataset and its high computation issue, so-called teacher-student (T-S) DNN which transfers the knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN has limited range of use, and the knowledge of T-DNN is insufficiently transferred to S-DNN. To improve the quality of the transferred knowledge from T-DNN, we propose a new knowledge distillation using singular value decomposition (SVD). In addition, we define a knowledge transfer as a self-supervised task and suggest a way to continuously receive information from T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of the T-DNN can be up to 1.1\% better than the T-DNN in terms of classification accuracy. Also assuming the same computational cost, our S-DNN outperforms the S-DNN driven by the state-of-the-art distillation with a performance advantage of 1.79\%. code is available on this https URL

##### Abstract (translated by Google)
为了解决深度神经网络（DNN）庞大的训练数据集及其高计算问题，已经提出了将T-DNN的知识传递给S-DNN的所谓的师生（T-S）DNN。然而，现有的T-S-DNN具有有限的使用范围，并且T-DNN的知识不充分地转移到S-DNN。为了提高T-DNN转移知识的质量，我们提出了一种使用奇异值分解（SVD）的新知识蒸馏。此外，我们将知识转移定义为自我监督的任务，并提出一种从T-DNN持续接收信息的方法。仿真结果表明，在分类精度方面，计算成本为T-DNN的1/5的S-DNN可以比T-DNN高出1.1％。同样假设相同的计算成本，我们的S-DNN优于由最先进的蒸馏驱动的S-DNN，性能优势为1.79 \％。代码在此https网址上提供

##### URL
[https://arxiv.org/abs/1807.06819](https://arxiv.org/abs/1807.06819)

##### PDF
[https://arxiv.org/pdf/1807.06819](https://arxiv.org/pdf/1807.06819)

