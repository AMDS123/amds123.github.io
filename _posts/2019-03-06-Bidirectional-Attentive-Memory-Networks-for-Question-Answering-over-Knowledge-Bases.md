---
layout: post
title: "Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases"
date: 2019-03-06 06:09:02
categories: arXiv_CL
tags: arXiv_CL Knowledge QA Attention Embedding Relation Memory_Networks
author: Yu Chen, Lingfei Wu, Mohammed J. Zaki
mathjax: true
---

* content
{:toc}

##### Abstract
When answering natural language questions over knowledge bases (KB), different question components and KB aspects play different roles. However, most existing embedding-based methods for knowledge base question answering (KBQA) ignore the subtle inter-relationships between the question and the KB (e.g., entity types, relation paths and context). In this work, we propose to directly model the two-way flow of interactions between the questions and the underlying KB via a novel two-layered bidirectional attention network, called BAMnet. Without requiring any external resources or hand-crafted features, on the WebQuestions benchmark, our method significantly outperforms existing information-retrieval based methods, and remains competitive with (hand-crafted) semantic parsing based methods. Also, since we use attention mechanisms, our method offers better interpretability compared to other baselines.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1903.02188](http://arxiv.org/abs/1903.02188)

##### PDF
[http://arxiv.org/pdf/1903.02188](http://arxiv.org/pdf/1903.02188)

