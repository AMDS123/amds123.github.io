---
layout: post
title: "Exploiting Web Images for Weakly Supervised Object Detection"
date: 2017-07-28 04:37:16
categories: arXiv_CV
tags: arXiv_CV Object_Detection Knowledge Weakly_Supervised CNN Detection
author: Qingyi Tao, Hao Yang, Jianfei Cai
mathjax: true
---

* content
{:toc}

##### Abstract
In recent years, the performance of object detection has advanced significantly with the evolving deep convolutional neural networks. However, the state-of-the-art object detection methods still rely on accurate bounding box annotations that require extensive human labelling. Object detection without bounding box annotations, i.e, weakly supervised detection methods, are still lagging far behind. As weakly supervised detection only uses image level labels and does not require the ground truth of bounding box location and label of each object in an image, it is generally very difficult to distill knowledge of the actual appearances of objects. Inspired by curriculum learning, this paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy web images to provide prior knowledge of object appearance as a good starting point. While exploiting large-scale free web imagery, we introduce a sophisticated labour free method to construct a web dataset with good diversity in object appearance. After that, semantic relevance and distribution relevance are introduced and utilized in the proposed curriculum training scheme. Our end-to-end learning with the constructed web data achieves remarkable improvement across most object classes especially for the classes that are often considered hard in other works.

##### Abstract (translated by Google)
近年来随着深度卷积神经网络的发展，目标检测的性能有了显着的提高。然而，最先进的对象检测方法仍然依赖于精确的边界框注释，这需要大量的人工标注。没有边界框注释的对象检测，即弱监督检测方法，仍然远远落后。由于弱监督检测仅使用图像级标签，不需要图像中每个对象的边界框位置和标签的基本事实，通常很难提炼出对象的实际外观的知识。受到课程学习的启发，本文提出了一种易于理解的知识转移方案，该方案融合了简单的网络图像，提供对象外观的先验知识作为一个很好的起点。在利用大规模免费网络图像的同时，我们引入了一种复杂的无劳动力的方法来构建一个具有良好多样性的Web数据集。之后，引入语义相关和分配相关性，并将其应用于课程设置的培训计划中。我们使用构建的Web数据进行端到端的学习，可以在大多数对象类中获得显着的改进，特别是对于在其他作品中经常被认为是困难的类。

##### URL
[https://arxiv.org/abs/1707.08721](https://arxiv.org/abs/1707.08721)

##### PDF
[https://arxiv.org/pdf/1707.08721](https://arxiv.org/pdf/1707.08721)

