---
layout: post
title: "Image Tranformer"
date: 2018-02-15 20:37:15
categories: arXiv_CV
tags: arXiv_CV Super_Resolution Attention CNN
author: Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku
mathjax: true
---

* content
{:toc}

##### Abstract
Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. We propose another extension of self-attention allowing it to efficiently take advantage of the two-dimensional nature of images. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published super-resolution models. Images generated by the model fool human observers three times more often than the previous state of the art.

##### Abstract (translated by Google)
图像生成已经被成功地演绎为自回归序列生成或转换问题。最近的研究表明，自我注意是对文本序列进行建模的有效方式。在这项工作中，我们将基于自我注意的最近提出的模型体系结构Transformer推广到具有易于理解的可能性的图像生成序列建模公式。通过限制自我注意机制来关注当地社区，我们显着增加了模型在实践中可以处理的图像的大小，尽管每层比起典型的卷积神经网络保持显着更大的感受野。我们提出了另一个自我注意的延伸，使其能够有效地利用图像的二维特性。虽然概念上简单，但我们的生成模型在ImageNet上的图像生成方面的性能明显优于当前的技术水平，从而将ImageNet上发布的最佳负对数似然从3.83提高到3.77。我们还以大的放大倍数呈现了图像超分辨率的结果，应用了我们架构的编码器 - 解码器配置。在一项人体评估研究中，我们表明，我们的超分辨率模型比以前发布的超分辨率模型显着改善。由模型生成的图像比先前的艺术更容易欺骗人类观察者三次。

##### URL
[https://arxiv.org/abs/1802.05751](https://arxiv.org/abs/1802.05751)

##### PDF
[https://arxiv.org/pdf/1802.05751](https://arxiv.org/pdf/1802.05751)

