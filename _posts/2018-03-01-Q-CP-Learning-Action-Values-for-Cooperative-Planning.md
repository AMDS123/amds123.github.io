---
layout: post
title: "Q-CP: Learning Action Values for Cooperative Planning"
date: 2018-03-01 10:53:04
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Francesco Riccio, Roberto Capobianco, Daniele Nardi
mathjax: true
---

* content
{:toc}

##### Abstract
Research on multi-robot systems has demonstrated promising results in manifold applications and domains. Still, efficiently learning an effective robot behaviors is very difficult, due to unstructured scenarios, high uncertainties, and large state dimensionality (e.g. hyper-redundant and groups of robot). To alleviate this problem, we present Q-CP a cooperative model-based reinforcement learning algorithm, which exploits action values to both (1) guide the exploration of the state space and (2) generate effective policies. Specifically, we exploit Q-learning to attack the curse-of-dimensionality in the iterations of a Monte-Carlo Tree Search. We implement and evaluate Q-CP on different stochastic cooperative (general-sum) games: (1) a simple cooperative navigation problem among 3 robots, (2) a cooperation scenario between a pair of KUKA YouBots performing hand-overs, and (3) a coordination task between two mobile robots entering a door. The obtained results show the effectiveness of Q-CP in the chosen applications, where action values drive the exploration and reduce the computational demand of the planning process while achieving good performance.

##### Abstract (translated by Google)
多机器人系统的研究已经证明了在多种应用和领域中的有希望的结果。由于非结构化场景，高不确定性和大状态维度（例如，超冗余和机器人组），高效学习有效的机器人行为非常困难。为了缓解这个问题，我们提出了一个基于合作模型的强化学习算法Q-CP，该算法利用动作值来（1）指导状态空间的探索和（2）产生有效的策略。具体而言，我们利用Q-learning在Monte-Carlo树搜索迭代中攻击维数灾难。我们在不同的随机合作（一般和）游戏中实现和评估Q-CP：（1）三个机器人之间的一个简单的合作导航问题，（2）一对KUKA YouBots进行交接的合作方案，以及）两个进入门的移动机器人之间的协调任务。所获得的结果显示了Q-CP在所选应用中的有效性，其中行动值驱动勘探并减少计划过程的计算需求，同时实现良好的性能。

##### URL
[http://arxiv.org/abs/1803.00297](http://arxiv.org/abs/1803.00297)

##### PDF
[http://arxiv.org/pdf/1803.00297](http://arxiv.org/pdf/1803.00297)

