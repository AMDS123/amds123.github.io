---
layout: post
title: "Multi-hop Federated Private Data Augmentation with Sample Compression"
date: 2019-07-15 10:54:18
categories: arXiv_AI
tags: arXiv_AI
author: Eunjeong Jeong, Seungeun Oh, Jihong Park, Hyesung Kim, Mehdi Bennis, Seong-Lyun Kim
mathjax: true
---

* content
{:toc}

##### Abstract
On-device machine learning (ML) has brought about the accessibility to a tremendous amount of data from the users while keeping their local data private instead of storing it in a central entity. However, for privacy guarantee, it is inevitable at each device to compensate for the quality of data or learning performance, especially when it has a non-IID training dataset. In this paper, we propose a data augmentation framework using a generative model: multi-hop federated augmentation with sample compression (MultFAug). A multi-hop protocol speeds up the end-to-end over-the-air transmission of seed samples by enhancing the transport capacity. The relaying devices guarantee stronger privacy preservation as well since the origin of each seed sample is hidden in those participants. For further privatization on the individual sample level, the devices compress their data samples. The devices sparsify their data samples prior to transmissions to reduce the sample size, which impacts the communication payload. This preprocessing also strengthens the privacy of each sample, which corresponds to the input perturbation for preserving sample privacy. The numerical evaluations show that the proposed framework significantly improves privacy guarantee, transmission delay, and local training performance with adjustment to the number of hops and compression rate.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1907.06426](https://arxiv.org/abs/1907.06426)

##### PDF
[https://arxiv.org/pdf/1907.06426](https://arxiv.org/pdf/1907.06426)

