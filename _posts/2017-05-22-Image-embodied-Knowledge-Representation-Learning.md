---
layout: post
title: "Image-embodied Knowledge Representation Learning"
date: 2017-05-22 08:14:27
categories: arXiv_CL
tags: arXiv_CL Image_Caption Knowledge_Graph Knowledge Attention Represenation_Learning Classification
author: Ruobing Xie, Zhiyuan Liu, Huanbo Luan, Maosong Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Entity images could provide significant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model (IKRL), where knowledge representations are learned with both triple facts and images. More specifically, we first construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.

##### Abstract (translated by Google)
实体图像可以为知识表示学习提供重要的视觉信息。大多数传统的方法只是从结构化的三元组中学习知识表示，忽略了从实体图像中提取丰富的视觉信息。在本文中，我们提出了一种新颖的图像化知识表示学习模型（IKRL），其中知识表示是学习三重事实和图像。更具体地说，我们首先用一个神经图像编码器构造一个实体的所有图像的表示。然后通过基于注意力的方法将这些图像表示集成到聚合的基于图像的表示中。我们评估我们的知识图完成和三重分类的IKRL模型。实验结果表明，我们的模型胜过了两个任务的所有基线，这表明了视觉信息对于知识表示的重要性，以及我们的模型在学习图像知识表示方面的能力。

##### URL
[https://arxiv.org/abs/1609.07028](https://arxiv.org/abs/1609.07028)

##### PDF
[https://arxiv.org/pdf/1609.07028](https://arxiv.org/pdf/1609.07028)

