---
layout: post
title: "Deep Abstract Q-Networks"
date: 2018-08-25 18:29:32
categories: arXiv_AI
tags: arXiv_AI Sparse Tracking Reinforcement_Learning
author: Melrose Roderick, Christopher Grimm, Stefanie Tellex
mathjax: true
---

* content
{:toc}

##### Abstract
We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.

##### Abstract (translated by Google)
我们研究了具有长视野和稀疏奖励的高维领域的学习和规划问题。最近的方法在许多Atari 2600域中取得了巨大成功。然而，具有长视野和稀疏奖励的领域，如Montezuma的Revenge和Venture，仍然对现有方法具有挑战性。使用抽象的方法（Dietterich 2000; Sutton，Precup和Singh 1999）已被证明可用于解决长期问题。我们使用专家提供的状态抽象将最近的深度强化学习技术与现有的基于模型的方法相结合。我们构建玩具领域，阐明长视野，稀疏奖励和高维输入的问题，并表明我们的算法明显优于以前的方法在这些领域。我们基于抽象的方法优于Deep Q-Networks（Mnih等人2015）关于Montezuma的Revenge and Venture，并展示了以前方法中缺少的回溯行为。

##### URL
[http://arxiv.org/abs/1710.00459](http://arxiv.org/abs/1710.00459)

##### PDF
[http://arxiv.org/pdf/1710.00459](http://arxiv.org/pdf/1710.00459)

