---
layout: post
title: "Transformational Sparse Coding"
date: 2017-12-08 19:21:15
categories: arXiv_CV
tags: arXiv_CV Sparse Face Deep_Learning Recognition
author: Dimitrios C. Gklezakos, Rajesh P. N. Rao
mathjax: true
---

* content
{:toc}

##### Abstract
A fundamental problem faced by object recognition systems is that objects and their features can appear in different locations, scales and orientations. Current deep learning methods attempt to achieve invariance to local translations via pooling, discarding the locations of features in the process. Other approaches explicitly learn transformed versions of the same feature, leading to representations that quickly explode in size. Instead of discarding the rich and useful information about feature transformations to achieve invariance, we argue that models should learn object features conjointly with their transformations to achieve equivariance. We propose a new model of unsupervised learning based on sparse coding that can learn object features jointly with their affine transformations directly from images. Results based on learning from natural images indicate that our approach matches the reconstruction quality of traditional sparse coding but with significantly fewer degrees of freedom while simultaneously learning transformations from data. These results open the door to scaling up unsupervised learning to allow deep feature+transformation learning in a manner consistent with the ventral+dorsal stream architecture of the primate visual cortex.

##### Abstract (translated by Google)
物体识别系统面临的一个基本问题是物体及其特征可能出现在不同的位置，尺度和方位上。目前的深度学习方法试图通过合并实现本地翻译的不变性，丢弃过程中的特征位置。其他方法明确地学习相同功能的转换版本，从而导致快速爆炸的表示。我们认为，模型应该学习对象的特征，与它们的变换相结合以达到等变性，而不是丢弃丰富而有用的关于特征变换的信息以实现不变性。我们提出了一种基于稀疏编码的无监督学习的新模型，可以直接从图像中学习对象特征和仿射变换。基于从自然图像学习的结果表明，我们的方法匹配传统稀疏编码的重构质量，但具有显着更少的自由度，同时学习从数据转换。这些结果打开了放大无监督学习的大门，允许以与灵长类动物视觉皮层的腹侧+背侧流结构一致的方式进行深度特征+转换学习。

##### URL
[http://arxiv.org/abs/1712.03257](http://arxiv.org/abs/1712.03257)

##### PDF
[http://arxiv.org/pdf/1712.03257](http://arxiv.org/pdf/1712.03257)

