---
layout: post
title: 'Coverage Embedding Models for Neural Machine Translation'
date: 2016-08-29 15:10:34
categories: arXiv_CL
tags: arXiv_CL NMT
author: Haitao Mi, Baskaran Sankaran, Zhiguo Wang, Abe Ittycheriah
---

* content
{:toc}

##### Abstract
In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.

##### Abstract (translated by Google)
在本文中，我们通过增加显式覆盖嵌入模型来加强基于注意力的神经机器翻译（NMT），以减轻在NMT中重复和丢弃翻译的问题。对于每个源词，我们的模型从一个完整的覆盖嵌入向量开始，跟踪覆盖状态，然后随着翻译的进行不断更新神经网络。大规模的中英文任务实验表明，我们的增强模型在强大的大词汇量NMT系统上，在各种测试集上显着提高了翻译质量。

##### URL
[https://arxiv.org/abs/1605.03148](https://arxiv.org/abs/1605.03148)

