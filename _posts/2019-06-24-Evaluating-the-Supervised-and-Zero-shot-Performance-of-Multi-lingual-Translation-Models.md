---
layout: post
title: "Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models"
date: 2019-06-24 00:25:18
categories: arXiv_CL
tags: arXiv_CL Knowledge NMT
author: Chris Hokamp, John Glover, Demian Gholipour
mathjax: true
---

* content
{:toc}

##### Abstract
We study several methods for full or partial sharing of the decoder parameters of multilingual NMT models. We evaluate both fully supervised and zero-shot translation performance in 110 unique translation directions using only the WMT 2019 shared task parallel datasets for training. We use additional test sets and re-purpose evaluation methods recently used for unsupervised MT in order to evaluate zero-shot translation performance for language pairs where no gold-standard parallel data is available. To our knowledge, this is the largest evaluation of multi-lingual translation yet conducted in terms of the total size of the training data we use, and in terms of the diversity of zero-shot translation pairs we evaluate. We conduct an in-depth evaluation of the translation performance of different models, highlighting the trade-offs between methods of sharing decoder parameters. We find that models which have task-specific decoder parameters outperform models where decoder parameters are fully shared across all tasks.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.09675](http://arxiv.org/abs/1906.09675)

##### PDF
[http://arxiv.org/pdf/1906.09675](http://arxiv.org/pdf/1906.09675)

