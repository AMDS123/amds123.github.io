---
layout: post
title: "Qualitative Measurements of Policy Discrepancy for Return-based Deep Q-Network"
date: 2018-06-14 12:12:18
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Wenjia Meng, Qian Zheng, Long Yang, Pengfei Li, Gang Pan
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we focus on policy discrepancy in return-based deep Q-network (R-DQN) learning. We propose a general framework for R-DQN, with which most of the return-based reinforcement learning algorithms can be combined with DQN. We show the performance of traditional DQN can be significantly improved by introducing returnbased reinforcement learning. In order to further improve the performance of R-DQN, we present a strategy with two measurements which can qualitatively measure the policy discrepancy. Moreover, we give two bounds for these two measurements under the R-DQN framework. Algorithms with our strategy can accurately express the trace coefficient and achieve a better approximation to return. The experiments are carried out on several representative tasks from the OpenAI Gym library. Results show the algorithms with our strategy outperform the state-of-the-art R-DQN methods.

##### Abstract (translated by Google)
在本文中，我们关注基于回报的深度Q网络（R-DQN）学习中的策略差异。我们提出了一个R-DQN的通用框架，大多数基于回报的强化学习算法可以与DQN相结合。我们展示了传统DQN的表现可以通过引入基于回归的强化学习来显着提高。为了进一步提高R-DQN的性能，我们提出了一种可以定性衡量政策差异的两种度量方法。此外，我们在R-DQN框架下为这两个测量给出两个界限。我们的策略算法可以准确地表达迹线系数并实现更好的返回逼近。这些实验是在来自OpenAI Gym图书馆的几项代表性任务中进行的。结果显示我们的策略的算法优于最先进的R-DQN方法。

##### URL
[http://arxiv.org/abs/1806.06953](http://arxiv.org/abs/1806.06953)

##### PDF
[http://arxiv.org/pdf/1806.06953](http://arxiv.org/pdf/1806.06953)

