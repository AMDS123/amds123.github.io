---
layout: post
title: "Factorization tricks for LSTM networks"
date: 2018-02-24 22:04:52
categories: arXiv_CL
tags: arXiv_CL RNN
author: Oleksii Kuchaiev, Boris Ginsburg
mathjax: true
---

* content
{:toc}

##### Abstract
We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.

##### Abstract (translated by Google)
我们提出了两种简单的方法来减少参数数量并加速大型长期短期记忆（LSTM）网络的训练：第一种方法是将LSTM矩阵的“设计矩阵分解”转换为两个较小矩阵的乘积，第二个是将LSTM矩阵，其输入和状态划分为独立组。这两种方法都允许我们训练大型LSTM网络，并且使用明显更少的RNN参数，从而显着加快到最近的艺术困境。

##### URL
[http://arxiv.org/abs/1703.10722](http://arxiv.org/abs/1703.10722)

##### PDF
[http://arxiv.org/pdf/1703.10722](http://arxiv.org/pdf/1703.10722)

