---
layout: post
title: "Word Representations, Tree Models and Syntactic Functions"
date: 2016-02-05 13:26:56
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning Relation Recognition
author: Simon Šuster, Gertjan van Noord, Ivan Titov
mathjax: true
---

* content
{:toc}

##### Abstract
Word representations induced from models with discrete latent variables (e.g.\ HMMs) have been shown to be beneficial in many NLP applications. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications. We evaluate the word representations on two tasks -- named entity recognition and semantic frame identification. We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods. Additionally, we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident.

##### Abstract (translated by Google)
由具有离散潜变量（例如，\ HMM）的模型引起的词表示已经被证明在许多NLP应用中是有益的。在这项工作中，我们利用带标签的句法依赖树，将感应问题形式化为树结构隐马尔可夫模型的无监督学习。句法功能被用作模型中的附加观察变量，影响转换和发射分量。这样的语法信息可能会导致捕获更多的单词之间的细粒度和功能区别，这反过来可能是许多NLP应用程序所需要的。我们评估两个任务的词表示 - 命名实体识别和语义框架识别。在这两种情况下，我们观察到了利用句法功能信息的改进，并且结果与那些最先进的表示学习方法相媲美。此外，我们重新审视顺序与非标记树模型之间的关系，并发现后者的优点并不是不言而喻的。

##### URL
[https://arxiv.org/abs/1508.07709](https://arxiv.org/abs/1508.07709)

##### PDF
[https://arxiv.org/pdf/1508.07709](https://arxiv.org/pdf/1508.07709)

