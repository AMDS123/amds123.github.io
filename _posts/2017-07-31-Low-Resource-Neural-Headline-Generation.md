---
layout: post
title: "Low-Resource Neural Headline Generation"
date: 2017-07-31 08:56:32
categories: arXiv_CL
tags: arXiv_CL
author: Ottokar Tilk, Tanel Alumäe
mathjax: true
---

* content
{:toc}

##### Abstract
Recent neural headline generation models have shown great results, but are generally trained on very large datasets. We focus our efforts on improving headline quality on smaller datasets by the means of pretraining. We propose new methods that enable pre-training all the parameters of the model and utilize all available text, resulting in improvements by up to 32.4% relative in perplexity and 2.84 points in ROUGE.

##### Abstract (translated by Google)
最近的神经标题生成模型已经显示出很好的结果，但通常在非常大的数据集上进行训练。我们通过预训练的方式，集中精力改善较小数据集的标题质量。我们提出了一种新的方法，可以预先训练模型的所有参数，并利用所有可用的文本，相对困惑度提高了32.4％，ROUGE提高了2.84个点。

##### URL
[https://arxiv.org/abs/1707.09769](https://arxiv.org/abs/1707.09769)

##### PDF
[https://arxiv.org/pdf/1707.09769](https://arxiv.org/pdf/1707.09769)

