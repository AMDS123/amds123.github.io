---
layout: post
title: "Multilingual Constituency Parsing with Self-Attention and Pre-Training"
date: 2018-12-31 11:01:02
categories: arXiv_CL
tags: arXiv_CL Attention
author: Nikita Kitaev, Dan Klein
mathjax: true
---

* content
{:toc}

##### Abstract
We extend our previous work on constituency parsing (Kitaev and Klein, 2018) by incorporating pre-training for ten additional languages, and compare the benefits of no pre-training, ELMo (Peters et al., 2018), and BERT (Devlin et al., 2018). Pre-training is effective across all languages evaluated, and BERT outperforms ELMo in large part due to the benefits of increased model capacity. Our parser obtains new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.11760](http://arxiv.org/abs/1812.11760)

##### PDF
[http://arxiv.org/pdf/1812.11760](http://arxiv.org/pdf/1812.11760)

