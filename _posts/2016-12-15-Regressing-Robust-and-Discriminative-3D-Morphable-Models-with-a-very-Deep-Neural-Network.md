---
layout: post
title: "Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network"
date: 2016-12-15 02:02:28
categories: arXiv_CV
tags: arXiv_CV Face CNN Recognition Face_Recognition
author: Anh Tuan Tran, Tal Hassner, Iacopo Masi, Gerard Medioni
mathjax: true
---

* content
{:toc}

##### Abstract
The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied "in the wild", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.

##### Abstract (translated by Google)
众所周知，人脸的三维形状是有区别的。但是，尽管如此，它们很少用于人脸识别，并始终在受控的观看条件下进行。我们声称，这是一个严重但常常被忽视的问题，现有的单视图3D面部重建方法的一个症状是：在“野外”应用时，他们的3D估计要么不稳定，要么针对同一主体的不同照片改变，要么过度规范化和通用化。作为回应，我们描述了用于回归判别性3D形变面部模型（3DMM）的稳健方法。我们使用卷积神经网络（CNN）直接从输入照片回归3DMM形状和纹理参数。我们通过提供生成大量标记示例的方法来克服为此需要的训练数据的不足。我们的CNN产生的3D估计超越了MICC数据集的艺术精确度。结合3D-3D人脸匹配流水线，我们在LFW，YTF和IJB-A基准上使用3D人脸形状作为表示，而不是其他现代系统使用的不透明深度特征向量，展示第一个竞争性人脸识别结果。

##### URL
[https://arxiv.org/abs/1612.04904](https://arxiv.org/abs/1612.04904)

##### PDF
[https://arxiv.org/pdf/1612.04904](https://arxiv.org/pdf/1612.04904)

