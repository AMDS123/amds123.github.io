---
layout: post
title: "Neural Motifs: Scene Graph Parsing with Global Context"
date: 2017-11-17 17:33:01
categories: arXiv_CV
tags: arXiv_CV Quantitative Detection Relation
author: Rowan Zellers, Mark Yatskar, Sam Thomson, Yejin Choi
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. This analysis leads to a new baseline that is simple, yet strikingly powerful. While hardly considering the overall visual context of an image, it outperforms previous approaches. We then introduce Stacked Motif Networks, a new architecture for encoding global context that is crucial for capturing higher order motifs in scene graphs. Our best model for scene graph detection achieves a 7.3% absolute improvement in recall@50 (41% relative gain) over prior state-of-the-art.

##### Abstract (translated by Google)
我们研究了生成视觉场景的结构化图形表示的问题。我们的工作分析了主题的作用：在场景图中定期出现子结构。我们在视觉基因组数据集中呈现这样的重复结构的新定量的见解。我们的分析表明，对象标签对关系标签有高度的预测能力，但反之亦然。我们还发现即使在较大的子图中也存在反复出现的模式：超过50％的图包含涉及至少两个关系的图案。这种分析导致了一个简单而又惊人的新基线。虽然几乎不考虑图像的整体视觉环境，但它胜过以前的方法。然后，我们介绍Stacked Motif Networks，这是一种用于编码全局上下文的新架构，对于捕获场景图中的高阶图形至关重要。与现有技术水平相比，我们最好的场景图检测模型在回忆率50％（相对增益41％）方面实现了7.3％的绝对改进。

##### URL
[https://arxiv.org/abs/1711.06640](https://arxiv.org/abs/1711.06640)

##### PDF
[https://arxiv.org/pdf/1711.06640](https://arxiv.org/pdf/1711.06640)

