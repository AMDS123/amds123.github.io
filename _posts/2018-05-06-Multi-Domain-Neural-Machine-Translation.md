---
layout: post
title: "Multi-Domain Neural Machine Translation"
date: 2018-05-06 22:14:47
categories: arXiv_CL
tags: arXiv_CL Knowledge NMT
author: Sander Tars, Mark Fishel
mathjax: true
---

* content
{:toc}

##### Abstract
We present an approach to neural machine translation (NMT) that supports multiple domains in a single model and allows switching between the domains when translating. The core idea is to treat text domains as distinct languages and use multilingual NMT methods to create multi-domain translation systems, we show that this approach results in significant translation quality gains over fine-tuning. We also explore whether the knowledge of pre-specified text domains is necessary, turns out that it is after all, but also that when it is not known quite high translation quality can be reached.

##### Abstract (translated by Google)
我们提出了一种在单一模型中支持多个域的神经机器翻译（NMT）方法，并允许在翻译时在域之间切换。核心思想是将文本域视为不同的语言，并使用多语言NMT方法创建多域翻译系统，我们证明这种方法通过微调获得了显着的翻译质量增益。我们还探讨了预先指定的文本域的知识是否有必要，事实证明它是毕竟的，而且当不知道可以达到的翻译质量时也是如此。

##### URL
[http://arxiv.org/abs/1805.02282](http://arxiv.org/abs/1805.02282)

##### PDF
[http://arxiv.org/pdf/1805.02282](http://arxiv.org/pdf/1805.02282)

