---
layout: post
title: "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks"
date: 2017-02-07 19:21:50
categories: arXiv_SD
tags: arXiv_SD Embedding Transfer_Learning Represenation_Learning
author: Stanisław Jastrzebski, Damian Leśniak, Wojciech Marian Czarnecki
mathjax: true
---

* content
{:toc}

##### Abstract
Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning). In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non--linearly encoded in the embedding space, which questions the cosine-based, unsupervised, evaluation methods. All results and analysis scripts are available online.

##### Abstract (translated by Google)
表示学习最重要的目标也许是加快后续学习速度。令人惊讶的是，这个事实并没有很好地反映在嵌入评估的方式。另外，最近的词嵌入实践指出了学习专业表示的重要性。我们认为词汇表征评估的重点应该反映这些趋势，并转向评估容易获得的有用信息。具体而言，我们建议评估应侧重于数据效率和简单的监督任务，其中可用数据量是变化的，每个子集报告监督模型的分数（通常在转移学习中进行）。为了说明这种分析的意义，提出了对选定的词嵌入的综合评估。提出的方法产生了一个更完整的图像，带来了性能特征的新的见解，例如关于单词相似性或类比信息往往是非线性编码的嵌入空间，质疑基于余弦的，无监督的评价方法。所有的结果和分析脚本都可以在线获得。

##### URL
[https://arxiv.org/abs/1702.02170](https://arxiv.org/abs/1702.02170)

##### PDF
[https://arxiv.org/pdf/1702.02170](https://arxiv.org/pdf/1702.02170)

