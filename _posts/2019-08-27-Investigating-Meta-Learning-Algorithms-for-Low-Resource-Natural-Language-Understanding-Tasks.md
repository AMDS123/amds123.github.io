---
layout: post
title: "Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks"
date: 2019-08-27 19:26:31
categories: arXiv_CL
tags: arXiv_CL Optimization Language_Model
author: Zi-Yi Dou, Keyi Yu, Antonios Anastasopoulos
mathjax: true
---

* content
{:toc}

##### Abstract
Learning general representations of text is a fundamental problem for many natural language understanding (NLU) tasks. Previously, researchers have proposed to use language model pre-training and multi-task learning to learn robust representations. However, these methods can achieve sub-optimal performance in low-resource scenarios. Inspired by the recent success of optimization-based meta-learning algorithms, in this paper, we explore the model-agnostic meta-learning algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.10423](http://arxiv.org/abs/1908.10423)

##### PDF
[http://arxiv.org/pdf/1908.10423](http://arxiv.org/pdf/1908.10423)

