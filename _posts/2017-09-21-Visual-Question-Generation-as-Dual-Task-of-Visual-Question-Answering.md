---
layout: post
title: "Visual Question Generation as Dual Task of Visual Question Answering"
date: 2017-09-21 08:04:48
categories: arXiv_CV
tags: arXiv_CV VQA
author: Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Recently visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, which have been explored separately. In this work, we propose an end-to-end unified framework, the Invertible Question Answering Network (iQAN), to leverage the complementary relations between questions and answers in images by jointly training the model on VQA and VQG tasks. Corresponding parameter sharing scheme and regular terms are proposed as constraints to explicitly leverage Q,A's dependencies to guide the training process. After training, iQAN can take either question or answer as input, then output the counterpart. Evaluated on the large-scale visual question answering datasets CLEVR and VQA2, our iQAN improves the VQA accuracy over the baselines. We also show the dual learning framework of iQAN can be generalized to other VQA architectures and consistently improve the results over both the VQA and VQG tasks.

##### Abstract (translated by Google)
最近，视觉问题回答（VQA）和视觉问题生成（VQG）是计算机视觉中的两个热门话题，分别进行了探讨。在这项工作中，我们提出了一个端到端的统一框架 - 可逆问题应答网络（iQAN），通过联合训练VQA和VQG任务的模型来利用图像中问题和答案的互补关系。提出了相应的参数共享方案和正则项作为约束来明确利用Q，A的依赖关系来指导培训过程。训练结束后，iQAN可以将问题或答案作为输入，然后输出相应的对象。我们的iQAN针对大型视觉问题解答数据集CLEVR和VQA2进行评估，提高了基线的VQA精度。我们还展示了iQAN的双重学习框架可以推广到其他VQA架构，并不断改进VQA和VQG任务的结果。

##### URL
[https://arxiv.org/abs/1709.07192](https://arxiv.org/abs/1709.07192)

