---
layout: post
title: "Visual Question Generation as Dual Task of Visual Question Answering"
date: 2017-09-21 08:04:48
categories: arXiv_CV
tags: arXiv_CV QA Relation VQA
author: Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Recently visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, which have been explored separately. In this work, we propose an end-to-end unified framework, the Invertible Question Answering Network (iQAN), to leverage the complementary relations between questions and answers in images by jointly training the model on VQA and VQG tasks. Corresponding parameter sharing scheme and regular terms are proposed as constraints to explicitly leverage Q,A's dependencies to guide the training process. After training, iQAN can take either question or answer as input, then output the counterpart. Evaluated on the large-scale visual question answering datasets CLEVR and VQA2, our iQAN improves the VQA accuracy over the baselines. We also show the dual learning framework of iQAN can be generalized to other VQA architectures and consistently improve the results over both the VQA and VQG tasks.

##### Abstract (translated by Google)
最近，视觉问答（VQA）和视觉问题生成（VQG）是计算机视觉中的两个主题，已经分别进行了探讨。在这项工作中，我们提出了一个端到端的统一框架，即可逆问题应答网络（iQAN），通过联合训练VQA和VQG任务模型，利用图像中问题和答案之间的互补关系。提出了相应的参数共享方案和常规项作为约束来明确地利用Q，A的依赖性来指导训练过程。训练结束后，iQAN可以将问题或答案作为输入，然后输出对应物。通过对大规模视觉问题解答数据集CLEVR和VQA2的评估，我们的iQAN提高了基线的VQA准确度。我们还展示了iQAN的双重学习框架可以推广到其他VQA架构，并不断改进VQA和VQG任务的结果。

##### URL
[https://arxiv.org/abs/1709.07192](https://arxiv.org/abs/1709.07192)

##### PDF
[https://arxiv.org/pdf/1709.07192](https://arxiv.org/pdf/1709.07192)

