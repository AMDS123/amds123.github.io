---
layout: post
title: "A Joint Model of Conversational Discourse and Latent Topics on Microblogs"
date: 2018-09-11 06:13:37
categories: arXiv_CL
tags: arXiv_CL Sparse GAN Summarization Relation
author: Jing Li, Yan Song, Zhongyu Wei, Kam-Fai Wong
mathjax: true
---

* content
{:toc}

##### Abstract
Conventional topic models are ineffective for topic extraction from microblog messages, because the data sparseness exhibited in short messages lacking structure and contexts results in poor message-level word co-occurrence patterns. To address this issue, we organize microblog messages as conversation trees based on their reposting and replying relations, and propose an unsupervised model that jointly learns word distributions to represent: 1) different roles of conversational discourse, 2) various latent topics in reflecting content information. By explicitly distinguishing the probabilities of messages with varying discourse roles in containing topical words, our model is able to discover clusters of discourse words that are indicative of topical content. In an automatic evaluation on large-scale microblog corpora, our joint model yields topics with better coherence scores than competitive topic models from previous studies. Qualitative analysis on model outputs indicates that our model induces meaningful representations for both discourse and topics. We further present an empirical study on microblog summarization based on the outputs of our joint model. The results show that the jointly modeled discourse and topic representations can effectively indicate summary-worthy content in microblog conversations.

##### Abstract (translated by Google)
传统的主题模型对于从微博消息中提取主题是无效的，因为在缺少结构和上下文的短消息中表现出的数据稀疏性导致差的消息级单词共现模式。为了解决这个问题，我们根据他们的重新发布和回复关系组织微博消息作为对话树，并提出一个无监督的模型，共同学习单词分布来表示：1）会话话语的不同角色，2）反映内容信息的各种潜在主题。通过在包含主题词中明确区分具有不同话语角色的消息的概率，我们的模型能够发现指示主题内容的话语词群。在对大型微博语料库的自动评估中，我们的联合模型产生的主题具有比先前研究中的竞争主题模型更好的一致性分数。对模型输出的定性分析表明，我们的模型引发了对话语和主题的有意义的表达。我们进一步基于联合模型的输出，对微博摘要进行了实证研究。结果表明，联合建模的话语和主题表达可以有效地表明微博对话中的总结性内容。

##### URL
[http://arxiv.org/abs/1809.03690](http://arxiv.org/abs/1809.03690)

##### PDF
[http://arxiv.org/pdf/1809.03690](http://arxiv.org/pdf/1809.03690)

