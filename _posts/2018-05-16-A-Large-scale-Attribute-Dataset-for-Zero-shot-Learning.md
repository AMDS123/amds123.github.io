---
layout: post
title: "A Large-scale Attribute Dataset for Zero-shot Learning"
date: 2018-05-16 04:13:44
categories: arXiv_CV
tags: arXiv_CV Knowledge Attention
author: Bo Zhao, Yanwei Fu, Rui Liang, Jiahong Wu, Yonggang Wang, Yizhou Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Zero-Shot Learning (ZSL) has attracted huge research attention over the past few years; it aims to learn the new concepts that have never been seen before. In classical ZSL algorithms, attributes are introduced as the intermediate semantic representation to realize the knowledge transfer from seen classes to unseen classes. Previous ZSL algorithms are tested on several benchmark datasets annotated with attributes. However, these datasets are defective in terms of the image distribution and attribute diversity. In addition, we argue that the "co-occurrence bias problem" of existing datasets, which is caused by the biased co-occurrence of objects, significantly hinders models from correctly learning the concept. To overcome these problems, we propose a Large-scale Attribute Dataset (LAD). Our dataset has 78,017 images of 5 super-classes, 230 classes. The image number of LAD is larger than the sum of the four most popular attribute datasets. 359 attributes of visual, semantic and subjective properties are defined and annotated in instance-level. We analyze our dataset by conducting both supervised learning and zero-shot learning tasks. Seven state-of-the-art ZSL algorithms are tested on this new dataset. The experimental results reveal the challenge of implementing zero-shot learning on our dataset.

##### Abstract (translated by Google)
零射击学习（ZSL）在过去几年引起了巨大的研究注意力，它旨在学习以前从未见过的新概念。在传统的ZSL算法中，属性被引入作为中间语义表示来实现从所看到的类到看不见的类的知识转移。先前的ZSL算法在几个基准数据集上进行测试，这些基准数据集用属性注释。但是，这些数据集在图像分布和属性多样性方面存在缺陷。另外，我们认为现有数据集的“共现偏差问题”是由对象的偏向共现导致的，这大大阻碍了模型正确地学习这个概念。为了克服这些问题，我们提出了一个大规模属性数据集（LAD）。我们的数据集包含78,017个5个超级课程230个课程的图像。 LAD的图像编号大于四个最流行的属性数据集的总和。视觉，语义和主观属性的359个属性在实例级定义和注释。我们通过进行监督学习和零点学习任务来分析我们的数据集。在这个新的数据集上测试了七种最先进的ZSL算法。实验结果揭示了在我们的数据集上实施零点学习的挑战。

##### URL
[http://arxiv.org/abs/1804.04314](http://arxiv.org/abs/1804.04314)

##### PDF
[http://arxiv.org/pdf/1804.04314](http://arxiv.org/pdf/1804.04314)

