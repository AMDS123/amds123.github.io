---
layout: post
title: "Object Perception and Grasping in Open-Ended Domains"
date: 2019-07-25 09:46:36
categories: arXiv_RO
tags: arXiv_RO Knowledge Deep_Learning Recognition
author: S. Hamidreza Kasaei
mathjax: true
---

* content
{:toc}

##### Abstract
Nowadays service robots are leaving the structured and completely known environments and entering human-centric settings. For these robots, object perception and grasping are two challenging tasks due to the high demand for accurate and real-time responses. Although many problems have already been understood and solved successfully, many challenges still remain. Open-ended learning is one of these challenges waiting for many improvements. Cognitive science revealed that humans learn to recognize object categories and grasp affordances ceaselessly over time. This ability allows adapting to new environments by enhancing their knowledge from the accumulation of experiences and the conceptualization of new object categories. Inspired by this, an autonomous robot must have the ability to process visual information and conduct learning and recognition tasks in an open-ended fashion. In this context, "open-ended" implies that the set of object categories to be learned is not known in advance, and the training instances are extracted from online experiences of a robot, and become gradually available over time, rather than being completely available at the beginning of the learning process. 
 In my research, I mainly focus on interactive open-ended learning approaches to recognize multiple objects and their grasp affordances concurrently. In particular, I try to address the following research questions: (i) What is the importance of open-ended learning for autonomous robots? (ii) How robots could learn incrementally from their own experiences as well as from interaction with humans? (iii) What are the limitations of Deep Learning approaches to be used in an open-ended manner? (iv) How to evaluate open-ended learning approaches and what are the right metrics to do so?

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.10932](http://arxiv.org/abs/1907.10932)

##### PDF
[http://arxiv.org/pdf/1907.10932](http://arxiv.org/pdf/1907.10932)

