---
layout: post
title: "Towards Automatic Construction of Diverse, High-quality Image Dataset"
date: 2017-08-22 04:36:12
categories: arXiv_CV
tags: arXiv_CV Image_Caption Object_Detection Weakly_Supervised Image_Classification Classification Detection
author: Yazhou Yao, Jian Zhang, Fumin Shen, Dongxiang Zhang, Zhenmin Tang, Heng Tao Shen
mathjax: true
---

* content
{:toc}

##### Abstract
The availability of labeled image datasets has been shown critical for high-level image understanding, which continuously drives the progress of feature designing and models developing. However, constructing labeled image datasets is laborious and monotonous. To eliminate manual annotation, in this work, we propose a novel image dataset construction framework by employing multiple textual metadata. We aim at collecting diverse and accurate images for given queries from the Web. Specifically, we formulate noisy textual metadata removing and noisy images filtering as a multi-view and multi-instance learning problem separately. Our proposed approach not only improves the accuracy but also enhances the diversity of the selected images. To verify the effectiveness of our proposed approach, we construct an image dataset with 100 categories. The experiments show significant performance gains by using the generated data of our approach on several tasks, such as image classification, cross-dataset generalization, and object detection. The proposed method also consistently outperforms existing weakly supervised and web-supervised approaches.

##### Abstract (translated by Google)
已标注图像数据集的可用性已被证明是高级图像理解的关键，这不断推动着特征设计和模型开发的进程。然而，构建标记的图像数据集是费力和单调的。为了消除手工标注，本文提出了一种新的图像数据集构建框架，采用多个文本元数据。我们的目标是收集来自网络的特定查询的各种准确的图像。具体来说，我们将噪声文本元数据去除和噪声图像过滤分别作为一个多视图和多实例的学习问题。我们提出的方法不仅提高了准确性，而且增强了所选图像的多样性。为了验证我们提出的方法的有效性，我们构建了一个包含100个类别的图像数据集。实验显示，通过使用我们的方法生成的数据在几个任务，如图像分类，跨数据集推广和对象检测显着性能增益。所提出的方法也始终优于现有的弱监督和网络监督方法。

##### URL
[https://arxiv.org/abs/1708.06495](https://arxiv.org/abs/1708.06495)

##### PDF
[https://arxiv.org/pdf/1708.06495](https://arxiv.org/pdf/1708.06495)

