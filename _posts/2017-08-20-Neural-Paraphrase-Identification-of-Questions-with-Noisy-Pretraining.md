---
layout: post
title: "Neural Paraphrase Identification of Questions with Noisy Pretraining"
date: 2017-08-20 02:41:42
categories: arXiv_CL
tags: arXiv_CL Attention
author: Gaurav Singh Tomar, Thyago Duque, Oscar Täckström, Jakob Uszkoreit, Dipanjan Das
mathjax: true
---

* content
{:toc}

##### Abstract
We present a solution to the problem of paraphrase identification of questions. We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model (Parikh et al., 2016) results in accurate performance on this task, while being far simpler than many competing neural architectures. Furthermore, when the model is pretrained on a noisy dataset of automatically collected question paraphrases, it obtains the best reported performance on the dataset.

##### Abstract (translated by Google)
我们提出解决问题的解释识别问题。我们专注于用二进制复述标签注释的问题对的最近数据集，并且表明可分解的注意模型（Parikh等，2016）的变体导致此任务的准确表现，而比许多竞争性神经架构简单得多。此外，当模型在自动收集的问题释义的嘈杂的数据集上预训练时，其在数据集上获得最好的报告的性能。

##### URL
[https://arxiv.org/abs/1704.04565](https://arxiv.org/abs/1704.04565)

##### PDF
[https://arxiv.org/pdf/1704.04565](https://arxiv.org/pdf/1704.04565)

