---
layout: post
title: "Optimal Bayesian Transfer Learning"
date: 2018-01-02 23:15:56
categories: arXiv_CV
tags: arXiv_CV Knowledge Attention Transfer_Learning Classification Prediction
author: Alireza Karbalayghareh, Xiaoning Qian, Edward R. Dougherty
mathjax: true
---

* content
{:toc}

##### Abstract
Transfer learning has recently attracted significant research attention, as it simultaneously learns from different source domains, which have plenty of labeled data, and transfers the relevant knowledge to the target domain with limited labeled data to improve the prediction performance. We propose a Bayesian transfer learning framework where the source and target domains are related through the joint prior density of the model parameters. The modeling of joint prior densities enables better understanding of the "transferability" between domains. We define a joint Wishart density for the precision matrices of the Gaussian feature-label distributions in the source and target domains to act like a bridge that transfers the useful information of the source domain to help classification in the target domain by improving the target posteriors. Using several theorems in multivariate statistics, the posteriors and posterior predictive densities are derived in closed forms with hypergeometric functions of matrix argument, leading to our novel closed-form and fast Optimal Bayesian Transfer Learning (OBTL) classifier. Experimental results on both synthetic and real-world benchmark data confirm the superb performance of the OBTL compared to the other state-of-the-art transfer learning and domain adaptation methods.

##### Abstract (translated by Google)
近年来，转移学习引起了学术界的广泛关注，同时从具有大量标注数据的不同来源领域进行学习，并将相关知识转移到标注数据有限的目标领域，提高了预测性能。我们提出了一个贝叶斯转移学习框架，其中源和目标域通过模型参数的联合先验密度相关。联合先前密度的建模能够更好地理解域之间的“可转移性”。我们为源域和目标域中的高斯特征标签分布的精度矩阵定义联合Wishart密度，以充当传递源域的有用信息以通过改进目标后验来帮助分类在目标域中的桥。利用多元统计量中的几个定理，推导出后验密度和后验密度函数的矩阵论域的超几何函数，从而导出了新颖的闭式快速最优贝叶斯传递学习（OBTL）分类器。综合和现实世界基准数据的实验结果证实了与其他最先进的转移学习和领域适应方法相比，OBTL的卓越性能。

##### URL
[http://arxiv.org/abs/1801.00857](http://arxiv.org/abs/1801.00857)

##### PDF
[http://arxiv.org/pdf/1801.00857](http://arxiv.org/pdf/1801.00857)

