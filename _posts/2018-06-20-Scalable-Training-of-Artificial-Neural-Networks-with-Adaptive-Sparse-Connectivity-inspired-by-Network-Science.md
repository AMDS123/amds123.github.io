---
layout: post
title: "Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science"
date: 2018-06-20 12:55:55
categories: arXiv_AI
tags: arXiv_AI Sparse CNN Deep_Learning
author: Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu, Antonio Liotta
mathjax: true
---

* content
{:toc}

##### Abstract
Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd\H{o}s-R\'enyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.

##### Abstract (translated by Google)
通过在各个领域深入学习的成功，人工神经网络是目前最常用的人工智能方法之一。从生物神经网络的网络特性（例如稀疏性，无标度）中获得灵感，我们认为（与一般实践相反），人工神经网络也不应该具有完全连接的层。在这里，我们提出了人工神经网络的稀疏进化训练，该算法在学习期间将两个连续的神经元层的初始稀疏拓扑（Erd \ H {o} s-R \ enyi随机图）演变为无尺度拓扑。我们的方法在训练之前用稀疏的人造神经网络完全连接的层代替，减少了参数数量的二次方，而不降低准确性。我们证明了我们对受限波尔兹曼机器，多层感知器和卷积神经网络的要求，用于15个数据集上的无监督和监督学习。我们的方法有可能使人造神经网络超出当前可能的范围。

##### URL
[http://arxiv.org/abs/1707.04780](http://arxiv.org/abs/1707.04780)

##### PDF
[http://arxiv.org/pdf/1707.04780](http://arxiv.org/pdf/1707.04780)

