---
layout: post
title: "Visual7W: Grounded Question Answering in Images"
date: 2016-04-09 07:18:10
categories: arXiv_CV
tags: arXiv_CV Image_Caption QA Attention RNN Detection VQA Recognition
author: Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei
mathjax: true
---

* content
{:toc}

##### Abstract
We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.

##### Abstract (translated by Google)
在基本的感知任务中，例如对象识别和检测，我们已经看到了很大的进步。然而，由于缺乏更深入的推理能力，AI模型在高级视觉任务中仍然无法与人类相匹配。最近提出了视觉问答（QA）的新任务来评估模型的深层图像理解能力。以前的作品已经建立了QA句子和图像之间的松散的全球关联。然而，在实践中，许多问题和答案都与图像中的地方区域有关。通过对象层次的接地建立文本描述与图像区域之间的语义链接。它除了在以前的工作中使用的文本答案外，还提供了一种新的视觉答案QA。我们研究了大量的7W多选QA对的基础环境下的视觉QA任务。此外，我们评估人力绩效和质量保证任务的几个基准模型。最后，我们提出了一个新颖的空间关注LSTM模型来解决7W质量保证任务。

##### URL
[https://arxiv.org/abs/1511.03416](https://arxiv.org/abs/1511.03416)

##### PDF
[https://arxiv.org/pdf/1511.03416](https://arxiv.org/pdf/1511.03416)

