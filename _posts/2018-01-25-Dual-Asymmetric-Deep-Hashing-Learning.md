---
layout: post
title: "Dual Asymmetric Deep Hashing Learning"
date: 2018-01-25 11:31:29
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Jinxing Li, Bob Zhang, Guangming Lu, David Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Due to the impressive learning power, deep learning has achieved a remarkable performance in supervised hash function learning. In this paper, we propose a novel asymmetric supervised deep hashing method to preserve the semantic structure among different categories and generate the binary codes simultaneously. Specifically, two asymmetric deep networks are constructed to reveal the similarity between each pair of images according to their semantic labels. The deep hash functions are then learned through two networks by minimizing the gap between the learned features and discrete codes. Furthermore, since the binary codes in the Hamming space also should keep the semantic affinity existing in the original space, another asymmetric pairwise loss is introduced to capture the similarity between the binary codes and real-value features. This asymmetric loss not only improves the retrieval performance, but also contributes to a quick convergence at the training phase. By taking advantage of the two-stream deep structures and two types of asymmetric pairwise functions, an alternating algorithm is designed to optimize the deep features and high-quality binary codes efficiently. Experimental results on three real-world datasets substantiate the effectiveness and superiority of our approach as compared with state-of-the-art.

##### Abstract (translated by Google)
由于学习能力强大，深度学习在监督哈希函数学习方面取得了显着的成绩。在本文中，我们提出了一种新的非对称监督深度哈希方法来保存不同类别之间的语义结构并同时生成二进制代码。具体而言，构造两个非对称深度网络，以根据其语义标签揭示每一对图像之间的相似性。然后通过两个网络学习深度散列函数，最小化学习特征和离散代码之间的差距。此外，由于汉明空间中的二进制码也应该保持原始空间中存在的语义亲和力，因此引入另一个非对称成对丢失来捕获二进制码与实值特征之间的相似度。这种不对称损失不仅提高了检索性能，而且有助于训练阶段的快速收敛。利用双流深层结构和两种非对称成对函数，设计了一种交替算法，有效地优化了深层特征和高质量的二进制编码。三个真实世界的数据集的实验结果证实了我们的方法与最先进的技术相比的有效性和优越性。

##### URL
[http://arxiv.org/abs/1801.08360](http://arxiv.org/abs/1801.08360)

##### PDF
[http://arxiv.org/pdf/1801.08360](http://arxiv.org/pdf/1801.08360)

