---
layout: post
title: "Marine Animal Classification with Correntropy Loss Based Multi-view Learning"
date: 2017-05-03 01:26:24
categories: arXiv_CV
tags: arXiv_CV Embedding Classification
author: Zheng Cao, Shujian Yu, Bing Ouyang, Fraser Dalgleish, Anni Vuorenkoski, Gabriel Alsenas, Jose Principe
mathjax: true
---

* content
{:toc}

##### Abstract
To analyze marine animals behavior, seasonal distribution and abundance, digital imagery can be acquired by visual or Lidar camera. Depending on the quantity and properties of acquired imagery, the animals are characterized as either features (shape, color, texture, etc.), or dissimilarity matrices derived from different shape analysis methods (shape context, internal distance shape context, etc.). For both cases, multi-view learning is critical in integrating more than one set of feature or dissimilarity matrix for higher classification accuracy. This paper adopts correntropy loss as cost function in multi-view learning, which has favorable statistical properties for rejecting noise. For the case of features, the correntropy loss-based multi-view learning and its entrywise variation are developed based on the multi-view intact space learning algorithm. For the case of dissimilarity matrices, the robust Euclidean embedding algorithm is extended to its multi-view form with the correntropy loss function. Results from simulated data and real-world marine animal imagery show that the proposed algorithms can effectively enhance classification rate, as well as suppress noise under different noise conditions.

##### Abstract (translated by Google)
为了分析海洋动物的行为，季节分布和丰度，可以通过视觉或激光雷达相机获取数字图像。根据获得的图像的数量和性质，动物被表征为特征（形状，颜色，纹理等），或者从不同的形状分析方法（形状上下文，内部距离形状上下文等等）导出的不相似性矩阵。对于这两种情况，多视图学习在集成多个特征或不相似矩阵以获得更高的分类准确度方面是至关重要的。本文采用相关损失作为多视点学习的代价函数，具有良好的拒绝噪声统计特性。针对特征的情况，基于多视图完整空间学习算法开发了基于相关损失的多视图学习及其入口变异。对于相异矩阵的情况，将鲁棒欧几里德嵌入算法扩展到具有相关损失函数的多视图形式。仿真数据和真实世界海洋动物图像的结果表明，所提出的算法能够有效提高分类率，并在不同噪声条件下抑制噪声。

##### URL
[https://arxiv.org/abs/1705.01217](https://arxiv.org/abs/1705.01217)

##### PDF
[https://arxiv.org/pdf/1705.01217](https://arxiv.org/pdf/1705.01217)

