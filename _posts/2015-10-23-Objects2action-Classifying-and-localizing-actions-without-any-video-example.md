---
layout: post
title: "Objects2action: Classifying and localizing actions without any video example"
date: 2015-10-23 14:23:44
categories: arXiv_CV
tags: arXiv_CV Embedding
author: Mihir Jain, Jan C. van Gemert, Thomas Mensink, Cees G. M. Snoek
mathjax: true
---

* content
{:toc}

##### Abstract
The goal of this paper is to recognize actions in video without the need for examples. Different from traditional zero-shot approaches we do not demand the design and specification of attribute classifiers and class-to-attribute mappings to allow for transfer from seen classes to unseen classes. Our key contribution is objects2action, a semantic word embedding that is spanned by a skip-gram model of thousands of object categories. Action labels are assigned to an object encoding of unseen video based on a convex combination of action and object affinities. Our semantic embedding has three main characteristics to accommodate for the specifics of actions. First, we propose a mechanism to exploit multiple-word descriptions of actions and objects. Second, we incorporate the automated selection of the most responsive objects per action. And finally, we demonstrate how to extend our zero-shot approach to the spatio-temporal localization of actions in video. Experiments on four action datasets demonstrate the potential of our approach.

##### Abstract (translated by Google)
本文的目标是识别视频中的操作，而不需要举例。与传统的零点方法不同，我们不要求属性分类器和类到属性映射的设计和规范允许从看到的类转移到看不见的类。我们的主要贡献是object2action，这是一个由数千个对象类别的跳跃模型跨越的语义词嵌入。动作标签被分配到基于动作和对象亲和性的凸面组合的未被看见的视频的对象编码。我们的语义嵌入有三个主要的特征来适应具体的行为。首先，我们提出了一个机制来利用行为和对象的多词描述。其次，我们将自动选择最具响应性的对象作为一项操作。最后，我们演示了如何将我们的零镜头方法扩展到视频中的动作的时空定位。四个行动数据集的实验证明了我们的方法的潜力。

##### URL
[https://arxiv.org/abs/1510.06939](https://arxiv.org/abs/1510.06939)

##### PDF
[https://arxiv.org/pdf/1510.06939](https://arxiv.org/pdf/1510.06939)

