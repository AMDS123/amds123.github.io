---
layout: post
title: "Neural Cross-Lingual Named Entity Recognition with Minimal Resources"
date: 2018-08-29 14:55:31
categories: arXiv_CL
tags: arXiv_CL Attention Embedding Recognition
author: Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A. Smith, Jaime Carbonell
mathjax: true
---

* content
{:toc}

##### Abstract
For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-rich languages would be an appealing capability. However, differences in words and word order across languages make it a challenging problem. To improve mapping of lexical items across languages, we propose a method that finds translations based on bilingual word embeddings. To improve robustness to word order differences, we propose to use self-attention, which allows for a degree of flexibility with respect to word order. We demonstrate that these methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a low-resource language.

##### Abstract (translated by Google)
对于没有注释资源的语言，从资源丰富的语言中无监督地传输自然语言处理模型（如命名实体识别（NER））将是一种吸引人的能力。但是，跨语言的单词和单词顺序的差异使其成为一个具有挑战性的问题。为了改进跨语言的词汇项的映射，我们提出了一种基于双语词嵌入来查找翻译的方法。为了提高对词序差异的鲁棒性，我们建议使用自我关注，这允许在词序方面具有一定程度的灵活性。我们证明这些方法在跨语言环境下在通常测试的语言上实现了最先进或竞争性的NER性能，其资源要求比过去的方法低得多。我们还评估了将这些方法应用于低资源语言维吾尔语的挑战。

##### URL
[http://arxiv.org/abs/1808.09861](http://arxiv.org/abs/1808.09861)

##### PDF
[http://arxiv.org/pdf/1808.09861](http://arxiv.org/pdf/1808.09861)

