---
layout: post
title: "Do latent tree learning models identify meaningful structure in sentences?"
date: 2018-02-26 15:59:38
categories: arXiv_CL
tags: arXiv_CL Classification
author: Adina Williams, Andrew Drozdov, Samuel R. Bowman
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.

##### Abstract (translated by Google)
最近在潜在树学习问题上的工作使得训练神经网络成为可能，这些神经网络可以学习解析一个句子，并使用最终的解析来解释句子，所有这些都不需要在训练时暴露于地面真值解析树。令人惊讶的是，与使用传统解析器的解析树的模型相比，这些模型在语句理解任务中常常表现得更好。本文旨在研究这些潜在树学习模型的学习内容。我们在共享代码库中复制两个这样的模型，发现（i）这些模型中只有一个模型在句子分类上优于传统的树型结构模型，（ii）其解析策略在随机重新启动时不是特别一致，（iii）解析它产出倾向于比标准Penn Treebank（PTB）解析更浅，并且（iv）它们不与PTB或作者意识到的任何其他语义或语法形式主义类似。

##### URL
[http://arxiv.org/abs/1709.01121](http://arxiv.org/abs/1709.01121)

##### PDF
[http://arxiv.org/pdf/1709.01121](http://arxiv.org/pdf/1709.01121)

