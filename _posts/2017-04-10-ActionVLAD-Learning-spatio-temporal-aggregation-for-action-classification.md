---
layout: post
title: "ActionVLAD: Learning spatio-temporal aggregation for action classification"
date: 2017-04-10 15:09:41
categories: arXiv_CV
tags: arXiv_CV CNN Video_Classification Classification
author: Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, Bryan Russell
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks with learnable spatio-temporal feature aggregation. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that: (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13% relative) as well as out-performs other baselines with comparable base architectures on HMDB51, UCF101, and Charades video classification benchmarks.

##### Abstract (translated by Google)
在这项工作中，我们引入了一个新的动作分类的视频表示，它在视频的整个时空范围内聚合了局部卷积特征。我们通过整合最先进的双流网络和可学习的时空特征聚合来实现这一点。由此产生的架构是端到端的可训练全视频分类。我们调查了不同的策略，以便跨空间和时间汇集，并结合来自不同流的信号。我们发现：（i）跨空间和时间共同汇集是重要的，但是（ii）外观和运动流最好汇总成它们各自的单独表示。最后，我们展示了我们的表示比HMDB51，UCF101和Charades视频分类基准在基础架构上的表现优于两流基本体系结构（相对13％），并超越其他基线。

##### URL
[https://arxiv.org/abs/1704.02895](https://arxiv.org/abs/1704.02895)

##### PDF
[https://arxiv.org/pdf/1704.02895](https://arxiv.org/pdf/1704.02895)

