---
layout: post
title: "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations"
date: 2018-10-18 04:35:49
categories: arXiv_CL
tags: arXiv_CL Sentiment Knowledge Recognition
author: Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, Rada Mihalcea
mathjax: true
---

* content
{:toc}

##### Abstract
Emotion recognition in conversations is a challenging Artificial Intelligence (AI) task. Recently, it has gained popularity due to its potential applications in many interesting AI tasks such as empathetic dialogue generation, user behavior understanding, and so on. To the best of our knowledge, there is no multimodal multi-party conversational dataset available, which contains more than two speakers in a dialogue. In this work, we propose the Multimodal EmotionLines Dataset (MELD), which we created by enhancing and extending the previously introduced EmotionLines dataset. MELD contains 13,708 utterances from 1433 dialogues of Friends TV series. MELD is superior to other conversational emotion recognition datasets SEMAINE and IEMOCAP as it consists of multiparty conversations and number of utterances in MELD is almost twice as these two datasets. Every utterance in MELD is associated with an emotion and a sentiment label. Utterances in MELD are multimodal encompassing audio and visual modalities along with the text. We have also addressed several shortcomings in EmotionLines and proposed a strong multimodal baseline. The baseline results show that both contextual and multimodal information play important role in emotion recognition in conversations.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.02508](http://arxiv.org/abs/1810.02508)

##### PDF
[http://arxiv.org/pdf/1810.02508](http://arxiv.org/pdf/1810.02508)

