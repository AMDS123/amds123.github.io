---
layout: post
title: "Improved Language Modeling by Decoding the Past"
date: 2018-08-14 18:44:58
categories: arXiv_AI
tags: arXiv_AI Regularization RNN Language_Model
author: Siddhartha Brahma
mathjax: true
---

* content
{:toc}

##### Abstract
Highly regularized LSTMs that model the auto-regressive conditional factorization of the joint probability distribution of words achieve state-of-the-art results in language modeling. These models have an implicit bias towards predicting the next word from a given context. We propose a new regularization term based on decoding words in the context from the predicted distribution of the next word. With relatively few additional parameters, our model achieves absolute improvements of 1.7\% and 2.3\% over the current state-of-the-art results on the Penn Treebank and WikiText-2 datasets.

##### Abstract (translated by Google)
高度正则化的LSTM模拟单词联合概率分布的自回归条件因子分解，在语言建模中实现了最先进的结果。这些模型隐含偏向于预测来自给定上下文的下一个单词。我们提出了一种新的正则化项，它基于从下一个单词的预测分布中解码上下文中的单词。由于附加参数相对较少，我们的模型在Penn Treebank和WikiText-2数据集上的当前最新结果上实现了1.7％和2.3％的绝对改进。

##### URL
[http://arxiv.org/abs/1808.05908](http://arxiv.org/abs/1808.05908)

##### PDF
[http://arxiv.org/pdf/1808.05908](http://arxiv.org/pdf/1808.05908)

