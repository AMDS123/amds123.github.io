---
layout: post
title: "Neural Tree Indexers for Text Understanding"
date: 2017-02-28 17:10:33
categories: arXiv_CL
tags: arXiv_CL Attention Inference RNN Classification
author: Tsendsuren Munkhdalai, Hong Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) process input text sequentially and model the conditional transition between word tokens. In contrast, the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language. However, the current recursive architecture is limited by its dependence on syntactic tree. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both structure and node function. We implemented and evaluated a binarytree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification, outperforming state-of-the-art recurrent and recursive neural networks.

##### Abstract (translated by Google)
递归神经网络（RNN）按顺序处理输入文本，并模拟字词标记之间的条件转换。相反，递归网络的优点在于它们明确地建模自然语言的组合性和递归结构。然而，目前的递归体系结构受句法树依赖的限制。在本文中，我们引入了一个强大的句法分析独立树结构模型，神经树索引器（NTI），提供了连续的RNN和基于句法树的递归模型之间的中间地带。 NTI通过以自下而上的方式处理具有节点功能的输入文本来构造完整的n元树。注意机制可以应用于结构和节点功能。我们实施并评估了一个NTI的二叉树模型，显示该模型在三种不同的NLP任务上达到了最先进的性能：自然语言推理，答句选择和句子分类，超越了最新的经常性循环和递归神经网络。

##### URL
[https://arxiv.org/abs/1607.04492](https://arxiv.org/abs/1607.04492)

##### PDF
[https://arxiv.org/pdf/1607.04492](https://arxiv.org/pdf/1607.04492)

