---
layout: post
title: "Adversarial Examples that Fool both Human and Computer Vision"
date: 2018-02-22 17:40:51
categories: arXiv_CV
tags: arXiv_CV Adversarial Classification
author: Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein
mathjax: true
---

* content
{:toc}

##### Abstract
Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we create the first adversarial examples designed to fool humans, by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by modifying models to more closely match the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.

##### Abstract (translated by Google)
机器学习模型容易受到对抗性例子的影响：对图像的微小改变会导致计算机视觉模型出现错误，例如将校车标识为鸵鸟。然而，人类是否容易出现类似的错误仍然是一个悬而未决的问题。在这里，我们通过利用最新技术创建第一个欺骗人类的对抗性例子，这些技术将具有已知参数和体系结构的计算机视觉模型的敌对示例转换为其他具有未知参数和体系结构的模型，并通过修改模型更加紧密地匹配初始处理的人类视觉系统。我们发现，在计算机视觉模型之间进行强烈转移的对抗性例子影响了时间有限的人类观察者所做的分类。

##### URL
[http://arxiv.org/abs/1802.08195](http://arxiv.org/abs/1802.08195)

##### PDF
[http://arxiv.org/pdf/1802.08195](http://arxiv.org/pdf/1802.08195)

