---
layout: post
title: "Transductive Auxiliary Task Self-Training for Neural Multi-Task Models"
date: 2019-08-16 19:31:13
categories: arXiv_CL
tags: arXiv_CL Relation
author: Johannes Bjerva, Katharina Kann, Isabelle Augenstein
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-task learning and self-training are two common ways to improve a machine learning model's performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training: training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the model has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56% over the pure multi-task model for dependency relation tagging and by up to 13.03% for semantic tagging.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.06136](http://arxiv.org/abs/1908.06136)

##### PDF
[http://arxiv.org/pdf/1908.06136](http://arxiv.org/pdf/1908.06136)

