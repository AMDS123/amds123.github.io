---
layout: post
title: "Track, then Decide: Category-Agnostic Vision-based Multi-Object Tracking"
date: 2017-12-21 13:05:06
categories: arXiv_CV
tags: arXiv_CV Object_Detection Segmentation Tracking Object_Tracking Detection
author: Aljo&#x161;a O&#x161;ep, Wolfgang Mehner, Paul Voigtlaender, Bastian Leibe
mathjax: true
---

* content
{:toc}

##### Abstract
The most common paradigm for vision-based multi-object tracking is tracking-by-detection, due to the availability of reliable detectors for several important object categories such as cars and pedestrians. However, future mobile systems will need a capability to cope with rich human-made environments, in which obtaining detectors for every possible object category would be infeasible. In this paper, we propose a model-free multi-object tracking approach that uses a category-agnostic image segmentation method to track objects. We present an efficient segmentation mask-based tracker which associates pixel-precise masks reported by the segmentation. Our approach can utilize semantic information whenever it is available for classifying objects at the track level, while retaining the capability to track generic unknown objects in the absence of such information. We demonstrate experimentally that our approach achieves performance comparable to state-of-the-art tracking-by-detection methods for popular object categories such as cars and pedestrians. Additionally, we show that the proposed method can discover and robustly track a large variety of other objects.

##### Abstract (translated by Google)
基于视觉的多目标跟踪的最常见范例是逐个检测，这是由于可靠的检测器可用于几个重要的目标类别，例如汽车和行人。然而，未来的移动通信系统将需要一个能够处理丰富的人造环境的能力，在这种环境中，为每个可能的目标类别获取探测器将是不可行的。在本文中，我们提出了一种无模型的多对象跟踪方法，它使用类别不可知的图像分割方法来跟踪对象。我们提出了一个有效的基于分割掩模的跟踪器，它将分割报告的像素精确的掩模关联起来。我们的方法可以利用语义信息，只要它可用于对轨道级别的对象进行分类，同时保留在没有这种信息的情况下跟踪通用未知对象的能力。我们通过实验证明，我们的方法达到了与最先进的逐行检测方法（例如汽车和行人）的性能相当的性能。此外，我们表明，提出的方法可以发现和强大的跟踪各种各样的其他对象。

##### URL
[http://arxiv.org/abs/1712.07920](http://arxiv.org/abs/1712.07920)

##### PDF
[http://arxiv.org/pdf/1712.07920](http://arxiv.org/pdf/1712.07920)

