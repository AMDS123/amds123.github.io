---
layout: post
title: "Language Generation with Recurrent Generative Adversarial Networks without Pre-training"
date: 2017-06-12 17:22:19
categories: arXiv_CL
tags: arXiv_CL Adversarial GAN CNN RNN
author: Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, Lior Wolf
mathjax: true
---

* content
{:toc}

##### Abstract
Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. We empirically show that our approach vastly improves the quality of generated sequences compared to a convolutional baseline.

##### Abstract (translated by Google)
生成对抗网络（GAN）最近在图像生成方面显示出巨大的希望。用于语言生成的训练GAN已经被证明是更困难的，因为用循环神经网络生成文本的不可区分性。因此，过去的工作要么采用最大似然的预训练，要么采用卷积网络来生成。在这项工作中，我们表明，循环神经网络可以被训练，从头开始使用课程学习，从头开始生成文本，通过慢慢地教学模型生成序列的增加和变长。我们凭经验证明，与卷积基线相比，我们的方法大大提高了生成序列的质量。

##### URL
[https://arxiv.org/abs/1706.01399](https://arxiv.org/abs/1706.01399)

##### PDF
[https://arxiv.org/pdf/1706.01399](https://arxiv.org/pdf/1706.01399)

