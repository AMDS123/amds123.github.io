---
layout: post
title: "Learning Compositional Visual Concepts with Mutual Consistency"
date: 2017-11-16 15:41:34
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Face Quantitative
author: Yunye Gong, Srikrishna Karanam, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Peter C. Doerschuk
mathjax: true
---

* content
{:toc}

##### Abstract
Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application.

##### Abstract (translated by Google)
图像合成和分析中的语义概念的合成性是有吸引力的，因为它可以帮助分解已知和有创造性地重组未知数据。例如，我们可以学习改变场景的照明，几何或反照率的概念，并尝试重新组合它们以产生有意义的，但是看不见的用于训练和测试的数据。然而在实践中，我们通常没有来自联合概念空间的样本：我们可能有关于一个数据集中的照度变化和另一个数据集中的几何变化的数据而没有完全重叠。我们提出如下问题：如何从不同的数据集合中共同学习两个或两个以上的概念？我们在本文中基于多个概念的循环一致性给出了一个新的答案，由生成对抗网络（GAN）单独表示。我们的方法ConceptGAN可以理解为数据增强的一个下降，以提高实际应用的弹性。定性和定量评估证明了其在生成语义上有意义的图像方面的功效，以及作为示例应用的一次镜头验证。

##### URL
[https://arxiv.org/abs/1711.06148](https://arxiv.org/abs/1711.06148)

##### PDF
[https://arxiv.org/pdf/1711.06148](https://arxiv.org/pdf/1711.06148)

