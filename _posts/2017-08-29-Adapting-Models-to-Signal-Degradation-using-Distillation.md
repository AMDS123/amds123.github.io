---
layout: post
title: "Adapting Models to Signal Degradation using Distillation"
date: 2017-08-29 17:14:25
categories: arXiv_CV
tags: arXiv_CV Knowledge Transfer_Learning Recognition
author: Jong-Chyi Su, Subhransu Maji
mathjax: true
---

* content
{:toc}

##### Abstract
Model compression and knowledge distillation have been successfully applied for cross-architecture and cross-domain transfer learning. However, a key requirement is that training examples are in correspondence across the domains. We show that in many scenarios of practical importance such aligned data can be synthetically generated using computer graphics pipelines allowing domain adaptation through distillation. We apply this technique to learn models for recognizing low-resolution images using labeled high-resolution images, non-localized objects using labeled localized objects, line-drawings using labeled color images, etc. Experiments on various fine-grained recognition datasets demonstrate that the technique improves recognition performance on the low-quality data and beats strong baselines for domain adaptation. Finally, we present insights into workings of the technique through visualizations and relating it to existing literature.

##### Abstract (translated by Google)
模型压缩和知识提炼已经成功应用于跨体系结构和跨域转移学习。然而，一个关键的要求是，培训范例在各个领域都是相互关联的。我们表明，在实际重要性的许多情况下，这种对齐的数据可以使用计算机图形管线通过蒸馏来允许域适应来合成地产生。我们应用这种技术来学习使用标记的高分辨率图像来识别低分辨率图像的模型，使用标记的本地化对象的非本地化对象，使用标记的彩色图像的线条图等。用于各种细粒度识别数据集的实验证明技术提高了对低质量数据的识别性能，并为领域适应打了坚实的基准。最后，我们通过可视化的方式来展示技术的运作，并将其与现有的文献相联系。

##### URL
[https://arxiv.org/abs/1604.00433](https://arxiv.org/abs/1604.00433)

##### PDF
[https://arxiv.org/pdf/1604.00433](https://arxiv.org/pdf/1604.00433)

