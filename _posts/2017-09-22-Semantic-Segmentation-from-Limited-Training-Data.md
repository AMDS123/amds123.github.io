---
layout: post
title: "Semantic Segmentation from Limited Training Data"
date: 2017-09-22 09:55:18
categories: arXiv_CV
tags: arXiv_CV Segmentation Face CNN Semantic_Segmentation Classification Detection
author: A. Milan, T. Pham, K. Vijay, D. Morrison, A.W. Tow, L. Liu, J. Erskine, R. Grinover, A. Gurman, T. Hunn, N. Kelly-Boxall, D. Lee, M. McTaggart, G. Rallos, A. Razjigaev, T. Rowntree, T. Shen, R. Smith, S. Wade-McCue, Z. Zhuang, C. Lehnert, G. Lin, I. Reid, P. Corke, J. Leitner
mathjax: true
---

* content
{:toc}

##### Abstract
We present our approach for robotic perception in cluttered scenes that led to winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small objects with shiny and transparent surfaces, the biggest challenge of the 2017 competition was the introduction of unseen categories. In contrast to traditional approaches which require large collections of annotated data and many hours of training, the task here was to obtain a robust perception pipeline with only few minutes of data acquisition and training time. To that end, we present two strategies that we explored. One is a deep metric learning approach that works in three separate steps: semantic-agnostic boundary detection, patch classification and pixel-wise voting. The other is a fully-supervised semantic segmentation approach with efficient dataset collection. We conduct an extensive analysis of the two methods on our ARC 2017 dataset. Interestingly, only few examples of each class are sufficient to fine-tune even very deep convolutional neural networks for this specific task.

##### Abstract (translated by Google)
我们在杂乱的场景中展示了我们的机器人感知方法，最终赢得了最近的亚马逊机器人挑战赛（ARC）2017。除了具有光泽和透明表面的小物件之外，2017年比赛的最大挑战是推出看不见的类别。与需要大量注释数据集合和多个小时训练的传统方法相比，这里的任务是获得一个鲁棒的感知流水线，只需要几分钟的数据采集和训练时间。为此，我们提出了两个我们探讨的策略。一种是深度量度学习方法，其在三个单独的步骤中工作：语义不可知的边界检测，片段分类和像素方式投票。另一种是完全监督的语义分割方法，具有高效的数据集合。我们对ARC 2017数据集上的两种方法进行了广泛的分析。有趣的是，对于这个特定的任务，仅仅几个每个类的例子都足以微调甚至非常深的卷积神经网络。

##### URL
[https://arxiv.org/abs/1709.07665](https://arxiv.org/abs/1709.07665)

##### PDF
[https://arxiv.org/pdf/1709.07665](https://arxiv.org/pdf/1709.07665)

