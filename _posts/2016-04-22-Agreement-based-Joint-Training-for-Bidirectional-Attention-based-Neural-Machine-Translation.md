---
layout: post
title: "Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation"
date: 2016-04-22 00:43:03
categories: arXiv_CL
tags: arXiv_CL Attention
author: Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu
mathjax: true
---

* content
{:toc}

##### Abstract
The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently,our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.

##### Abstract (translated by Google)
注意机制已被证明是有效的改善端到端的神经机器翻译。然而，由于自然语言之间错综复杂的结构差异，基于单向注意的模型可能只能捕捉注意规律的部分方面。我们提出基于协议的双向注意型端到端神经机器翻译联合训练。我们的方法不是独立地训练源到目标和目标到源的翻译模型，而是鼓励两个互补模型在相同的训练数据上达成字对齐矩阵。对汉英和英法翻译任务的实验表明，基于协议的联合训练比单独训练显着提高了对齐和翻译质量。

##### URL
[https://arxiv.org/abs/1512.04650](https://arxiv.org/abs/1512.04650)

##### PDF
[https://arxiv.org/pdf/1512.04650](https://arxiv.org/pdf/1512.04650)

