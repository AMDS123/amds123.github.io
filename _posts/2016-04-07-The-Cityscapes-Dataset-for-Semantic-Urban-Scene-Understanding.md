---
layout: post
title: "The Cityscapes Dataset for Semantic Urban Scene Understanding"
date: 2016-04-07 15:39:22
categories: arXiv_CV
tags: arXiv_CV Object_Detection Deep_Learning Detection
author: Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele
mathjax: true
---

* content
{:toc}

##### Abstract
Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.

##### Abstract (translated by Google)
对复杂的城市街景的视觉理解是广泛应用的一个促成因素。对象检测已经从大型数据集中受益匪浅，尤其是在深度学习的背景下。然而，对于语义城市场景的理解，目前的数据集还不足以捕捉真实世界城市场景的复杂性。为了解决这个问题，我们引入Cityscapes，一个基准测试套件和大规模数据集来训练和测试像素级和实例级语义标签的方法。城市风景包括来自50个不同城市的街道上记录的大量不同的立体视频序列。这些图像中的5000个具有高质量的像素级注释; 20000个附加图像具有粗略的注释，以启用利用大量弱标记数据的方法。至关重要的是，我们的努力在数据集大小，注释丰富性，场景变化性和复杂性方面超越了先前的尝试。我们随附的实证研究提供了对数据集特征的深入分析，以及基于我们基准的几种最新方法的绩效评估。

##### URL
[https://arxiv.org/abs/1604.01685](https://arxiv.org/abs/1604.01685)

##### PDF
[https://arxiv.org/pdf/1604.01685](https://arxiv.org/pdf/1604.01685)

