---
layout: post
title: "A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management"
date: 2017-11-29 18:51:14
categories: arXiv_CL
tags: arXiv_CL Reinforcement_Learning
author: Iñigo Casanueva, Paweł Budzianowski, Pei-Hao Su, Nikola Mrkšić, Tsung-Hsien Wen, Stefan Ultes, Lina Rojas-Barahona, Steve Young, Milica Gašić
mathjax: true
---

* content
{:toc}

##### Abstract
Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid the significant effort needed to hand-craft the required dialogue flow, the Dialogue Management (DM) module can be cast as a continuous Markov Decision Process (MDP) and trained through Reinforcement Learning (RL). Several RL models have been investigated over recent years. However, the lack of a common benchmarking framework makes it difficult to perform a fair comparison between different models and their capability to generalise to different environments. Therefore, this paper proposes a set of challenging simulated environments for dialogue model development and evaluation. To provide some baselines, we investigate a number of representative parametric algorithms, namely deep reinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and compare them to a non-parametric model, GP-SARSA. Both the environments and policy models are implemented using the publicly available PyDial toolkit and released on-line, in order to establish a testbed framework for further experiments and to facilitate experimental reproducibility.

##### Abstract (translated by Google)
对话助理正在迅速成为不可或缺的日常援助。为了避免需要手工操作所需对话流程的大量工作，对话管理（DM）模块可以作为一个连续的马尔可夫决策过程（MDP），并通过强化学习（RL）进行训练。几个RL模型近年来一直在进行调查。但是，由于缺乏共同的基准测试框架，难以在不同的模型之间进行公平的比较，并且难以推广到不同的环境。因此，本文提出了一套具有挑战性的对话模型开发和评估的模拟环境。为了提供一些基线，我们研究了许多具有代表性的参数化算法，即深度强化学习算法--DQN，A2C和Natural Actor-Critic，并将它们与非参数模型GP-SARSA进行比较。环境和策略模型都是使用公开可用的PyDial工具包实现的，并在线发布，以便为进一步的实验建立测试平台框架并促进实验的可重复性。

##### URL
[https://arxiv.org/abs/1711.11023](https://arxiv.org/abs/1711.11023)

