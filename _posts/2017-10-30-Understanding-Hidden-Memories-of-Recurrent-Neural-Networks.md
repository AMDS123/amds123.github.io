---
layout: post
title: "Understanding Hidden Memories of Recurrent Neural Networks"
date: 2017-10-30 05:37:25
categories: arXiv_CV
tags: arXiv_CV Review Knowledge RNN
author: Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, Huamin Qu
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1710.10777](https://arxiv.org/abs/1710.10777)

##### PDF
[https://arxiv.org/pdf/1710.10777](https://arxiv.org/pdf/1710.10777)

