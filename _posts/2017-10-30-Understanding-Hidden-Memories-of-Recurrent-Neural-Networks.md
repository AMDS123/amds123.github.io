---
layout: post
title: "Understanding Hidden Memories of Recurrent Neural Networks"
date: 2017-10-30 05:37:25
categories: arXiv_CV
tags: arXiv_CV Review Knowledge RNN
author: Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, Huamin Qu
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.

##### Abstract (translated by Google)
递归神经网络（RNN）已经成功应用于各种自然语言处理（NLP）任务，取得了比传统方法更好的结果。但是，缺乏对其有效性背后机制的理解限制了其架构的进一步改进。在本文中，我们提出了一个视觉分析方法来理解和比较RNP模型的NLP任务。我们提出了一种技术来解释单个隐藏状态单元的功能，基于他们对输入文本的预期响应。然后，我们根据预期的响应将隐藏的状态单元和单词进行聚类，并将协作聚类结果可视化为内存芯片和单词云，从而提供更多有关RNN隐藏状态的结构化知识。我们还提出了基于集合信息的基于字形的序列可视化来分析RNN隐藏状态在句级的行为。我们的方法的可用性和有效性通过案例研究和领域专家的评论来证明。

##### URL
[https://arxiv.org/abs/1710.10777](https://arxiv.org/abs/1710.10777)

##### PDF
[https://arxiv.org/pdf/1710.10777](https://arxiv.org/pdf/1710.10777)

