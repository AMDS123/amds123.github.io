---
layout: post
title: "Sequential Attention: A Context-Aware Alignment Function for Machine Reading"
date: 2017-06-26 22:25:55
categories: arXiv_CL
tags: arXiv_CL Attention
author: Sebastian Brarda, Philip Yeres, Samuel R. Bowman
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (on the Who did What and CNN datasets) and show that it dramatically improves a strong baseline--the Stanford Reader--and is competitive with the state of the art.

##### Abstract (translated by Google)
在本文中，我们提出了一个带有新型顺序注意层的神经网络模型，该模型通过给输入序列中的词赋予权重来扩展软注意力，这种方式不仅考虑了单词与查询匹配的程度，比赛。我们对阅读理解任务（“Who was What”和“CNN数据集”）的评估方法进行了评估，并表明它大大提高了强大的基线 - 斯坦福大学的读者 - 并且与最先进的技术竞争。

##### URL
[https://arxiv.org/abs/1705.02269](https://arxiv.org/abs/1705.02269)

##### PDF
[https://arxiv.org/pdf/1705.02269](https://arxiv.org/pdf/1705.02269)

