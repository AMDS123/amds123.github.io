---
layout: post
title: "Retrosynthesis with Attention-Based NMT Model and Chemical Analysis of the 'Wrong' Predictions"
date: 2019-08-02 07:34:38
categories: arXiv_CL
tags: arXiv_CL Attention NMT Prediction
author: Hongliang Duan, Ling Wang, Chengyun Zhang, Jianjun Li
mathjax: true
---

* content
{:toc}

##### Abstract
We cast retrosynthesis as a machine translation problem by introducing a special Tensor2Tensor, an entire attention-based and fully data-driven model. Given a data set comprising about 50,000 diverse reactions extracted from USPTO patents, the model significantly outperforms seq2seq model (34.7%) on a top-1 accuracy by achieving 54.1%. For yielding better results, parameters such as batch size and training time are thoroughly investigated to train the model. Additionally, we offer a novel insight into the causes of grammatically invalid SMILES, and conduct a test in which experienced chemists pick out and analyze the "wrong" predictions that may be chemically plausible but differ from the ground truth. Actually, the effectiveness of our model is un-derestimated and the "true" top-1 accuracy can reach to 64.6%.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.00727](https://arxiv.org/abs/1908.00727)

##### PDF
[https://arxiv.org/pdf/1908.00727](https://arxiv.org/pdf/1908.00727)

