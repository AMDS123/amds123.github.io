---
layout: post
title: "Safe Option-Critic: Learning Safety in the Option-Critic Architecture"
date: 2018-07-21 00:39:23
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Arushi Jain, Khimya Khetarpal, Doina Precup
mathjax: true
---

* content
{:toc}

##### Abstract
Designing hierarchical reinforcement learning algorithms that induce a notion of safety is not only vital for safety-critical applications, but also, brings better understanding of an artificially intelligent agent's decisions. While learning end-to-end options automatically has been fully realized recently, we propose a solution to learning safe options. We introduce the idea of controllability of states based on the temporal difference errors in the option-critic framework. We then derive the policy-gradient theorem with controllability and propose a novel framework called safe option-critic. We demonstrate the effectiveness of our approach in the four-rooms grid-world, cartpole, and three games in the Arcade Learning Environment (ALE): MsPacman, Amidar and Q*Bert. Learning of end-to-end options with the proposed notion of safety achieves reduction in the variance of return and boosts the performance in environments with intrinsic variability in the reward structure. More importantly, the proposed algorithm outperforms the vanilla options in all the environments and primitive actions in two out of three ALE games.

##### Abstract (translated by Google)
设计引入安全概念的分层强化学习算法不仅对安全关键应用至关重要，而且可以更好地理解人工智能代理的决策。虽然最近已经完全实现了自动学习端到端选项，但我们提出了一种学习安全选项的解决方案。我们基于期权 - 评论框架中的时间差异误差，引入了状态可控性的概念。然后，我们推导出具有可控性的策略梯度定理，并提出一种称为安全期权 - 评论者的新框架。我们在街机学习环境（ALE）的四室网格世界，推车和三场比赛中证明了我们的方法的有效性：MsPacman，Amidar和Q * Bert。利用所提出的安全概念来学习端到端选项可以减少回报的方差，并在奖励结构中具有内在可变性的环境中提高绩效。更重要的是，所提出的算法在三种ALE游戏中的两种中都优于所有环境中的香草选项和原始动作。

##### URL
[http://arxiv.org/abs/1807.08060](http://arxiv.org/abs/1807.08060)

##### PDF
[http://arxiv.org/pdf/1807.08060](http://arxiv.org/pdf/1807.08060)

