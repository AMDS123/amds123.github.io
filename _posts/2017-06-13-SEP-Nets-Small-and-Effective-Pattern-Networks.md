---
layout: post
title: "SEP-Nets: Small and Effective Pattern Networks"
date: 2017-06-13 06:07:26
categories: arXiv_CV
tags: arXiv_CV Attention CNN
author: Zhe Li, Xiaoyu Wang, Xutao Lv, Tianbao Yang
mathjax: true
---

* content
{:toc}

##### Abstract
While going deeper has been witnessed to improve the performance of convolutional neural networks (CNN), going smaller for CNN has received increasing attention recently due to its attractiveness for mobile/embedded applications. It remains an active and important topic how to design a small network while retaining the performance of large and deep CNNs (e.g., Inception Nets, ResNets). Albeit there are already intensive studies on compressing the size of CNNs, the considerable drop of performance is still a key concern in many designs. This paper addresses this concern with several new contributions. First, we propose a simple yet powerful method for compressing the size of deep CNNs based on parameter binarization. The striking difference from most previous work on parameter binarization/quantization lies at different treatments of $1\times 1$ convolutions and $k\times k$ convolutions ($k>1$), where we only binarize $k\times k$ convolutions into binary patterns. The resulting networks are referred to as pattern networks. By doing this, we show that previous deep CNNs such as GoogLeNet and Inception-type Nets can be compressed dramatically with marginal drop in performance. Second, in light of the different functionalities of $1\times 1$ (data projection/transformation) and $k\times k$ convolutions (pattern extraction), we propose a new block structure codenamed the pattern residual block that adds transformed feature maps generated by $1\times 1$ convolutions to the pattern feature maps generated by $k\times k$ convolutions, based on which we design a small network with $\sim 1$ million parameters. Combining with our parameter binarization, we achieve better performance on ImageNet than using similar sized networks including recently released Google MobileNets.

##### Abstract (translated by Google)
在深入研究卷积神经网络（CNN）性能的同时，由于CNN对移动/嵌入式应用的吸引力，CNN越来越受到关注。如何设计一个小型网络，同时保留大型和深度的CNN（如网络，ResNets）的性能仍然是一个活跃而重要的话题。虽然目前已经有很多关于压缩CNN尺寸的深入研究，但性能的显着下降仍然是许多设计中的关键问题。本文通过几个新的贡献来解决这个问题。首先，我们提出了一种简单但强大的基于参数二值化的深度CNN尺寸压缩方法。与以前大多数关于参数二值化/量化的工作的惊人区别在于不同的处理：$ 1 \ times 1 $卷积和$ k \ times k $卷积（$ k> 1 $），其中我们只是二元化$ k \ times k $卷积进入二进制模式。结果网络被称为模式网络。通过这样做，我们发现以前的深度CNN如GoogLeNet和初始型网络可以被大幅度压缩，并且性能的下降。其次，根据$ 1 \ times 1 $（数据投影/变换）和$ k \ times k $卷积（模式提取）的不同功能，我们提出了一个新的块结构代码模式残差块，由$ 1 \ times 1 $卷积到由$ k \ times k $卷积生成的模式特征映射，在此基础上我们设计一个具有$ \ sim 1 $ million参数的小型网络。结合我们的参数二值化，我们在ImageNet上实现比使用包括最近发布的Google MobileNets的类似大小的网络更好的性能。

##### URL
[https://arxiv.org/abs/1706.03912](https://arxiv.org/abs/1706.03912)

##### PDF
[https://arxiv.org/pdf/1706.03912](https://arxiv.org/pdf/1706.03912)

