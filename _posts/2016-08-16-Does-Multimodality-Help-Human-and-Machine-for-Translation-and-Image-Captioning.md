---
layout: post
title: "Does Multimodality Help Human and Machine for Translation and Image Captioning?"
date: 2016-08-16 12:11:29
categories: arXiv_CL
tags: arXiv_CL Image_Caption Attention Caption RNN
author: Ozan Caglayan, Walid Aransa, Yaxing Wang, Marc Masana, Mercedes García-Martínez, Fethi Bougares, Loïc Barrault, Joost van de Weijer
mathjax: true
---

* content
{:toc}

##### Abstract
This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.

##### Abstract (translated by Google)
本文介绍了由LIUM和CVC开发的用于WMT16多模态机器翻译挑战的系统。我们探索了各种比较方法，即基于短语的系统和使用单模或多模数据训练的注意力递归神经网络模型。我们还进行了人类评估，以估计多模态数据在人机翻译和图像描述生成中的作用。根据自动评估指标BLEU和METEOR，我们的系统获得了两项任务的最佳结果。

##### URL
[https://arxiv.org/abs/1605.09186](https://arxiv.org/abs/1605.09186)

##### PDF
[https://arxiv.org/pdf/1605.09186](https://arxiv.org/pdf/1605.09186)

