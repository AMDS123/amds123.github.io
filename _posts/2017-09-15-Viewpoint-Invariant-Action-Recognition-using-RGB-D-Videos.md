---
layout: post
title: "Viewpoint Invariant Action Recognition using RGB-D Videos"
date: 2017-09-15 07:39:34
categories: arXiv_CV
tags: arXiv_CV Sparse Knowledge Action_Recognition Classification Recognition
author: Jian Liu, Naveed Akhtar, Ajmal Mian
mathjax: true
---

* content
{:toc}

##### Abstract
In video-based action recognition, viewpoint variations often pose major challenges because the same actions can appear different from different views. We use the complementary RGB and Depth information from the RGB-D cameras to address this problem. The proposed technique capitalizes on the spatio-temporal information available in the two data streams to the extract action features that are largely insensitive to the viewpoint variations. We use the RGB data to compute dense trajectories that are translated to viewpoint insensitive deep features under a non-linear knowledge transfer model. Similarly, the Depth stream is used to extract CNN-based view invariant features on which Fourier Temporal Pyramid is computed to incorporate the temporal information. The heterogeneous features from the two streams are combined and used as a dictionary to predict the label of the test samples. To that end, we propose a sparse-dense collaborative representation classification scheme that strikes a balance between the discriminative abilities of the dense and the sparse representations of the samples over the extracted heterogeneous dictionary.

##### Abstract (translated by Google)
在基于视频的动作识别中，视点变化经常带来重大挑战，因为相同的动作可能会出现与不同视图不同的情况。我们使用来自RGB-D摄像机的补充RGB和深度信息来解决这个问题。所提出的技术利用两个数据流中可用的时空信息来获取对视点变化很不敏感的提取动作特征。我们使用RGB数据来计算在非线性知识转移模型下转换为视点不敏感的深度特征的稠密轨迹。类似地，深度流被用于提取基于CNN的视图不变特征，傅立叶时间金字塔被计算以合并时间信息。来自两个流的异质特征被组合并用作字典来预测测试样本的标签。为此，我们提出了一个稀疏密集的协同表示分类方案，在提取的异构词典中对样本的稠密和稀疏表示的区分能力进行平衡。

##### URL
[https://arxiv.org/abs/1709.05087](https://arxiv.org/abs/1709.05087)

##### PDF
[https://arxiv.org/pdf/1709.05087](https://arxiv.org/pdf/1709.05087)

