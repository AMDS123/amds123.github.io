---
layout: post
title: "Multimodal Unified Attention Networks for Vision-and-Language Interactions"
date: 2019-08-12 12:12:17
categories: arXiv_CV
tags: arXiv_CV QA Attention VQA
author: Zhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, Qi Tian
mathjax: true
---

* content
{:toc}

##### Abstract
Learning an effective attention mechanism for multimodal data is important in many vision-and-language tasks that require a synergic understanding of both the visual and textual contents. Existing state-of-the-art approaches use co-attention models to associate each visual object (e.g., image region) with each textual object (e.g., query word). Despite the success of these co-attention models, they only model inter-modal interactions while neglecting intra-modal interactions. Here we propose a general `unified attention' model that simultaneously captures the intra- and inter-modal interactions of multimodal features and outputs their corresponding attended representations. By stacking such unified attention blocks in depth, we obtain the deep Multimodal Unified Attention Network (MUAN), which can seamlessly be applied to the visual question answering (VQA) and visual grounding tasks. We evaluate our MUAN models on two VQA datasets and three visual grounding datasets, and the results show that MUAN achieves top-level performance on both tasks without bells and whistles.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1908.04107](https://arxiv.org/abs/1908.04107)

##### PDF
[https://arxiv.org/pdf/1908.04107](https://arxiv.org/pdf/1908.04107)

