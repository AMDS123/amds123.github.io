---
layout: post
title: "Multi-View Image Generation from a Single-View"
date: 2018-02-26 03:55:09
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN Inference
author: Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Jiashi Feng
mathjax: true
---

* content
{:toc}

##### Abstract
This paper addresses a challenging problem -- how to generate multi-view cloth images from only a single view input. To generate realistic-looking images with different views from the input, we propose a new image generation model termed VariGANs that combines the strengths of the variational inference and the Generative Adversarial Networks (GANs). Our proposed VariGANs model generates the target image in a coarse-to-fine manner instead of a single pass which suffers from severe artifacts. It first performs variational inference to model global appearance of the object (e.g., shape and color) and produce a coarse image with a different view. Conditioned on the generated low resolution images, it then proceeds to perform adversarial learning to fill details and generate images of consistent details with the input. Extensive experiments conducted on two clothing datasets, MVC and DeepFashion, have demonstrated that images of a novel view generated by our model are more plausible than those generated by existing approaches, in terms of more consistent global appearance as well as richer and sharper details.

##### Abstract (translated by Google)
本文讨论了一个具有挑战性的问题 - 如何从一个视图输入生成多视图布图像。为了生成具有不同视角的逼真图像，我们提出了一种称为VariGAN的新图像生成模型，它结合了变分推理和生成对抗网络（GAN）的优势。我们提出的VariGANs模型以粗到细的方式生成目标图像，而不是一次通过严重伪影。它首先执行变化推理以模拟对象的全局外观（例如，形状和颜色）并产生具有不同视图的粗略图像。通过对生成的低分辨率图像进行条件处理，然后进行敌对学习以填充细节并生成与输入一致的细节图像。在两个服装数据集MVC和DeepFashion上进行的大量实验已经证明，由我们的模型生成的新视图的图像比现有方法生成的图像更加合理，就全局外观更加一致以及更丰富和更清晰的细节而言。

##### URL
[http://arxiv.org/abs/1704.04886](http://arxiv.org/abs/1704.04886)

##### PDF
[http://arxiv.org/pdf/1704.04886](http://arxiv.org/pdf/1704.04886)

