---
layout: post
title: "Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear Subspace Learning"
date: 2018-07-10 02:29:39
categories: arXiv_CV
tags: arXiv_CV
author: Rudrasis Chakraborty, S&#xf8;ren Hauberg, Baba C. Vemuri
mathjax: true
---

* content
{:toc}

##### Abstract
Principal Component Analysis (PCA) and Kernel Principal Component Analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional Reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by $K$-tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.

##### Abstract (translated by Google)
主成分分析（PCA）和核主成分分析（KPCA）是降低维数的机器学习的基本方法。前者是用于在有限维中找到这种近似的技术，后者通常是无限维再生核希尔伯特空间（RKHS）。在本文中，我们提出了一个几何框架，用于计算两种情况下的主要线性子空间以及稳健的PCA情况，这相当于计算所有子空间空间的内在平均值：格拉斯曼流形。这个流形上的点被定义为跨越$ K $ -tuples观测的子空间。当从高斯分布中绘制时，这些子空间的固有格拉斯曼平均值显示与观测的主要分量一致。我们在RKHS案例中显示了类似的结果，并提供了一种计算投影到这个平均子空间的有效算法。结果是类似于KPCA的方法，其快得多。此外，我们使用我们的几何框架呈现了一个新的KPCA在线版本。我们所有算法的竞争性能都在各种真实和合成数据集上得到证明。

##### URL
[http://arxiv.org/abs/1702.01005](http://arxiv.org/abs/1702.01005)

##### PDF
[http://arxiv.org/pdf/1702.01005](http://arxiv.org/pdf/1702.01005)

