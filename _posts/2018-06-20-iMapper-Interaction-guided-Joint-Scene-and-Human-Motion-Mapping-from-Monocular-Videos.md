---
layout: post
title: "iMapper: Interaction-guided Joint Scene and Human Motion Mapping from Monocular Videos"
date: 2018-06-20 17:47:50
categories: arXiv_CV
tags: arXiv_CV Quantitative Detection Relation
author: Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Yumer, Niloy J. Mitra
mathjax: true
---

* content
{:toc}

##### Abstract
A long-standing challenge in scene analysis is the recovery of scene arrangements under moderate to heavy occlusion, directly from monocular video. While the problem remains a subject of active research, concurrent advances have been made in the context of human pose reconstruction from monocular video, including image-space feature point detection and 3D pose recovery. These methods, however, start to fail under moderate to heavy occlusion as the problem becomes severely under-constrained. We approach the problems differently. We observe that people interact similarly in similar scenes. Hence, we exploit the correlation between scene object arrangement and motions performed in that scene in both directions: first, typical motions performed when interacting with objects inform us about possible object arrangements; and second, object arrangements, in turn, constrain the possible motions. 
 We present iMapper, a data-driven method that focuses on identifying human-object interactions, and jointly reasons about objects and human movement over space-time to recover both a plausible scene arrangement and consistent human interactions. We first introduce the notion of characteristic interactions as regions in space-time when an informative human-object interaction happens. This is followed by a novel occlusion-aware matching procedure that searches and aligns such characteristic snapshots from an interaction database to best explain the input monocular video. Through extensive evaluations, both quantitative and qualitative, we demonstrate that iMapper significantly improves performance over both dedicated state-of-the-art scene analysis and 3D human pose recovery approaches, especially under medium to heavy occlusion.

##### Abstract (translated by Google)
场景分析中长期存在的挑战是直接从单眼视频恢复中度至重度遮挡下的场景布置。虽然问题仍然是一个积极的研究课题，但在单眼视频人体姿态重建的背景下，包括图像空间特征点检测和三维姿态恢复已经取得了进展。然而，这些方法在中度至重度阻塞下开始失败，因为问题严重受到限制。我们以不同的方式处理问题我们观察到人们在类似的场景中以相似的方式进行交互因此，我们利用场景对象排列和在该场景中在两个方向上执行的运动之间的相关性：首先，与对象交互时执行的典型运动告诉我们可能的对象布置;其次，目标安排反过来限制了可能的动作。
 我们介绍iMapper，一种数据驱动的方法，重点是识别人与物体的相互作用，以及关于物体和人类在时空中移动的共同原因，以恢复合理的场景布置和一致的人类互动。我们首先将特征相互作用的概念作为信息人对象相互作用发生时的时空区域。接下来是一种新颖的遮挡感知匹配过程，该过程从交互数据库中搜索并对齐这些特征快照以最好地解释输入单眼视频。通过广泛的定量和定性评估，我们证明iMapper能够显着提高性能，同时还可以在专用的最先进的场景分析和3D人体姿势恢复方法上，尤其是在中等到重度遮挡的情况下。

##### URL
[http://arxiv.org/abs/1806.07889](http://arxiv.org/abs/1806.07889)

##### PDF
[http://arxiv.org/pdf/1806.07889](http://arxiv.org/pdf/1806.07889)

