---
layout: post
title: "Learning Personalized Representation for Inverse Problems in Medical Imaging Using Deep Neural Network"
date: 2018-07-04 20:00:00
categories: arXiv_CV
tags: arXiv_CV Image_Caption Represenation_Learning Optimization
author: Kuang Gong, Kyungsang Kim, Jianan Cui, Ning Guo, Ciprian Catana, Jinyi Qi, Quanzheng Li
mathjax: true
---

* content
{:toc}

##### Abstract
Recently deep neural networks have been widely and successfully applied in computer vision tasks and attracted growing interests in medical imaging. One barrier for the application of deep neural networks to medical imaging is the need of large amounts of prior training pairs, which is not always feasible in clinical practice. In this work we propose a personalized representation learning framework where no prior training pairs are needed, but only the patient's own prior images. The representation is expressed using a deep neural network with the patient's prior images as network input. We then applied this novel image representation to inverse problems in medical imaging in which the original inverse problem was formulated as a constraint optimization problem and solved using the alternating direction method of multipliers (ADMM) algorithm. Anatomically guided brain positron emission tomography (PET) image reconstruction and image denoising were employed as examples to demonstrate the effectiveness of the proposed framework. Quantification results based on simulation and real datasets show that the proposed personalized representation framework outperform other widely adopted methods.

##### Abstract (translated by Google)
最近，深度神经网络已经广泛且成功地应用于计算机视觉任务中，并且引起了对医学成像的日益增长将深度神经网络应用于医学成像的一个障碍是需要大量的先前训练对，这在临床实践中并不总是可行的。在这项工作中，我们提出了一种个性化的表征学习框架，其中不需要先前的训练对，而只需要患者自己的先前图像。使用深度神经网络表示表示，其中患者的先前图像作为网络输入。然后，我们将这种新颖的图像表示应用于医学成像中的逆问题，其中将原始逆问题公式化为约束优化问题，并使用交替方向乘法器（ADMM）算法求解。采用解剖学引导的脑正电子发射断层扫描（PET）图像重建和图像去噪作为实例来证明所提出的框架的有效性。基于模拟和真实数据集的量化结果表明，所提出的个性化表示框架优于其他广泛采用的方法。

##### URL
[http://arxiv.org/abs/1807.01759](http://arxiv.org/abs/1807.01759)

##### PDF
[http://arxiv.org/pdf/1807.01759](http://arxiv.org/pdf/1807.01759)

