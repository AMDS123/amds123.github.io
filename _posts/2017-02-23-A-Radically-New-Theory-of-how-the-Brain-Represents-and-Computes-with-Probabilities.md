---
layout: post
title: "A Radically New Theory of how the Brain Represents and Computes with Probabilities"
date: 2017-02-23 19:16:39
categories: arXiv_CV
tags: arXiv_CV Sparse Embedding Inference Relation
author: Gerard Rinkus
mathjax: true
---

* content
{:toc}

##### Abstract
The brain is believed to implement probabilistic reasoning and to represent information via population, or distributed, coding. Most previous population-based probabilistic (PPC) theories share several basic properties: 1) continuous-valued neurons; 2) fully(densely)-distributed codes, i.e., all(most) units participate in every code; 3) graded synapses; 4) rate coding; 5) units have innate unimodal tuning functions (TFs); 6) intrinsically noisy units; and 7) noise/correlation is considered harmful. We present a radically different theory that assumes: 1) binary units; 2) only a small subset of units, i.e., a sparse distributed code (SDC) (cell assembly, ensemble), comprises any individual code; 3) binary synapses; 4) signaling formally requires only single (first) spikes; 5) units initially have completely flat TFs (all weights zero); 6) units are not inherently noisy; but rather 7) noise is a resource generated/used to cause similar inputs to map to similar codes, controlling a tradeoff between storage capacity and embedding the input space statistics in the pattern of intersections over stored codes, indirectly yielding correlation patterns. The theory, Sparsey, was introduced 20 years ago as a canonical cortical circuit/algorithm model, but not elaborated as an alternative to PPC theories. Here, we show that the active SDC simultaneously represents both the most similar/likely input and the coarsely-ranked distribution over all stored inputs (hypotheses). Crucially, Sparsey's code selection algorithm (CSA), used for both learning and inference, achieves this with a single pass over the weights for each successive item of a sequence, thus performing spatiotemporal pattern learning/inference with a number of steps that remains constant as the number of stored items increases. We also discuss our approach as a radically new implementation of graphical probability modeling.

##### Abstract (translated by Google)
大脑被认为是实施概率推理，并通过人口，或分布式，编码来表示信息。大多数先前的基于人口的概率（PPC）理论共享几个基本属性：1）连续值神经元; 2）完全（密集）分发代码，即所有（大多数）单元参与每个代码; 3）渐变突触; 4）码率编码; 5）单元具有固有的单峰调谐功能（TF）; 6）本质上嘈杂的单位; 7）噪声/相关性被认为是有害的。我们提出一个完全不同的理论，假设：1）二元单位; 2）只有一小部分单元，即稀疏分布式代码（SDC）（单元组合，整体）包括任何单独的代码; 3）二元突触; 4）信号正式要求只有单一（第一）尖峰; 5）单位最初具有完全平坦的TF（所有权重为零）; 6）单位不是固有的嘈杂;而是7）噪声是产生/用来使类似输入映射到相似代码的资源，控制存储容量之间的折衷，并将输入空间统计量嵌入到存储代码的交叉点模式中，从而间接地产生相关模式。 Sparsey理论在20年前被引入，作为典型的皮质电路/算法模型，但是没有被阐述为PPC理论的替代品。在这里，我们显示主动SDC同时代表最相似/可能的输入和所有存储的输入（假设）的粗略排序分布。最重要的是，用于学习和推理的Sparsey的代码选择算法（CSA）通过对序列中的每个连续项目的权重进行一次遍历来实现，从而执行时间模式学习/推理，其中保持常数的步骤为存储的项目数量增加。我们也讨论我们的方法，作为图形概率建模的全新实现。

##### URL
[https://arxiv.org/abs/1701.07879](https://arxiv.org/abs/1701.07879)

##### PDF
[https://arxiv.org/pdf/1701.07879](https://arxiv.org/pdf/1701.07879)

