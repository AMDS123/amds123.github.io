---
layout: post
title: "Investigating how well contextual features are captured by bi-directional recurrent neural network models"
date: 2017-11-29 16:02:36
categories: arXiv_CL
tags: arXiv_CL RNN
author: Kushal Chawla, Sunil Kumar Sahu, Ashish Anand
mathjax: true
---

* content
{:toc}

##### Abstract
Learning algorithms for natural language processing (NLP) tasks traditionally rely on manually defined relevant contextual features. On the other hand, neural network models using an only distributional representation of words have been successfully applied for several NLP tasks. Such models learn features automatically and avoid explicit feature engineering. Across several domains, neural models become a natural choice specifically when limited characteristics of data are known. However, this flexibility comes at the cost of interpretability. In this paper, we define three different methods to investigate ability of bi-directional recurrent neural networks (RNNs) in capturing contextual features. In particular, we analyze RNNs for sequence tagging tasks. We perform a comprehensive analysis on general as well as biomedical domain datasets. Our experiments focus on important contextual words as features, which can easily be extended to analyze various other feature types. We also investigate positional effects of context words and show how the developed methods can be used for error analysis.

##### Abstract (translated by Google)
自然语言处理（NLP）任务的学习算法传统上依赖于手动定义的相关上下文特征。另一方面，神经网络模型使用单词的分布表示已经成功应用于多个NLP任务。这些模型自动学习功能，避免显式的特征工程。在几个领域中，神经模型特别是在有限的数据特征已知的情况下成为自然的选择。然而，这种灵活性是以可解释性为代价的。在本文中，我们定义了三种不同的方法来研究双向递归神经网络（RNN）捕获上下文特征的能力。特别是，我们分析序列标记任务的RNN。我们对一般以及生物医学领域数据集进行综合分析。我们的实验侧重于重要的上下文单词作为功能，可以很容易地扩展到分析各种其他功能类型。我们还研究上下文单词的位置效应，并展示如何使用开发的方法进行错误分析。

##### URL
[https://arxiv.org/abs/1709.00659](https://arxiv.org/abs/1709.00659)

##### PDF
[https://arxiv.org/pdf/1709.00659](https://arxiv.org/pdf/1709.00659)

