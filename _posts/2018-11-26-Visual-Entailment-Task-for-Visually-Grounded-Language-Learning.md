---
layout: post
title: "Visual Entailment Task for Visually-Grounded Language Learning"
date: 2018-11-26 18:37:25
categories: arXiv_CV
tags: arXiv_CV QA Inference VQA
author: Ning Xie, Farley Lai, Derek Doran, Asim Kadav
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a new inference task - Visual Entailment (VE) - which differs from traditional Textual Entailment (TE) tasks whereby a premise is defined by an image, rather than a natural language sentence as in TE tasks. A novel dataset SNLI-VE is proposed for VE tasks based on the Stanford Natural Language Inference corpus and Flickr30K. We introduce a differentiable architecture called the Explainable Visual Entailment model (EVE) to tackle the VE problem. EVE and several other state-of-the-art visual question answering (VQA) based models are evaluated on the SNLI-VE dataset, facilitating grounded language understanding and providing insights on how modern VQA based models perform.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1811.10582](https://arxiv.org/abs/1811.10582)

##### PDF
[https://arxiv.org/pdf/1811.10582](https://arxiv.org/pdf/1811.10582)

