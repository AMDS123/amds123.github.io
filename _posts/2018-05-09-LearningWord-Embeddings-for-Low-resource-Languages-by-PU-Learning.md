---
layout: post
title: "LearningWord Embeddings for Low-resource Languages by PU Learning"
date: 2018-05-09 04:05:37
categories: arXiv_CL
tags: arXiv_CL Sparse Embedding
author: Chao Jiang, Hsiang-Fu Yu, Cho-Jui Hsieh, Kai-Wei Chang
mathjax: true
---

* content
{:toc}

##### Abstract
Word embedding is a key component in many downstream applications in processing natural languages. Existing approaches often assume the existence of a large collection of text for learning effective word embedding. However, such a corpus may not be available for some low-resource languages. In this paper, we study how to effectively learn a word embedding model on a corpus with only a few million tokens. In such a situation, the co-occurrence matrix is sparse as the co-occurrences of many word pairs are unobserved. In contrast to existing approaches often only sample a few unobserved word pairs as negative samples, we argue that the zero entries in the co-occurrence matrix also provide valuable information. We then design a Positive-Unlabeled Learning (PU-Learning) approach to factorize the co-occurrence matrix and validate the proposed approaches in four different languages.

##### Abstract (translated by Google)
字嵌入是处理自然语言的许多下游应用程序中的关键组件。现有的方法通常假定存在大量用于学习有效词嵌入的文本集合。但是，这样的语料库可能不适用于某些低资源语言。在本文中，我们研究如何在仅有几百万个令牌的语料库上有效地学习单词嵌入模型。在这种情况下，共生矩阵是稀疏的，因为许多单词对的共现不被发现。与现有方法相反，通常只将少数未观察到的单词对作为负样本，我们认为共现矩阵中的零条目也提供了有价值的信息。然后，我们设计一个正向 - 未标记学习（PU-Learning）方法来分解共生矩阵，并用四种不同的语言验证提出的方法。

##### URL
[http://arxiv.org/abs/1805.03366](http://arxiv.org/abs/1805.03366)

##### PDF
[http://arxiv.org/pdf/1805.03366](http://arxiv.org/pdf/1805.03366)

