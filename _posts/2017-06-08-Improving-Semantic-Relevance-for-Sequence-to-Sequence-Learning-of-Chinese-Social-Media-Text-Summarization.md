---
layout: post
title: "Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization"
date: 2017-06-08 07:05:56
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Shuming Ma, Xu Sun, Jingjing Xu, Houfeng Wang, Wenjie Li, Qi Su
mathjax: true
---

* content
{:toc}

##### Abstract
Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.

##### Abstract (translated by Google)
目前的中国社交媒体文本摘要模型是基于编解码器框架。尽管其生成的摘要与原文相似，但它们的语义相关性较低。在这项工作中，我们的目标是提高中文社交媒体摘要的源文本和摘要之间的语义相关性。我们引入基于语义相关性的神经模型来鼓励文本和摘要之间的高度语义相似性。在我们的模型中，源文本由门控注意编码器表示，而汇总表示由解码器生成。此外，在训练期间，表示之间的相似度得分最大化。我们的实验表明，该模型胜过社交媒体语料库上的基线系统。

##### URL
[https://arxiv.org/abs/1706.02459](https://arxiv.org/abs/1706.02459)

##### PDF
[https://arxiv.org/pdf/1706.02459](https://arxiv.org/pdf/1706.02459)

