---
layout: post
title: "Towards an Understanding of Neural Networks in Natural-Image Spaces"
date: 2019-02-11 05:18:00
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Yifei Fan, Anthony Yezzi
mathjax: true
---

* content
{:toc}

##### Abstract
Two major uncertainties, dataset bias and adversarial examples, prevail in state-of-the-art AI algorithms with deep neural networks. In this paper, we present an intuitive explanation for these issues as well as an interpretation of the performance of deep networks in a natural-image space. The explanation consists of two parts: the philosophy of neural networks and a hypothetical model of natural-image spaces. Following the explanation, we 1) demonstrate that the values of training samples differ, 2) provide incremental boost to the accuracy of a CIFAR-10 classifier by introducing an additional "random-noise" category during training, 3) alleviate over-fitting thereby enhancing the robustness against adversarial examples by detecting and excluding illusive training samples that are consistently misclassified. Our overall contribution is therefore twofold. First, while most existing algorithms treat data equally and have a strong appetite for more data, we demonstrate in contrast that an individual datum can sometimes have disproportionate and counterproductive influence and that it is not always better to train neural networks with more data. Next, we consider more thoughtful strategies by taking into account the geometric and topological properties of natural-image spaces to which deep networks are applied.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1801.09097](http://arxiv.org/abs/1801.09097)

##### PDF
[http://arxiv.org/pdf/1801.09097](http://arxiv.org/pdf/1801.09097)

