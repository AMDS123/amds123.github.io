---
layout: post
title: "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
date: 2018-09-18 21:17:15
categories: arXiv_CL
tags: arXiv_CL Knowledge Transfer_Learning
author: Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman
mathjax: true
---

* content
{:toc}

##### Abstract
For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.

##### Abstract (translated by Google)
对于自然语言理解（NLU）技术而言，无论是实际上还是作为科学研究对象，它都必须是通用的：它必须能够以不完全适合任何特定任务或数据集的方式处理语言。为了实现这一目标，我们引入了通用语言理解评估基准（GLUE），这是一种用于评估和分析各种现有NLU任务中模型性能的工具。 GLUE与模型无关，但它鼓励跨任务共享知识，因为某些任务的训练数据非常有限。我们进一步提供手工制作的诊断测试套件，可对NLU模型进行详细的语言分析。我们基于当前的多任务和转移学习方法评估基线，发现它们不会立即对每项任务单独训练模型的总体性能进行实质性改进，这表明在开发通用和强大的NLU系统方面存在改进的空间。

##### URL
[http://arxiv.org/abs/1804.07461](http://arxiv.org/abs/1804.07461)

##### PDF
[http://arxiv.org/pdf/1804.07461](http://arxiv.org/pdf/1804.07461)

