---
layout: post
title: "Bi-directional Attention with Agreement for Dependency Parsing"
date: 2016-09-22 08:52:31
categories: arXiv_CL
tags: arXiv_CL Attention Embedding Prediction
author: Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao, Li Deng
mathjax: true
---

* content
{:toc}

##### Abstract
We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.

##### Abstract (translated by Google)
我们为依赖分析开发了一个新的双向注意模型，从前向和后向解析方向学习了对词条预测的一致性。每个方向的解析过程被制定为顺序查询存储连续词条嵌入的存储器组件。所提出的解析器利用{\ it soft}词条嵌入，允许模型隐式捕获高阶解析历史，而不会显着增加计算复杂度。我们在CoNLL 2006共享任务中对英语，中文和其他12种语言进行了实验，结果表明，该模型能够实现6种语言的最新的无标签附件分数。

##### URL
[https://arxiv.org/abs/1608.02076](https://arxiv.org/abs/1608.02076)

##### PDF
[https://arxiv.org/pdf/1608.02076](https://arxiv.org/pdf/1608.02076)

