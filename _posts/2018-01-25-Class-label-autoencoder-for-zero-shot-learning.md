---
layout: post
title: "Class label autoencoder for zero-shot learning"
date: 2018-01-25 08:00:36
categories: arXiv_CV
tags: arXiv_CV Embedding Classification Relation
author: Guangfeng Lin, Caixia Fan, Wanjun Chen, Yajun Chen, Fan Zhao
mathjax: true
---

* content
{:toc}

##### Abstract
Existing zero-shot learning (ZSL) methods usually learn a projection function between a feature space and a semantic embedding space(text or attribute space) in the training seen classes or testing unseen classes. However, the projection function cannot be used between the feature space and multi-semantic embedding spaces, which have the diversity characteristic for describing the different semantic information of the same class. To deal with this issue, we present a novel method to ZSL based on learning class label autoencoder (CLA). CLA can not only build a uniform framework for adapting to multi-semantic embedding spaces, but also construct the encoder-decoder mechanism for constraining the bidirectional projection between the feature space and the class label space. Moreover, CLA can jointly consider the relationship of feature classes and the relevance of the semantic classes for improving zero-shot classification. The CLA solution can provide both unseen class labels and the relation of the different classes representation(feature or semantic information) that can encode the intrinsic structure of classes. Extensive experiments demonstrate the CLA outperforms state-of-art methods on four benchmark datasets, which are AwA, CUB, Dogs and ImNet-2.

##### Abstract (translated by Google)
现有的零点学习（ZSL）方法通常学习特征空间和语义嵌入空间（文本或属性空间）之间的投影函数在训练看到的类或测试看不见的类。然而，在特征空间和多义语义嵌入空间之间不能使用投影函数，它们具有描述同一类别不同语义信息的多样性特征。为了解决这个问题，我们提出了一种基于学习类标签自动编码器（CLA）的ZSL新方法。 CLA不仅可以构建一个适应多语义嵌入空间的统一框架，而且还可以构建用于约束特征空间与类标签空间之间双向投影的编码 - 解码机制。而且，CLA可以共同考虑特征类与语义类的相关性来改善零点分类。 CLA解决方案既可以提供看不见的类标签，也可以提供可以对类的内部结构进行编码的不同类表示（特征或语义信息）的关系。广泛的实验证明CLA在四个基准数据集（AwA，CUB，Dogs和ImNet-2）上优于最先进的方法。

##### URL
[http://arxiv.org/abs/1801.08301](http://arxiv.org/abs/1801.08301)

##### PDF
[http://arxiv.org/pdf/1801.08301](http://arxiv.org/pdf/1801.08301)

