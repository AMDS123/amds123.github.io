---
layout: post
title: "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution"
date: 2017-06-06 02:47:41
categories: arXiv_CL
tags: arXiv_CL GAN
author: Ting Liu, Yiming Cui, Qingyu Yin, Weinan Zhang, Shijin Wang, Guoping Hu
mathjax: true
---

* content
{:toc}

##### Abstract
Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.

##### Abstract (translated by Google)
大多数现有的零代词解决方法主要依赖于注释数据，这通常由共享任务组织者发布。因此，注释数据的缺失成为零代词解析任务进程中的主要障碍。另外，花费人力来标记数据以获得更好的性能也是昂贵的。为了缓解上述问题，本文提出了一种简单而新颖的自动生成零代词解析的大规模伪训练数据的方法。此外，我们成功地将填充式阅读理解神经网络模型转化为零代词解析任务，并提出了两步式训练机制来克服伪训练数据与实际训练数据之间的差距。实验结果表明，所提出的方法显着优于最先进的系统，在OntoNotes 5.0数据上绝对改善了3.1％F-score。

##### URL
[https://arxiv.org/abs/1606.01603](https://arxiv.org/abs/1606.01603)

##### PDF
[https://arxiv.org/pdf/1606.01603](https://arxiv.org/pdf/1606.01603)

