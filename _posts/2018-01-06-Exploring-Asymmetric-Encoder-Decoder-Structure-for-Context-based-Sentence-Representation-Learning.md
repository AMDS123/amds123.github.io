---
layout: post
title: "Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning"
date: 2018-01-06 00:12:18
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning RNN
author: Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, Virginia R. de Sa
mathjax: true
---

* content
{:toc}

##### Abstract
Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder. We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.

##### Abstract (translated by Google)
语境信息在人类语言理解中起着重要的作用，对于机器学习语言的矢量表征也是很有用的。在本文中，我们探索了一个非对称的编码器 - 解码器结构，用于无监督的基于上下文的句子表示学习。因此，我们建立了一个带RNN编码器和CNN解码器的编码器 - 解码器架构。我们进一步结合了一套有效的设计来显着提高模型效率，同时实现更好的性能。我们的模型是在两个不同的大型未标记语料库上进行训练的，在这两种情况下，可转移性都是在一组下游语言理解任务上评估的。我们凭经验表明，我们的模型简单快捷，同时产生丰富的句子表示，在下游任务中表现突出。

##### URL
[http://arxiv.org/abs/1710.10380](http://arxiv.org/abs/1710.10380)

##### PDF
[http://arxiv.org/pdf/1710.10380](http://arxiv.org/pdf/1710.10380)

