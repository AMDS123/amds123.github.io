---
layout: post
title: "A Novel Neural Sequence Model with Multiple Attentions for Word Sense Disambiguation"
date: 2018-09-04 16:28:36
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention
author: Mahtab Ahmed, Muhammad Rifayat Samee, Robert E. Mercer
mathjax: true
---

* content
{:toc}

##### Abstract
Word sense disambiguation (WSD) is a well researched problem in computational linguistics. Different research works have approached this problem in different ways. Some state of the art results that have been achieved for this problem are by supervised models in terms of accuracy, but they often fall behind flexible knowledge-based solutions which use engineered features as well as human annotators to disambiguate every target word. This work focuses on bridging this gap using neural sequence models incorporating the well-known attention mechanism. The main gist of our work is to combine multiple attentions on different linguistic features through weights and to provide a unified framework for doing this. This weighted attention allows the model to easily disambiguate the sense of an ambiguous word by attending over a suitable portion of a sentence. Our extensive experiments show that multiple attention enables a more versatile encoder-decoder model leading to state of the art results.

##### Abstract (translated by Google)
词义消歧（WSD）是计算语言学中一个研究得很好的问题。不同的研究工作以不同的方式解决了这个问题。对于该问题已经实现的一些现有技术结果是在监督模型的准确性方面，但是它们通常落后于基于知识的灵活解决方案，其使用工程特征以及人类注释器来消除每个目标词的歧义。这项工作的重点是利用结合了众所周知的注意机制的神经序列模型弥合这一差距。我们工作的主要目的是通过权重结合不同语言特征的多个注意事项，并为此提供统一的框架。这种加权注意允许模型通过参与句子的适当部分来容易地消除模糊词的意义。我们的广泛实验表明，多种注意力使得更通用的编码器 - 解码器模型能够产生最先进的结果。

##### URL
[http://arxiv.org/abs/1809.01074](http://arxiv.org/abs/1809.01074)

##### PDF
[http://arxiv.org/pdf/1809.01074](http://arxiv.org/pdf/1809.01074)

