---
layout: post
title: "Visual Explanations from Hadamard Product in Multimodal Deep Networks"
date: 2017-12-18 02:37:20
categories: arXiv_CV
tags: arXiv_CV Attention VQA
author: Jin-Hwa Kim, Byoung-Tak Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
The visual explanation of learned representation of models helps to understand the fundamentals of learning. The attentional models of previous works used to visualize the attended regions over an image or text using their learned weights to confirm their intended mechanism. Kim et al. (2016) show that the Hadamard product in multimodal deep networks, which is well-known for the joint function of visual question answering tasks, implicitly performs an attentional mechanism for visual inputs. In this work, we extend their work to show that the Hadamard product in multimodal deep networks performs not only for visual inputs but also for textual inputs simultaneously using the proposed gradient-based visualization technique. The attentional effect of Hadamard product is visualized for both visual and textual inputs by analyzing the two inputs and an output of the Hadamard product with the proposed method and compared with learned attentional weights of a visual question answering model.

##### Abstract (translated by Google)
学习模型的表达的视觉解释有助于理解学习的基础。以前的作品的注意模型用于使用他们学习的权重对图像或文本上的出席区域进行可视化以确认其预期的机制。 Kim等人（2016）表明，多模式深度网络中的哈达玛积（Hadamard product），以视觉问题应答任务的共同作用而着称，隐含地实现了视觉输入的注意机制。在这项工作中，我们扩展他们的工作，以表明多模深网络中的Hadamard产品不仅可以用于视觉输入，而且可以同时使用提出的基于梯度的可视化技术来处理文本输入。 Hadamard产品的注意效果通过分析两个输入和用所提出的方法的Hadamard产品的输出，并与视觉问题应答模型的学习注意力权重进行比较，用于视觉和文本输入的可视化。

##### URL
[http://arxiv.org/abs/1712.06228](http://arxiv.org/abs/1712.06228)

##### PDF
[http://arxiv.org/pdf/1712.06228](http://arxiv.org/pdf/1712.06228)

