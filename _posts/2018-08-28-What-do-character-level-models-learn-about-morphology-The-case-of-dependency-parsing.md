---
layout: post
title: "What do character-level models learn about morphology? The case of dependency parsing"
date: 2018-08-28 09:02:48
categories: arXiv_CL
tags: arXiv_CL Face
author: Clara Vania, Andreas Grivas, Adam Lopez
mathjax: true
---

* content
{:toc}

##### Abstract
When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn morphology. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying morphological typologies. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.

##### Abstract (translated by Google)
在使用神经模型解析形态丰富的语言时，在字符级别对输入进行建模是有益的，并且声称这是因为字符级模型学习形态学。我们通过将字符级模型与oracle进行比较来测试这些声明，并且可以访问具有不同形态类型的12种语言的显式形态分析。我们的结果突出了角色模型的许多优点，但也表明他们在消除某些词语方面很差，特别是面对案例融合。然后，我们证明显式建模形态学案例改进了我们的最佳模型，表明角色级模型可以从显式形态建模的目标形式中受益。

##### URL
[http://arxiv.org/abs/1808.09180](http://arxiv.org/abs/1808.09180)

##### PDF
[http://arxiv.org/pdf/1808.09180](http://arxiv.org/pdf/1808.09180)

