---
layout: post
title: "Sample Efficient Deep Reinforcement Learning for Dialogue Systems with Large Action Spaces"
date: 2018-02-11 15:37:37
categories: arXiv_AI
tags: arXiv_AI Attention Reinforcement_Learning Deep_Learning
author: Gell&#xe9;rt Weisz, Pawe&#x142; Budzianowski, Pei-Hao Su, Milica Ga&#x161;i&#x107;
mathjax: true
---

* content
{:toc}

##### Abstract
In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimisation task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments however are fully observable and have a relatively small action set so in this paper we examine the application of ACER to dialogue policy optimisation. We show that this method beats the current state-of-the-art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state-of-the-art.

##### Abstract (translated by Google)
在口语对话系统中，我们的目标是部署人工智能来构建可与人交谈的自动对话代理。这项工作的一部分是政策优化任务，该任务尝试寻找描述如何对人类作出反应的政策，以功能的形式考虑当前的对话状态并返回系统的响应。在本文中，我们调查深层强化学习方法来解决这个问题。特别关注演员评论方法，带有经验回放的非政策强化学习，以及旨在减少估计人员偏差和方差的各种方法。结合时，这些方法产生了先前提出的ACER算法，该算法在游戏环境中提供了有竞争力的结果。然而，这些环境是完全可观察的，并且具有相对较小的动作集，所以在本文中我们考察了ACER在对话策略优化中的应用。我们表明，这种方法击败了目前最先进的语音对话系统的深度学习方法。这不仅会导致更快的样本更高效的算法，而且还使我们能够在比以前更困难的环境中应用算法。因此，我们在非常大的行动空间中尝试学习，其行动比先前考虑的多两个数量级。我们发现ACER的培训速度比目前最先进的技术要快得多。

##### URL
[http://arxiv.org/abs/1802.03753](http://arxiv.org/abs/1802.03753)

##### PDF
[http://arxiv.org/pdf/1802.03753](http://arxiv.org/pdf/1802.03753)

