---
layout: post
title: "Adversarial Examples for Evaluating Reading Comprehension Systems"
date: 2017-07-23 18:26:29
categories: arXiv_CL
tags: arXiv_CL Adversarial
author: Robin Jia, Percy Liang
mathjax: true
---

* content
{:toc}

##### Abstract
Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of $75\%$ F1 score to $36\%$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to $7\%$. We hope our insights will motivate the development of new models that understand language more precisely.

##### Abstract (translated by Google)
标准的准确性指标表明，阅读理解系统正在快速发展，但这些系统真正理解语言的程度仍不清楚。为了奖励具有真正语言理解能力的系统，我们提出了斯坦福问题回答数据集（SQUAD）的对抗评估方案。我们的方法测试系统是否可以回答关于含有敌对插入句子的段落，这些段落是自动生成的，以分散计算机系统而不改变正确答案或误导人类。在这种对抗的背景下，十六个已发布车型的准确性从平均$ 75％F1 $降到$ 36 $％$;当对手被允许添加不合语法序列的单词时，四个模型的平均精确度进一步降低到$ 7 \％$。我们希望我们的见解能够激发更精确理解语言的新模型的发展。

##### URL
[https://arxiv.org/abs/1707.07328](https://arxiv.org/abs/1707.07328)

##### PDF
[https://arxiv.org/pdf/1707.07328](https://arxiv.org/pdf/1707.07328)

