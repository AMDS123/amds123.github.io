---
layout: post
title: "Deep Residual Learning for Image Recognition"
date: 2015-12-10 19:51:55
categories: arXiv_CV
tags: arXiv_CV Object_Detection Segmentation Classification Detection Recognition
author: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.

##### Abstract (translated by Google)
更深的神经网络更难以训练。我们提出了一个残留的学习框架，以减轻网络的培训，比以前使用的网络要深得多。我们明确地将层次重新定义为参照层次输入学习残差函数，而不是学习未引用的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从深度上增加精度。在ImageNet数据集上，我们评估了深度达152层的网络 - 比VGG网络深8倍，但复杂度较低。这些残余网络的集合在ImageNet测试集上达到了3.57％的误差。这个结果在ILSVRC 2015分类任务中赢得了第一名。我们还分析了100和1000层的CIFAR-10。表示的深度对于许多视觉识别任务来说是至关重要的。仅仅由于我们极其深刻的表示，我们在COCO目标检测数据集上获得了28％的相对改进。深度残留网络是我们提交ILSVRC＆COCO 2015比赛的基础，在这里我们还赢得了ImageNet检测，ImageNet本地化，COCO检测和COCO分割任务的第一名。

##### URL
[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)

##### PDF
[https://arxiv.org/pdf/1512.03385](https://arxiv.org/pdf/1512.03385)

