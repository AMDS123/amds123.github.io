---
layout: post
title: "Multi-domain Neural Network Language Generation for Spoken Dialogue Systems"
date: 2016-03-03 19:49:32
categories: arXiv_SD
tags: arXiv_SD RNN
author: Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, Steve Young
mathjax: true
---

* content
{:toc}

##### Abstract
Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain.

##### Abstract (translated by Google)
从有限域自然语言生成（NLG）转向开放域是困难的，因为语义输入组合的数量随着域的数量呈指数增长。因此，利用现有资源并利用域之间的相似性来促进域名适应是非常重要的。在本文中，我们提出了一种通过多个适应步骤来训练多域递归神经网络（RNN）语言生成器的程序。在这个过程中，一个模型首先训练从一个域外数据集合成的假冒数据，然后用一个有区别的目标函数对一小部分域内话语进行微调。基于语料库的评估结果表明，所提出的程序可以在BLEU分数和时隙错误率方面实现竞争性能，同时显着减少在新的，看不见的领域中训练发电机所需的数据。在主观测试中，人类评审确认，当只有少量的数据在域中可用时，程序大大提高了发电机的性能。

##### URL
[https://arxiv.org/abs/1603.01232](https://arxiv.org/abs/1603.01232)

##### PDF
[https://arxiv.org/pdf/1603.01232](https://arxiv.org/pdf/1603.01232)

