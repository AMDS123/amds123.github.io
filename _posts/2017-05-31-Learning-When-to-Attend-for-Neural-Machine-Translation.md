---
layout: post
title: "Learning When to Attend for Neural Machine Translation"
date: 2017-05-31 16:05:49
categories: arXiv_CL
tags: arXiv_CL Attention
author: Junhui Li, Muhua Zhu
mathjax: true
---

* content
{:toc}

##### Abstract
In the past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline.

##### Abstract (translated by Google)
在过去的几年中，注意机制已经成为端到端神经机器翻译模型不可缺少的组成部分。然而，以往的注意模型在预测目标词时，往往指的是某些源词，与某些目标词没有相应的源词相矛盾。受此观察的启发，我们提出了一种新颖的关注模型，它能够确定解码器何时应该关注源词以及何时不应该关注。 NIST中英文翻译任务的实验结果表明，新模型在一个最先进的基线上达到了0.8 BLEU分数的提高。

##### URL
[https://arxiv.org/abs/1705.11160](https://arxiv.org/abs/1705.11160)

##### PDF
[https://arxiv.org/pdf/1705.11160](https://arxiv.org/pdf/1705.11160)

