---
layout: post
title: "FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence"
date: 2017-02-03 07:36:44
categories: arXiv_CV
tags: arXiv_CV CNN
author: Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul Jeon, Stephen Lin, Kwanghoon Sohn
mathjax: true
---

* content
{:toc}

##### Abstract
We present a descriptor, called fully convolutional self-similarity (FCSS), for dense semantic correspondence. To robustly match points among different instances within the same object class, we formulate FCSS using local self-similarity (LSS) within a fully convolutional network. In contrast to existing CNN-based descriptors, FCSS is inherently insensitive to intra-class appearance variations because of its LSS-based structure, while maintaining the precise localization ability of deep neural networks. The sampling patterns of local structure and the self-similarity measure are jointly learned within the proposed network in an end-to-end and multi-scale manner. As training data for semantic correspondence is rather limited, we propose to leverage object candidate priors provided in existing image datasets and also correspondence consistency between object pairs to enable weakly-supervised learning. Experiments demonstrate that FCSS outperforms conventional handcrafted descriptors and CNN-based descriptors on various benchmarks.

##### Abstract (translated by Google)
我们提出一个描述符，称为完全卷积自相似（FCSS），用于密集的语义对应。为了在同一对象类中的不同实例之间强有力地匹配点，我们使用完全卷积网络中的局部自相似性（LSS）来制定FCSS。与现有的基于CNN的描述符相比，由于其基于LSS的结构，FCSS本质上对类内外观变化不敏感，同时保持深度神经网络的精确定位能力。提出的网络中，端到端，多尺度的方式共同学习局部结构的抽样模式和自相似性度量。由于语义对应的训练数据相当有限，我们建议利用现有图像数据集中提供的对象候选先验以及对象对之间的对应一致性来实现弱监督学习。实验表明，在各种基准测试中，FCSS优于传统的手工描述符和基于CNN的描述符。

##### URL
[https://arxiv.org/abs/1702.00926](https://arxiv.org/abs/1702.00926)

##### PDF
[https://arxiv.org/pdf/1702.00926](https://arxiv.org/pdf/1702.00926)

