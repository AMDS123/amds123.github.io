---
layout: post
title: "Stress Test Evaluation for Natural Language Inference"
date: 2018-06-02 19:14:39
categories: arXiv_CL
tags: arXiv_CL Inference
author: Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig
mathjax: true
---

* content
{:toc}

##### Abstract
Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed "stress tests" that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area.

##### Abstract (translated by Google)
自然语言推理（NLI）是确定自然语言假设是否可以以合理的方式从给定前提推断的任务。 NLI被提出作为自然语言理解的基准任务。现有模型在NLI的标准数据集上表现良好，在不同类型的文本中实现了令人印象深刻的结果。然而，这些模型在多大程度上理解句子的语义内容还不清楚。在这项工作中，我们提出了一种由自动构建的“压力测试”组成的评估方法，它使我们能够检查系统是否有能力做出真正的推理决策。我们在这些压力测试中对六种句子编码器模型的评估揭示了这些模型在挑战语言现象方面的长处和弱点，并为这一领域的未来工作提出了重要的方向。

##### URL
[http://arxiv.org/abs/1806.00692](http://arxiv.org/abs/1806.00692)

##### PDF
[http://arxiv.org/pdf/1806.00692](http://arxiv.org/pdf/1806.00692)

