---
layout: post
title: "Improving Reinforcement Learning Based Image Captioning with Natural Language Prior"
date: 2018-09-13 17:21:56
categories: arXiv_CV
tags: arXiv_CV Image_Caption Reinforcement_Learning Caption Quantitative
author: Tszhang Guo, Shiyu Chang, Mo Yu, Kun Bai
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.

##### Abstract (translated by Google)
最近，通过直接优化用于测试的度量，强化学习（RL）方法已经证明了图像字幕的高级性能。然而，这种形状的奖励引入了学习偏见，这降低了生成文本的可读性。此外，大样本空间使训练不稳定且缓慢。为了缓解这些问题，我们提出了一种简单的连贯解决方案，它使用n-gram语言优先约束动作空间。基准测试的定量和定性评估表明，RL具有简单的附加模块，在可读性和收敛速度方面均优于其对应物。人体评估结果表明，我们的模型更具人性化和优雅。在接受该文件后，该实施将公开。

##### URL
[http://arxiv.org/abs/1809.06227](http://arxiv.org/abs/1809.06227)

##### PDF
[http://arxiv.org/pdf/1809.06227](http://arxiv.org/pdf/1809.06227)

