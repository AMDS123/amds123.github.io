---
layout: post
title: "Learning Deep Disentangled Embeddings with the F-Statistic Loss"
date: 2018-02-14 20:28:38
categories: arXiv_AI
tags: arXiv_AI Embedding
author: Karl Ridgeway, Michael C. Mozer
mathjax: true
---

* content
{:toc}

##### Abstract
Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm for discovering disentangled representations of class structure; these representations reveal the underlying factors that jointly determine class. We propose and evaluate a novel loss function based on the $F$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding procedure matches or beats state-of-the-art procedures on deep embeddings, as evaluated by performance on recall@$k$ and few-shot learning tasks. To evaluate alternative approaches on disentangling, we formalize two key properties of a disentangled representation: modularity and explicitness. By these criteria, our procedure yields disentangled representations, whereas traditional procedures fail. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.

##### Abstract (translated by Google)
深度嵌入方法的目的是发现一个域的表示，使域的类结构更清晰。解构方法的目的是明确组成或因子结构。我们结合了这两条主动但独立的研究路线，并提出了一种新的范式来发现类结构的解体表征;这些表述揭示了共同决定阶级的基本因素。我们基于$ F $统计量提出并评估了一种新的损失函数，它描述了两个或更多分布的分离。通过确保不同的类在嵌入维度的子集上很好地分离，我们获得对于少量学习有用的嵌入。通过不要求在所有维度上分离，我们鼓励发现解开表示。我们的嵌入过程匹配或击败深度嵌入的最先进的程序，通过回忆@ $ k $的表现和少量学习任务进行评估。为了评估替代解决方法，我们将解构表示的两个关键属性形式化：模块化和显式。通过这些标准，我们的程序产生了解开的表示，而传统程序失败。我们工作的目标是获得更多的可解释，可操作和可概括的概念和类别深层表示。

##### URL
[http://arxiv.org/abs/1802.05312](http://arxiv.org/abs/1802.05312)

##### PDF
[http://arxiv.org/pdf/1802.05312](http://arxiv.org/pdf/1802.05312)

