---
layout: post
title: "CIFT: Crowd-Informed Fine-Tuning to Improve Machine Learning Ability"
date: 2017-06-28 23:59:15
categories: arXiv_SD
tags: arXiv_SD Knowledge
author: John P. Lalor, Hao Wu, Hong Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose Crowd-Informed Fine-Tuning (CIFT) as a new training process, where a pre-trained model is fine-tuned with a specialized supplemental training set obtained via IRT model-fitting on a large set of crowdsourced response patterns. With CIFT we can leverage the specialized set of data obtained through IRT to inform parameter tuning in DNNs. We experiment with two loss functions in CIFT to represent (i) memorization of fine-tuning items and (ii) learning a probability distribution over potential labels that is similar to the crowdsourced distribution over labels to simulate crowd knowledge. Our results show that CIFT improves ability for a state-of-the art DNN model for Recognizing Textual Entailment (RTE) tasks and is generalizable to a large-scale RTE test set.

##### Abstract (translated by Google)
项目反应理论（IRT）允许测量机器学习模型与人群相比的能力。然而，很难创建一个大的数据集来训练深度神经网络模型（DNNs）的能力。我们提出了人群知情微调（CIFT）作为一个新的训练过程，其中预先训练的模型是通过IRT模型拟合获得的一个专门的补充训练集，在众多的众包响应模式中进行微调。借助CIFT，我们可以利用通过IRT获得的专用数据集来通知DNN中的参数调整。我们在CIFT中试验了两个损失函数来表示（i）微调项目的记忆和（ii）学习潜在标签的概率分布，类似于标签上的众包分布以模拟人群知识。我们的研究结果表明，CIFT提高了用于识别文本完成（RTE）任务的最先进的DNN模型的能力，并且可推广到大规模RTE测试集。

##### URL
[https://arxiv.org/abs/1702.08563](https://arxiv.org/abs/1702.08563)

##### PDF
[https://arxiv.org/pdf/1702.08563](https://arxiv.org/pdf/1702.08563)

