---
layout: post
title: "Mean Actor Critic"
date: 2018-05-22 20:20:59
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Cameron Allen, Kavosh Asadi, Melrose Roderick, Abdel-rahman Mohamed, George Konidaris, Michael Littman
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action continuous-state reinforcement learning. MAC is a policy gradient algorithm that uses the agent's explicit representation of all action values to estimate the gradient of the policy, rather than using only the actions that were actually executed. We prove that this approach reduces variance in the policy gradient estimate relative to traditional actor-critic methods. We show empirical results on two control domains and on six Atari games, where MAC is competitive with state-of-the-art policy search algorithms.

##### Abstract (translated by Google)
针对离散动作连续状态强化学习，我们提出了一种新的算法 - 平均演员 - 评论员（MAC）。 MAC是一种策略梯度算法，它使用代理程序显式表示所有操作值来估计策略的梯度，而不是仅使用实际执行的操作。我们证明，相对于传统的演员评论方法，这种方法减少了政策梯度估计的方差。我们展示了两个控制域和六个Atari游戏的实证结果，其中MAC与最先进的策略搜索算法竞争。

##### URL
[http://arxiv.org/abs/1709.00503](http://arxiv.org/abs/1709.00503)

##### PDF
[http://arxiv.org/pdf/1709.00503](http://arxiv.org/pdf/1709.00503)

