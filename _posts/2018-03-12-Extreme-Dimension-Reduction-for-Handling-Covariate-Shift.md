---
layout: post
title: "Extreme Dimension Reduction for Handling Covariate Shift"
date: 2018-03-12 15:44:49
categories: arXiv_AI
tags: arXiv_AI
author: Fulton Wang, Cynthia Rudin
mathjax: true
---

* content
{:toc}

##### Abstract
In the covariate shift learning scenario, the training and test covariate distributions differ, so that a predictor's average loss over the training and test distributions also differ. In this work, we explore the potential of extreme dimension reduction, i.e. to very low dimensions, in improving the performance of importance weighting methods for handling covariate shift, which fail in high dimensions due to potentially high train/test covariate divergence and the inability to accurately estimate the requisite density ratios. We first formulate and solve a problem optimizing over linear subspaces a combination of their predictive utility and train/test divergence within. Applying it to simulated and real data, we show extreme dimension reduction helps sometimes but not always, due to a bias introduced by dimension reduction.

##### Abstract (translated by Google)
在协变量移动学习场景中，训练和测试协变量分布不同，因此预测变量在训练和测试分布上的平均损失也不同。在这项工作中，我们探讨极端降维的可能性，即极低维度，改善处理协变量变换的重要性加权方法的性能，由于潜在的高的训练/测试协变量分歧以及不能够准确估计必要的密度比。我们首先制定并解决线性子空间上的一个问题，它们的预测效用和训练/测试背离的组合。将它应用于模拟和实际数据，我们显示极端降维有时但并非总是有助于降低维度引起的偏差。

##### URL
[http://arxiv.org/abs/1711.10938](http://arxiv.org/abs/1711.10938)

##### PDF
[http://arxiv.org/pdf/1711.10938](http://arxiv.org/pdf/1711.10938)

