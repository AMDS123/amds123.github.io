---
layout: post
title: "Low Resource Multi-modal Data Augmentation for End-to-end ASR"
date: 2018-12-10 16:57:21
categories: arXiv_SD
tags: arXiv_SD Attention
author: Matthew Wiesner, Adithya Renduchintala, Shinji Watanabe, Chunxi Liu, Najim Dehak, Sanjeev Khudanpur
mathjax: true
---

* content
{:toc}

##### Abstract
We explore training attention-based encoder-decoder ASR for low-resource languages and present techniques that result in a 50% relative improvement in character error rate compared to a standard baseline. The performance of encoder-decoder ASR systems depends on having sufficient target-side text to train the attention and decoder networks. The lack of such data in low-resource contexts results in severely degraded performance. In this paper we present a data augmentation scheme tailored for low-resource ASR in diverse languages. Across 3 test languages, our approach resulted in a 20% average relative improvement over a baseline text-based augmentation technique. We further compare the performance of our monolingual text-based data augmentation to speech-based data augmentation from nearby languages and find that this gives a further 20-30% relative reduction in character error rate.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.03919](http://arxiv.org/abs/1812.03919)

##### PDF
[http://arxiv.org/pdf/1812.03919](http://arxiv.org/pdf/1812.03919)

