---
layout: post
title: "Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search"
date: 2018-07-18 13:11:08
categories: arXiv_AI
tags: arXiv_AI Optimization Deep_Learning Relation
author: Arber Zela, Aaron Klein, Stefan Falkner, Frank Hutter
mathjax: true
---

* content
{:toc}

##### Abstract
While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.

##### Abstract (translated by Google)
虽然现有的神经架构搜索（NAS）工作在单独的后处理步骤中调整超参数，但我们证明架构选择和其他超参数设置以一种可以使这种分离不理想的方式进行交互。同样地，我们证明了在主要NAS期间使用非常少的时期和在后处理步骤期间使用更多数量的时期的常见做法是低效的，因为这两种训练方案的相对排名几乎没有相关性。为了解决这两个问题，我们建议使用最近的贝叶斯优化和高带的组合来实现高效的联合神经结构和超参数搜索。

##### URL
[https://arxiv.org/abs/1807.06906](https://arxiv.org/abs/1807.06906)

##### PDF
[https://arxiv.org/pdf/1807.06906](https://arxiv.org/pdf/1807.06906)

