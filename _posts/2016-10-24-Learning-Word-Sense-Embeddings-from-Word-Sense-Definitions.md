---
layout: post
title: "Learning Word Sense Embeddings from Word Sense Definitions"
date: 2016-10-24 00:56:54
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Qi Li, Tianshi Li, Baobao Chang
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings play a significant role in many modern NLP systems. Since learning one representation per word is problematic for polysemous words and homonymous words, researchers propose to use one embedding per word sense. Their approaches mainly train word sense embeddings on a corpus. In this paper, we propose to use word sense definitions to learn one embedding per word sense. Experimental results on word similarity tasks and a word sense disambiguation task show that word sense embeddings produced by our approach are of high quality.

##### Abstract (translated by Google)
词嵌入在许多现代NLP系统中起着重要的作用。由于对于多义词和同义词来说，学习每个单词的一个表示是有问题的，因此研究人员提出使用一个单词意义上的嵌入。他们的方法主要是在语料库上训练词义嵌入。在本文中，我们建议使用单词意义定义来学习一个单词意义上的嵌入。词相似任务的实验结果和词义消歧任务表明，我们的方法产生的词义嵌入是高质量的。

##### URL
[https://arxiv.org/abs/1606.04835](https://arxiv.org/abs/1606.04835)

##### PDF
[https://arxiv.org/pdf/1606.04835](https://arxiv.org/pdf/1606.04835)

