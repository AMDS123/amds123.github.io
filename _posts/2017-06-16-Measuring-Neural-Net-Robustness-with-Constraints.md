---
layout: post
title: "Measuring Neural Net Robustness with Constraints"
date: 2017-06-16 11:58:51
categories: arXiv_CV
tags: arXiv_CV Adversarial
author: Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, Antonio Criminisi
mathjax: true
---

* content
{:toc}

##### Abstract
Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness "overfit" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.

##### Abstract (translated by Google)
尽管精确度很高，但神经网络已经被证明容易受到敌对的例子的影响，其中对输入的小扰动可能导致其被误标。我们提出度量神经网络鲁棒性的指标，并设计一种新的算法来逼近这些度量基于鲁棒性的编码作为线性程序。我们展示了如何使用MNIST和CIFAR-10数据集上的实验来评估深度神经网络的鲁棒性。与基于现有算法的估计相比，我们的算法生成更多的鲁棒性度量的信息估计。此外，我们展示了现有的方法如何提高鲁棒性“overfit”到使用特定算法生成的敌对示例。最后，我们展示了我们的技术可以用来根据我们提出的度量额外地提高神经网络鲁棒性，还可以根据先前提出的度量来提高神经网络鲁棒性。

##### URL
[https://arxiv.org/abs/1605.07262](https://arxiv.org/abs/1605.07262)

##### PDF
[https://arxiv.org/pdf/1605.07262](https://arxiv.org/pdf/1605.07262)

