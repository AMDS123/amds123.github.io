---
layout: post
title: "Text Understanding with the Attention Sum Reader Network"
date: 2016-06-24 13:04:47
categories: arXiv_SD
tags: arXiv_SD Attention
author: Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, Jan Kleindienst
mathjax: true
---

* content
{:toc}

##### Abstract
Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets.

##### Abstract (translated by Google)
最近引入了几个大型的填充式上下文问答数据集：CNN和每日邮报新闻数据以及儿童图书测试。由于这些数据集的大小，相关的文本理解任务非常适合目前似乎超越所有替代方法的深度学习技术。我们提出了一个新的，简单的模型，使用注意力直接从上下文中选择答案，而不是像在相似模型中常见的那样，使用文档中单词的混合表示来计算答案。这使得该模型特别适合回答问题，即答案是文件中的单个单词。我们模型的集合为所有评估的数据集设置了新的艺术状态。

##### URL
[https://arxiv.org/abs/1603.01547](https://arxiv.org/abs/1603.01547)

##### PDF
[https://arxiv.org/pdf/1603.01547](https://arxiv.org/pdf/1603.01547)

