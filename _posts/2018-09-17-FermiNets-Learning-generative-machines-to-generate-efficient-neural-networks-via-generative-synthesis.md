---
layout: post
title: "FermiNets: Learning generative machines to generate efficient neural networks via generative synthesis"
date: 2018-09-17 01:26:57
categories: arXiv_AI
tags: arXiv_AI Object_Detection Segmentation Image_Classification Semantic_Segmentation Inference Classification Deep_Learning Detection
author: Alexander Wong, Mohammad Javad Shafiee, Brendan Chwyl, Francis Li
mathjax: true
---

* content
{:toc}

##### Abstract
The tremendous potential exhibited by deep learning is often offset by architectural and computational complexity, making widespread deployment a challenge for edge scenarios such as mobile and other consumer devices. To tackle this challenge, we explore the following idea: Can we learn generative machines to automatically generate deep neural networks with efficient network architectures? In this study, we introduce the idea of generative synthesis, which is premised on the intricate interplay between a generator-inquisitor pair that work in tandem to garner insights and learn to generate highly efficient deep neural networks that best satisfies operational requirements. What is most interesting is that, once a generator has been learned through generative synthesis, it can be used to generate not just one but a large variety of different, unique highly efficient deep neural networks that satisfy operational requirements. Experimental results for image classification, semantic segmentation, and object detection tasks illustrate the efficacy of generative synthesis in producing generators that automatically generate highly efficient deep neural networks (which we nickname FermiNets) with higher model efficiency and lower computational costs (reaching &gt;10x more efficient and fewer multiply-accumulate operations than several tested state-of-the-art networks), as well as higher energy efficiency (reaching &gt;4x improvements in image inferences per joule consumed on a Nvidia Tegra X2 mobile processor). As such, generative synthesis can be a powerful, generalized approach for accelerating and improving the building of deep neural networks for on-device edge scenarios.

##### Abstract (translated by Google)
深度学习所带来的巨大潜力往往被架构和计算复杂性所抵消，使得广泛部署成为移动和其他消费设备等边缘场景的挑战。为了应对这一挑战，我们探索了以下想法：我们能否学习生成机器以自动生成具有高效网络架构的深度神经网络？在这项研究中，我们介绍了生成合成的概念，它以发电机 - 检测器对之间错综复杂的相互作用为前提，它们协同工作以获得洞察力并学习生成最能满足操作要求的高效深度神经网络。最有趣的是，一旦通过生成合成学习了发生器，它就可以用来生成不仅仅是一种，而且可以生成满足操作要求的各种不同的，独特的高效深度神经网络。图像分类，语义分割和目标检测任务的实验结果说明了生成合成在生成自动生成高效深度神经网络（我们称之为FermiNets）的生成器中的效率，模型效率更高，计算成本更低（达到> 10倍以上）与几个经过测试的最先进的网络相比，有效且更少的乘法累加运算，以及更高的能量效率（在Nvidia Tegra X2移动处理器上消耗的每个焦耳的图像推断达到> 4倍）。因此，生成合成可以是用于加速和改进用于设备边缘场景的深度神经网络的构建的强大的通用方法。

##### URL
[http://arxiv.org/abs/1809.05989](http://arxiv.org/abs/1809.05989)

##### PDF
[http://arxiv.org/pdf/1809.05989](http://arxiv.org/pdf/1809.05989)

