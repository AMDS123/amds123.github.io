---
layout: post
title: "A Visual Attention Grounding Neural Model for Multimodal Machine Translation"
date: 2018-08-24 18:47:29
categories: arXiv_CL
tags: arXiv_CL Attention Embedding
author: Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, Zhou Yu
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our model jointly optimizes the learning of a shared visual-language embedding and translating languages. It does this with the aid of a visual attention grounding mechanism which links the visual semantics in the image with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this dataset, our visual attention grounding model outperforms other methods by a large margin.

##### Abstract (translated by Google)
我们介绍了一种利用并行视觉和文本信息的新型多模式机器翻译模型。我们的模型共同优化了共享可视语言嵌入和翻译语言的学习。它借助于视觉注意力接地机制来实现这一点，该机制将图像中的视觉语义与相应的文本语义联系起来。我们的方法在Multi30K和Ambiguous COCO数据集上实现了最具竞争力的最先进结果。我们还收集了一个新的多语言多模式产品描述数据集，以模拟真实世界的国际在线购物场景。在这个数据集上，我们的视觉注意力接地模型大大优于其他方法。

##### URL
[http://arxiv.org/abs/1808.08266](http://arxiv.org/abs/1808.08266)

##### PDF
[http://arxiv.org/pdf/1808.08266](http://arxiv.org/pdf/1808.08266)

