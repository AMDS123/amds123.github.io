---
layout: post
title: "Intra-Ensemble in Neural Networks"
date: 2019-04-09 04:53:17
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Yuan Gao, Zixiang Cai, Yimin Chen, Wenke Chen, Kan Yang, Chen Sun, Cong Yao
mathjax: true
---

* content
{:toc}

##### Abstract
Improving model performance is always the key problem in machine learning including deep learning. However, stand-alone neural networks always suffer from marginal effect when stacking more layers. At the same time, ensemble is a useful technique to further enhance model performance. Nevertheless, training several independent stand-alone deep neural networks costs multiple resources. In this work, we propose Intra-Ensemble, an end-to-end strategy with stochastic training operations to train several sub-networks simultaneously within one neural network. Additional parameter size is marginal since the majority of parameters are mutually shared. Meanwhile, stochastic training increases the diversity of sub-networks with weight sharing, which significantly enhances intra-ensemble performance. Extensive experiments prove the applicability of intra-ensemble on various kinds of datasets and network architectures. Our models achieve comparable results with the state-of-the-art architectures on CIFAR-10 and CIFAR-100.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.04466](http://arxiv.org/abs/1904.04466)

##### PDF
[http://arxiv.org/pdf/1904.04466](http://arxiv.org/pdf/1904.04466)

