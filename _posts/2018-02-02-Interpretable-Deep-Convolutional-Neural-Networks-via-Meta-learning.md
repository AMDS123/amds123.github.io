---
layout: post
title: "Interpretable Deep Convolutional Neural Networks via Meta-learning"
date: 2018-02-02 05:09:10
categories: arXiv_AI
tags: arXiv_AI CNN
author: Xuan Liu, Xiaoguang Wang, Stan Matwin
mathjax: true
---

* content
{:toc}

##### Abstract
Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for "algorithmic fairness" also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the deep CNN model, which leads to reliable interpretations.

##### Abstract (translated by Google)
模型可解释性是许多应用程序的一个要求，在这些应用程序中，依赖于模型输出的用户做出了关键决策。最近“算法公平”的运动也规定了可解释性，因此也规定了学习模式的可解释性。然而，最成功的当代机器学习方法，深度神经网络，产生的模型是高度不可解释的。我们试图通过提出一种称为CNN-INTE的技术来通过元学习来解释深度卷积神经网络（CNN）来解决这个挑战。在这项工作中，我们解释MNIST图像数据集上的CNN深层模型的一个特定的隐藏层。我们使用两级结构的聚类算法来查找元级训练数据和随机森林作为基础学习算法来生成元级测试数据。解释结果通过图表可视化显示，清楚地表明特定测试实例如何分类。我们的方法实现了所有测试实例的全局解释，而不会牺牲原有的深度CNN模型所获得的准确性。这意味着我们的模型忠实于CNN深度模型，从而得到可靠的解释。

##### URL
[https://arxiv.org/abs/1802.00560](https://arxiv.org/abs/1802.00560)

##### PDF
[https://arxiv.org/pdf/1802.00560](https://arxiv.org/pdf/1802.00560)

