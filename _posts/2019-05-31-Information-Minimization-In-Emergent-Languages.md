---
layout: post
title: "Information Minimization In Emergent Languages"
date: 2019-05-31 15:54:41
categories: arXiv_CL
tags: arXiv_CL Adversarial Represenation_Learning
author: Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, Marco Baroni
mathjax: true
---

* content
{:toc}

##### Abstract
There is a growing interest in studying the languages emerging when neural agents are jointly trained to solve tasks that require communication through discrete messages. We investigate here the information-theoretic complexity of such languages, focusing on the most basic two-agent, one-symbol, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an information minimization pressure: The mutual information between the communicating agent's inputs and the messages is close to the minimum that still allows the task to be solved. After verifying this information minimization property, we perform experiments showing that a stronger discrete-channel-driven information minimization pressure leads to increased robustness to overfitting and to adversarial attacks. We conclude by discussing the implications of our findings for the studies of artificial and natural language emergence, and for representation learning.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.13687](http://arxiv.org/abs/1905.13687)

##### PDF
[http://arxiv.org/pdf/1905.13687](http://arxiv.org/pdf/1905.13687)

