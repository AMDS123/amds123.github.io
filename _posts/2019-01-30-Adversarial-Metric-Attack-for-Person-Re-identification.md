---
layout: post
title: "Adversarial Metric Attack for Person Re-identification"
date: 2019-01-30 02:41:50
categories: arXiv_CV
tags: arXiv_CV Re-identification Adversarial Attention Person_Re-identification Classification Prediction
author: Song Bai, Yingwei Li, Yuyin Zhou, Qizhu Li, Philip H.S. Torr
mathjax: true
---

* content
{:toc}

##### Abstract
Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance, especially considering the highly strict requirement of public safety. 
 Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks, which can effectively generate adversarial examples for re-ID. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Moreover, by benchmarking various adversarial settings, we expect that our work can facilitate the development of robust feature learning with the experimental conclusions we have drawn.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1901.10650](http://arxiv.org/abs/1901.10650)

##### PDF
[http://arxiv.org/pdf/1901.10650](http://arxiv.org/pdf/1901.10650)

