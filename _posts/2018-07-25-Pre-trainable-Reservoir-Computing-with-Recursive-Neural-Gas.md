---
layout: post
title: "Pre-trainable Reservoir Computing with Recursive Neural Gas"
date: 2018-07-25 10:05:46
categories: arXiv_AI
tags: arXiv_AI Knowledge RNN
author: Luca Carcano, Emanuele Plebani, Danilo Pietro Pau, Marco Piastra
mathjax: true
---

* content
{:toc}

##### Abstract
Echo State Networks (ESN) are a class of Recurrent Neural Networks (RNN) that has gained substantial popularity due to their effectiveness, ease of use and potential for compact hardware implementation. An ESN contains the three network layers input, reservoir and readout where the reservoir is the truly recurrent network. The input and reservoir layers of an ESN are initialized at random and never trained afterwards and the training of the ESN is applied to the readout layer only. The alternative of Recursive Neural Gas (RNG) is one of the many proposals of fully-trainable reservoirs that can be found in the literature. Although some improvements in performance have been reported with RNG, to the best of authors' knowledge, no experimental comparative results are known with benchmarks for which ESN is known to yield excellent results. This work describes an accurate model of RNG together with some extensions to the models presented in the literature and shows comparative results on three well-known and accepted datasets. The experimental results obtained show that, under specific circumstances, RNG-based reservoirs can achieve better performance.

##### Abstract (translated by Google)
回声状态网络（ESN）是一类回归神经网络（RNN），由于其有效性，易用性和紧凑硬件实现的潜力而获得了广泛的普及。 ESN包含三个网络层输入，储存和读出，其中储层是真正的循环网络。 ESN的输入和储层随机初始化，之后从未进行过训练，ESN的训练仅应用于读出层。递归神经气体（RNG）的替代方案是可在文献中找到的可完全训练的储层的许多提议之一。尽管RNG已经报告了一些性能方面的改进，但据作者所知，没有任何实验性比较结果已知基准，已知ESN可产生优异的结果。这项工作描述了RNG的精确模型以及文献中提供的模型的一些扩展，并显示了三个众所周知和可接受的数据集的比较结果。实验结果表明，在特定情况下，基于RNG的储层可以获得更好的性能。

##### URL
[http://arxiv.org/abs/1807.09510](http://arxiv.org/abs/1807.09510)

##### PDF
[http://arxiv.org/pdf/1807.09510](http://arxiv.org/pdf/1807.09510)

