---
layout: post
title: "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation"
date: 2017-08-18 11:28:05
categories: arXiv_CL
tags: arXiv_CL
author: Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer
mathjax: true
---

* content
{:toc}

##### Abstract
Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text usingAbstract Meaning Representation (AMR)has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.

##### Abstract (translated by Google)
序列到序列模型在广泛的应用中表现出强大的性能。然而，由于标注数据的数量相对有限以及AMR图的非连续性，使用抽象意义表示（AMR）解析和生成文本的应用受到了限制。我们提出了一种新颖的训练过程，可以使用数以百万计的无标签句子和仔细预处理AMR图解除这个限制。对于AMR解析，我们的模型实现了62.1SMATCH的竞争结果，当前最好的分数报告没有显着使用外部语义资源。 AMR一代，我们的模型建立了一个新的BLEU 33.8的最先进的性能。我们提出了广泛的烧蚀和定性分析，包括强有力的证据表明，基于序列的AMR模型对图形到序列转换的排序变化是强大的。

##### URL
[https://arxiv.org/abs/1704.08381](https://arxiv.org/abs/1704.08381)

##### PDF
[https://arxiv.org/pdf/1704.08381](https://arxiv.org/pdf/1704.08381)

