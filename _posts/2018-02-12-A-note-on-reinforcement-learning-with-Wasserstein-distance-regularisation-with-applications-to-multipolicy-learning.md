---
layout: post
title: "A note on reinforcement learning with Wasserstein distance regularisation, with applications to multipolicy learning"
date: 2018-02-12 11:08:43
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Mohammed Amin Abdullah, Aldo Pacchiano, Moez Draief
mathjax: true
---

* content
{:toc}

##### Abstract
In this note we describe an application of Wasserstein distance to Reinforcement Learning. The Wasserstein distance in question is between the distribution of mappings of trajectories of a policy into some metric space, and some other fixed distribution (which may, for example, come from another policy). Different policies induce different distributions, so given an underlying metric, the Wasserstein distance quantifies how different policies are. This can be used to learn multiple polices which are different in terms of such Wasserstein distances by using a Wasserstein regulariser. Changing the sign of the regularisation parameter, one can learn a policy for which its trajectory mapping distribution is attracted to a given fixed distribution.

##### Abstract (translated by Google)
在这个笔记中，我们描述了Wasserstein距离到强化学习的应用。所讨论的Wasserstein距离介于一个策略的轨迹映射到某个度量空间的分布和其他一些固定分布（可能来自另一个策略）。不同的政策会导致不同的分配，所以给定一个基础指标，Wasserstein距离可以量化不同的政策。这可以用来通过使用Wasserstein regulariser来学习在这样的Wasserstein距离方面不同的多个策略。改变正则化参数的符号，可以学习一种策略，其轨迹映射分布被吸引到给定的固定分布。

##### URL
[http://arxiv.org/abs/1802.03976](http://arxiv.org/abs/1802.03976)

##### PDF
[http://arxiv.org/pdf/1802.03976](http://arxiv.org/pdf/1802.03976)

