---
layout: post
title: "Non-Parametric Transformation Networks"
date: 2018-01-14 06:48:45
categories: arXiv_AI
tags: arXiv_AI CNN Gradient_Descent
author: Dipan K. Pal, Marios Savvides
mathjax: true
---

* content
{:toc}

##### Abstract
ConvNets have been very effective in many applications where it is required to learn invariances to within-class nuisance transformations. However, through their architecture, ConvNets only enforce invariance to translation. In this paper, we introduce a new class of convolutional architectures called Non-Parametric Transformation Networks (NPTNs) which can learn general invariances and symmetries directly from data. NPTNs are a direct and natural generalization of ConvNets and can be optimized directly using gradient descent. They make no assumption regarding structure of the invariances present in the data and in that aspect are very flexible and powerful. We also model ConvNets and NPTNs under a unified framework called Transformation Networks which establishes the natural connection between the two. We demonstrate the efficacy of NPTNs on natural data such as MNIST and CIFAR 10 where it outperforms ConvNet baselines with the same number of parameters. We show it is effective in learning invariances unknown apriori directly from data from scratch. Finally, we apply NPTNs to Capsule Networks and show that they enable them to perform even better.

##### Abstract (translated by Google)
ConvNets在许多应用中非常有效，在这些应用中需要学习类内滋扰转换的不变性。然而，ConvNets通过他们的体系结构只强制执行翻译。在本文中，我们引入一类称为非参数变换网络（Non-Parametric Transformation Networks，NPTNs）的新型卷积结构，它可以直接从数据中学习到一般的不变量和对称性。 NPTN是ConvNets的直接和自然的推广，可以直接使用梯度下降进行优化。他们不假定数据中存在的不变性的结构，在这方面非常灵活和强大。我们还在ConvNets和NPTNs下建立了一个名为Transformation Networks的统一框架，从而建立了两者之间的自然联系。我们证明了NPTNs对MNIST和CIFAR 10等自然数据的有效性，其中它的性能优于ConvNet基线和相同数量的参数。我们显示它是有效的，直接从数据从头学习不断未知的apriori。最后，我们将NPTN应用于胶囊网络，并显示它们使其性能更好。

##### URL
[https://arxiv.org/abs/1801.04520](https://arxiv.org/abs/1801.04520)

##### PDF
[https://arxiv.org/pdf/1801.04520](https://arxiv.org/pdf/1801.04520)

