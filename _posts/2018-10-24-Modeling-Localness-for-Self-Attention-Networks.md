---
layout: post
title: "Modeling Localness for Self-Attention Networks"
date: 2018-10-24 04:08:25
categories: arXiv_AI
tags: arXiv_AI Attention Quantitative
author: Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, Tong Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Self-attention networks have proven to be of profound value for its strength of capturing global dependencies. In this work, we propose to model localness for self-attention networks, which enhances the ability of capturing useful local context. We cast localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to be paid more attention. The bias is then incorporated into the original attention distribution to form a revised distribution. To maintain the strength of capturing long distance dependencies and enhance the ability of capturing short-range dependencies, we only apply localness modeling to lower layers of self-attention networks. Quantitative and qualitative analyses on Chinese-English and English-German translation tasks demonstrate the effectiveness and universality of the proposed approach.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.10182](http://arxiv.org/abs/1810.10182)

##### PDF
[http://arxiv.org/pdf/1810.10182](http://arxiv.org/pdf/1810.10182)

