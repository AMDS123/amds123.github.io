---
layout: post
title: "Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems"
date: 2015-08-18 12:42:42
categories: arXiv_CL
tags: arXiv_CL Knowledge Reinforcement_Learning RNN
author: Pei-Hao Su, David Vandyke, Milica Gasic, Nikola Mrksic, Tsung-Hsien Wen, Steve Young
mathjax: true
---

* content
{:toc}

##### Abstract
Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users. However in the reinforcement learning paradigm the dialogue manager (agent) often requires significant time to explore the state-action space to learn to behave in a desirable manner. This is a critical issue when the system is trained on-line with real users where learning costs are expensive. Reward shaping is one promising technique for addressing these concerns. Here we examine three recurrent neural network (RNN) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback. These RNNs are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster. In both simulated and real user scenarios these RNNs are shown to increase policy learning speed. Importantly, they do not require prior knowledge of the user's goal.

##### Abstract (translated by Google)
统计口语对话系统具有能够通过与真实用户交互的数据进行优化的吸引力特性。然而，在强化学习范式中，对话管理者（代理人）往往需要花费大量的时间来探索国家行动空间，学习如何行事。当系统在实际用户的在线学习时，这是一个关键的问题，因为学习成本昂贵。奖励塑造是解决这些问题的一种有前景的技术。在这里，我们研究了三种递归神经网络（RNN）方法，除了提供主要的（任务导向的）环境反馈之外，还提供奖励整形信息。这些RNN是根据模拟用户产生的对话的回报进行训练的，并试图将对话的整体评价扩散到转向水平，以指导代理人更快地行为。在模拟和真实用户场景中，这些RNN都显示出增加策略学习速度。重要的是，他们不需要事先知道用户的目标。

##### URL
[https://arxiv.org/abs/1508.03391](https://arxiv.org/abs/1508.03391)

##### PDF
[https://arxiv.org/pdf/1508.03391](https://arxiv.org/pdf/1508.03391)

