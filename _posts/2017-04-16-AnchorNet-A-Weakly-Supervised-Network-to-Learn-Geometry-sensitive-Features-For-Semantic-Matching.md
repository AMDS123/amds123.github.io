---
layout: post
title: "AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive Features For Semantic Matching"
date: 2017-04-16 11:07:02
categories: arXiv_CV
tags: arXiv_CV Image_Caption Weakly_Supervised Classification Deep_Learning
author: David Novotny, Diane Larlus, Andrea Vedaldi
mathjax: true
---

* content
{:toc}

##### Abstract
Despite significant progress of deep learning in recent years, state-of-the-art semantic matching methods still rely on legacy features such as SIFT or HoG. We argue that the strong invariance properties that are key to the success of recent deep architectures on the classification task make them unfit for dense correspondence tasks, unless a large amount of supervision is used. In this work, we propose a deep network, termed AnchorNet, that produces image representations that are well-suited for semantic matching. It relies on a set of filters whose response is geometrically consistent across different object instances, even in the presence of strong intra-class, scale, or viewpoint variations. Trained only with weak image-level labels, the final representation successfully captures information about the object structure and improves results of state-of-the-art semantic matching methods such as the deformable spatial pyramid or the proposal flow methods. We show positive results on the cross-instance matching task where different instances of the same object category are matched as well as on a new cross-category semantic matching task aligning pairs of instances each from a different object class.

##### Abstract (translated by Google)
尽管近年来深度学习取得了重大进展，但最先进的语义匹配方法仍然依赖于遗留特征，如SIFT或HoG。我们认为强大的不变性属性是近期深层次体系结构在分类任务中取得成功的关键，使得它们不适合密集的通信任务，除非使用大量的监督。在这项工作中，我们提出了一个称为AnchorNet的深层网络，它产生非常适合于语义匹配的图像表示。它依赖于一组过滤器，其响应在不同的对象实例中几何一致，即使存在强大的类内，尺度或视点变化。只有使用较弱的图像级标签进行训练，最终的表示才能成功捕获关于对象结构的信息，并改进最先进的语义匹配方法（如可变形空间金字塔或提议流方法）的结果。我们在交叉实例匹配任务中显示出正面的结果，其中同一对象类别的不同实例匹配以及在新的跨类别语义匹配任务上对齐来自不同对象类别的实例对。

##### URL
[https://arxiv.org/abs/1704.04749](https://arxiv.org/abs/1704.04749)

##### PDF
[https://arxiv.org/pdf/1704.04749](https://arxiv.org/pdf/1704.04749)

