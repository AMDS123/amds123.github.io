---
layout: post
title: "Mitigating Planner Overfitting in Model-Based Reinforcement Learning"
date: 2018-12-03 23:11:30
categories: arXiv_AI
tags: arXiv_AI Regularization Face Reinforcement_Learning
author: Dilip Arumugam, David Abel, Kavosh Asadi, Nakul Gopalan, Christopher Grimm, Jun Ki Lee, Lucas Lehnert, Michael L. Littman
mathjax: true
---

* content
{:toc}

##### Abstract
An agent with an inaccurate model of its environment faces a difficult choice: it can ignore the errors in its model and act in the real world in whatever way it determines is optimal with respect to its model. Alternatively, it can take a more conservative stance and eschew its model in favor of optimizing its behavior solely via real-world interaction. This latter approach can be exceedingly slow to learn from experience, while the former can lead to "planner overfitting" - aspects of the agent's behavior are optimized to exploit errors in its model. This paper explores an intermediate position in which the planner seeks to avoid overfitting through a kind of regularization of the plans it considers. We present three different approaches that demonstrably mitigate planner overfitting in reinforcement-learning environments.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.01129](http://arxiv.org/abs/1812.01129)

##### PDF
[http://arxiv.org/pdf/1812.01129](http://arxiv.org/pdf/1812.01129)

