---
layout: post
title: "Federated Learning for Mobile Keyboard Prediction"
date: 2018-11-08 18:37:03
categories: arXiv_CL
tags: arXiv_CL Language_Model Prediction Gradient_Descent
author: Andrew Hard, Kanishka Rao, Rajiv Mathews, Fran&#xe7;oise Beaufays, Sean Augenstein, Hubert Eichner, Chlo&#xe9; Kiddon, Daniel Ramage
mathjax: true
---

* content
{:toc}

##### Abstract
We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the Federated Averaging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. 
 This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.03604](http://arxiv.org/abs/1811.03604)

##### PDF
[http://arxiv.org/pdf/1811.03604](http://arxiv.org/pdf/1811.03604)

