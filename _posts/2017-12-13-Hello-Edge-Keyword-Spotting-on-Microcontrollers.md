---
layout: post
title: "Hello Edge: Keyword Spotting on Microcontrollers"
date: 2017-12-13 23:54:52
categories: arXiv_CL
tags: arXiv_CL CNN
author: Yundong Zhang, Naveen Suda, Liangzhen Lai, Vikas Chandra
mathjax: true
---

* content
{:toc}

##### Abstract
Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4%, which is ~10% higher than the DNN model with similar number of parameters.

##### Abstract (translated by Google)
关键字识别（KWS）是在智能设备上启用基于语音的用户交互的关键组件。它需要实时响应和高准确度，以获得良好的用户体验。最近，神经网络已经成为KWS体系结构的一个有吸引力的选择，因为它比传统的语音处理算法具有更高的精度。由于其始终在线的性质，KWS应用程序具有高度的功耗预算限制，通常运行在内存和计算能力有限的微型控制器上。 KWS的神经网络结构的设计必须考虑这些约束条件。在这项工作中，我们对资源受限的微控制器进行KWS的神经网络结构评估和探索。我们训练各种神经网络架构，用于比较文献中发现的关键词点的准确性和内存/计算需求。我们表明，可以优化这些神经网络架构，以适应微控制器的内存和计算约束，而不会牺牲精度。我们进一步探索深度可分卷积神经网络（DS-CNN），并将其与其他神经网络架构进行比较。 DS-CNN的准确率达到95.4％，比参数相近的DNN模型高出约10％。

##### URL
[http://arxiv.org/abs/1711.07128](http://arxiv.org/abs/1711.07128)

##### PDF
[http://arxiv.org/pdf/1711.07128](http://arxiv.org/pdf/1711.07128)

