---
layout: post
title: "Learning Multi-Modal Word Representation Grounded in Visual Context"
date: 2017-11-09 17:28:07
categories: arXiv_CV
tags: arXiv_CV Embedding
author: Éloi Zablocki, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari
mathjax: true
---

* content
{:toc}

##### Abstract
Representing the semantics of words is a long-standing problem for the natural language processing community. Most methods compute word semantics given their textual context in large corpora. More recently, researchers attempted to integrate perceptual and visual features. Most of these works consider the visual appearance of objects to enhance word representations but they ignore the visual environment and context in which objects appear. We propose to unify text-based techniques with vision-based techniques by simultaneously leveraging textual and visual context to learn multimodal word embeddings. We explore various choices for what can serve as a visual context and present an end-to-end method to integrate visual context elements in a multimodal skip-gram model. We provide experiments and extensive analysis of the obtained results.

##### Abstract (translated by Google)
代表词的语义是自然语言处理社区长期存在的问题。大多数方法在大语料库中给定文本语境的情况下计算词语义。最近，研究人员试图整合感知和视觉特征。这些作品中的大多数考虑了对象的视觉外观以增强词语表示，但是忽略了对象出现的视觉环境和上下文。我们建议通过同时利用文本和视觉上下文来学习多模态词嵌入，将基于文本的技术与基于视觉的技术统一起来。我们探索了多种可以作为视觉上下文的选择，并提出了一种将视觉上下文元素集成到多模跳跃模型中的端到端方法。我们提供实验和广泛的分析获得的结果。

##### URL
[https://arxiv.org/abs/1711.03483](https://arxiv.org/abs/1711.03483)

##### PDF
[https://arxiv.org/pdf/1711.03483](https://arxiv.org/pdf/1711.03483)

