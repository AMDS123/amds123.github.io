---
layout: post
title: "Toward Optimal Feature Selection in Naive Bayes for Text Categorization"
date: 2016-02-09 03:43:21
categories: arXiv_SD
tags: arXiv_SD Classification
author: Bo Tang, Steven Kay, Haibo He
mathjax: true
---

* content
{:toc}

##### Abstract
Automated feature selection is important for text categorization to reduce the feature size and to speed up the learning process of classifiers. In this paper, we present a novel and efficient feature selection framework based on the Information Theory, which aims to rank the features with their discriminative capacity for classification. We first revisit two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyze their asymptotic properties relating to type I and type II errors of a Bayesian classifier. We then introduce a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, we develop two efficient feature selection methods, termed maximum discrimination ($MD$) and $MD-\chi^2$ methods, for text categorization. The promising results of extensive experiments demonstrate the effectiveness of the proposed approaches.

##### Abstract (translated by Google)
自动特征选择对于文本分类非常重要，以减少特征尺寸并加快分类器的学习过程。在本文中，我们提出了一个基于信息论的新颖高效的特征选择框架，该框架旨在将特征与其区分能力进行排序。我们首先回顾两个信息测度：二元假设检验的Kullback-Leibler散度和Jeffreys散度，并分析它们与贝叶斯分类器的类型I和类型II误差有关的渐近性质。然后，我们引入一种称为Jeffreys-Multi-Hypothesis（JMH）分歧的新的分歧度量来衡量多分类分类的多分布分歧。基于JMH散度，我们开发了两种有效的特征选择方法，称为最大歧视（$ MD $）和$ MD- \ chi ^ 2 $方法，用于文本分类。广泛的实验的有希望的结果证明了所提出的方法的有效性。

##### URL
[https://arxiv.org/abs/1602.02850](https://arxiv.org/abs/1602.02850)

##### PDF
[https://arxiv.org/pdf/1602.02850](https://arxiv.org/pdf/1602.02850)

