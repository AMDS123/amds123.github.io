---
layout: post
title: "The word entropy of natural languages"
date: 2016-06-22 16:00:52
categories: arXiv_CL
tags: arXiv_CL Quantitative
author: Christian Bentz, Dimitrios Alikaniotis
mathjax: true
---

* content
{:toc}

##### Abstract
The average uncertainty associated with words is an information-theoretic concept at the heart of quantitative and computational linguistics. The entropy has been established as a measure of this average uncertainty - also called average information content. We here use parallel texts of 21 languages to establish the number of tokens at which word entropies converge to stable values. These convergence points are then used to select texts from a massively parallel corpus, and to estimate word entropies across more than 1000 languages. Our results help to establish quantitative language comparisons, to understand the performance of multilingual translation systems, and to normalize semantic similarity measures.

##### Abstract (translated by Google)
与词相关的平均不确定性是定量和计算语言学核心的信息论概念。熵被建立为衡量这种平均不确定性的一种度量 - 也称为平均信息量。我们在这里使用21种语言的平行文本来建立词汇熵收敛于稳定值的令牌数量。然后使用这些收敛点从大规模平行语料库中选择文本，并估计跨越1000多种语言的单词熵。我们的研究结果有助于建立定量语言比较，了解多语言翻译系统的性能，并规范语义相似性度量。

##### URL
[https://arxiv.org/abs/1606.06996](https://arxiv.org/abs/1606.06996)

##### PDF
[https://arxiv.org/pdf/1606.06996](https://arxiv.org/pdf/1606.06996)

