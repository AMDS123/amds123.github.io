---
layout: post
title: "Concurrent Learning of Semantic Relations"
date: 2018-07-26 11:44:19
categories: arXiv_CL
tags: arXiv_CL Prediction Relation
author: Georgios Balikas, Gael Dias, Rumen Moraliyski, Massih-Reza Amini
mathjax: true
---

* content
{:toc}

##### Abstract
Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is of crucial importance for NLP as it is essential for tasks like query expansion in IR. Within this context, different methodologies have been proposed that either exclusively focus on a single lexical relation (e.g. hypernymy vs. random) or learn specific classifiers capable of identifying multiple semantic relations (e.g. hypernymy vs. synonymy vs. random). In this paper, we propose another way to look at the problem that relies on the multi-task learning paradigm. In particular, we want to study whether the learning process of a given semantic relation (e.g. hypernymy) can be improved by the concurrent learning of another semantic relation (e.g. co-hyponymy). Within this context, we particularly examine the benefits of semi-supervised learning where the training of a prediction function is performed over few labeled data jointly with many unlabeled ones. Preliminary results based on simple learning strategies and state-of-the-art distributional feature representations show that concurrent learning can lead to improvements in a vast majority of tested situations.

##### Abstract (translated by Google)
发现单词是否与语义相关并识别它们之间存在的特定语义关系对于NLP至关重要，因为它对于IR中的查询扩展等任务至关重要。在此上下文中，已经提出了不同的方法，其或者专注于单个词汇关系（例如，上位词与随机）或者学习能够识别多个语义关系的特定分类器（例如，上位词与同义词与随机相比）。在本文中，我们提出了另一种方法来研究依赖于多任务学习范式的问题。特别地，我们想要研究是否可以通过同时学习另一个语义关系（例如，同义词）来改进给定语义关系（例如，上位词）的学习过程。在此背景下，我们特别研究了半监督学习的好处，其中预测函数的训练是通过少量标记数据与许多未标记数据一起进行的。基于简单学习策略和最先进的分布特征表示的初步结果表明，并发学习可以导致绝大多数测试情况的​​改进。

##### URL
[http://arxiv.org/abs/1807.10076](http://arxiv.org/abs/1807.10076)

##### PDF
[http://arxiv.org/pdf/1807.10076](http://arxiv.org/pdf/1807.10076)

