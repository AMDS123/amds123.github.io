---
layout: post
title: "Multi-view Sentence Representation Learning"
date: 2018-05-18 21:04:08
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning RNN
author: Shuai Tang, Virginia R. de Sa
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-view learning can provide self-supervision when different views are available of the same data. The distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora. Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we create a unified multi-view sentence representation learning framework, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model, and the training objective is to maximise the agreement specified by the adjacent context information between two views. We show that, after training, the vectors produced from our multi-view training provide improved representations over the single-view training, and the combination of different views gives further representational improvement and demonstrates solid transferability on standard downstream tasks.

##### Abstract (translated by Google)
多视图学习可以在相同数据的不同视图可用时提供自我监督。分布假设提供了另一种形式的有用的自我监督，从邻近的句子，在大型未标记的语料库中丰富。受人脑两个半球的不对称性以及不同学习架构倾向于强调句子意义的不同方面的观察，我们创建了一个统一的多视角句子表征学习框架，其中一个视图编码输入句子与循环神经网络（RNN），另一个视图用一个简单的线性模型对其进行编码，训练目标是最大化两个视图之间相邻上下文信息所指定的协议。我们证明，在训练之后，我们的多视图训练产生的向量提供了对单视图训练的改进表示，并且不同视图的组合提供了进一步的代表性改进并证明了对标准下游任务的可靠转移性。

##### URL
[https://arxiv.org/abs/1805.07443](https://arxiv.org/abs/1805.07443)

##### PDF
[https://arxiv.org/pdf/1805.07443](https://arxiv.org/pdf/1805.07443)

