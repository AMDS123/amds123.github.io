---
layout: post
title: "A Radically New Theory of how the Brain Represents and Computes with Probabilities"
date: 2018-02-21 23:00:01
categories: arXiv_CV
tags: arXiv_CV Sparse Embedding Inference Relation Recognition
author: Gerard Rinkus
mathjax: true
---

* content
{:toc}

##### Abstract
The brain is believed to implement probabilistic reasoning and to represent information via population, or distributed, coding. Most previous population-based probabilistic (PPC) theories share several basic properties: 1) continuous-valued neurons; 2) fully(densely)-distributed codes, i.e., all(most) units participate in every code; 3) graded synapses; 4) rate coding; 5) units have innate unimodal tuning functions (TFs); 6) intrinsically noisy units; and 7) noise/correlation is considered harmful. We present a radically different theory that assumes: 1) binary units; 2) only a small subset of units, i.e., a sparse distributed representation (SDR) (cell assembly), comprises any individual code; 3) binary synapses; 4) signaling formally requires only single (i.e., first) spikes; 5) units initially have completely flat TFs (all weights zero); 6) units are far less intrinsically noisy than traditionally thought; rather 7) noise is a resource generated/used to cause similar inputs to map to similar codes, controlling a tradeoff between storage capacity and embedding the input space statistics in the pattern of intersections over stored codes, epiphenomenally determining correlation patterns across neurons. The theory, Sparsey, was introduced 20+ years ago as a canonical cortical circuit/algorithm model achieving efficient sequence learning/recognition, but not elaborated as an alternative to PPC theories. Here, we show that: a) the active SDR simultaneously represents both the most similar/likely input and the entire (coarsely-ranked) similarity likelihood/distribution over all stored inputs (hypotheses); and b) given an input, the SDR code selection algorithm, which underlies both learning and inference, updates both the most likely hypothesis and the entire likelihood distribution (cf. belief update) with a number of steps that remains constant as the number of stored items increases.

##### Abstract (translated by Google)
人们相信大脑实施概率推理并通过人口或分布式编码来表示信息。大多数以前的基于人口的概率（PPC）理论共享几个基本属性：1）连续值神经元; 2）完全（密集）分发代码，即所有（大多数）单元参与每个代码; 3）渐变突触; 4）速率编码; 5）单元具有先天的单峰调谐功能（TF）; 6）本质上嘈杂的单位;和7）噪声/相关性被认为是有害的。我们提出一个完全不同的理论，假定：1）二元单位; 2）只有一小部分单元，即稀疏分布表示（SDR）（单元组件），包括任何单独的代码; 3）二元突触; 4）信令正式地只需要单个（即，第一个）尖峰; 5）单位最初具有完全平坦的TF（所有权重为零）; 6）单位远不如传统认为的那样内在地嘈杂;而是7）噪声是生成/用于使类似输入映射到相似代码的资源，控制存储容量之间的权衡并将输入空间统计量嵌入到存储代码的交叉点模式中，以及附带地确定跨神经元的相关模式。理论上，Sparsey在20多年前被引入，作为实现高效序列学习/识别的典型皮质电路/算法模型，但未被阐述为PPC理论的替代方案。在这里，我们表明：a）主动SDR同时代表所有存储的输入（假设）中最相似/可能的输入和整个（粗略排序）的相似性/分布;和b）给出一个输入，在学习和推理两者之下的SDR代码选择算法更新最有可能的假设和整个似然分布（参见信念更新），其中多个步骤保持不变，因为存储的数量物品增加。

##### URL
[http://arxiv.org/abs/1701.07879](http://arxiv.org/abs/1701.07879)

##### PDF
[http://arxiv.org/pdf/1701.07879](http://arxiv.org/pdf/1701.07879)

