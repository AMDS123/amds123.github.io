---
layout: post
title: "Graph Convolutional Networks based Word Embeddings"
date: 2018-09-12 07:31:06
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding CNN Represenation_Learning Relation
author: Shikhar Vashishth, Prateek Yadav, Manik Bhandari, Piyush Rai, Chiranjib Bhattacharyya, Partha Talukdar
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, word embeddings have been widely adopted across several NLP applications. However, most word embedding methods solely rely on linear context and do not provide a framework for incorporating word relationships like hypernym, nmod in a principled manner. In this paper, we propose WordGCN, a Graph Convolution based word representation learning approach which provides a framework for exploiting multiple types of word relationships. WordGCN operates at sentence as well as corpus level and allows to incorporate dependency parse based context in an efficient manner without increasing the vocabulary size. To the best of our knowledge, this is the first approach which effectively incorporates word relationships via Graph Convolutional Networks for learning word representations. Through extensive experiments on various intrinsic and extrinsic tasks, we demonstrate WordGCN's effectiveness over existing word embedding approaches. We make WordGCN's source code available to encourage reproducible research.

##### Abstract (translated by Google)
最近，在几个NLP应用程序中广泛采用了字嵌入。然而，大多数单词嵌入方法仅依赖于线性上下文，并且没有提供用于以原则方式合并诸如hypernym，nmod之类的单词关系的框架。在本文中，我们提出了WordGCN，一种基于图形卷积的单词表示学习方法，它提供了一个利用多种类型的单词关系的框架。 WordGCN在句子和语料库级别操作，并允许以有效的方式合并基于依赖性解析的上下文而不增加词汇量。据我们所知，这是第一种通过图形卷积网络有效地结合单词关系来学习单词表示的方法。通过对各种内在和外在任务的广泛实验，我们证明了WordGCN对现有单词嵌入方法的有效性。我们提供WordGCN的源代码，以鼓励可重复的研究。

##### URL
[http://arxiv.org/abs/1809.04283](http://arxiv.org/abs/1809.04283)

##### PDF
[http://arxiv.org/pdf/1809.04283](http://arxiv.org/pdf/1809.04283)

