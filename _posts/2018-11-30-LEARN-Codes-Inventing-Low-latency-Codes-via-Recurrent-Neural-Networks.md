---
layout: post
title: "LEARN Codes: Inventing Low-latency Codes via Recurrent Neural Networks"
date: 2018-11-30 10:40:31
categories: arXiv_AI
tags: arXiv_AI CNN RNN Deep_Learning
author: Yihan Jiang, Hyeji Kim, Himanshu Asnani, Sreeram Kannan, Sewoong Oh, Pramod Viswanath
mathjax: true
---

* content
{:toc}

##### Abstract
Designing channel codes under low latency constraints is one of the most demanding requirements in 5G standards. However, sharp characterizations of the performances of traditional codes are only available in the large block-length limit. Code designs are guided by those asymptotic analyses and require large block lengths and long latency to achieve the desired error rate. Furthermore, when the codes designed for one channel (e.g. Additive White Gaussian Noise (AWGN) channel) are used for another (e.g. non-AWGN channels), heuristics are necessary to achieve any nontrivial performance -thereby severely lacking in robustness as well as adaptivity. 
 Obtained by jointly designing Recurrent Neural Network (RNN) based encoder and decoder, we propose an end-to-end learned neural code which outperforms canonical convolutional code under block settings. With this gained experience of designing a novel neural block code, we propose a new class of codes under low latency constraint - Low-latency Efficient Adaptive Robust Neural (LEARN) codes, which outperforms the state-of-the-art low latency codes as well as exhibits robustness and adaptivity properties. LEARN codes show the potential of designing new versatile and universal codes for future communications via tools of modern deep learning coupled with communication engineering insights.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.12707](http://arxiv.org/abs/1811.12707)

##### PDF
[http://arxiv.org/pdf/1811.12707](http://arxiv.org/pdf/1811.12707)

