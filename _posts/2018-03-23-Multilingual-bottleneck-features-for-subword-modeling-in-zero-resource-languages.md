---
layout: post
title: "Multilingual bottleneck features for subword modeling in zero-resource languages"
date: 2018-03-23 16:18:27
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition Recognition
author: Enno Hermann, Sharon Goldwater
mathjax: true
---

* content
{:toc}

##### Abstract
How can we effectively develop speech technology for languages where no transcribed data is available? Many existing approaches use no annotated resources at all, yet it makes sense to leverage information from large annotated corpora in other languages, for example in the form of multilingual bottleneck features (BNFs) obtained from a supervised speech recognition system. In this work, we evaluate the benefits of BNFs for subword modeling (feature extraction) in six unseen languages on a word discrimination task. First we establish a strong unsupervised baseline by combining two existing methods: vocal tract length normalisation (VTLN) and the correspondence autoencoder (cAE). We then show that BNFs trained on a single language already beat this baseline; including up to 10 languages results in additional improvements which cannot be matched by just adding more data from a single language. Finally, we show that the cAE can improve further on the BNFs if high-quality same-word pairs are available.

##### Abstract (translated by Google)
我们如何才能有效地开发语音技术，用于没有转录数据的语言？许多现有的方法根本不使用注释资源，但是利用来自其他语言的大型注释语料库的信息是有意义的，例如以从监督语音识别系统获得的多语言瓶颈特征（BNF）的形式。在这项工作中，我们评估BNFs在字歧视任务中以六种不可见语言进行子字建模（特征提取）的好处。首先，我们通过结合两种现有方法建立强有力的无监督基线：声道长度归一化（VTLN）和对应自动编码器（cAE）。然后，我们显示用单一语言训练的BNF已经超过了这个基线;包括多达10种语言的结果会得到额外的改进，仅仅通过添加来自单一语言的更多数据是无法匹配的。最后，我们表明如果高质量的相同字对可用，cAE可以在BNF上进一步提高。

##### URL
[https://arxiv.org/abs/1803.08863](https://arxiv.org/abs/1803.08863)

##### PDF
[https://arxiv.org/pdf/1803.08863](https://arxiv.org/pdf/1803.08863)

