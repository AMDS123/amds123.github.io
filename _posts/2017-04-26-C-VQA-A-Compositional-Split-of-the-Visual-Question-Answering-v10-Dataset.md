---
layout: post
title: "C-VQA: A Compositional Split of the Visual Question Answering v1.0 Dataset"
date: 2017-04-26 17:57:59
categories: arXiv_CV
tags: arXiv_CV QA Attention Deep_Learning Relation VQA
author: Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, Devi Parikh
mathjax: true
---

* content
{:toc}

##### Abstract
Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.

##### Abstract (translated by Google)
视觉问答（VQA）在过去几年得到了很多关注。已经提出了许多深度学习模型来完成这个任务。然而，已经表明，这些模型主要由训练数据中的表面相关性驱动，并且缺乏合成性 - 能够回答关于所看到的概念的不可见组合的问题。这种组合性对于情报是理想的和重要的。在本文中，我们提出了一个新的视觉问题答案设置，其中测试问题 - 答案对在训练问题 - 答案对相比，在组成上是新颖的。为了便于在此设置下开发模型，我们提出了VQA v1.0数据集的一个新的组合分割，我们称之为Compositional VQA（C-VQA）。我们分析C-VQA拆分中问题和答案的分布。最后，我们评估几个现有的VQA模型在这个新的设置下，并显示这些模型的性能比原来的VQA设置显着降低。

##### URL
[https://arxiv.org/abs/1704.08243](https://arxiv.org/abs/1704.08243)

##### PDF
[https://arxiv.org/pdf/1704.08243](https://arxiv.org/pdf/1704.08243)

