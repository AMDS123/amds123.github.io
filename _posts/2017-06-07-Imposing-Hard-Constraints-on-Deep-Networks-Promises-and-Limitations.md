---
layout: post
title: "Imposing Hard Constraints on Deep Networks: Promises and Limitations"
date: 2017-06-07 02:03:00
categories: arXiv_CV
tags: arXiv_CV Prediction
author: Pablo Márquez-Neila, Mathieu Salzmann, Pascal Fua
mathjax: true
---

* content
{:toc}

##### Abstract
Imposing constraints on the output of a Deep Neural Net is one way to improve the quality of its predictions while loosening the requirements for labeled training data. Such constraints are usually imposed as soft constraints by adding new terms to the loss function that is minimized during training. An alternative is to impose them as hard constraints, which has a number of theoretical benefits but has not been explored so far due to the perceived intractability of the problem. In this paper, we show that imposing hard constraints can in fact be done in a computationally feasible way and delivers reasonable results. However, the theoretical benefits do not materialize and the resulting technique is no better than existing ones relying on soft constraints. We analyze the reasons for this and hope to spur other researchers into proposing better solutions.

##### Abstract (translated by Google)
对深度神经网络的输出施加约束是提高其预测质量的一种方式，同时放宽了标注训练数据的要求。这样的约束通常通过在训练期间将损失函数添加到最小化的损失函数中作为软约束来施加。另一种办法是把它们强加给硬约束，这样做有许多理论上的好处，但是由于被认为是棘手的问题，至今还没有探索过。在本文中，我们表明，施加硬约束事实上可以用计算可行的方式完成，并提供合理的结果。然而，理论上的收益并没有实现，而且由此产生的技术并不比现有的软约束技术更好。我们分析其原因，并希望刺激其他研究人员提出更好的解决方案。

##### URL
[https://arxiv.org/abs/1706.02025](https://arxiv.org/abs/1706.02025)

##### PDF
[https://arxiv.org/pdf/1706.02025](https://arxiv.org/pdf/1706.02025)

