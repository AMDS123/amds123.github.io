---
layout: post
title: "Learning to Reconstruct Texture-less Deformable Surfaces from a Single View"
date: 2018-07-27 14:11:25
categories: arXiv_CV
tags: arXiv_CV Face
author: Jan Bedna&#x159;&#xed;k, Pascal Fua, Mathieu Salzmann
mathjax: true
---

* content
{:toc}

##### Abstract
Recent years have seen the development of mature solutions for reconstructing deformable surfaces from a single image, provided that they are relatively well-textured. By contrast, recovering the 3D shape of texture-less surfaces remains an open problem, and essentially relates to Shape-from-Shading. In this paper, we introduce a data-driven approach to this problem. We introduce a general framework that can predict diverse 3D representations, such as meshes, normals, and depth maps. Our experiments show that meshes are ill-suited to handle texture-less 3D reconstruction in our context. Furthermore, we demonstrate that our approach generalizes well to unseen objects, and that it yields higher-quality reconstructions than a state-of-the-art SfS technique, particularly in terms of normal estimates. Our reconstructions accurately model the fine details of the surfaces, such as the creases of a T-Shirt worn by a person.

##### Abstract (translated by Google)
近年来已经看到开发用于从单个图像重建可变形表面的成熟解决方案，只要它们具有相对良好的纹理。相比之下，恢复无纹理表面的3D形状仍然是一个悬而未决的问题，并且主要涉及从形状到阴影的形状。在本文中，我们介绍了一种针对此问题的数据驱动方法。我们介绍了一个可以预测各种3D表示的通用框架，例如网格，法线和深度图。我们的实验表明，网格不适合在我们的上下文中处理无纹理的3D重建。此外，我们证明了我们的方法很好地概括了看不见的对象，并且它产生了比最先进的SfS技术更高质量的重建，特别是在正态估计方面。我们的重建精确地模拟了表面的精细细节，例如人们穿着的T恤的折痕。

##### URL
[http://arxiv.org/abs/1803.08908](http://arxiv.org/abs/1803.08908)

##### PDF
[http://arxiv.org/pdf/1803.08908](http://arxiv.org/pdf/1803.08908)

