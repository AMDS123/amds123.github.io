---
layout: post
title: "The loss surface of deep and wide neural networks"
date: 2017-06-12 19:43:39
categories: arXiv_CV
tags: arXiv_CV Face Optimization
author: Quynh Nguyen, Matthias Hein
mathjax: true
---

* content
{:toc}

##### Abstract
While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.

##### Abstract (translated by Google)
虽然深度神经网络背后的优化问题是高度非凸的，但在实践中经常观察到训练深度网络似乎是可能的，而不会陷入次优点。有人认为，所有的地方最小值都接近于全球最佳状态。我们证明这是（几乎）真实的，实际上几乎所有的局部极小值都是全局最优的，因为一个完全连通的网络具有平方损失和分析激活函数，因为网络中一层隐藏单元的数量大于数量训练点和网络结构从这一层是金字塔。

##### URL
[https://arxiv.org/abs/1704.08045](https://arxiv.org/abs/1704.08045)

##### PDF
[https://arxiv.org/pdf/1704.08045](https://arxiv.org/pdf/1704.08045)

