---
layout: post
title: "Few-shot Learning by Exploiting Visual Concepts within CNNs"
date: 2018-02-13 12:30:09
categories: arXiv_CV
tags: arXiv_CV Face CNN
author: Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille
mathjax: true
---

* content
{:toc}

##### Abstract
Convolutional neural networks (CNNs) are one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is that CNNs are complex and hard to interpret. Another is that standard CNNs require large amounts of annotated data, which is sometimes hard to obtain, and it is desirable to learn to recognize objects from few examples. In this work, we address these limitations of CNNs by developing novel, flexible, and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts (VCs), which are interpretable visual cues represented by the feature vectors within CNNs. We first adapt the learning of VCs to the few-shot setting, and then uncover two key properties of feature encoding using VCs, which we call category sensitivity and spatial pattern. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being more flexible and interpretable than alternative state-of-the-art few-shot learning methods. We conclude that using VCs helps expose the natural capability of CNNs for few-shot learning.

##### Abstract (translated by Google)
卷积神经网络（CNN）是推动计算机视觉发展的驱动力之一。尽管他们在许多任务中表现出色，但CNN在实现理想机器智能的道路上仍面临重大障碍。一个是CNN很复杂，很难解释。另一个原因是标准的CNN需要大量的注释数据，这些数据有时很难获得，并且需要学习识别少数示例中的对象。在这项工作中，我们通过为少数学习开发新颖，灵活和可解释的模型来解决CNN的这些局限性。我们的模型基于以视觉概念（VC）为单位对对象进行编码的想法，VC是由CNN内的特征向量表示的可解释的视觉线索。我们首先将VC的学习适应于少数几个设置，然后使用VC来揭示特征编码的两个关键属性，我们称之为类别敏感度和空间模式。受到这些特性的驱动，我们针对少量学习问题提出了两种直观的模型。实验表明，我们的模型实现了有竞争力的表现，同时比其他最先进的少数学习方法更具灵活性和可解释性。我们得出结论，使用风险投资有助于暴露CNNs的自然能力，以进行少量学习。

##### URL
[http://arxiv.org/abs/1711.08277](http://arxiv.org/abs/1711.08277)

##### PDF
[http://arxiv.org/pdf/1711.08277](http://arxiv.org/pdf/1711.08277)

