---
layout: post
title: "Unsupervised Learning of Long-Term Motion Dynamics for Videos"
date: 2017-04-11 22:09:03
categories: arXiv_CV
tags: arXiv_CV Represenation_Learning Classification Relation
author: Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei
mathjax: true
---

* content
{:toc}

##### Abstract
We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos. Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions. To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality. We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D. Our framework is generic to any input modality, i.e., RGB, Depth, and RGB-D videos.

##### Abstract (translated by Google)
我们提出一种无监督的表示学习方法，紧凑地编码视频中的运动依赖关系。给定一对来自视频剪辑的图像，我们的框架学习预测长期的3D运动。为了减少学习框架的复杂性，我们建议将运动描述为用RGB-D模态计算的原子3D流的序列。我们使用基于循环神经网络的编码器 - 解码器框架来预测这些流程序列。我们认为，为了解码器重建这些序列，编码器必须学习一个强大的视频表示，捕捉长期的运动依赖和空间 - 时间关系。我们展示了我们学习的时间表示对跨多种模态和数据集（如NTU RGB + D和MSR日常活动3D）的活动分类的有效性。我们的框架通用于任何输入模式，即RGB，深度和RGB-D视频。

##### URL
[https://arxiv.org/abs/1701.01821](https://arxiv.org/abs/1701.01821)

##### PDF
[https://arxiv.org/pdf/1701.01821](https://arxiv.org/pdf/1701.01821)

