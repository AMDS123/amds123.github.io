---
layout: post
title: "Word Sense Disambiguation with LSTM: Do We Really Need 100 Billion Words?"
date: 2017-12-09 10:47:19
categories: arXiv_CL
tags: arXiv_CL RNN
author: Minh Le, Marten Postma, Jacopo Urbani
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, Yuan et al. (2016) have shown the e ectiveness of using Long Short-Term Memory (LSTM) for performing Word Sense Disambiguation (WSD). Their proposed technique outperformed the previous state-of-the-art with several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study of this technique using only openly available datasets (GigaWord, SemCore, OMSTI) and software (TensorFlow). From them, it emerged that state-of-the-art results can be obtained with much less data than hinted by Yuan et al. All code and trained models are made freely available.

##### Abstract (translated by Google)
最近，袁等人。 （2016）已经显示了使用长期短期记忆（LSTM）进行词义消歧（WSD）的有效性。他们提出的技术在几个基准测试中胜过了以前的技术水平，但是训练数据和源代码都没有公布。本文介绍了这种技术的复制研究的结果只使用公开可用的数据集（GigaWord，SemCore，OMSTI）和软件（TensorFlow）。从这些数据中可以看出，远远低于元等人提出的数据。所有的代码和训练模型都是免费提供的。

##### URL
[http://arxiv.org/abs/1712.03376](http://arxiv.org/abs/1712.03376)

##### PDF
[http://arxiv.org/pdf/1712.03376](http://arxiv.org/pdf/1712.03376)

