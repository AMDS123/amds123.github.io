---
layout: post
title: "Compression of Deep Neural Networks for Image Instance Retrieval"
date: 2017-01-18 01:57:55
categories: arXiv_CV
tags: arXiv_CV CNN
author: Vijay Chandrasekhar, Jie Lin, Qianli Liao, Olivier Morère, Antoine Veillard, Lingyu Duan, Tomaso Poggio
mathjax: true
---

* content
{:toc}

##### Abstract
Image instance retrieval is the problem of retrieving images from a database which contain the same object. Convolutional Neural Network (CNN) based descriptors are becoming the dominant approach for generating {\it global image descriptors} for the instance retrieval problem. One major drawback of CNN-based {\it global descriptors} is that uncompressed deep neural network models require hundreds of megabytes of storage making them inconvenient to deploy in mobile applications or in custom hardware. In this work, we study the problem of neural network model compression focusing on the image instance retrieval task. We study quantization, coding, pruning and weight sharing techniques for reducing model size for the instance retrieval problem. We provide extensive experimental results on the trade-off between retrieval performance and model size for different types of networks on several data sets providing the most comprehensive study on this topic. We compress models to the order of a few MBs: two orders of magnitude smaller than the uncompressed models while achieving negligible loss in retrieval performance.

##### Abstract (translated by Google)
图像实例检索是从包含相同对象的数据库检索图像的问题。基于卷积神经网络（CNN）的描述符正在成为实例检索问题生成{\ it全局图像描述符}的主要方法。基于CNN的{\ it全局描述符}的一个主要缺点是未压缩的深度神经网络模型需要数百兆字节的存储空间，这使得在移动应用程序或定制硬件中部署不方便。在这项工作中，我们研究神经网络模型压缩问题的重点是图像实例检索任务。我们研究量化，编码，修剪和权重共享技术，以减少实例检索问题的模型大小。我们提供了广泛的实验结果之间的权衡取舍性能和模型大小为不同类型的网络在几个数据集提供了最全面的研究这个问题。我们将模型压缩到几MB的数量级：比未压缩模型小两个数量级，同时在检索性能上实现可忽略的损失。

##### URL
[https://arxiv.org/abs/1701.04923](https://arxiv.org/abs/1701.04923)

##### PDF
[https://arxiv.org/pdf/1701.04923](https://arxiv.org/pdf/1701.04923)

