---
layout: post
title: 'Automatic Generation of Grounded Visual Questions'
date: 2017-12-06 07:54:26
categories: arXiv_CV
tags: arXiv_CV Caption VQA
author: Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, Jiawan Zhang
---

* content
{:toc}

##### Abstract
In this paper, we propose the first model to be able to generate visually grounded questions with diverse types for a single image. Visual question generation is an emerging topic which aims to ask questions in natural language based on visual input. To the best of our knowledge, it lacks automatic methods to generate meaningful questions with various types for the same visual input. To circumvent the problem, we propose a model that automatically generates visually grounded questions with varying types. Our model takes as input both images and the captions generated by a dense caption model, samples the most probable question types, and generates the questions in sequel. The experimental results on two real world datasets show that our model outperforms the strongest baseline in terms of both correctness and diversity with a wide margin.

##### Abstract (translated by Google)
在本文中，我们提出了第一个模型，能够为单个图像生成不同类型的视觉基础问题。视觉问题的产生是一个新兴的主题，其目的是以视觉输入为基础，用自然语言提出问题。据我们所知，它没有自动的方法来产生相同的视觉输入的各种类型的有意义的问题。为了避免这个问题，我们提出了一个模型，可以自动生成不同类型的视觉基础问题。我们的模型同时输入图像和密集字幕模型生成的字幕，对最可能的问题类型进行采样，并在后续中产生问题。两个现实世界的数据集的实验结果表明，我们的模型在正确性和多样性方面优于最强的基线。

##### URL
[https://arxiv.org/abs/1612.06530](https://arxiv.org/abs/1612.06530)

