---
layout: post
title: "Composition of Sentence Embeddings:Lessons from Statistical Relational Learning"
date: 2019-04-04 10:38:33
categories: arXiv_CL
tags: arXiv_CL Knowledge Embedding Represenation_Learning Inference Prediction Relation
author: Damien Sileo, Tim Van-De-Cruys, Camille Pradel, Philippe Muller
mathjax: true
---

* content
{:toc}

##### Abstract
Various NLP problems -- such as the prediction of sentence similarity, entailment, and discourse relations -- are all instances of the same general task: the modeling of semantic relations between a pair of textual elements. A popular model for such problems is to embed sentences into fixed size vectors, and use composition functions (e.g. concatenation or sum) of those vectors as features for the prediction. At the same time, composition of embeddings has been a main focus within the field of Statistical Relational Learning (SRL) whose goal is to predict relations between entities (typically from knowledge base triples). In this article, we show that previous work on relation prediction between texts implicitly uses compositions from baseline SRL models. We show that such compositions are not expressive enough for several tasks (e.g. natural language inference). We build on recent SRL models to address textual relational problems, showing that they are more expressive, and can alleviate issues from simpler compositions. The resulting models significantly improve the state of the art in both transferable sentence representation learning and relation prediction.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1904.02464](http://arxiv.org/abs/1904.02464)

##### PDF
[http://arxiv.org/pdf/1904.02464](http://arxiv.org/pdf/1904.02464)

