---
layout: post
title: "Improving the Performance of the LSTM and HMM Models via Hybridization"
date: 2019-07-09 15:12:51
categories: arXiv_CL
tags: arXiv_CL Survey RNN Language_Model
author: Larkin Liu, Yu-Chung Lin, Joshua Reid
mathjax: true
---

* content
{:toc}

##### Abstract
Language models based on deep neural neural networks and traditionalstochastic modelling has become both highly functional and effective in recenttimes. In this work a general survey into the two types of language modelling is conducted. We investigate the effectiveness of a combination of the Hidden Markov Model (HMM) with the Long Short-Term Memory (LSTM) model via a process known as hybridization, which we introduce in this paper. This process involves combining the substitution of hidden state probabilities of the HMM into those of the LSTM. We conduct Monte Carlo sampling to produce training and validation of the data in order to produce robust results. The experimental results of this work displayed an increase in the predictive accuracy of LSTM model when hybridized with the HMM.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.04670](http://arxiv.org/abs/1907.04670)

##### PDF
[http://arxiv.org/pdf/1907.04670](http://arxiv.org/pdf/1907.04670)

