---
layout: post
title: "AceKG: A Large-scale Knowledge Graph for Academic Data Mining"
date: 2018-07-23 08:57:44
categories: arXiv_AI
tags: arXiv_AI Knowledge_Graph Knowledge Ontology Embedding Represenation_Learning Inference Classification Prediction Detection Relation
author: Ruijie Wang, Yuchen Yan, Jialu Wang, Yuting Jia, Ye Zhang, Weinan Zhang, Xinbing Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Most existing knowledge graphs (KGs) in academic domains suffer from problems of insufficient multi-relational information, name ambiguity and improper data format for large-scale machine pro- cessing. In this paper, we present AceKG, a new large-scale KG in academic domain. AceKG not only provides clean academic information, but also offers a large-scale benchmark dataset for researchers to conduct challenging data mining projects including link prediction, community detection and scholar classification. Specifically, AceKG describes 3.13 billion triples of academic facts based on a consistent ontology, including necessary properties of papers, authors, fields of study, venues and institutes, as well as the relations among them. To enrich the proposed knowledge graph, we also perform entity alignment with existing databases and rule-based inference. Based on AceKG, we conduct experiments of three typical academic data mining tasks and evaluate several state-of- the-art knowledge embedding and network representation learning approaches on the benchmark datasets built from AceKG. Finally, we discuss several promising research directions that benefit from AceKG.

##### Abstract (translated by Google)
学术领域中的大多数现有知识图（KG）都存在多关系信息不足，名称模糊以及大规模机器处理的不正确数据格式等问题。在本文中，我们介绍AceKG，一个新的大型KG在学术领域。 AceKG不仅提供清晰的学术信息，还为研究人员提供了一个大规模的基准数据集，用于开展具有挑战性的数据挖掘项目，包括链接预测，社区检测和学者分类。具体而言，AceKG根据一致的本体论描述了31.3亿三倍的学术事实，包括论文，作者，研究领域，场地和研究所的必要属性，以及它们之间的关系。为了丰富提出的知识图，我们还执行与现有数据库和基于规则的推理的实体对齐。基于AceKG，我们进行了三个典型的学术数据挖掘任务的实验，并在AceKG构建的基准数据集上评估了几种最先进的知识嵌入和网络表示学习方法。最后，我们讨论了受益于AceKG的几个有前途的研究方向。

##### URL
[http://arxiv.org/abs/1807.08484](http://arxiv.org/abs/1807.08484)

##### PDF
[http://arxiv.org/pdf/1807.08484](http://arxiv.org/pdf/1807.08484)

