---
layout: post
title: "Query-adaptive Video Summarization via Quality-aware Relevance Estimation"
date: 2017-09-28 13:18:56
categories: arXiv_CL
tags: arXiv_CL Attention Summarization Embedding Prediction
author: Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin, Luc Van Gool
mathjax: true
---

* content
{:toc}

##### Abstract
Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.

##### Abstract (translated by Google)
尽管自动视频摘要的问题最近受到了很多关注，但是创建视频摘要的问题还突出显示了与搜索查询有关的元素，这一点已经被较少研究。我们通过将查询相关摘要作为视频帧子集选择问题来解决这个问题，这使得我们可以优化同时多样化的摘要，代表整个视频，并且与文本查询相关。我们通过在由神经网络诱导的普通文本 - 视觉语义嵌入空间中测量帧与查询之间的距离来量化相关性。另外，我们扩展模型以捕捉与查询无关的属性，例如框架质量。我们比较我们的方法与先前的艺术在缩略图选择的文本视觉嵌入，并显示我们的模型胜过他们的相关性预测。此外，我们引入一个新的数据集，注释与多样性和查询特定相关标签。在这个数据集上，我们训练和测试我们的视频摘要的完整模型，并且显示出它优于标准基线，如最大边缘相关性。

##### URL
[https://arxiv.org/abs/1705.00581](https://arxiv.org/abs/1705.00581)

##### PDF
[https://arxiv.org/pdf/1705.00581](https://arxiv.org/pdf/1705.00581)

