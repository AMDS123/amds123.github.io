---
layout: post
title: "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making"
date: 2018-05-23 16:46:13
categories: arXiv_AI
tags: arXiv_AI Knowledge Reinforcement_Learning
author: Fangkai Yang, Daoming Lyu, Bo Liu, Steven Gustafson
mathjax: true
---

* content
{:toc}

##### Abstract
Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic planning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework {\em PEORL} that integrates symbolic planning with hierarchical reinforcement learning (HRL) to cope with decision-making in a dynamic environment with uncertainties. 
 Symbolic plans are used to guide the agent's task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL.

##### Abstract (translated by Google)
强化学习和符号计划都被用于构建智能自治代理。强化学习依赖于与现实世界的交互学习，这往往需要大量的经验。符号规划依赖于人工制作的符号知识，这对于领域的不确定性和变化可能不够强大。在本文中，我们提出了一个统一的框架{\ em PEORL}，它将符号规划与分层强化学习（HRL）相结合，以应对具有不确定性的动态环境中的决策。
 符号计划用于指导代理人的任务执行和学习，并将学到的经验反馈给符号知识以改进计划。这种方法导致快速的政策搜索和强大的复杂领域的符号计划。该框架在HRL的基准域进行测试。

##### URL
[http://arxiv.org/abs/1804.07779](http://arxiv.org/abs/1804.07779)

##### PDF
[http://arxiv.org/pdf/1804.07779](http://arxiv.org/pdf/1804.07779)

