---
layout: post
title: "Sequence Generation with Guider Network"
date: 2018-11-02 01:21:17
categories: arXiv_CL
tags: arXiv_CL Sparse Attention Reinforcement_Learning Optimization Prediction
author: Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Liqun Chen, Dinghan Shen, Guoyin Wang, Lawrence Carin
mathjax: true
---

* content
{:toc}

##### Abstract
Sequence generation with reinforcement learning (RL) has received significant attention recently. However, a challenge with such methods is the sparse-reward problem in the RL training process, in which a scalar guiding signal is often only available after an entire sequence has been generated. This type of sparse reward tends to ignore the global structural information of a sequence, causing generation of sequences that are semantically inconsistent. In this paper, we present a model-based RL approach to overcome this issue. Specifically, we propose a novel guider network to model the sequence-generation environment, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments show that the proposed method leads to improved performance for both unconditional and conditional sequence-generation tasks.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.00696](http://arxiv.org/abs/1811.00696)

##### PDF
[http://arxiv.org/pdf/1811.00696](http://arxiv.org/pdf/1811.00696)

