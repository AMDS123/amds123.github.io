---
layout: post
title: 'Excitation Backprop for RNNs'
date: 2017-11-30 19:23:46
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption Action_Recognition RNN Recognition
author: Sarah Adel Bargal, Andrea Zunino, Donghyun Kim, Jianming Zhang, Vittorio Murino, Stan Sclaroff
---

* content
{:toc}

##### Abstract
Deep models are state-of-the-art for many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks.

##### Abstract (translated by Google)
深度模型是许多视觉任务的最新技术，包括视频动作识别和视频字幕。对模型进行培训，以对视频中的活动进行标题或分类，但对用于做出此类决定的证据知之甚少。深度网络做出的接地决策已经在空间视觉内容中进行了研究，为图像的模型预测提供了更多的信息。然而，这些研究对于时空视觉内容的模型 - 视频相对缺乏。在这项工作中，我们设计了一个同时使用自上而下的显着性在一个时间空间和时间的证据。我们使用模型的内部表示形象化了对深层模型的分类/字幕输出有贡献的时空线索。基于这些时空提示，我们能够在视频中对与特定动作相对应的片段进行本地化，或者对来自标题的短语进行本地化，而不对这些任务进行明确的优化/训练。

##### URL
[https://arxiv.org/abs/1711.06778](https://arxiv.org/abs/1711.06778)

