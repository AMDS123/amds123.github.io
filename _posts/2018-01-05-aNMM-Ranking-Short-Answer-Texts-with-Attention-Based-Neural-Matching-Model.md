---
layout: post
title: "aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model"
date: 2018-01-05 06:06:17
categories: arXiv_CL
tags: arXiv_CL QA Attention CNN RNN Deep_Learning
author: Liu Yang, Qingyao Ai, Jiafeng Guo, W. Bruce Croft
mathjax: true
---

* content
{:toc}

##### Abstract
As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have recently been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In this paper, we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network. Using the popular benchmark TREC QA data, we show that the relatively simple aNMM model can significantly outperform other neural network models that have been used for the question answering task, and is competitive with models that are combined with additional features. When aNMM is combined with additional features, it outperforms all baselines.

##### Abstract (translated by Google)
作为基于特征工程的问题答案方法的替代方法，最近已经提出了诸如卷积神经网络（CNN）和长期短期记忆模型（LSTM）的深度学习方法用于问题和答案的语义匹配。然而，为了获得好的结果，这些模型已经结合了词汇重叠或BM25分数等附加特征。没有这种组合，这些模型比基于语言特征工程的方法表现得更差。在本文中，我们提出了一种基于注意力的神经匹配模型来排序简答题文本。我们采用价值共享加权方案，而不是位置分享加权方案来组合不同的匹配信号，并且使用问题关注网络结合问题重要性学习。使用流行的基准TREC QA数据，我们表明，相对简单的aNMM模型可以显着超越已经用于问题回答任务的其他神经网络模型，并且与具有附加特征的模型相竞争。当aNMM与附加功能组合时，它的性能优于所有基线。

##### URL
[http://arxiv.org/abs/1801.01641](http://arxiv.org/abs/1801.01641)

##### PDF
[http://arxiv.org/pdf/1801.01641](http://arxiv.org/pdf/1801.01641)

