---
layout: post
title: "Dynamic Fusion with Intra- and Inter- Modality Attention Flow for Visual Question Answering"
date: 2018-12-13 03:41:18
categories: arXiv_CV
tags: arXiv_CV QA Attention VQA
author: Gao Peng, Hongsheng Li, Haoxuan You, Zhengkai Jiang, Pan Lu, Steven Hoi, Xiaogang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Learning effective fusion of multi-modality features is at the heart of visual question answering. We propose a novel method of dynamically fusing multi-modal features with intra- and inter-modality information flow, which alternatively pass dynamic information between and across the visual and language modalities. It can robustly capture the high-level interactions between language and vision domains, thus significantly improves the performance of visual question answering. We also show that the proposed dynamic intra-modality attention flow conditioned on the other modality can dynamically modulate the intra-modality attention of the target modality, which is vital for multimodality feature fusion. Experimental evaluations on the VQA 2.0 dataset show that the proposed method achieves state-of-the-art VQA performance. Extensive ablation studies are carried out for the comprehensive analysis of the proposed method.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.05252](http://arxiv.org/abs/1812.05252)

##### PDF
[http://arxiv.org/pdf/1812.05252](http://arxiv.org/pdf/1812.05252)

