---
layout: post
title: "Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning"
date: 2018-09-08 01:20:45
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning
author: Shuai Tang, Virginia R. de Sa
mathjax: true
---

* content
{:toc}

##### Abstract
The encoder-decoder models for unsupervised sentence representation learning tend to discard the decoder after being trained on a large unlabelled corpus, since only the encoder is needed to map the input sentence into a vector representation. However, parameters learnt in the decoder also contain useful information about language. In order to utilise the decoder after learning, we present two types of decoding functions whose inverse can be easily derived without expensive inverse calculation. Therefore, the inverse of the decoding function serves as another encoder that produces sentence representations. We show that, with careful design of the decoding functions, the model learns good sentence representations, and the ensemble of the representations produced from the encoder and the inverse of the decoder demonstrate even better generalisation ability and solid transferability.

##### Abstract (translated by Google)
用于无监督语句表示学习的编码器 - 解码器模型倾向于在大型未标记语料库上训练之后丢弃解码器，因为仅需要编码器将输入语句映射到矢量表示。但是，在解码器中学习的参数也包含有关语言的有用信息。为了在学习之后利用解码器，我们提出了两种类型的解码函数，其逆可以容易地导出而无需昂贵的逆计算。因此，解码函数的逆函数用作产生句子表示的另一编码器。我们表明，通过对解码函数的精心设计，该模型学习了良好的句子表示，并且从编码器和解码器的逆产生的表示的集合表现出更好的泛化能力和可靠的可转移性。

##### URL
[http://arxiv.org/abs/1809.02731](http://arxiv.org/abs/1809.02731)

##### PDF
[http://arxiv.org/pdf/1809.02731](http://arxiv.org/pdf/1809.02731)

