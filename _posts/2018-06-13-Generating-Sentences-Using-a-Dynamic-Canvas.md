---
layout: post
title: "Generating Sentences Using a Dynamic Canvas"
date: 2018-06-13 12:57:19
categories: arXiv_CL
tags: arXiv_CL Attention
author: Harshil Shah, Bowen Zheng, David Barber
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word level generative model for natural language. It uses a recurrent neural network with a dynamic attention and canvas memory mechanism to iteratively construct sentences. By viewing the state of the memory at intermediate stages and where the model is placing its attention, we gain insight into how it constructs sentences. We demonstrate that AUTR learns a meaningful latent representation for each sentence, and achieves competitive log-likelihood lower bounds whilst being computationally efficient. It is effective at generating and reconstructing sentences, as well as imputing missing words.

##### Abstract (translated by Google)
我们引入注意无监督文本（W）riter（AUTR），它是一种自然语言的词级生成模型。它使用具有动态注意力和画布记忆机制的循环神经网络来迭代构造句子。通过查看中间阶段的记忆状态以及模型关注的位置，我们可以深入了解它如何构造句子。我们证明AUTR为每个句子学习一个有意义的潜在表示，并且在计算效率上达到竞争对数似然下界。它对于生成和重构句子以及填补遗漏的单词非常有效。

##### URL
[http://arxiv.org/abs/1806.05178](http://arxiv.org/abs/1806.05178)

##### PDF
[http://arxiv.org/pdf/1806.05178](http://arxiv.org/pdf/1806.05178)

