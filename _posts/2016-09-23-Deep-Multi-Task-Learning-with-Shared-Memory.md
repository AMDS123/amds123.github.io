---
layout: post
title: "Deep Multi-Task Learning with Shared Memory"
date: 2016-09-23 03:35:27
categories: arXiv_CL
tags: arXiv_CL Text_Classification Classification
author: Pengfei Liu, Xipeng Qiu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks.

##### Abstract (translated by Google)
基于神经网络的模型在各种具体任务上取得了令人印象深刻的成果然而，在以前的工作中，大多数模型是基于单任务监督目标而分别学习的，这些目标经常受到训练数据不足的困扰。在本文中，我们提出两个深层架构，可以共同训练多个相关的任务。更具体地说，我们用一个外部存储器来增加神经模型，这是由几个任务共享的。对两组文本分类任务的实验表明，我们提出的体系结构可以在其他相关任务的帮助下提高任务的性能。

##### URL
[https://arxiv.org/abs/1609.07222](https://arxiv.org/abs/1609.07222)

##### PDF
[https://arxiv.org/pdf/1609.07222](https://arxiv.org/pdf/1609.07222)

