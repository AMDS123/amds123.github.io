---
layout: post
title: "Context Models for OOV Word Translation in Low-Resource Languages"
date: 2018-01-26 02:50:03
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention Language_Model
author: Angli Liu, Katrin Kirchhoff
mathjax: true
---

* content
{:toc}

##### Abstract
Out-of-vocabulary word translation is a major problem for the translation of low-resource languages that suffer from a lack of parallel training data. This paper evaluates the contributions of target-language context models towards the translation of OOV words, specifically in those cases where OOV translations are derived from external knowledge sources, such as dictionaries. We develop both neural and non-neural context models and evaluate them within both phrase-based and self-attention based neural machine translation systems. Our results show that neural language models that integrate additional context beyond the current sentence are the most effective in disambiguating possible OOV word translations. We present an efficient second-pass lattice-rescoring method for wide-context neural language models and demonstrate performance improvements over state-of-the-art self-attention based neural MT systems in five out of six low-resource language pairs.

##### Abstract (translated by Google)
词汇之外的单词翻译是翻译缺乏平行训练数据的低资源语言的主要问题。本文评估了目标语言语境模式对翻译OOV词的贡献，特别是在OOV翻译源自词典等外部知识源的情况下。我们开发神经和非神经背景模型，并在基于短语和基于自我注意的神经机器翻译系统中进行评估。我们的研究结果显示，在当前句子之外整合附加语境的神经语言模型在消除可能的OOV单词翻译中是最有效的。我们提出了一个有效的第二遍格子重新调整方法的广泛上下文的神经语言模型，并展示性能改善超过最先进的基于自我注意的神经机器翻译系统六个低资源语言对中的五个。

##### URL
[https://arxiv.org/abs/1801.08660](https://arxiv.org/abs/1801.08660)

##### PDF
[https://arxiv.org/pdf/1801.08660](https://arxiv.org/pdf/1801.08660)

