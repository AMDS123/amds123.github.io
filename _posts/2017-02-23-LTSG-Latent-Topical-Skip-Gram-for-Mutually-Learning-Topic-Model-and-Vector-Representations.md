---
layout: post
title: "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations"
date: 2017-02-23 07:16:03
categories: arXiv_SD
tags: arXiv_SD Knowledge Embedding
author: Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu Rong (Dept. of Computer Science, Sun Yat-Sen University, GuangZhou, China.)
mathjax: true
---

* content
{:toc}

##### Abstract
Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.

##### Abstract (translated by Google)
主题模型已被广泛用于发现文本挖掘中文档间共享的潜在主题。向量表示，单词嵌入和主题嵌入，将单词和主题映射到低维和密集的实值向量空间，在NLP任务中获得了高性能。然而，现有的大多数模型都假设其中一个模型的训练结果是完全正确的，并被用作改进其他模型的先验知识。其他一些模型使用外部大语料库训练的信息来帮助改进较小的语料库。在本文中，我们的目标是建立一个算法框架，使主题模型和向量表示在相同语料库中相互提高。 EM风格的算法框架被用来迭代地优化主题模型和向量表示。实验结果表明，我们的模型胜过了各种NLP任务的最先进的方法。

##### URL
[https://arxiv.org/abs/1702.07117](https://arxiv.org/abs/1702.07117)

##### PDF
[https://arxiv.org/pdf/1702.07117](https://arxiv.org/pdf/1702.07117)

