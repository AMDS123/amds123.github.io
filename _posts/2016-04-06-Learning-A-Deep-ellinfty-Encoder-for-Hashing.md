---
layout: post
title: "Learning A Deep $ell_infty$ Encoder for Hashing"
date: 2016-04-06 03:54:33
categories: arXiv_CV
tags: arXiv_CV Regularization Deep_Learning
author: Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, Thomas S. Huang
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate the $\ell_\infty$-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named \textit{Deep $\ell_\infty$ Encoder}, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a \textit{Deep Siamese $\ell_\infty$ Network}, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.

##### Abstract (translated by Google)
我们研究了$ \ ell_ \ infty $ -constrained表示法，它表现出对量化误差的鲁棒性，利用深度学习的工具。基于交替方向乘子法（ADMM），通过引入新的有界线性单元（BLU），将原始凸极小化问题作为前馈神经网络，命名为\ textit {Deep $ \ ell_ \ infty $ Encoder} ）神经元和拉格朗日乘子作为网络偏见建模。这样的结构先验作为一个有效的网络正则化，并促进模型初始化。然后，我们研究了所提议的模型在散列应用中的有效使用，通过将所提出的编码器在监督的成对损失下耦合来开发一个可以从末端进行优化的\ textit {深度连接$ \ ell_ \ infty $网络}结束。广泛的实验证明了该模型的令人印象深刻的性能。我们还提供了对竞争对手行为的深入分析。

##### URL
[https://arxiv.org/abs/1604.01475](https://arxiv.org/abs/1604.01475)

##### PDF
[https://arxiv.org/pdf/1604.01475](https://arxiv.org/pdf/1604.01475)

