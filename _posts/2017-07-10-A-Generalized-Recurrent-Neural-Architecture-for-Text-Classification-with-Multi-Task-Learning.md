---
layout: post
title: "A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning"
date: 2017-07-10 14:58:53
categories: arXiv_CL
tags: arXiv_CL Text_Classification Classification Relation
author: Honglun Zhang, Liqiang Xiao, Yongkun Wang, Yaohui Jin
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-task learning leverages potential correlations among related tasks to extract common features and yield performance gains. However, most previous works only consider simple or weak interactions, thereby failing to model complex correlations among three or more tasks. In this paper, we propose a multi-task learning architecture with four types of recurrent neural layers to fuse information across multiple related tasks. The architecture is structurally flexible and considers various interactions among tasks, which can be regarded as a generalized case of many previous works. Extensive experiments on five benchmark datasets for text classification show that our model can significantly improve performances of related tasks with additional information from others.

##### Abstract (translated by Google)
多任务学习利用相关任务之间潜在的相关性来提取共同特征并提高性能收益。然而，大多数以前的作品只考虑简单或弱的相互作用，因此不能模拟三个或更多任务之间的复杂相关性。在本文中，我们提出了一种多任务学习架构，它具有四种递归神经层来融合多个相关任务的信息。这个架构在结构上是灵活的，考虑了各种任务之间的各种交互，可以看作是以前许多作品的一个普遍的例子。对五个文本分类基准数据集的大量实验表明，我们的模型可以显着提高其他相关任务的性能。

##### URL
[https://arxiv.org/abs/1707.02892](https://arxiv.org/abs/1707.02892)

##### PDF
[https://arxiv.org/pdf/1707.02892](https://arxiv.org/pdf/1707.02892)

