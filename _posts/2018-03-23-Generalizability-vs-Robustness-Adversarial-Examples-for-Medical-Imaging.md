---
layout: post
title: "Generalizability vs. Robustness: Adversarial Examples for Medical Imaging"
date: 2018-03-23 10:43:16
categories: arXiv_CV
tags: arXiv_CV Adversarial Segmentation Classification Deep_Learning
author: Magdalini Paschali, Sailesh Conjeti, Fernando Navarro, Nassir Navab
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, for the first time, we propose an evaluation method for deep learning models that assesses the performance of a model not only in an unseen test scenario, but also in extreme cases of noise, outliers and ambiguous input data. To this end, we utilize adversarial examples, images that fool machine learning models, while looking imperceptibly different from original data, as a measure to evaluate the robustness of a variety of medical imaging models. Through extensive experiments on skin lesion classification and whole brain segmentation with state-of-the-art networks such as Inception and UNet, we show that models that achieve comparable performance regarding generalizability may have significant variations in their perception of the underlying data manifold, leading to an extensive performance gap in their robustness.

##### Abstract (translated by Google)
在本文中，我们首次提出了一种深度学习模型的评估方法，该方法不仅在一个看不见的测试场景中评估模型的性能，而且在噪声，异常值和模糊输入数据的极端情况下评估模型的性能。为此，我们利用对抗性的例子，愚弄机器学习模型的图像，同时看起来与原始数据不可分辨地不同，作为评估各种医学成像模型的稳健性的措施。通过使用最先进的网络，如Inception和UNet，对皮肤病变分类和整个脑部分割进行广泛的实验，我们表明，实现类似于普遍性的性能的模型可能在其对基础数据流形的感知方面有显着变化，在稳健性方面存在广泛的性能差距。

##### URL
[http://arxiv.org/abs/1804.00504](http://arxiv.org/abs/1804.00504)

##### PDF
[http://arxiv.org/pdf/1804.00504](http://arxiv.org/pdf/1804.00504)

