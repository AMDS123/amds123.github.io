---
layout: post
title: "Practical Adversarial Attack Against Object Detector"
date: 2018-12-26 04:14:08
categories: arXiv_CV
tags: arXiv_CV Adversarial Object_Detection Attention Detection
author: Yue Zhao, Hong Zhu, Qintao Shen, Ruigang Liang, Kai Chen, Shengzhi Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we proposed the first practical adversarial attacks against object detectors in realistic situations: the adversarial examples are placed in different angles and distances, especially in the long distance (over 20m) and wide angles 120 degree. To improve the robustness of adversarial examples, we proposed the nested adversarial examples and introduced the image transformation techniques. Transformation methods aim to simulate the variance factors such as distances, angles, illuminations, etc., in the physical world. Two kinds of attacks were implemented on YOLO V3, a state-of-the-art real-time object detector: hiding attack that fools the detector unable to recognize the object, and appearing attack that fools the detector to recognize the non-existent object. The adversarial examples are evaluated in three environments: indoor lab, outdoor environment, and the real road, and demonstrated to achieve the success rate up to 92.4% based on the distance range from 1m to 25m. In particular, the real road testing of hiding attack on a straight road and a crossing road produced the success rate of 75% and 64% respectively, and the appearing attack obtained the success rates of 63% and 81% respectively, which we believe, should catch the attention of the autonomous driving community.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.10217](http://arxiv.org/abs/1812.10217)

##### PDF
[http://arxiv.org/pdf/1812.10217](http://arxiv.org/pdf/1812.10217)

