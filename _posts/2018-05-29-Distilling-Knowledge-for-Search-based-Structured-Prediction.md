---
layout: post
title: "Distilling Knowledge for Search-based Structured Prediction"
date: 2018-05-29 02:39:43
categories: arXiv_CL
tags: arXiv_CL Knowledge Prediction
author: Yijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin, Ting Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Many natural language processing tasks can be modeled into structured prediction and solved as a search problem. In this paper, we distill an ensemble of multiple models trained with different initialization into a single model. In addition to learning to match the ensemble's probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration. Experimental results on two typical search-based structured prediction tasks -- transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model's performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.

##### Abstract (translated by Google)
许多自然语言处理任务可以建模为结构化预测，并作为搜索问题解决。在本文中，我们将用不同初始化训练的多个模型集成到一个模型中。除了学习在参考状态上匹配集合的概率输出之外，我们还使用集合来探索搜索空间并从勘探中遇到的状态中学习。在两个典型的基于搜索的结构化预测任务 - 基于转换的依赖分析和神经机器翻译的实验结果表明，蒸馏可以有效地提高单个模型的性能，最终模型在这两个方面实现了LAS的1.32和BLEU得分的2.65的改善任务分别超过强基线，并且胜过以前文献中的贪心结构预测模型。

##### URL
[http://arxiv.org/abs/1805.11224](http://arxiv.org/abs/1805.11224)

##### PDF
[http://arxiv.org/pdf/1805.11224](http://arxiv.org/pdf/1805.11224)

