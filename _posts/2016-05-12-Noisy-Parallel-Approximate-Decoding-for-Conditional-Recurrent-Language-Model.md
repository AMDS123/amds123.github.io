---
layout: post
title: "Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model"
date: 2016-05-12 14:39:50
categories: arXiv_CV
tags: arXiv_CV Attention Speech_Recognition Language_Model Recognition
author: Kyunghyun Cho
mathjax: true
---

* content
{:toc}

##### Abstract
Recent advances in conditional recurrent language modelling have mainly focused on network architectures (e.g., attention mechanism), learning algorithms (e.g., scheduled sampling and sequence-level training) and novel applications (e.g., image/video description generation, speech recognition, etc.) On the other hand, we notice that decoding algorithms/strategies have not been investigated as much, and it has become standard to use greedy or beam search. In this paper, we propose a novel decoding strategy motivated by an earlier observation that nonlinear hidden layers of a deep neural network stretch the data manifold. The proposed strategy is embarrassingly parallelizable without any communication overhead, while improving an existing decoding algorithm. We extensively evaluate it with attention-based neural machine translation on the task of En->Cz translation.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1605.03835](https://arxiv.org/abs/1605.03835)

##### PDF
[https://arxiv.org/pdf/1605.03835](https://arxiv.org/pdf/1605.03835)

