---
layout: post
title: "Propositional Knowledge Representation and Reasoning in Restricted Boltzmann Machines"
date: 2018-05-29 04:44:31
categories: arXiv_AI
tags: arXiv_AI Knowledge Attention Inference
author: Son N. Tran
mathjax: true
---

* content
{:toc}

##### Abstract
While knowledge representation and reasoning are considered the keys for human-level artificial intelligence, connectionist networks have been shown successful in a broad range of applications due to their capacity for robust learning and flexible inference under uncertainty. The idea of representing symbolic knowledge in connectionist networks has been well-received and attracted much attention from research community as this can establish a foundation for integration of scalable learning and sound reasoning. In previous work, there exist a number of approaches that map logical inference rules with feed-forward propagation of artificial neural networks (ANN). However, the discriminative structure of an ANN requires the separation of input/output variables which makes it difficult for general reasoning where any variables should be inferable. Other approaches address this issue by employing generative models such as symmetric connectionist networks, however, they are difficult and convoluted. In this paper we propose a novel method to represent propositional formulas in restricted Boltzmann machines which is less complex, especially in the cases of logical implications and Horn clauses. An integration system is then developed and evaluated in real datasets which shows promising results.

##### Abstract (translated by Google)
尽管知识表示和推理被认为是人类级人工智能的关键，但连接主义网络由于其在不确定性下的强大学习和灵活推理的能力而在广泛的应用中已经被证明是成功的。在连接网络中表示象征知识的想法已经得到了广泛的接受，并且引起了研究界的广泛关注，因为这可以为可扩展学习和合理推理的整合奠定基础。在以前的工作中，存在许多将逻辑推理规则与人工神经网络（ANN）的前馈传播相映射的方法。然而，人工神经网络的辨别结构要求分离输入/输出变量，这使得在任何变量应该可推断的情况下，难以进行一般推理。其他方法通过使用生成模型来解决这个问题，例如对称连接网络，然而，它们很难并且令人费解。在本文中，我们提出了一种新颖的方法来表示限制玻尔兹曼机器中的命题公式，这种方法不那么复杂，特别是在逻辑含义和Horn子句的情况下。然后在真实数据集中开发并评估一个集成系统，这些数据集显示出有希望的结果

##### URL
[http://arxiv.org/abs/1705.10899](http://arxiv.org/abs/1705.10899)

##### PDF
[http://arxiv.org/pdf/1705.10899](http://arxiv.org/pdf/1705.10899)

