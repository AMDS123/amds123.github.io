---
layout: post
title: "Semantic Composition via Probabilistic Model Theory"
date: 2017-09-01 10:01:52
categories: arXiv_CL
tags: arXiv_CL
author: Guy Emerson, Ann Copestake
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic composition remains an open problem for vector space models of semantics. In this paper, we explain how the probabilistic graphical model used in the framework of Functional Distributional Semantics can be interpreted as a probabilistic version of model theory. Building on this, we explain how various semantic phenomena can be recast in terms of conditional probabilities in the graphical model. This connection between formal semantics and machine learning is helpful in both directions: it gives us an explicit mechanism for modelling context-dependent meanings (a challenge for formal semantics), and also gives us well-motivated techniques for composing distributed representations (a challenge for distributional semantics). We present results on two datasets that go beyond word similarity, showing how these semantically-motivated techniques improve on the performance of vector models.

##### Abstract (translated by Google)
语义构成仍然是向量空间语义模型的开放问题。在本文中，我们解释了在功能分布语义框架中使用的概率图模型如何被解释为模型理论的概率版本。在此基础上，我们解释了在图形模型中，各种语义现象如何根据条件概率进行重构。形式语义和机器学习之间的这种联系在两个方面都是有帮助的：它为我们提供了一个建模上下文相关意义的明确机制（对形式语义的一个挑战），也为我们组织分布式表示提供了积极的技巧分布式语义）。我们在超出单词相似性的两个数据集上呈现结果，显示这些语义驱动的技术如何改善向量模型的性能。

##### URL
[https://arxiv.org/abs/1709.00226](https://arxiv.org/abs/1709.00226)

##### PDF
[https://arxiv.org/pdf/1709.00226](https://arxiv.org/pdf/1709.00226)

