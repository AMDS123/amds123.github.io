---
layout: post
title: "Long Short-Term Memory-Networks for Machine Reading"
date: 2016-09-20 21:20:09
categories: arXiv_CV
tags: arXiv_CV Sentiment Attention Inference Language_Model Relation
author: Jianpeng Cheng, Li Dong, Mirella Lapata
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.

##### Abstract (translated by Google)
在本文中，我们解决了如何更好地处理序列级网络以处理结构化输入的问题。我们提出了一个机器阅读模拟器，它从左到右递增地处理文本，并用记忆和注意力进行浅层推理。读者用存储器网络代替单个存储器单元来扩展长期短期存储器结构。这使得在神经关注的复发期间适应性记忆的使用，提供弱诱导令牌之间的关系的方式。该系统最初设计为处理单个序列，但我们也演示如何将其与编码器 - 解码器架构集成。在语言建模，情感分析和自然语言推理方面的实验表明，我们的模型匹配或超越了现有技术水平。

##### URL
[https://arxiv.org/abs/1601.06733](https://arxiv.org/abs/1601.06733)

##### PDF
[https://arxiv.org/pdf/1601.06733](https://arxiv.org/pdf/1601.06733)

