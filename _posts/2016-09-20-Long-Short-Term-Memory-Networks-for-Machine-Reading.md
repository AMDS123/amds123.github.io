---
layout: post
title: "Long Short-Term Memory-Networks for Machine Reading"
date: 2016-09-20 21:20:09
categories: arXiv_CV
tags: arXiv_CV Sentiment Attention Inference Language_Model Relation
author: Jianpeng Cheng, Li Dong, Mirella Lapata
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1601.06733](https://arxiv.org/abs/1601.06733)

##### PDF
[https://arxiv.org/pdf/1601.06733](https://arxiv.org/pdf/1601.06733)

