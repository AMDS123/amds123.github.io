---
layout: post
title: "Neural Variational Inference for Text Processing"
date: 2016-06-04 06:41:58
categories: arXiv_CL
tags: arXiv_CL Attention Inference
author: Yishu Miao, Lei Yu, Phil Blunsom
mathjax: true
---

* content
{:toc}

##### Abstract
Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.

##### Abstract (translated by Google)
神经变分推理的最新进展已经催生了深潜变量模型的复兴。在本文中，我们介绍一个通用的变分推理框架的生成和条件模型的文本。虽然传统的变分方法导出了对潜变量难以处理的分布的解析近似，但是在这里我们构造了一个以离散文本输入为条件的推理网络来提供变分布。我们验证了这个框架在两个非常不同的文本建模应用程序，生成文档建模和监督问题回答。我们的神经变分文档模型将连续的随机文档表示与词袋生成模型相结合，并在两个标准测试语料库上达到最低的报告困惑度。神经应答选择模型使用关注机制内的随机表示层来提取问题和答案对之间的语义。在两个问题回答基准上，这个模型超出了所有以前公布的基准。

##### URL
[https://arxiv.org/abs/1511.06038](https://arxiv.org/abs/1511.06038)

##### PDF
[https://arxiv.org/pdf/1511.06038](https://arxiv.org/pdf/1511.06038)

