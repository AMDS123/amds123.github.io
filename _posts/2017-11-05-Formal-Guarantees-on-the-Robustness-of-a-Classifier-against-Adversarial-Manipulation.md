---
layout: post
title: "Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation"
date: 2017-11-05 20:58:09
categories: arXiv_CV
tags: arXiv_CV Regularization Adversarial Classification Prediction
author: Matthias Hein, Maksym Andriushchenko
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.

##### Abstract (translated by Google)
最近的工作表明，最先进的分类器是相当脆弱的，就是说，原本具有高置信度的正确分类输入的小的对抗性变化导致了高信度的错误分类。这引起了人们的担忧，即这种分类器容易受到攻击，并质疑其在安全关键系统中的使用。在本文中，我们首先通过给出改变分类器决策所需的输入操作的规范的实例特定的下限来首次正式保证分类器的健壮性。基于这个分析，我们提出了交叉Lipschitz正则化函数。我们展示了在内核方法中使用这种正则化形式。神经网络提高了分类器的鲁棒性，而不会有任何预测性能的损失。

##### URL
[https://arxiv.org/abs/1705.08475](https://arxiv.org/abs/1705.08475)

##### PDF
[https://arxiv.org/pdf/1705.08475](https://arxiv.org/pdf/1705.08475)

