---
layout: post
title: "A General Theory of Equivariant CNNs on Homogeneous Spaces"
date: 2018-11-05 20:22:10
categories: arXiv_AI
tags: arXiv_AI CNN Classification
author: Taco Cohen, Mario Geiger, Maurice Weiler
mathjax: true
---

* content
{:toc}

##### Abstract
Group equivariant convolutional neural networks (G-CNNs) have recently emerged as a very effective model class for learning from signals in the context of known symmetries. A wide variety of equivariant layers has been proposed for signals on 2D and 3D Euclidean space, graphs, and the sphere, and it has become difficult to see how all of these methods are related, and how they may be generalized. In this paper, we present a fairly general theory of equivariant convolutional networks. Convolutional feature spaces are described as fields over a homogeneous base space, such as the plane $\mathbb{R}^2$, sphere $S^2$ or a graph $\mathcal{G}$. The theory enables a systematic classification of all existing G-CNNs in terms of their group of symmetry, base space, and field type (e.g. scalar, vector, or tensor field, etc.). In addition to this classification, we use Mackey theory to show that convolutions with equivariant kernels are the most general class of equivariant maps between such fields, thus establishing G-CNNs as a universal class of equivariant networks. The theory also explains how the space of equivariant kernels can be parameterized for learning, thereby simplifying the development of G-CNNs for new spaces and symmetries. Finally, the theory introduces a rich geometric semantics to learned feature spaces, thus improving interpretability of deep networks, and establishing a connection to central ideas in mathematics and physics.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1811.02017](https://arxiv.org/abs/1811.02017)

##### PDF
[https://arxiv.org/pdf/1811.02017](https://arxiv.org/pdf/1811.02017)

