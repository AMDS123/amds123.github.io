---
layout: post
title: "Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral-Regularization Algorithms"
date: 2018-01-22 18:14:11
categories: arXiv_AI
tags: arXiv_AI Regularization
author: Junhong Lin, Volkan Cevher
mathjax: true
---

* content
{:toc}

##### Abstract
We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We first investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds can be retained for distributed SGM provided that the partition level is not too large. We then extend our results to spectral-regularization algorithms (SRA), including kernel ridge regression (KRR), kernel principal component analysis, and gradient methods. Our results are superior to the state-of-the-art theory. Particularly, our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed KRR and classic SGM. Moreover, even for non-distributed SRA, they provide the first optimal, capacity-dependent convergence rates, considering the case that the regression function may not be in the RKHS.

##### Abstract (translated by Google)
我们研究分布式算法在再生核Hilbert空间（RKHS）上设置非参数回归的泛化特性。我们首先研究分布式随机梯度法（SGM），通过小批量和多遍的数据。我们表明，如果分区级别不是太大，可以保留分布式SGM的最优泛化误差界限。然后，我们将结果扩展到光谱正则化算法（SRA），包括核岭回归（KRR），核主成分分析和梯度方法。我们的成果优于最先进的理论。特别是，我们的结果表明，与分布式KRR和传统的SGM相比，分布式SGM具有更小的理论计算复杂度。此外，考虑到回归函数可能不在RKHS中的情况，即使对于非分布式SRA，它们也提供了第一优化的与容量有关的收敛速率。

##### URL
[https://arxiv.org/abs/1801.07226](https://arxiv.org/abs/1801.07226)

##### PDF
[https://arxiv.org/pdf/1801.07226](https://arxiv.org/pdf/1801.07226)

