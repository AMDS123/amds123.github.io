---
layout: post
title: "Detecting Adversarial Examples Based on Steganalysis"
date: 2018-06-21 04:57:20
categories: arXiv_CV
tags: arXiv_CV Adversarial Object_Detection GAN Face Image_Classification Classification Detection Recognition Face_Recognition
author: Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Nenghai Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks (DNNs) have recently led to significant improvement in many fields, such as image classification. However, these machine learning models are vulnerable to adversarial examples which can mislead machine learning classifiers to give incorrect classifications. Adversarial examples pose security concerns in areas where privacy requirements are strict, such as face recognition, autonomous cars and malware detection. What's more, they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. In this paper, we focus on detecting adversarial examples. We propose to augment deep neural networks with a detector. The detector is constructed by modeling the differences between adjacent pixels in natural images. And then we identify deviations from this model and assume that such deviations are due to adversarial attack. We construct the detector based on steganalysis which can detect minor modifications to an image because the adversarial attack can be treated as a sort of accidental steganography.

##### Abstract (translated by Google)
深度神经网络（DNN）最近在诸如图像分类等许多领域取得了显着的进步。然而，这些机器学习模型容易受到敌对性的例子的影响，这些例子会误导机器学习分类器给出不正确的分类。敌对案例在隐私要求严格的领域引起安全问题，如人脸识别，自动驾驶汽车和恶意软件检测。更重要的是，即使攻击者无法访问底层模型，他们也可以用来对机器学习系统进行攻击。在本文中，我们着重探讨敌对的例子。我们建议用检测器来增强深度神经网络。探测器是通过对自然图像中相邻像素之间的差异建模而构建的。然后我们确定这个模型的偏差，并假设这种偏差是由对抗性攻击造成的。我们构建了基于隐写分析的检测器，可以检测图像的微小修改，因为敌对攻击可以被看作是一种意外的隐写术。

##### URL
[http://arxiv.org/abs/1806.09186](http://arxiv.org/abs/1806.09186)

##### PDF
[http://arxiv.org/pdf/1806.09186](http://arxiv.org/pdf/1806.09186)

