---
layout: post
title: "Attention Strategies for Multi-Source Sequence-to-Sequence Learning"
date: 2017-04-21 14:39:27
categories: arXiv_CL
tags: arXiv_CL Attention
author: Jindřich Libovický, Jindřich Helcl
mathjax: true
---

* content
{:toc}

##### Abstract
Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.

##### Abstract (translated by Google)
神经多源序列到序列学习中的注意力建模仍然是一个相对未开发的领域，尽管它在包含多种源语言或模态的任务中是有用的。我们提出两种新颖的方法来将每个源序列上的注意力机制的输出结合起来，是平坦的和分层的。我们将所提出的方法与现有技术进行了比较，并对WMT16多模态翻译和自动后期编辑任务中的这些方法进行了系统评估。我们表明，提出的方法在两个任务上取得竞争结果。

##### URL
[https://arxiv.org/abs/1704.06567](https://arxiv.org/abs/1704.06567)

##### PDF
[https://arxiv.org/pdf/1704.06567](https://arxiv.org/pdf/1704.06567)

