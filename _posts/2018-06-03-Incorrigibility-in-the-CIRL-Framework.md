---
layout: post
title: "Incorrigibility in the CIRL Framework"
date: 2018-06-03 17:43:18
categories: arXiv_AI
tags: arXiv_AI
author: Ryan Carey
mathjax: true
---

* content
{:toc}

##### Abstract
A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.

##### Abstract (translated by Google)
假设关机指令提供了关于哪些行为导致有价值的结果的信息（在技术意义上），价值学习系统具有遵循关机指示的动机。然而，这种假设对于模型错误规范（例如，在编程错误的情况下）是不稳健的。我们通过提出一些受监督的POMDP场景来证明这一点，其中参数化奖励函数中的错误消除了遵循关闭命令的激励。这些困难与Soares等人讨论的那些相同。 （2015年）在关于腐蚀性的论文中。我们认为，在一些较弱的假设条件下（例如，一个小的验证模块被正确实现;与整个先验概率分布和/或参数化奖励函数相对），考虑遵循关闭命令的系统是很重要的。我们用简单的方法来讨论一些困难，试图在价值学习框架中获得这些保证。

##### URL
[http://arxiv.org/abs/1709.06275](http://arxiv.org/abs/1709.06275)

##### PDF
[http://arxiv.org/pdf/1709.06275](http://arxiv.org/pdf/1709.06275)

