---
layout: post
title: "Maneuver Control based on Reinforcement Learning for Automated Vehicles in An Interactive Environment"
date: 2018-03-25 04:42:57
categories: arXiv_RO
tags: arXiv_RO Adversarial Reinforcement_Learning
author: Pin Wang, Ching-Yao Chan, Hanhan Li
mathjax: true
---

* content
{:toc}

##### Abstract
Operating a robot safely and efficiently can be considerably challenging in an interactive and complex environment. Other surrounding agents may be cooperative or adversarial in their interactions with the robot. It will be desirable to develop control strategies that can enable the robot agent to handle diverse situations and respond with appropriate behaviors in an interactive environment. In this paper, we focus on automated vehicles, and propose a reinforcement learning based approach to train the vehicle agent for safe, comfortable, and efficient maneuvers under interactive driving situations. Particularly, we design a form of the Q-function approximator that consists of neural networks but also has a closed-form greedy policy. In this way, we avoid the complication of invoking an additional function that learns to take actions, as in actor-critic algorithms. Additionally, we formulate the vehicle control maneuvers with continuous state and action space to enhance the practicability and feasibility of the proposed approach. We test our algorithm in simulation with a challenging use case, the lane change maneuver. Results show that the vehicle robot successfully learns a desirable driving policy that allows it to drive safely, comfortably, and efficiently in complex driving scenarios.

##### Abstract (translated by Google)
在交互式和复杂的环境中安全高效地操作机器人可能非常具有挑战性。其他周边代理在与机器人的交互中可能是合作或敌对的。开发可以使机器人代理处理不同情况并在交互式环境中以适当行为做出响应的控制策略将是可取的。在本文中，我们专注于自动驾驶汽车，并提出了一种基于强化学习的方法来训练车辆代理人在交互式驾驶情况下进行安全，舒适和高效的机动。特别是，我们设计了一种由神经网络组成的Q函数逼近器，但也有一个封闭的贪婪策略。通过这种方式，我们避免了调用一个学习采取行动的附加函数的复杂性，就像在actor-critic算法中一样。此外，我们制定具有连续状态和行动空间的车辆控制演习，以增强所提出的方法的实用性和可行性。我们在具有挑战性的用例仿真中测试我们的算法，即车道变换机动。结果表明，车辆机器人成功地学习了理想的驾驶策略，使其能够在复杂的驾驶场景中安全，舒适和高效地驾驶。

##### URL
[https://arxiv.org/abs/1803.09200](https://arxiv.org/abs/1803.09200)

##### PDF
[https://arxiv.org/pdf/1803.09200](https://arxiv.org/pdf/1803.09200)

