---
layout: post
title: 'Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning'
date: 2017-12-06 02:59:39
categories: arXiv_CL
tags: arXiv_CL
author: Jan Niehues, Eunah Cho
---

* content
{:toc}

##### Abstract
Linguistic resources such as part-of-speech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances. However, usage of such linguistic annotations in neural machine translation (NMT) systems has been left under-explored. In this work, we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model. By jointly training several natural language processing (NLP) tasks in one system, we are able to leverage common information and improve the performance of the individual task. We analyze the impact of three design decisions in multi-task learning: the tasks used in training, the training schedule, and the degree of parameter sharing across the tasks, which is defined by the network architecture. The experiments are conducted for an German to English translation task. As additional linguistic resources, we exploit POS information and named-entities (NE). Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition. The performance of the POS tagger is also improved using the multi-task learning scheme.

##### Abstract (translated by Google)
词性（POS）标签等语言资源已广泛应用于统计机器翻译（SMT）框架，并取得了较好的表现。然而，在神经机器翻译（NMT）系统中使用这样的语言注释尚处于探索之中。在这项工作中，我们表明，多任务学习是一个成功和简单的方法来引入额外的知识到端到端的神经注意力模型。通过在一个系统中联合训练几个自然语言处理（NLP）任务，我们可以利用共同的信息，提高个人任务的性能。我们分析了三种设计决策在多任务学习中的作用：训练中使用的任务，训练时间表，以及网络体系结构定义的跨任务参数的程度。这个实验是针对德语到英语的翻译任务而进行的。作为额外的语言资源，我们利用POS信息和命名实体（NE）。实验表明，在资源不足的情况下，翻译质量可以提高1.5 BLEU点。使用多任务学习方案，POS标记器的性能也得到改善。

##### URL
[https://arxiv.org/abs/1708.00993](https://arxiv.org/abs/1708.00993)

