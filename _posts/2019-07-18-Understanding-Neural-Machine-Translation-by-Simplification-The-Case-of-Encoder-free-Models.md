---
layout: post
title: "Understanding Neural Machine Translation by Simplification: The Case of Encoder-free Models"
date: 2019-07-18 16:59:40
categories: arXiv_CL
tags: arXiv_CL Attention Embedding NMT
author: Gongbo Tang, Rico Sennrich, Joakim Nivre
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we try to understand neural machine translation (NMT) via simplifying NMT architectures and training encoder-free NMT models. In an encoder-free model, the sums of word embeddings and positional embeddings represent the source. The decoder is a standard Transformer or recurrent neural network that directly attends to embeddings via attention mechanisms. Experimental results show (1) that the attention mechanism in encoder-free models acts as a strong feature extractor, (2) that the word embeddings in encoder-free models are competitive to those in conventional models, (3) that non-contextualized source representations lead to a big performance drop, and (4) that encoder-free models have different effects on alignment quality for German-English and Chinese-English.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.08158](http://arxiv.org/abs/1907.08158)

##### PDF
[http://arxiv.org/pdf/1907.08158](http://arxiv.org/pdf/1907.08158)

