---
layout: post
title: "Size vs. Structure in Training Corpora for Word Embedding Models: Araneum Russicum Maximum and Russian National Corpus"
date: 2018-01-19 14:11:16
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Andrey Kutuzov, Maria Kunilovskaya
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we present a distributional word embedding model trained on one of the largest available Russian corpora: Araneum Russicum Maximum (over 10 billion words crawled from the web). We compare this model to the model trained on the Russian National Corpus (RNC). The two corpora are much different in their size and compilation procedures. We test these differences by evaluating the trained models against the Russian part of the Multilingual SimLex999 semantic similarity dataset. We detect and describe numerous issues in this dataset and publish a new corrected version. Aside from the already known fact that the RNC is generally a better training corpus than web corpora, we enumerate and explain fine differences in how the models process semantic similarity task, what parts of the evaluation set are difficult for particular models and why. Additionally, the learning curves for both models are described, showing that the RNC is generally more robust as training material for this task.

##### Abstract (translated by Google)
在本文中，我们提出了一个分布式词语嵌入模型训练在最大的可用俄语语料库之一：Araneum Russicum Maximum（超过100亿字从网页爬行）。我们将这个模型与在俄罗斯国家语料库（RNC）上训练的模型进行比较。这两个语料库的大小和编译程序大不相同。我们通过评估多语种SimLex999语义相似性数据集的俄语部分的训练模型来测试这些差异。我们检测并描述这个数据集中的许多问题，并发布一个新的更正的版本。除了已知的事实，即RNC通常比网络语料库更好的训练语料库，我们列举和解释模型如何处理语义相似性任务，评估集的哪些部分对特定模型是困难的，以及为什么会有细微差别。此外，还介绍了两种模型的学习曲线，显示RNC作为此任务的培训材料通常更加健壮。

##### URL
[http://arxiv.org/abs/1801.06407](http://arxiv.org/abs/1801.06407)

##### PDF
[http://arxiv.org/pdf/1801.06407](http://arxiv.org/pdf/1801.06407)

