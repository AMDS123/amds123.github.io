---
layout: post
title: "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling"
date: 2016-09-06 09:29:12
categories: arXiv_CL
tags: arXiv_CL Attention Speech_Recognition RNN Classification Prediction Detection Recognition
author: Bing Liu, Ian Lane
mathjax: true
---

* content
{:toc}

##### Abstract
Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models.

##### Abstract (translated by Google)
基于注意力的编码器 - 解码器神经网络模型最近在机器翻译和语音识别中显示出有希望的结果。在这项工作中，我们提出了一个基于注意力的联合意图检测和槽填充神经网络模型，这两个模型是许多语音理解和对话系统的关键步骤。与机器翻译和语音识别不同，对齐在插槽填充中是明确的。我们探索将这种对齐信息结合到编码器 - 解码器框架的不同策略。从编码器 - 解码器模型的注意机制中学习，我们进一步提出注意基于对齐的RNN模型。这样的注意提供了意图分类和时隙标签预测的附加信息。我们的独立任务模型在基准ATIS任务上实现了最新的意图检测错误率和时隙填充F1分数。我们的联合训练模型在独立任务模型上进一步获得了0.56％的绝对误差减少（23.8％相对误差）和0.23％绝对增益。

##### URL
[https://arxiv.org/abs/1609.01454](https://arxiv.org/abs/1609.01454)

##### PDF
[https://arxiv.org/pdf/1609.01454](https://arxiv.org/pdf/1609.01454)

