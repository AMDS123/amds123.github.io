---
layout: post
title: "Multiplicative Models for Recurrent Language Modeling"
date: 2019-06-30 20:51:43
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model Relation
author: Diego Maupom&#xe9;, Marie-Jean Meurs
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, there has been interest in multiplicative recurrent neural networks for language modeling. Indeed, simple Recurrent Neural Networks (RNNs) encounter difficulties recovering from past mistakes when generating sequences due to high correlation between hidden states. These challenges can be mitigated by integrating second-order terms in the hidden-state update. One such model, multiplicative Long Short-Term Memory (mLSTM) is particularly interesting in its original formulation because of the sharing of its second-order term, referred to as the intermediate state. We explore these architectural improvements by introducing new models and testing them on character-level language modeling tasks. This allows us to establish the relevance of shared parametrization in recurrent language modeling.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.00455](http://arxiv.org/abs/1907.00455)

##### PDF
[http://arxiv.org/pdf/1907.00455](http://arxiv.org/pdf/1907.00455)

