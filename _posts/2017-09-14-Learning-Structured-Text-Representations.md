---
layout: post
title: "Learning Structured Text Representations"
date: 2017-09-14 20:42:25
categories: arXiv_CL
tags: arXiv_CL Attention
author: Yang Liu, Mirella Lapata
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.

##### Abstract (translated by Google)
在本文中，我们关注的是从数据中学习结构感知的文档表示，而不用求助于话语解析器或附加的注释。从最近努力使神经网络具有结构性偏见中汲取灵感，我们提出了一个模型，可以对文档进行编码，同时自动引发丰富的结构依赖性。具体而言，我们将可微分的非投影解析算法嵌入到神经模型中，并使用注意机制来结合结构偏差。跨不同任务和数据集的实验评估表明，所提出的模型在文档建模任务上达到了最新的结果，同时引入了可解释和有意义的中间结构。

##### URL
[https://arxiv.org/abs/1705.09207](https://arxiv.org/abs/1705.09207)

##### PDF
[https://arxiv.org/pdf/1705.09207](https://arxiv.org/pdf/1705.09207)

