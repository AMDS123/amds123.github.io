---
layout: post
title: "#phramacovigilance - Exploring Deep Learning Techniques for Identifying Mentions of Medication Intake from Twitter"
date: 2018-05-16 15:43:21
categories: arXiv_CL
tags: arXiv_CL Sentiment CNN Classification Deep_Learning
author: Debanjan Mahata, Jasper Friedrichs, Hitkul, Rajiv Ratn Shah
mathjax: true
---

* content
{:toc}

##### Abstract
Mining social media messages for health and drug related information has received significant interest in pharmacovigilance research. Social media sites (e.g., Twitter), have been used for monitoring drug abuse, adverse reactions of drug usage and analyzing expression of sentiments related to drugs. Most of these studies are based on aggregated results from a large population rather than specific sets of individuals. In order to conduct studies at an individual level or specific cohorts, identifying posts mentioning intake of medicine by the user is necessary. Towards this objective, we train different deep neural network classification models on a publicly available annotated dataset and study their performances on identifying mentions of personal intake of medicine in tweets. We also design and train a new architecture of a stacked ensemble of shallow convolutional neural network (CNN) ensembles. We use random search for tuning the hyperparameters of the models and share the details of the values taken by the hyperparameters for the best learnt model in different deep neural network architectures. Our system produces state-of-the-art results, with a micro- averaged F-score of 0.693.

##### Abstract (translated by Google)
挖掘与健康和药物相关信息的社交媒体消息已经引起了药物警戒研究的巨大兴趣。社交媒体网站（例如Twitter）已被用于监控药物滥用，药物使用的不良反应以及分析与药物有关的情绪表达。这些研究大多基于大量人群的综合结果，而不是特定人群。为了在个人或特定人群中进行研究，需要确定提到用户摄入药物的职位。为实现这一目标，我们在公开可用的注释数据集上训练不同的深层神经网络分类模型，并研究他们在识别提及在推特中个人摄入药物方面的表现。我们还设计和训练了一种新型的浅层卷积神经网络（CNN）合奏集合体系结构。我们使用随机搜索来调整模型的超参数，并在不同的深度神经网络体系结构中分享超参数为最佳学习模型所采用的值的细节。我们的系统产生最新的结果，微平均F值为0.693。

##### URL
[http://arxiv.org/abs/1805.06375](http://arxiv.org/abs/1805.06375)

##### PDF
[http://arxiv.org/pdf/1805.06375](http://arxiv.org/pdf/1805.06375)

