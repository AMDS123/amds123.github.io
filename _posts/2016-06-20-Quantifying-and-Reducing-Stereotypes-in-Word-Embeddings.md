---
layout: post
title: "Quantifying and Reducing Stereotypes in Word Embeddings"
date: 2016-06-20 13:58:45
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai
mathjax: true
---

* content
{:toc}

##### Abstract
Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes. In this paper, we initiate the study of gender stereotypes in {\em word embedding}, a popular framework to represent text data. As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding. We developed an efficient algorithm that reduces gender stereotype using just a handful of training examples while preserving the useful geometric properties of the embedding. We evaluated our algorithm on several metrics. While we focus on male/female stereotypes, our framework may be applicable to other types of embedding biases.

##### Abstract (translated by Google)
机器学习算法经过优化以模拟训练数据的统计特性。如果输入的数据反映了更广泛的社会的成见和偏见，那么学习算法的输出也捕捉这些刻板印象。在本文中，我们开始研究在文本数据的流行框架{em word embedding}中的性别刻板印象。随着它们的使用变得越来越普遍，应用程序可能会无意中放大不想要的刻板印象。我们在多个数据集中显示嵌入包含显着的性别刻板印象，尤其是在职业方面。我们创建了一个新的性别比喻任务，并将其与众包联合起来，系统地量化给定嵌入中的性别偏见。我们开发了一种高效的算法，在保留嵌入的有用几何特性的同时，仅使用少量的训练样例来减少性别刻板印象。我们评估了几个指标的算法。虽然我们专注于男性/女性的刻板印象，但我们的框架可能适用于其他类型的嵌入偏见。

##### URL
[https://arxiv.org/abs/1606.06121](https://arxiv.org/abs/1606.06121)

##### PDF
[https://arxiv.org/pdf/1606.06121](https://arxiv.org/pdf/1606.06121)

