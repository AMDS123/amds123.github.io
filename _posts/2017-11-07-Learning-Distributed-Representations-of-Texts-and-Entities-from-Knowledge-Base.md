---
layout: post
title: "Learning Distributed Representations of Texts and Entities from Knowledge Base"
date: 2017-11-07 15:27:55
categories: arXiv_CL
tags: arXiv_CL Knowledge
author: Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji
mathjax: true
---

* content
{:toc}

##### Abstract
We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.

##### Abstract (translated by Google)
我们描述了一个联合学习文本和知识库（KB）实体的分布式表示的神经网络模型。给定知识库中的文本，我们训练我们提出的模型来预测与文本相关的实体。我们的模型被设计成通用的，能够轻松处理各种NLP任务。我们使用从维基百科提取的大量文本和它们的实体注释来训练模型。我们对三个重要的NLP任务（即句子文本相似性，实体链接和真实问题回答）的模型进行了评估，涉及无监督和监督设置。因此，我们在所有这三项任务中取得了最先进的成果。我们的代码和训练有素的模型可供公众进一步学术研究。

##### URL
[https://arxiv.org/abs/1705.02494](https://arxiv.org/abs/1705.02494)

##### PDF
[https://arxiv.org/pdf/1705.02494](https://arxiv.org/pdf/1705.02494)

