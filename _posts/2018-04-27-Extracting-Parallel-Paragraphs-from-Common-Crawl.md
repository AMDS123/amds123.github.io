---
layout: post
title: "Extracting Parallel Paragraphs from Common Crawl"
date: 2018-04-27 09:33:49
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Jakub Kúdela, Irena Holubová, Ondřej Bojar
mathjax: true
---

* content
{:toc}

##### Abstract
Most of the current methods for mining parallel texts from the web assume that web pages of web sites share same structure across languages. We believe that there still exists a non-negligible amount of parallel data spread across sources not satisfying this assumption. We propose an approach based on a combination of bivec (a bilingual extension of word2vec) and locality-sensitive hashing which allows us to efficiently identify pairs of parallel segments located anywhere on pages of a given web domain, regardless their structure. We validate our method on realigning segments from a large parallel corpus. Another experiment with real-world data provided by Common Crawl Foundation confirms that our solution scales to hundreds of terabytes large set of web-crawled data.

##### Abstract (translated by Google)
目前大多数从网上挖掘并行文本的方法都假定网站的网页在不同语言之间共享相同的结构。我们相信，在不满足此假设的情况下，仍存在不可忽视的并行数据量。我们提出了一种基于bivec（word2vec的双语扩展）和局部敏感哈希的组合的方法，该方法使我们能够高效地识别位于给定Web域的页面上的任何位置的平行段对，而不管它们的结构如何。我们验证了我们的方法重新排列大型平行语料库中的分段。 Common Crawl Foundation提供的实际数据的另一个实验证实，我们的解决方案可扩展到数百TB的大型网页爬行数据。

##### URL
[https://arxiv.org/abs/1804.10413](https://arxiv.org/abs/1804.10413)

##### PDF
[https://arxiv.org/pdf/1804.10413](https://arxiv.org/pdf/1804.10413)

