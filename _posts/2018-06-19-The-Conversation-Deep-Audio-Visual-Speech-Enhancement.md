---
layout: post
title: "The Conversation: Deep Audio-Visual Speech Enhancement"
date: 2018-06-19 17:51:32
categories: arXiv_CV
tags: arXiv_CV Quantitative
author: Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman
mathjax: true
---

* content
{:toc}

##### Abstract
Our goal is to isolate individual speakers from multi-talker simultaneous speech in videos. Existing works in this area have focussed on trying to separate utterances from known speakers in controlled environments. In this paper, we propose a deep audio-visual speech enhancement network that is able to separate a speaker's voice given lip regions in the corresponding video, by predicting both the magnitude and the phase of the target signal. The method is applicable to speakers unheard and unseen during training, and for unconstrained environments. We demonstrate strong quantitative and qualitative results, isolating extremely challenging real-world examples.

##### Abstract (translated by Google)
我们的目标是将视频中的多个讲话者同时发言的讲话人分离出来。该领域的现有作品集中在试图在受控环境中将已知说话者的话语分开。在本文中，我们提出了一个深度的视听语音增强网络，通过预测目标信号的大小和相位，能够在给定的相应视频中给出唇部区域来分离讲话者的语音。该方法适用于在训练期间未听到和看不见的扬声器以及无限制的环境。我们展示了强大的定量和定性结果，隔离了极具挑战性的现实世界示例。

##### URL
[http://arxiv.org/abs/1804.04121](http://arxiv.org/abs/1804.04121)

##### PDF
[http://arxiv.org/pdf/1804.04121](http://arxiv.org/pdf/1804.04121)

