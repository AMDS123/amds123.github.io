---
layout: post
title: "Object Level Visual Reasoning in Videos"
date: 2018-06-16 00:33:50
categories: arXiv_CV
tags: arXiv_CV Segmentation Recognition
author: Fabien Baradel, Natalia Neverova, Christian Wolf, Julien Mille, Greg Mori
mathjax: true
---

* content
{:toc}

##### Abstract
Human activity recognition is typically addressed by training models to detect key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this, requiring fine distinctions and a detailed comprehension of the interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatio-temporal interactions in videos. Key to our approach is the choice of performing this reasoning on an object level through the integration of state of the art object instance segmentation networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluated our method on three standard datasets: the Twenty-BN Something-Something dataset, the VLOG dataset and the EPIC Kitchens dataset, and achieve state of the art results on both. Finally, we also show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.

##### Abstract (translated by Google)
人类活动识别通常通过训练模型解决，以检测全局和局部运动等关键概念，与场景中存在的对象类别相关的特征以及与全局背景有关的特征。活动识别中的下一个公开挑战需要一定程度的理解力，超越了这个层次，需要对场景中演员和对象之间的相互作用进行细致的区分和详细的理解。我们提出了一个能够学习推理视频中语义上有意义的时空交互的模型。我们方法的关键是通过整合最先进的对象实例分割网络来选择在对象级别上执行此推理。这允许模型学习存在于语义对象交互相关级别的详细空间交互。我们在三个标准数据集上评估了我们的方法：Twenty-BN Something-Something数据集，VLOG数据集和EPIC Kitchens数据集，并在两者上实现了最先进的结果。最后，我们还展示了模型学习到的交互的可视化，这些交互说明了对象类以及它们与不同活动类相对应的交互。

##### URL
[http://arxiv.org/abs/1806.06157](http://arxiv.org/abs/1806.06157)

##### PDF
[http://arxiv.org/pdf/1806.06157](http://arxiv.org/pdf/1806.06157)

