---
layout: post
title: "Can Active Memory Replace Attention?"
date: 2017-03-07 04:04:33
categories: arXiv_CV
tags: arXiv_CV Image_Caption Attention Speech_Recognition Caption Image_Classification Classification Deep_Learning Recognition
author: Łukasz Kaiser, Samy Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.

##### Abstract (translated by Google)
近年来，几种将神经网络的注意力集中在其输入或记忆的选定部分上的机制已经成功地用于深度学习模型中。注意力已经改进了图像分类，图像字幕，语音识别，生成模型和学习算法任务，但它可能对神经机器翻译产生了最大的影响。最近，使用替代机制已经获得了类似的改进，这些机制不专注于存储器的单个部分，而是以统一的方式并行地对所有机制进行操作。这种机制，我们称之为主动内存，在算法任务，图像处理和生成建模中提高了注意力。然而，到目前为止，对于大多数自然语言处理任务，特别是对于机器翻译而言，主动存储器没有改善注意力。我们在本文中分析了这个缺点，并提出了一个扩展的主动记忆模型，该模型匹配现有的神经机器翻译注意模型，并更好地推广到更长的句子。我们调查这个模型并解释为什么以前的活动内存模型没有成功。最后，我们讨论主动记忆何时带来最大好处，哪些注意力可以成为更好的选择。

##### URL
[https://arxiv.org/abs/1610.08613](https://arxiv.org/abs/1610.08613)

##### PDF
[https://arxiv.org/pdf/1610.08613](https://arxiv.org/pdf/1610.08613)

