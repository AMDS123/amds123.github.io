---
layout: post
title: "Bayesian Sparsification of Gated Recurrent Neural Networks"
date: 2018-12-12 14:32:16
categories: arXiv_CL
tags: arXiv_CL Sparse RNN
author: Ekaterina Lobacheva, Nadezhda Chirkova, Dmitry Vetrov
mathjax: true
---

* content
{:toc}

##### Abstract
Bayesian methods have been successfully applied to sparsify weights of neural networks and to remove structure units from the networks, e. g. neurons. We apply and further develop this approach for gated recurrent architectures. Specifically, in addition to sparsification of individual weights and neurons, we propose to sparsify preactivations of gates and information flow in LSTM. It makes some gates and information flow components constant, speeds up forward pass and improves compression. Moreover, the resulting structure of gate sparsity is interpretable and depends on the task. Code is available on github: https://github.com/tipt0p/SparseBayesianRNN

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.05692](http://arxiv.org/abs/1812.05692)

##### PDF
[http://arxiv.org/pdf/1812.05692](http://arxiv.org/pdf/1812.05692)

