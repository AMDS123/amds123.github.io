---
layout: post
title: "Deep Learning Works in Practice. But Does it Work in Theory?"
date: 2018-01-31 13:12:30
categories: arXiv_AI
tags: arXiv_AI Speech_Recognition Deep_Learning Recognition
author: L&#xea; Nguy&#xea;n Hoang, Rachid Guerraoui
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning relies on a very specific kind of neural networks: those superposing several neural layers. In the last few years, deep learning achieved major breakthroughs in many tasks such as image analysis, speech recognition, natural language processing, and so on. Yet, there is no theoretical explanation of this success. In particular, it is not clear why the deeper the network, the better it actually performs. 
 We argue that the explanation is intimately connected to a key feature of the data collected from our surrounding universe to feed the machine learning algorithms: large non-parallelizable logical depth. Roughly speaking, we conjecture that the shortest computational descriptions of the universe are algorithms with inherently large computation times, even when a large number of computers are available for parallelization. Interestingly, this conjecture, combined with the folklore conjecture in theoretical computer science that $ P \neq NC$, explains the success of deep learning.

##### Abstract (translated by Google)
深度学习依赖于一种非常特殊的神经网络：叠加几个神经层的那些神经网络。在过去的几年里，深度学习在图像分析，语音识别，自然语言处理等诸多领域取得了重大突破。然而，这个成功没有理论上的解释。特别是，为什么网络越深，它的实际效果就越好。
 我们认为这个解释与从我们周围的宇宙收集的数据的关键特征密切相关，以提供机器学习算法：大的不可并行化的逻辑深度。粗略地说，我们猜测宇宙中最短的计算描述是具有固有的大计算时间的算法，即使大量的计算机可用于并行化。有趣的是，这个猜想与理论计算机科学中的民俗猜想相结合，解释了深度学习的成功。

##### URL
[http://arxiv.org/abs/1801.10437](http://arxiv.org/abs/1801.10437)

##### PDF
[http://arxiv.org/pdf/1801.10437](http://arxiv.org/pdf/1801.10437)

