---
layout: post
title: "Whitening Black-Box Neural Networks"
date: 2017-11-06 07:58:48
categories: arXiv_CV
tags: arXiv_CV Adversarial Recognition
author: Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz
mathjax: true
---

* content
{:toc}

##### Abstract
Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models.

##### Abstract (translated by Google)
许多部署的学习模型是黑匣子：给定输入，返回输出。关于模型的内部信息，如体系结构，优化过程或培训数据，没有明确的披露，因为它可能包含专有信息或使系统更易受到攻击。这项工作表明神经网络的这些属性可以从一系列查询中暴露出来。这具有多重含义。一方面，我们的工作暴露了黑盒神经网络对不同类型攻击的脆弱性 - 我们表明，揭示的内部信息有助于产生更有效的对抗黑盒模型的对抗性例子。另一方面，这种技术可以用于利用敌对的例子更好地保护来自自动识别模型的私人内容。我们的论文表明，实际上很难在白盒和黑盒模型之间画一条线。

##### URL
[https://arxiv.org/abs/1711.01768](https://arxiv.org/abs/1711.01768)

##### PDF
[https://arxiv.org/pdf/1711.01768](https://arxiv.org/pdf/1711.01768)

