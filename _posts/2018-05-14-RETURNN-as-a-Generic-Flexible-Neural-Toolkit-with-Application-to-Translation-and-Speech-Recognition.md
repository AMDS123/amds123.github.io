---
layout: post
title: "RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition"
date: 2018-05-14 15:23:40
categories: arXiv_AI
tags: arXiv_AI Attention Speech_Recognition RNN Recognition
author: Albert Zeyer, Tamer Alkhouli, Hermann Ney
mathjax: true
---

* content
{:toc}

##### Abstract
We compare the fast training and decoding speed of RETURNN of attention models for translation, due to fast CUDA LSTM kernels, and a fast pure TensorFlow beam search decoder. We show that a layer-wise pretraining scheme for recurrent attention models gives over 1% BLEU improvement absolute and it allows to train deeper recurrent encoder networks. Promising preliminary results on max. expected BLEU training are presented. We are able to train state-of-the-art models for translation and end-to-end models for speech recognition and show results on WMT 2017 and Switchboard. The flexibility of RETURNN allows a fast research feedback loop to experiment with alternative architectures, and its generality allows to use it on a wide range of applications.

##### Abstract (translated by Google)
我们比较了由于快速CUDA LSTM内核和快速纯粹的TensorFlow波束搜索解码器，翻译注意模型的RETURNN的快速训练和解码速度。我们展示了针对经常性关注模型的分层预训练方案，可以提供绝对1％以上的BLEU改进，并允许训练更深层的经常性编码器网络。有前途的最大成果介绍了预期的BLEU培训。我们能够训练用于语音识别的翻译和端到端模型的最先进模型，并在WMT 2017和交换机上展示结果。 RETURNN的灵活性允许快速的研究反馈循环对其他架构进行试验，并且其通用性允许在广泛的应用中使用它。

##### URL
[https://arxiv.org/abs/1805.05225](https://arxiv.org/abs/1805.05225)

##### PDF
[https://arxiv.org/pdf/1805.05225](https://arxiv.org/pdf/1805.05225)

