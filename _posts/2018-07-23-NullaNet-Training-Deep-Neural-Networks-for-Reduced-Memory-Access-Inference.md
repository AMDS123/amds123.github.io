---
layout: post
title: "NullaNet: Training Deep Neural Networks for Reduced-Memory-Access Inference"
date: 2018-07-23 16:50:31
categories: arXiv_AI
tags: arXiv_AI Speech_Recognition Inference Recognition
author: Mahdi Nazemi, Ghasem Pasandi, Massoud Pedram
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks have been successfully deployed in a wide variety of applications including computer vision and speech recognition. However, computational and storage complexity of these models has forced the majority of computations to be performed on high-end computing platforms or on the cloud. To cope with computational and storage complexity of these models, this paper presents a training method that enables a radically different approach for realization of deep neural networks through Boolean logic minimization. The aforementioned realization completely removes the energy-hungry step of accessing memory for obtaining model parameters, consumes about two orders of magnitude fewer computing resources compared to realizations that use floatingpoint operations, and has a substantially lower latency.

##### Abstract (translated by Google)
深度神经网络已成功应用于各种应用，包括计算机视觉和语音识别。然而，这些模型的计算和存储复杂性迫使大多数计算在高端计算平台或云上执行。为了应对这些模型的计算和存储复杂性，本文提出了一种训练方法，通过布尔逻辑最小化，实现了一种完全不同的方法来实现深度神经网络。上述实现完全消除了访问存储器以获得模型参数的耗能步骤，与使用浮点运算的实现相比，消耗大约两个数量级的计算资源，并且具有显着更低的延迟。

##### URL
[http://arxiv.org/abs/1807.08716](http://arxiv.org/abs/1807.08716)

##### PDF
[http://arxiv.org/pdf/1807.08716](http://arxiv.org/pdf/1807.08716)

