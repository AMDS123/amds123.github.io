---
layout: post
title: "Automated Curriculum Learning by Rewarding Temporally Rare Events"
date: 2018-03-19 19:35:44
categories: arXiv_AI
tags: arXiv_AI Sparse Reinforcement_Learning
author: Niels Justesen, Sebastian Risi
mathjax: true
---

* content
{:toc}

##### Abstract
Reward shaping allows reinforcement learning (RL) agents to accelerate learning by receiving additional reward signals. However, these signals can be difficult to design manually, especially for complex RL tasks. We propose a simple and general approach that determines the reward of pre-defined events by their rarity alone. Here events become less rewarding as they are experienced more often, which encourages the agent to continually explore new types of events as it learns. The adaptiveness of this reward function results in a form of automated curriculum learning that does not have to be specified by the experimenter. We demonstrate that this Rarity of Events (RoE) approach enables the agent to succeed in challenging VizDoom scenarios without access to the extrinsic reward from the environment. Furthermore, the results demonstrate that RoE learns a more versatile policy that adapts well to critical changes in the environment. Rewarding events based on their rarity could help in many unsolved RL environments that are characterized by sparse extrinsic rewards but a plethora of known event types.

##### Abstract (translated by Google)
奖励整形允许强化学习（RL）代理通过接收额外的奖励信号来加速学习。但是，这些信号可能难以手动设计，特别是对于复杂的RL任务。我们提出了一种简单而通用的方法，仅凭其稀有性就能确定预定义事件的回报。在这种情况下，事件变得不那么有价值，因为它们更经常地经历，这促使代理人在学习时不断探索新类型的事件。这种奖励功能的适应性产生了一种自动化课程学习的形式，不必由实验者指定。我们证明这种事件（RoE）方法可以使代理成功挑战VizDoom场景，而无需获得来自环境的外部奖励。此外，结果表明，RoE学习了一种更适用于环境关键变化的更通用的策略。根据其稀有度奖励事件可能有助于许多未解决的RL环境，这些环境的特点是稀疏的外在奖励，但大量已知的事件类型。

##### URL
[http://arxiv.org/abs/1803.07131](http://arxiv.org/abs/1803.07131)

##### PDF
[http://arxiv.org/pdf/1803.07131](http://arxiv.org/pdf/1803.07131)

