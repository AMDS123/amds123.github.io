---
layout: post
title: "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks"
date: 2019-07-25 14:03:06
categories: arXiv_CL
tags: arXiv_CL Regularization Attention CNN
author: Lin Zehui, Pengfei Liu, Luyao Huang, Jie Fu, Junkun Chen, Xipeng Qiu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Variants dropout methods have been designed for the fully-connected layer, convolutional layer and recurrent layer in neural networks, and shown to be effective to avoid overfitting. As an appealing alternative to recurrent and convolutional layers, the fully-connected self-attention layer surprisingly lacks a specific dropout method. This paper explores the possibility of regularizing the attention weights in Transformers to prevent different contextualized feature vectors from co-adaption. Experiments on a wide range of tasks show that DropAttention can improve performance and reduce overfitting.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.11065](http://arxiv.org/abs/1907.11065)

##### PDF
[http://arxiv.org/pdf/1907.11065](http://arxiv.org/pdf/1907.11065)

