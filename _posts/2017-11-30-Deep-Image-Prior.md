---
layout: post
title: 'Deep Image Prior'
date: 2017-11-30 10:14:39
categories: arXiv_CV
tags: arXiv_CV Super_Resolution CNN
author: Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky
---

* content
{:toc}

##### Abstract
Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at this https URL .

##### Abstract (translated by Google)
深卷积网络已经成为图像生成和恢复的流行工具。一般来说，他们的优秀表现被归功于他们从大量的示例图像中学习现实的图像先验的能力。在本文中，我们表明，相反，发生器网络的结构足以在任何学习之前捕获大量的低级图像统计。为了做到这一点，我们展示了一个随机初始化的神经网络可以用来作为一个手工制作的先验，在标准的反问题，如去噪，超分辨率和修补等方面有出色的结果。此外，同样的先验可以用来反转深层神经表象来诊断它们，并且基于闪光 - 无闪光输入对来恢复图像。除了其多样化的应用，我们的方法突出了由标准发生器网络架构捕获的归纳偏差。它还弥补了两个非常受欢迎的图像恢复方法家族之间的差距：使用深度卷积网络的基于学习的方法和基于手工图像先验（例如自相似性）的免学习方法。代码和补充材料可在此https URL中获得。

##### URL
[https://arxiv.org/abs/1711.10925](https://arxiv.org/abs/1711.10925)

