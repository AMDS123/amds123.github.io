---
layout: post
title: "Modeling Latent Attention Within Neural Networks"
date: 2017-12-30 08:08:50
categories: arXiv_AI
tags: arXiv_AI Attention Reinforcement_Learning
author: Christopher Grimm, Dilip Arumugam, Siddharth Karamcheti, David Abel, Lawson L.S. Wong, Michael L. Littman
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed "attention masks" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.

##### Abstract (translated by Google)
深层神经网络能够解决各种领域和数据模式的任务。尽管取得了许多经验成功，但我们缺乏清楚地理解和解释有助于实现这种有效行为的内在机制的能力，或者更关键的是失败模式。在这项工作中，我们提出了一个可视化的任意神经网络的内在机制，其权力和局限性的一般方法。我们以数据集为中心的方法产生了一个训练好的网络如何参与其输入组件的可视化。计算出的“注意掩码”通过突出显示哪些输入属性在确定输出中至关重要来支持提高的可解释性。我们在计算机视觉，自然语言处理和强化学习等领域展示了我们的框架在各种深度神经网络架构上的有效性。我们的方法的主要贡献是一个可解释的关注的可视化，提供独特的见解网络的基本决策过程，而不考虑数据的形式。

##### URL
[http://arxiv.org/abs/1706.00536](http://arxiv.org/abs/1706.00536)

##### PDF
[http://arxiv.org/pdf/1706.00536](http://arxiv.org/pdf/1706.00536)

