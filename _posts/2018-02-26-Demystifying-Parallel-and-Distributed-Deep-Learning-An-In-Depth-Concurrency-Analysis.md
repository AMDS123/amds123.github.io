---
layout: post
title: "Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis"
date: 2018-02-26 08:47:34
categories: arXiv_CV
tags: arXiv_CV Survey Deep_Learning Gradient_Descent
author: Tal Ben-Nun, Torsten Hoefler
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. Specifically, we present trends in DNN architectures and the resulting implications on parallelization strategies. We discuss the different types of concurrency in DNNs; synchronous and asynchronous stochastic gradient descent; distributed system architectures; communication schemes; and performance modeling. Based on these approaches, we extrapolate potential directions for parallelism in deep learning.

##### Abstract (translated by Google)
深度神经网络（DNN）正在成为现代计算应用中的重要工具。加速培训是一项重大挑战，技术范围从分布式算法到低级电路设计。在这次调查中，我们从理论角度描述问题，其次是并行化方法。具体而言，我们介绍了DNN体系结构的趋势以及由此产生的对并行策略的影响。我们讨论DNN中不同类型的并发性;同步和异步随机梯度下降;分布式系统架构;沟通计划;和性能建模。基于这些方法，我们推断了深度学习中可能的并行方向。

##### URL
[https://arxiv.org/abs/1802.09941](https://arxiv.org/abs/1802.09941)

##### PDF
[https://arxiv.org/pdf/1802.09941](https://arxiv.org/pdf/1802.09941)

