---
layout: post
title: "Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following"
date: 2019-07-23 03:11:07
categories: arXiv_AI
tags: arXiv_AI
author: David Gaddy, Dan Klein
mathjax: true
---

* content
{:toc}

##### Abstract
We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.09671](http://arxiv.org/abs/1907.09671)

##### PDF
[http://arxiv.org/pdf/1907.09671](http://arxiv.org/pdf/1907.09671)

