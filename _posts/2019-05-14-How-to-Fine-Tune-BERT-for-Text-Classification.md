---
layout: post
title: "How to Fine-Tune BERT for Text Classification?"
date: 2019-05-14 13:17:26
categories: arXiv_CL
tags: arXiv_CL Text_Classification Classification Language_Model
author: Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1905.05583](https://arxiv.org/abs/1905.05583)

##### PDF
[https://arxiv.org/pdf/1905.05583](https://arxiv.org/pdf/1905.05583)

