---
layout: post
title: "Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models"
date: 2019-02-01 02:42:55
categories: arXiv_CL
tags: arXiv_CL Attention Text_Generation
author: Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang, Jianfeng Gao, Lawrence Carin
mathjax: true
---

* content
{:toc}

##### Abstract
Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation with latent variables. In this paper, we investigate several multi-level structures to learn a VAE model to generate long, and coherent text. In particular, we use a hierarchy of stochastic layers between the encoder and decoder networks to generate more informative latent codes. We also investigate a multi-level decoder structure to learn a coherent long-term structure by generating intermediate sentence representations as high-level plan vectors. Empirical results demonstrate that a multi-level VAE model produces more coherent and less repetitive long text compared to the standard VAE models and can further mitigate the posterior-collapse issue.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.00154](http://arxiv.org/abs/1902.00154)

##### PDF
[http://arxiv.org/pdf/1902.00154](http://arxiv.org/pdf/1902.00154)

