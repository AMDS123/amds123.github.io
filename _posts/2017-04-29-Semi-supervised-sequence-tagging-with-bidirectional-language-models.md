---
layout: post
title: "Semi-supervised sequence tagging with bidirectional language models"
date: 2017-04-29 01:13:04
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model Recognition
author: Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power
mathjax: true
---

* content
{:toc}

##### Abstract
Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.

##### Abstract (translated by Google)
从未标记文本中学习的预训练词嵌入已经成为NLP任务的神经网络体系结构的标准组件。然而，在大多数情况下，运用字级表示来产生上下文敏感表示的循环网络在相对较少的标记数据上进行训练。在本文中，我们演示了一种通用的半监督方法，用于将预先训练的双向语言模型的上下文嵌入添加到NLP系统，并将其应用于序列标记任务。我们在两个标准数据集上评估我们的模型，用于命名实体识别（NER）和分块，并且在这两种情况下实现了最先进的结果，超越了以前使用其他形式的传输或联合学习的附加标记数据和任务特定地名录。

##### URL
[https://arxiv.org/abs/1705.00108](https://arxiv.org/abs/1705.00108)

##### PDF
[https://arxiv.org/pdf/1705.00108](https://arxiv.org/pdf/1705.00108)

