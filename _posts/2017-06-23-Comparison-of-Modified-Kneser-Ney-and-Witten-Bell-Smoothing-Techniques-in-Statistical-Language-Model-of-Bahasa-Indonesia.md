---
layout: post
title: "Comparison of Modified Kneser-Ney and Witten-Bell Smoothing Techniques in Statistical Language Model of Bahasa Indonesia"
date: 2017-06-23 17:43:20
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Ismail Rusli
mathjax: true
---

* content
{:toc}

##### Abstract
Smoothing is one technique to overcome data sparsity in statistical language model. Although in its mathematical definition there is no explicit dependency upon specific natural language, different natures of natural languages result in different effects of smoothing techniques. This is true for Russian language as shown by Whittaker (1998). In this paper, We compared Modified Kneser-Ney and Witten-Bell smoothing techniques in statistical language model of Bahasa Indonesia. We used train sets of totally 22M words that we extracted from Indonesian version of Wikipedia. As far as we know, this is the largest train set used to build statistical language model for Bahasa Indonesia. The experiments with 3-gram, 5-gram, and 7-gram showed that Modified Kneser-Ney consistently outperforms Witten-Bell smoothing technique in term of perplexity values. It is interesting to note that our experiments showed 5-gram model for Modified Kneser-Ney smoothing technique outperforms that of 7-gram. Meanwhile, Witten-Bell smoothing is consistently improving over the increase of n-gram order.

##### Abstract (translated by Google)
平滑是克服统计语言模型中数据稀疏性的一种技术。尽管在数学定义上没有明确的依赖于特定的自然语言，但是自然语言的不同性质导致了平滑技术的不同效果。如Whittaker（1998）所示，这对俄语是正确的。在本文中，我们比较了修正的Kneser-Ney和Witten-Bell平滑技术在印度尼西亚统计语言模型中的应用。我们使用了从印度尼西亚版维基百科中提取的总共22M字的训练集。据我们所知，这是印度尼西亚最大的用于建立统计语言模型的训练集。 3克，5克和7克的实验表明，改进的Kneser-Ney一直比Witten-Bell平滑技术更胜一筹。有趣的是，我们的实验显示修改的Kneser-Ney平滑技术的5-gram模型胜过了7-gram。同时，随着n-gram顺序的增加，Witten-Bell平滑也在不断提高。

##### URL
[https://arxiv.org/abs/1706.07786](https://arxiv.org/abs/1706.07786)

##### PDF
[https://arxiv.org/pdf/1706.07786](https://arxiv.org/pdf/1706.07786)

