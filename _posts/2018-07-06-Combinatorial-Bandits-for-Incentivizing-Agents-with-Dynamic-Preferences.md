---
layout: post
title: "Combinatorial Bandits for Incentivizing Agents with Dynamic Preferences"
date: 2018-07-06 08:03:39
categories: arXiv_AI
tags: arXiv_AI Recommendation
author: Tanner Fiez, Shreyas Sekar, Liyuan Zheng, Lillian J. Ratliff
mathjax: true
---

* content
{:toc}

##### Abstract
The design of personalized incentives or recommendations to improve user engagement is gaining prominence as digital platform providers continually emerge. We propose a multi-armed bandit framework for matching incentives to users, whose preferences are unknown a priori and evolving dynamically in time, in a resource constrained environment. We design an algorithm that combines ideas from three distinct domains: (i) a greedy matching paradigm, (ii) the upper confidence bound algorithm (UCB) for bandits, and (iii) mixing times from the theory of Markov chains. For this algorithm, we provide theoretical bounds on the regret and demonstrate its performance via both synthetic and realistic (matching supply and demand in a bike-sharing platform) examples.

##### Abstract (translated by Google)
随着数字平台提供商的不断涌现，个性化激励或建议以提高用户参与度的设计越来越受到重视。我们提出了一种多臂强盗框架，用于将激励与用户匹配，这些用户的偏好在资源受限的环境中是先验的，并且在时间上动态演变。我们设计了一种算法，它结合了三个不同领域的思想：（i）贪婪的匹配范式，（ii）强盗的上置信界限算法（UCB），以及（iii）马尔可夫链理论的混合时间。对于这个算法，我们提供遗憾的理论界限，并通过合成和现实（在自行车共享平台中匹配供应和需求）的例子展示其性能。

##### URL
[http://arxiv.org/abs/1807.02297](http://arxiv.org/abs/1807.02297)

##### PDF
[http://arxiv.org/pdf/1807.02297](http://arxiv.org/pdf/1807.02297)

