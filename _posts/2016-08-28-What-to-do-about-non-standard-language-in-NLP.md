---
layout: post
title: "What to do about non-standard language in NLP"
date: 2016-08-28 17:51:41
categories: arXiv_CL
tags: arXiv_CL Review
author: Barbara Plank
mathjax: true
---

* content
{:toc}

##### Abstract
Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., socio-demographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language. In this paper, I review the notion of canonicity, and how it shapes our community's approach to language. I argue for leveraging what I call fortuitous data, i.e., non-obvious data that is hitherto neglected, hidden in plain sight, or raw data that needs to be refined. If we embrace the variety of this heterogeneous data by combining it with proper algorithms, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation.

##### Abstract (translated by Google)
真实世界的数据与我们在自然语言处理（NLP）中使用的基准语料完全不同。只要我们将技术应用于现实世界，性能就会下降。造成这个问题的原因是显而易见的：NLP模型接受了一系列被认为是标准，最显着的英语新闻专线的典型品种样本的培训。然而，有许多维度，例如社会人口统计学，语言，流派，句子类型等等，文本可以与标准不同。解决方案并不明显：我们无法控制所有因素，也不清楚如何最好地超越目前单一领域和语言对同质数据进行培训的做法。在本文中，我回顾了canonicity的概念，以及它如何塑造我们社区的语言方法。我主张利用所谓的偶然性数据，也就是迄今为止被忽略的隐藏在显眼处的非显而易见的数据或需要改进的原始数据。如果我们通过将这些异构数据与适当的算法相结合来支持这些异构数据的多样性，那么我们不仅会产生更多的鲁棒模型，而且还将使自适应语言技术能够解决自然语言的变化。

##### URL
[https://arxiv.org/abs/1608.07836](https://arxiv.org/abs/1608.07836)

##### PDF
[https://arxiv.org/pdf/1608.07836](https://arxiv.org/pdf/1608.07836)

