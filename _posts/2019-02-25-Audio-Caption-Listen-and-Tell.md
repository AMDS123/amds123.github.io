---
layout: post
title: "Audio Caption: Listen and Tell"
date: 2019-02-25 13:27:13
categories: arXiv_CL
tags: arXiv_CL Image_Caption Caption Classification Detection Relation
author: Mengyue Wu, Heinrich Dinkel, Kai Yu
mathjax: true
---

* content
{:toc}

##### Abstract
Increasing amount of research has shed light on machine perception of audio events, most of which concerns detection and classification tasks. However, human-like perception of audio scenes involves not only detecting and classifying audio sounds, but also summarizing the relationship between different audio events. Comparable research such as image caption has been conducted, yet the audio field is still quite barren. This paper introduces a manually-annotated dataset for audio caption. The purpose is to automatically generate natural sentences for audio scene description and to bridge the gap between machine perception of audio and image. The whole dataset is labelled in Mandarin and we also include translated English annotations. A baseline encoder-decoder model is provided for both English and Mandarin. Similar BLEU scores are derived for both languages: our model can generate understandable and data-related captions based on the dataset.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.09254](http://arxiv.org/abs/1902.09254)

##### PDF
[http://arxiv.org/pdf/1902.09254](http://arxiv.org/pdf/1902.09254)

