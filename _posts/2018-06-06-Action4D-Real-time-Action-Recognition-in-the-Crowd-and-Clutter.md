---
layout: post
title: "Action4D: Real-time Action Recognition in the Crowd and Clutter"
date: 2018-06-06 20:59:40
categories: arXiv_CV
tags: arXiv_CV Attention Action_Recognition Recognition
author: Quanzeng You, Hao Jiang
mathjax: true
---

* content
{:toc}

##### Abstract
Recognizing every person's action in a crowded and cluttered environment is a challenging task. In this paper, we propose a real-time action recognition method, Action4D, which gives reliable and accurate results in the real-world settings. We propose to tackle the action recognition problem using a holistic 4D "scan" of a cluttered scene to include every detail about the people and environment. Recognizing multiple people's actions in the cluttered 4D representation is a new problem. In this paper, we propose novel methods to solve this problem. We propose a new method to track people in 4D, which can reliably detect and follow each person in real time. We propose a new deep neural network, the Action4D-Net, to recognize the action of each tracked person. The Action4D-Net's novel structure uses both the global feature and the focused attention to achieve state-of-the-art result. Our real-time method is invariant to camera view angles, resistant to clutter and able to handle crowd. The experimental results show that the proposed method is fast, reliable and accurate. Our method paves the way to action recognition in the real-world applications and is ready to be deployed to enable smart homes, smart factories and smart stores.

##### Abstract (translated by Google)
认识到每个人在拥挤和混乱的环境中的行为是一项具有挑战性的任务。在本文中，我们提出了一种实时动作识别方法Action4D，它可以在现实环境中提供可靠和准确的结果。我们建议使用对杂乱场景的全面4D“扫描”来处理动作识别问题，以包含有关人员和环境的每个细节。认识到多个人在混乱的4D表示中的行为是一个新问题。在本文中，我们提出了解决这个问题的新方法。我们提出了一种追踪4D人的新方法，可以实时可靠地检测并跟踪每个人。我们提出了一个新的深度神经网络Action4D-Net来识别每个被跟踪人的行为。 Action4D-Net的新颖结构既利用了全球特点，也集中了注意力来实现最新的结果。我们的实时方法对摄像机视角不变，抗杂乱和能够处理人群。实验结果表明，该方法快速，可靠，准确。我们的方法为实际应用中的动作识别铺平了道路，并准备部署以实现智能家居，智能工厂和智能商店。

##### URL
[http://arxiv.org/abs/1806.02424](http://arxiv.org/abs/1806.02424)

##### PDF
[http://arxiv.org/pdf/1806.02424](http://arxiv.org/pdf/1806.02424)

