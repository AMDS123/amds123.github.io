---
layout: post
title: "Depth and nonlinearity induce implicit exploration for RL"
date: 2018-05-29 21:21:18
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Justas Dauparas, Ryota Tomioka, Katja Hofmann
mathjax: true
---

* content
{:toc}

##### Abstract
The question of how to explore, i.e., take actions with uncertain outcomes to learn about possible future rewards, is a key question in reinforcement learning (RL). Here, we show a surprising result: We show that Q-learning with nonlinear Q-function and no explicit exploration (i.e., a purely greedy policy) can learn several standard benchmark tasks, including mountain car, equally well as, or better than, the most commonly-used $\epsilon$-greedy exploration. We carefully examine this result and show that both the depth of the Q-network and the type of nonlinearity are important to induce such deterministic exploration.

##### Abstract (translated by Google)
如何探索，即采取具有不确定结果的行动来了解未来可能的奖励，这是强化学习（RL）中的一个关键问题。在这里，我们展示了一个令人惊讶的结果：我们表明，具有非线性Q函数的Q学习并没有明确的探索（即纯粹的贪婪政策）可以学习包括山地车在内的几项标准基准任务，最常用的$ \ epsilon $ -greedy探索。我们仔细研究了这个结果，并且证明Q网络的深度和非线性的类型对于诱导这样的确定性探索是重要的。

##### URL
[https://arxiv.org/abs/1805.11711](https://arxiv.org/abs/1805.11711)

##### PDF
[https://arxiv.org/pdf/1805.11711](https://arxiv.org/pdf/1805.11711)

