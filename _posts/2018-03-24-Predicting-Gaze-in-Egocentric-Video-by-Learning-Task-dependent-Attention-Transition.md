---
layout: post
title: "Predicting Gaze in Egocentric Video by Learning Task-dependent Attention Transition"
date: 2018-03-24 15:31:55
categories: arXiv_CV
tags: arXiv_CV Salient Attention Prediction
author: Yifei Huang, Minjie Cai, Zhenqiang Li, Yoichi Sato
mathjax: true
---

* content
{:toc}

##### Abstract
We present a new computational model for gaze prediction in egocentric videos by exploring patterns in temporal shift of gaze fixations (attention transition) that are dependent on egocentric manipulation tasks. Our assumption is that the high-level context of how a task is completed in a certain way has a strong influence on attention transition and should be modeled for gaze prediction in natural dynamic scenes. Specifically, we propose a hybrid model based on deep neural networks which integrates task-dependent attention transition with bottom-up saliency prediction. In particular, the task-dependent attention transition is learned with a recurrent neural network to exploit the temporal context of gaze fixations, e.g. looking at a cup after moving gaze away from a grasped bottle. Experiments on public egocentric activity datasets show that our model significantly outperforms state-of-the-art gaze prediction methods and is able to learn meaningful transition of human attention.

##### Abstract (translated by Google)
我们通过探索依赖于自我中心操纵任务的凝视注视（注意力转移）时间转移模式，提出了一种新的用于以自我为中心的视频注视预测的计算模型。我们的假设是，任务如何以某种方式完成的高层次背景对注意力转移具有强烈的影响，并且应该模拟在自然动态场景中进行注视预测。具体而言，我们提出了一种基于深度神经网络的混合模型，该模型将任务依赖注意力转移与自下而上显着性预测相结合。特别地，利用循环神经网络来学习依赖于任务的注意力转换，以利用凝视注意力的时间背景，例如，从注视的瓶子移开凝视后，看着杯子。对公共自我中心活动数据集的实验表明，我们的模型显着优于最先进的凝视预测方法，并能够学习人类注意力的有意义转变。

##### URL
[https://arxiv.org/abs/1803.09125](https://arxiv.org/abs/1803.09125)

##### PDF
[https://arxiv.org/pdf/1803.09125](https://arxiv.org/pdf/1803.09125)

