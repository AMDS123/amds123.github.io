---
layout: post
title: "Maximum a Posteriori Policy Optimisation"
date: 2018-06-14 12:46:23
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, Martin Riedmiller
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.

##### Abstract (translated by Google)
基于相对熵目标的坐标上升，我们引入了一种称为最大后验政策优化（MPO）的强化学习新算法。我们展示了几种现有的方法可以直接与我们的推导相关。我们开发两种关闭策略算法，并证明它们与深度强化学习中的最新技术竞争。特别是对于连续控制，我们的方法在样本效率，早熟收敛和对超参数设置的鲁棒性方面优于现有方法，同时实现类似或更好的最终性能。

##### URL
[http://arxiv.org/abs/1806.06920](http://arxiv.org/abs/1806.06920)

##### PDF
[http://arxiv.org/pdf/1806.06920](http://arxiv.org/pdf/1806.06920)

