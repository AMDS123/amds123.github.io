---
layout: post
title: "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation"
date: 2017-04-20 00:36:35
categories: arXiv_SD
tags: arXiv_SD
author: Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios P. Spithourakis, Lucy Vanderwende
mathjax: true
---

* content
{:toc}

##### Abstract
The popularity of image sharing on social media and the engagement it creates between users reflects the important role that visual context plays in everyday conversations. We present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiple-reference dataset of crowd-sourced, event-centric conversations on images. IGC falls on the continuum between chit-chat and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialogue research.

##### Abstract (translated by Google)
图像共享在社交媒体上的流行以及用户之间的交互反映了视觉情境在日常对话中扮演的重要角色。我们提出了一个新的任务，即基于图像的对话（IGC），在这个任务中，关于共享图像生成自然的声音对话。为了进行基准测试，我们在图像上引入了一个新的多参考数据集，这个数据集包含众多的以事件为中心的对话。 IGC落在闲聊和目标导向的对话模式之间的连续性，视觉接地限制了话题与事件驱动的话语。对社交媒体数据进行训练的模型的实验表明，视觉和文本环境的结合提高了生成的会话轮次的质量。在人类评价中，人类表现与神经和检索结构之间的差距表明，多模IGC为对话研究提出了一个有趣的挑战。

##### URL
[https://arxiv.org/abs/1701.08251](https://arxiv.org/abs/1701.08251)

##### PDF
[https://arxiv.org/pdf/1701.08251](https://arxiv.org/pdf/1701.08251)

