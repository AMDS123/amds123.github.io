---
layout: post
title: "Taking a machine's perspective: Human deciphering of adversarial images"
date: 2018-09-11 19:39:51
categories: arXiv_CV
tags: arXiv_CV Adversarial CNN Classification Recognition
author: Zhenglong Zhou, Chaz Firestone
mathjax: true
---

* content
{:toc}

##### Abstract
How similar is the human mind to the sophisticated machine-learning systems that mirror its performance? Models of object categorization based on convolutional neural networks (CNNs) have achieved human-level benchmarks in assigning known labels to novel images. These advances support transformative technologies such as autonomous vehicles and machine diagnosis; beyond this, they also serve as candidate models for the visual system itself -- not only in their output but perhaps even in their underlying mechanisms and principles. However, unlike human vision, CNNs can be "fooled" by adversarial examples -- carefully crafted images that appear as nonsense patterns to humans but are recognized as familiar objects by machines, or that appear as one object to humans and a different object to machines. This seemingly extreme divergence between human and machine classification challenges the promise of these new advances, both as applied image-recognition systems and also as models of the human mind. Surprisingly, however, little work has empirically investigated human classification of such adversarial stimuli: Does human and machine performance fundamentally diverge? Or could humans decipher such images and predict the machine's preferred labels? Here, we show that human and machine classification of adversarial stimuli are robustly related: In seven experiments on five prominent and diverse adversarial imagesets, human subjects reliably identified the machine's chosen label over relevant foils. This pattern persisted for images with strong antecedent identities, and even for images described as "totally unrecognizable to human eyes". We suggest that human intuition may be a more reliable guide to machine (mis)classification than has typically been imagined, and we explore the consequences of this result for minds and machines alike.

##### Abstract (translated by Google)
人类思维与反映其绩效的复杂机器学习系统有多相似？基于卷积神经网络（CNN）的对象分类模型已经在将已知标签分配给新图像方面达到了人类基准。这些进步支持变革性技术，如自动驾驶汽车和机器诊断;除此之外，它们还可以作为视觉系统本身的候选模型 - 不仅在其输出中，甚至可能在其基本机制和原则中。然而，与人类视觉不同，CNN可以被对抗性的例子“愚弄” - 精心制作的图像对人类来说是无意义的模式，但被机器认为是熟悉的对象，或者作为人的一个对象和机器的不同对象出现。 。人类和机器分类之间看似极端的分歧挑战了这些新进展的前景，既作为应用的图像识别系统，也作为人类思维的模型。然而，令人惊讶的是，很少有工作经验性地研究了这种对抗性刺激的人类分类：人类和机器的表现是否从根本上分歧？或者人类可以破译这些图像并预测机器的首选标签吗？在这里，我们表明对抗性刺激的人类和机器分类是强有力的相关：在五个突出和不同的对抗性图像集的七个实验中，人类受试者可靠地识别机器在相关箔上选择的标签。对于具有强烈先行身份的图像，甚至对于描述为“人眼完全无法辨认”的图像，这种模式仍然存在。我们建议人类直觉可能是比通常想象的机器（错误）分类更可靠的指导，我们探讨这种结果对于思想和机器的影响。

##### URL
[http://arxiv.org/abs/1809.04120](http://arxiv.org/abs/1809.04120)

##### PDF
[http://arxiv.org/pdf/1809.04120](http://arxiv.org/pdf/1809.04120)

