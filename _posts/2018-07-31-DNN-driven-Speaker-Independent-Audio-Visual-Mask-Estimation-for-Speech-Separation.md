---
layout: post
title: "DNN driven Speaker Independent Audio-Visual Mask Estimation for Speech Separation"
date: 2018-07-31 20:12:15
categories: arXiv_CV
tags: arXiv_CV Attention RNN
author: Mandar Gogate, Ahsan Adeel, Ricard Marxer, Jon Barker, Amir Hussain
mathjax: true
---

* content
{:toc}

##### Abstract
Human auditory cortex excels at selectively suppressing background noise to focus on a target speaker. The process of selective attention in the brain is known to contextually exploit the available audio and visual cues to better focus on target speaker while filtering out other noises. In this study, we propose a novel deep neural network (DNN) based audiovisual (AV) mask estimation model. The proposed AV mask estimation model contextually integrates the temporal dynamics of both audio and noise-immune visual features for improved mask estimation and speech separation. For optimal AV features extraction and ideal binary mask (IBM) estimation, a hybrid DNN architecture is exploited to leverages the complementary strengths of a stacked long short term memory (LSTM) and convolution LSTM network. The comparative simulation results in terms of speech quality and intelligibility demonstrate significant performance improvement of our proposed AV mask estimation model as compared to audio-only and visual-only mask estimation approaches for both speaker dependent and independent scenarios.

##### Abstract (translated by Google)
人类听觉皮层擅长选择性地抑制背景噪声以聚焦于目标说话者。已知大脑中选择性注意的过程在上下文中利用可用的音频和视觉提示以更好地聚焦于目标说话者，同时滤除其他噪声。在这项研究中，我们提出了一种新的基于深度神经网络（DNN）的视听（AV）模板估计模型。所提出的AV掩模估计模型在上下文中集成了音频和噪声免疫视觉特征的时间动态，以改进掩模估计和语音分离。为了获得最佳的AV特征提取和理想的二元掩模（IBM）估计，利用混合DNN架构来利用堆叠长短期存储器（LSTM）和卷积LSTM网络的互补优势。在语音质量和可懂度方面的比较模拟结果证明了我们提出的AV掩模估计模型与用于说话者相关和独立场景的仅音频和仅视觉掩模估计方法相比的显着性能改进。

##### URL
[https://arxiv.org/abs/1808.00060](https://arxiv.org/abs/1808.00060)

##### PDF
[https://arxiv.org/pdf/1808.00060](https://arxiv.org/pdf/1808.00060)

