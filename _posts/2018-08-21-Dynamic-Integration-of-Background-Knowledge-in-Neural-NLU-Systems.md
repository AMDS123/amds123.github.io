---
layout: post
title: "Dynamic Integration of Background Knowledge in Neural NLU Systems"
date: 2018-08-21 08:57:43
categories: arXiv_AI
tags: arXiv_AI Knowledge QA
author: Dirk Weissenborn, Tom&#xe1;&#x161; Ko&#x10d;isk&#xfd;, Chris Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way.

##### Abstract (translated by Google)
理解自然语言需要常识和背景知识，但在大多数神经自然语言理解（NLU）系统中，这些知识必须在学习过程中从训练语料库中获取，然后在测试时保持静态。我们引入了一种新的架构，用于在NLU模型中动态集成显式背景知识。通用读取模块以自由文本语句的形式（连同任务特定的文本输入）读取背景知识，并将精炼的单词表示产生到特定于任务的NLU体系结构，该体系结构使用这些表示来重新处理任务输入。文档问答（DQA）和识别文本蕴涵（RTE）的实验证明了该方法的有效性和灵活性。分析表明，我们的模型学会以语义上适当的方式利用知识。

##### URL
[http://arxiv.org/abs/1706.02596](http://arxiv.org/abs/1706.02596)

##### PDF
[http://arxiv.org/pdf/1706.02596](http://arxiv.org/pdf/1706.02596)

