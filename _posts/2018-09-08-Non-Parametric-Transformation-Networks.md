---
layout: post
title: "Non-Parametric Transformation Networks"
date: 2018-09-08 22:45:23
categories: arXiv_AI
tags: arXiv_AI Knowledge CNN Gradient_Descent
author: Dipan K. Pal, Marios Savvides
mathjax: true
---

* content
{:toc}

##### Abstract
ConvNets, through their architecture, only enforce invariance to translation. In this paper, we introduce a new class of deep convolutional architectures called Non-Parametric Transformation Networks (NPTNs) which can learn \textit{general} invariances and symmetries directly from data. NPTNs are a natural generalization of ConvNets and can be optimized directly using gradient descent. Unlike almost all previous works in deep architectures, they make no assumption regarding the structure of the invariances present in the data and in that aspect are flexible and powerful. We also model ConvNets and NPTNs under a unified framework called Transformation Networks (TN), which yields a better understanding of the connection between the two. We demonstrate the efficacy of NPTNs on data such as MNIST with extreme transformations and CIFAR10 where they outperform baselines, and further outperform several recent algorithms on ETH-80. They do so while having the same number of parameters. We also show that they are more effective than ConvNets in modelling symmetries and invariances from data, without the explicit knowledge of the added arbitrary nuisance transformations. Finally, we replace ConvNets with NPTNs within Capsule Networks and show that this enables Capsule Nets to perform even better.

##### Abstract (translated by Google)
ConvNets通过其架构仅强制执行翻译不变性。在本文中，我们引入了一类称为非参数变换网络（NPTN）的深度卷积体系结构，它可以直接从数据中学习\ textit {general}不变性和对称性。 NPTN是ConvNets的自然概括，可以使用梯度下降直接优化。与深层体系结构中几乎所有以前的工作不同，它们不对数据中存在的不变性的结构做出任何假设，并且在该方面是灵活且强大的。我们还在一个名为Transformation Networks（TN）的统一框架下对ConvNets和NPTN进行建模，这可以更好地理解两者之间的联系。我们证明了NPTNs对数据的有效性，例如具有极端变换的MNIST和CIFAR10，它们的表现优于基线，并且在ETH-80上的表现优于最近的几种算法。他们这样做的同时具有相同数量的参数。我们还表明它们在建模对称性和数据不变性方面比ConvNets更有效，而没有明确知道增加的任意扰乱变换。最后，我们用胶囊网络中的NPTN替换ConvNets，并表明这使得Capsule Nets能够表现得更好。

##### URL
[http://arxiv.org/abs/1801.04520](http://arxiv.org/abs/1801.04520)

##### PDF
[http://arxiv.org/pdf/1801.04520](http://arxiv.org/pdf/1801.04520)

