---
layout: post
title: "Train Once, Test Anywhere: Zero-Shot Learning for Text Classification"
date: 2017-12-16 15:17:07
categories: arXiv_CL
tags: arXiv_CL Text_Classification Embedding Classification Relation
author: Pushpankar Kumar Pushp, Muktabh Mayank Srivastava
mathjax: true
---

* content
{:toc}

##### Abstract
Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence's tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space. The model learns to predict whether a given sentence is related to a tag or not; unlike other classifiers that learn to classify the sentence as one of the possible classes. We propose three different neural networks for the task and report their accuracy on the test set of the dataset used for training them as well as two other standard datasets for which no retraining was done. We show that our models generalize well across new unseen classes in both cases. Although the models do not achieve the accuracy level of the state of the art supervised models, yet it evidently is a step forward towards general intelligence in natural language processing.

##### Abstract (translated by Google)
零点学习者是能够预测看不见的课程的模型。在这项工作中，我们提出了一个零射击学习方法的文本分类。我们的方法涉及对大量语料库的训练模型，以学习句子与语句标签嵌入之间的关系。学习这种关系使得模型可以推广到看不见的句子，标签，甚至新的数据集，只要它们可以放入相同的嵌入空间。模型学习预测给定句子是否与标签有关;不像其他分类器，学习将句子分类为可能的类别之一。我们提出三个不同的神经网络的任务，并报告他们的准确性在数据集的测试集用于训练他们以及两个其他标准数据集，没有再训练。我们表明，我们的模型在两种情况下通过新的看不见的类概括得很好。尽管模型没有达到现有监督模型的精确度水平，但它显然是向自然语言处理中的一般智能迈出的一步。

##### URL
[http://arxiv.org/abs/1712.05972](http://arxiv.org/abs/1712.05972)

##### PDF
[http://arxiv.org/pdf/1712.05972](http://arxiv.org/pdf/1712.05972)

