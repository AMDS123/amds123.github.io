---
layout: post
title: "Plan Explanations as Model Reconciliation -- An Empirical Study"
date: 2018-02-03 19:17:58
categories: arXiv_AI
tags: arXiv_AI Knowledge
author: Tathagata Chakraborti, Sarath Sreedharan, Sachin Grover, Subbarao Kambhampati
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work in explanation generation for decision making agents has looked at how unexplained behavior of autonomous systems can be understood in terms of differences in the model of the system and the human's understanding of the same, and how the explanation process as a result of this mismatch can be then seen as a process of reconciliation of these models. Existing algorithms in such settings, while having been built on contrastive, selective and social properties of explanations as studied extensively in the psychology literature, have not, to the best of our knowledge, been evaluated in settings with actual humans in the loop. As such, the applicability of such explanations to human-AI and human-robot interactions remains suspect. In this paper, we set out to evaluate these explanation generation algorithms in a series of studies in a mock search and rescue scenario with an internal semi-autonomous robot and an external human commander. We demonstrate to what extent the properties of these algorithms hold as they are evaluated by humans, and how the dynamics of trust between the human and the robot evolve during the process of these interactions.

##### Abstract (translated by Google)
最近对决策代理人的解释生成工作研究了自治系统的无法解释的行为如何根据系统模型和人的理解的差异来理解，以及由于这种不匹配的结果可以被看作是这些模型的调和过程。在这样的设置中，现有的算法虽然已经建立在心理学文献中广泛研究的解释的对比，选择性和社会性质上，但是据我们所知，并没有在循环中的实际人类的环境中进行评估。因此，这种解释对于人机交互和人机交互的适用性仍然值得怀疑。在本文中，我们开始评估这些解释生成算法在一系列的研究中，在一个内部的半自主机器人和一个外部的人类指挥官的模拟搜索和救援方案。我们证明了这些算法的特性在何种程度上被人类所评估，以及人与机器人之间的信任动态如何在这些相互作用的过程中发展。

##### URL
[http://arxiv.org/abs/1802.01013](http://arxiv.org/abs/1802.01013)

##### PDF
[http://arxiv.org/pdf/1802.01013](http://arxiv.org/pdf/1802.01013)

