---
layout: post
title: "Knowledge Aggregation via Epsilon Model Spaces"
date: 2018-05-20 15:58:14
categories: arXiv_AI
tags: arXiv_AI Knowledge
author: Neel Guha
mathjax: true
---

* content
{:toc}

##### Abstract
In many practical applications, machine learning is divided over multiple agents, where each agent learns a different task and/or learns from a different dataset. We present Epsilon Model Spaces (EMS), a framework for learning a global model by aggregating local learnings performed by each agent. Our approach forgoes sharing of data between agents, makes no assumptions on the distribution of data across agents, and requires minimal communication between agents. We empirically validate our techniques on MNIST experiments and discuss how EMS can generalize to a wide range of problem settings, including federated averaging and catastrophic forgetting. We believe our framework to be among the first to lay out a general methodology for "combining" distinct models.

##### Abstract (translated by Google)
在许多实际应用中，机器学习分为多个代理，每个代理学习不同的任务和/或从不同的数据集中学习。我们提出Epsilon模型空间（EMS），这是一个通过汇总每个代理人执行的本地学习来学习全球模型的框架。我们的方法放弃代理之间的数据共享，不会假设代理之间的数据分布，并且需要代理之间的最小通信。我们凭经验验证了我们在MNIST实验上的技​​术，并讨论EMS如何推广到广泛的问题设置，包括联邦平均和灾难性遗忘。我们相信我们的框架是第一个为“合并”不同模型制定一般方法论的机构之一。

##### URL
[https://arxiv.org/abs/1805.07782](https://arxiv.org/abs/1805.07782)

##### PDF
[https://arxiv.org/pdf/1805.07782](https://arxiv.org/pdf/1805.07782)

