---
layout: post
title: "Exploring Domain Shift in Extractive Text Summarization"
date: 2019-08-30 11:40:14
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Danqing Wang, Pengfei Liu, Ming Zhong, Jie Fu, Xipeng Qiu, Xuanjing Huang
mathjax: true
---

* content
{:toc}

##### Abstract
Although domain shift has been well explored in many NLP applications, it still has received little attention in the domain of extractive text summarization. As a result, the model is under-utilizing the nature of the training data due to ignoring the difference in the distribution of training sets and shows poor generalization on the unseen domain. 
 With the above limitation in mind, in this paper, we first extend the conventional definition of the domain from categories into data sources for the text summarization task. Then we re-purpose a multi-domain summarization dataset and verify how the gap between different domains influences the performance of neural summarization models. 
 Furthermore, we investigate four learning strategies and examine their abilities to deal with the domain shift problem. 
 Experimental results on three different settings show their different characteristics in our new testbed. 
 Our source code including \textit{BERT-based}, \textit{meta-learning} methods for multi-domain summarization learning and the re-purposed dataset \textsc{Multi-SUM} will be available on our project: \url{<a href="http://pfliu.com/TransferSum/">this http URL</a>}.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.11664](http://arxiv.org/abs/1908.11664)

##### PDF
[http://arxiv.org/pdf/1908.11664](http://arxiv.org/pdf/1908.11664)

