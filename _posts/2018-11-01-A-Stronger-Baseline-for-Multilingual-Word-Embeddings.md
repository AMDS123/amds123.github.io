---
layout: post
title: "A Stronger Baseline for Multilingual Word Embeddings"
date: 2018-11-01 18:48:57
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model
author: Philipp Dufter, Hinrich Sch&#xfc;tze
mathjax: true
---

* content
{:toc}

##### Abstract
Levy, S{\o}gaard and Goldberg's (2017) S-ID (sentence ID) method applies word2vec on tuples containing a sentence ID and a word from the sentence. It has been shown to be a strong baseline for learning multilingual embeddings. Inspired by recent work on concept based embedding learning we propose SC-ID, an extension to S-ID: given a sentence aligned corpus, we use sampling to extract concepts that are then processed in the same manner as S-IDs. We perform experiments on the Parallel Bible Corpus across 1000+ languages and show that SC-ID yields up to 6% performance increase in a word translation task. In addition, we provide evidence that SC-ID is easily and widely applicable by reporting competitive results across 8 tasks on a EuroParl based corpus.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1811.00586](http://arxiv.org/abs/1811.00586)

##### PDF
[http://arxiv.org/pdf/1811.00586](http://arxiv.org/pdf/1811.00586)

