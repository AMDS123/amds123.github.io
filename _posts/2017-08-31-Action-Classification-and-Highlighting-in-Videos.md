---
layout: post
title: "Action Classification and Highlighting in Videos"
date: 2017-08-31 01:19:57
categories: arXiv_CV
tags: arXiv_CV Salient Attention RNN Classification Quantitative Recognition
author: Atousa Torabi, Leonid Sigal
mathjax: true
---

* content
{:toc}

##### Abstract
Inspired by recent advances in neural machine translation, that jointly align and translate using encoder-decoder networks equipped with attention, we propose an attentionbased LSTM model for human activity recognition. Our model jointly learns to classify actions and highlight frames associated with the action, by attending to salient visual information through a jointly learned soft-attention networks. We explore attention informed by various forms of visual semantic features, including those encoding actions, objects and scenes. We qualitatively show that soft-attention can learn to effectively attend to important objects and scene information correlated with specific human actions. Further, we show that, quantitatively, our attention-based LSTM outperforms the vanilla LSTM and CNN models used by stateof-the-art methods. On a large-scale youtube video dataset, ActivityNet, our model outperforms competing methods in action classification.

##### Abstract (translated by Google)
受到最近神经机器翻译的进展的启发，我们提出了一种基于注意力的LSTM模型来进行人类活动识别。我们的模式共同学习，通过共同学习的软关注网络来关注显着的视觉信息，从而对动作进行分类并突出显示与动作相关的帧。我们探索各种形式的视觉语义特征，包括编码动作，物体和场景的注意力。我们定性地表明，软注意力可以学习有效地关注与特定人类行为相关的重要对象和场景信息。此外，我们表明，在数量上，我们的注意力为基础的LSTM优于国家的最先进的方法所使用的香草LSTM和CNN模型。在大规模的YouTube视频数据集ActivityNet上，我们的模型胜过了行为分类中的竞争方法。

##### URL
[https://arxiv.org/abs/1708.09522](https://arxiv.org/abs/1708.09522)

##### PDF
[https://arxiv.org/pdf/1708.09522](https://arxiv.org/pdf/1708.09522)

