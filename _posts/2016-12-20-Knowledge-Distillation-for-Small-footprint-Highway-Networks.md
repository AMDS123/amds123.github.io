---
layout: post
title: "Knowledge Distillation for Small-footprint Highway Networks"
date: 2016-12-20 22:21:48
categories: arXiv_CL
tags: arXiv_CL Knowledge Speech_Recognition Deep_Learning Recognition
author: Liang Lu, Michelle Guo, Steve Renals
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning has significantly advanced state-of-the-art of speech recognition in the past few years. However, compared to conventional Gaussian mixture acoustic models, neural network models are usually much larger, and are therefore not very deployable in embedded devices. Previously, we investigated a compact highway deep neural network (HDNN) for acoustic modelling, which is a type of depth-gated feedforward neural network. We have shown that HDNN-based acoustic models can achieve comparable recognition accuracy with much smaller number of model parameters compared to plain deep neural network (DNN) acoustic models. In this paper, we push the boundary further by leveraging on the knowledge distillation technique that is also known as {\it teacher-student} training, i.e., we train the compact HDNN model with the supervision of a high accuracy cumbersome model. Furthermore, we also investigate sequence training and adaptation in the context of teacher-student training. Our experiments were performed on the AMI meeting speech recognition corpus. With this technique, we significantly improved the recognition accuracy of the HDNN acoustic model with less than 0.8 million parameters, and narrowed the gap between this model and the plain DNN with 30 million parameters.

##### Abstract (translated by Google)
深度学习在过去几年中已经大大提高了语音识别的最新水平。然而，与传统的高斯混合声学模型相比，神经网络模型通常要大得多，因此在嵌入式设备中不能很好地部署。之前，我们研究了一种用于声学建模的紧凑型高速公路深度神经网络（HDNN），这是一种深度门控前馈神经网络。我们已经表明，与普通深度神经网络（DNN）声学模型相比，基于HDNN的声学模型可以实现可比较的识别准确度，且模型参数的数量少得多。在本文中，我们利用知识蒸馏技术进一步推进边界，也称为{\ it老师 - 学生}训练，即在高精度繁琐的模型的监督下训练紧​​凑的HDNN模型。此外，我们还调查在师生培训中的顺序训练和适应。我们的实验是在AMI会议语音识别语料库上进行的。利用这一技术，我们大大提高了参数少于80万的HDNN声学模型的识别精度，并缩小了该模型与原始DNN之间的3000万参数的差距。

##### URL
[https://arxiv.org/abs/1608.00892](https://arxiv.org/abs/1608.00892)

##### PDF
[https://arxiv.org/pdf/1608.00892](https://arxiv.org/pdf/1608.00892)

