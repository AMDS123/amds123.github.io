---
layout: post
title: "Why do These Match? Explaining the Behavior of Image Similarity Models"
date: 2019-05-26 12:48:23
categories: arXiv_CV
tags: arXiv_CV Salient Image_Classification Classification Deep_Learning VQA Recognition
author: Bryan A. Plummer, Mariya I. Vasileva, Vitali Petsiuk, Kate Saenko, David Forsyth
mathjax: true
---

* content
{:toc}

##### Abstract
Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering. In this paper, we introduce an explanation approach for image similarity models, where a model's output is a semantic feature representation rather than a classification. In this task, an explanation depends on both of the input images, so standard methods do not apply. We propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match. We find that our explanations are more human-interpretable than saliency maps alone, and can also improve performance on the classic task of attribute recognition. The ability of our approach to generalize is demonstrated on two datasets from very different domains, Polyvore Outfits and Animals with Attributes 2.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1905.10797](http://arxiv.org/abs/1905.10797)

##### PDF
[http://arxiv.org/pdf/1905.10797](http://arxiv.org/pdf/1905.10797)

