---
layout: post
title: "Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation"
date: 2016-10-13 07:40:37
categories: arXiv_CL
tags: arXiv_CL Attention
author: Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, Zhi Jin
mathjax: true
---

* content
{:toc}

##### Abstract
Using neural networks to generate replies in human-computer dialogue systems is attracting increasing attention over the past few years. However, the performance is not satisfactory: the neural network tends to generate safe, universally relevant replies which carry little meaning. In this paper, we propose a content-introducing approach to neural network-based generative dialogue systems. We first use pointwise mutual information (PMI) to predict a noun as a keyword, reflecting the main gist of the reply. We then propose seq2BF, a "sequence to backward and forward sequences" model, which generates a reply containing the given keyword. Experimental results show that our approach significantly outperforms traditional sequence-to-sequence models in terms of human evaluation and the entropy measure, and that the predicted keyword can appear at an appropriate position in the reply.

##### Abstract (translated by Google)
在人机对话系统中使用神经网络产生回复在过去几年引起越来越多的关注。然而，表现并不令人满意：神经网络倾向于产生安全，普遍相关的答复，这些答案意义不大。在本文中，我们提出了一种基于神经网络的生成对话系统的内容引入方法。我们首先使用点对点互信息（PMI）来预测名词作为关键词，这反映了答复的主要依据。然后，我们提出seq2BF，一个“序列向后和向前序列”模型，它生成一个包含给定关键字的回复。实验结果表明，我们的方法在人类评估和熵测量方面明显优于传统的序列到序列模型，并且预测的关键字可以出现在回复中的适当位置。

##### URL
[https://arxiv.org/abs/1607.00970](https://arxiv.org/abs/1607.00970)

##### PDF
[https://arxiv.org/pdf/1607.00970](https://arxiv.org/pdf/1607.00970)

