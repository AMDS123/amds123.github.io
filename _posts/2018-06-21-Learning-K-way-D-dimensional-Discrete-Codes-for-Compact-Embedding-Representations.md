---
layout: post
title: "Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations"
date: 2018-06-21 18:59:05
categories: arXiv_AI
tags: arXiv_AI Embedding CNN Optimization Gradient_Descent
author: Ting Chen, Martin Renqiang Min, Yizhou Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying a linear transformation based on a "one-hot" encoding of the discrete symbols. Despite its simplicity, such approach yields the number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work, we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the "one-hot" encoding. In the proposed "KD encoding", each symbol is represented by a $D$-dimensional code with a cardinality of $K$, and the final symbol embedding vector is generated by composing the code embedding vectors. To end-to-end learn semantically meaningful codes, we derive a relaxed discrete optimization approach based on stochastic gradient descent, which can be generally applied to any differentiable computational graph with an embedding layer. In our experiments with various applications from natural language processing to graph convolutional networks, the total size of the embedding layer can be reduced up to 98\% while achieving similar or better performance.

##### Abstract (translated by Google)
传统的嵌入方法直接将每个符号与连续的嵌入矢量相关联，这相当于基于离散符号的“单热”编码应用线性变换。尽管简单，但这种方法会产生与词汇大小呈线性增长并可能导致过度拟合的参数数目。在这项工作中，我们提出了一种更紧凑的K路D维离散编码方案来取代“单热”编码。在所提出的“KD编码”中，每个符号由基数为$ K $的$ D $维度代码表示，并且通过组合代码嵌入矢量来生成最终符号嵌入矢量。为了端到端地学习语义上有意义的代码，我们推导了一种基于随机梯度下降的松散离散优化方法，该方法通常可以应用于任何具有嵌入层的可微分计算图。在我们对从自然语言处理到图形卷积网络的各种应用的实验中，嵌入层的总大小可以降低到98％，同时达到相似或更好的性能。

##### URL
[http://arxiv.org/abs/1806.09464](http://arxiv.org/abs/1806.09464)

##### PDF
[http://arxiv.org/pdf/1806.09464](http://arxiv.org/pdf/1806.09464)

