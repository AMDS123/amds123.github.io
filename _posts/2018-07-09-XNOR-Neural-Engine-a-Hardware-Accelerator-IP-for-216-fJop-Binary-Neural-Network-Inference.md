---
layout: post
title: "XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary Neural Network Inference"
date: 2018-07-09 09:40:37
categories: arXiv_AI
tags: arXiv_AI CNN Inference
author: Francesco Conti, Pasquale Davide Schiavone, Luca Benini
mathjax: true
---

* content
{:toc}

##### Abstract
Binary Neural Networks (BNNs) are promising to deliver accuracy comparable to conventional deep neural networks at a fraction of the cost in terms of memory and energy. In this paper, we introduce the XNOR Neural Engine (XNE), a fully digital configurable hardware accelerator IP for BNNs, integrated within a microcontroller unit (MCU) equipped with an autonomous I/O subsystem and hybrid SRAM / standard cell memory. The XNE is able to fully compute convolutional and dense layers in autonomy or in cooperation with the core in the MCU to realize more complex behaviors. We show post-synthesis results in 65nm and 22nm technology for the XNE IP and post-layout results in 22nm for the full MCU indicating that this system can drop the energy cost per binary operation to 21.6fJ per operation at 0.4V, and at the same time is flexible and performant enough to execute state-of-the-art BNN topologies such as ResNet-34 in less than 2.2mJ per frame at 8.9 fps.

##### Abstract (translated by Google)
二进制神经网络（BNN）有望提供与传统深度神经网络相当的精度，其成本仅为内存和能量的一小部分。在本文中，我们介绍了XNOR神经引擎（XNE），一种用于BNN的全数字可配置硬件加速器IP，集成在配备有自主I / O子系统和混合SRAM /标准单元存储器的微控制器单元（MCU）中。 XNE能够在自治中完全计算卷积和密集层，或者与MCU中的核心协作，以实现更复杂的行为。我们展示了针对XNE IP的65nm和22nm技术的后合成结果以及针对整个MCU的22nm后布局结果，表明该系统可以将每个二进制操作的能量成本降低到每个操作在61V时的21.6fJ，并且在同时具有足够的灵活性和高性能，可以以每帧不到2.2mJ的速率执行最先进的BNN拓扑结构，如ResNet-34，速度为8.9 fps。

##### URL
[http://arxiv.org/abs/1807.03010](http://arxiv.org/abs/1807.03010)

##### PDF
[http://arxiv.org/pdf/1807.03010](http://arxiv.org/pdf/1807.03010)

