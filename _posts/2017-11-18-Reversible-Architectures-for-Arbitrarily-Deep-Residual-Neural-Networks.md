---
layout: post
title: "Reversible Architectures for Arbitrarily Deep Residual Neural Networks"
date: 2017-11-18 22:10:57
categories: arXiv_CV
tags: arXiv_CV
author: Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, Elliot Holtham
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memory-efficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.

##### Abstract (translated by Google)
最近，深度残差网络已成功应用于许多计算机视觉和自然语言处理任务中，以更深更广的体系结构推动最先进的性能。在这项工作中，我们将深度残差网络解释为常微分方程（ODEs），这些方程在数理和物理学方面已经有很长时间的研究，并且在理论和实践上都取得了丰硕的成果。根据这一解释，我们发展了一个关于深度神经网络的稳定性和可逆性的理论框架，从理论上导出了三种可以任意深入的可逆神经网络结构。可逆属性允许一个高效的内存实现，不需要存储大多数隐藏层的激活。加上我们的架构的稳定性，这使得只使用适度的计算资源来训练更深的网络。我们提供了理论分析和实证结果。实验结果证明了我们的体系结构对CIFAR-10，CIFAR-100和STL-10上的几条强基线的有效性，具有卓越的性能或性能优异的性能。此外，我们展示了当使用较少的训练数据进行训练时，我们的架构会产生出色的结果

##### URL
[https://arxiv.org/abs/1709.03698](https://arxiv.org/abs/1709.03698)

##### PDF
[https://arxiv.org/pdf/1709.03698](https://arxiv.org/pdf/1709.03698)

