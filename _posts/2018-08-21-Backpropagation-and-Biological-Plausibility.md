---
layout: post
title: "Backpropagation and Biological Plausibility"
date: 2018-08-21 14:41:56
categories: arXiv_AI
tags: arXiv_AI Optimization Deep_Learning
author: Alessandro Betti, Marco Gori, Giuseppe Marra
mathjax: true
---

* content
{:toc}

##### Abstract
By and large, Backpropagation (BP) is regarded as one of the most important neural computation algorithms at the basis of the progress in machine learning, including the recent advances in deep learning. However, its computational structure has been the source of many debates on its arguable biological plausibility. In this paper, it is shown that when framing supervised learning in the Lagrangian framework, while one can see a natural emergence of Backpropagation, biologically plausible local algorithms can also be devised that are based on the search for saddle points in the learning adjoint space composed of weights, neural outputs, and Lagrangian multipliers. This might open the doors to a truly novel class of learning algorithms where, because of the introduction of the notion of support neurons, the optimization scheme also plays a fundamental role in the construction of the architecture.

##### Abstract (translated by Google)
总的来说，反向传播（BP）被认为是机器学习进步的最重要的神经计算算法之一，包括深度学习的最新进展。然而，它的计算结构一直是许多关于其可论证的生物合理性的辩论的根源。在本文中，表明当在拉格朗日框架中构建监督学习时，虽然可以看到反向传播的自然出现，但也可以设计出基于在学习伴随空间中搜索鞍点的生物学似是而非的局部算法。权重，神经输出和拉格朗日乘数。这可能为真正新颖的学习算法打开了大门，由于引入了支持神经元的概念，优化方案在构造体系结构中也起着重要作用。

##### URL
[http://arxiv.org/abs/1808.06934](http://arxiv.org/abs/1808.06934)

##### PDF
[http://arxiv.org/pdf/1808.06934](http://arxiv.org/pdf/1808.06934)

