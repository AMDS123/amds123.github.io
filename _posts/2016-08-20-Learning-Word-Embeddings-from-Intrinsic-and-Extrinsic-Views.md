---
layout: post
title: "Learning Word Embeddings from Intrinsic and Extrinsic Views"
date: 2016-08-20 17:34:38
categories: arXiv_CL
tags: arXiv_CL Embedding Classification Prediction
author: Jifan Chen, Kan Chen, Xipeng Qiu, Qi Zhang, Xuanjing Huang, Zheng Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
While word embeddings are currently predominant for natural language processing, most of existing models learn them solely from their contexts. However, these context-based word embeddings are limited since not all words' meaning can be learned based on only context. Moreover, it is also difficult to learn the representation of the rare words due to data sparsity problem. In this work, we address these issues by learning the representations of words by integrating their intrinsic (descriptive) and extrinsic (contextual) information. To prove the effectiveness of our model, we evaluate it on four tasks, including word similarity, reverse dictionaries,Wiki link prediction, and document classification. Experiment results show that our model is powerful in both word and document modeling.

##### Abstract (translated by Google)
虽然词汇嵌入目前主要用于自然语言处理，但现有的大多数模型仅从其语境中学习。然而，这些基于上下文的单词嵌入是有限的，因为不是所有单词的意义都可以仅仅基于上下文来学习。而且，由于数据稀疏性问题，学习稀有词的表示也是困难的。在这项工作中，我们通过整合他们的内在（描述性）和外在（上下文）信息来学习单词的表达来解决这些问题。为了证明我们的模型的有效性，我们评估了四个任务，包括词相似度，反向词典，维基链接预测和文档分类。实验结果表明，我们的模型在文字和文档建模方面都很强大。

##### URL
[https://arxiv.org/abs/1608.05852](https://arxiv.org/abs/1608.05852)

##### PDF
[https://arxiv.org/pdf/1608.05852](https://arxiv.org/pdf/1608.05852)

