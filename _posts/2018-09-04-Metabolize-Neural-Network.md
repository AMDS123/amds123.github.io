---
layout: post
title: "Metabolize Neural Network"
date: 2018-09-04 08:42:52
categories: arXiv_AI
tags: arXiv_AI Deep_Learning
author: Dan Dai, Zhiwen Yu, Yang Hu, Wenming Cao, Mingnan Luo
mathjax: true
---

* content
{:toc}

##### Abstract
The metabolism of cells is the most basic and important part of human function. Neural networks in deep learning stem from neuronal activity. It is self-evident that the significance of metabolize neuronal network(MetaNet) in model construction. In this study, we explore neuronal metabolism for shallow network from proliferation and autophagy two aspects. First, we propose different neuron proliferate methods that constructive the selfgrowing network in metabolism cycle. Proliferate neurons alleviate resources wasting and insufficient model learning problem when network initializes more or less parameters. Then combined with autophagy mechanism in the process of model self construction to ablate under-expressed neurons. The MetaNet can automatically determine the number of neurons during training, further, save more resource consumption. We verify the performance of the proposed methods on datasets: MNIST, Fashion-MNIST and CIFAR-10.

##### Abstract (translated by Google)
细胞的新陈代谢是人体功能最基本，最重要的部分。深度学习中的神经网络源于神经元活动。在模型构建中代谢神经元网络（MetaNet）的重要性是不言而喻的。在这项研究中，我们从增殖和自噬两个方面探讨了浅层网络的神经元代谢。首先，我们提出了不同的神经元增殖方法，即在代谢周期中构建自我生长网络。当网络初始化或多或少的参数时，增殖神经元减轻资源浪费和模型学习问题不足。然后结合自噬机制在模型自我构建过程中消融表达不足的神经元。 MetaNet可以自动确定训练期间的神经元数量，进一步节省更多的资源消耗。我们验证了所提方法在数据集上的性能：MNIST，Fashion-MNIST和CIFAR-10。

##### URL
[http://arxiv.org/abs/1809.00837](http://arxiv.org/abs/1809.00837)

##### PDF
[http://arxiv.org/pdf/1809.00837](http://arxiv.org/pdf/1809.00837)

