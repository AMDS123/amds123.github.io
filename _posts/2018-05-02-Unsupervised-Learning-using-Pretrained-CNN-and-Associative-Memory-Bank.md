---
layout: post
title: "Unsupervised Learning using Pretrained CNN and Associative Memory Bank"
date: 2018-05-02 21:32:08
categories: arXiv_CV
tags: arXiv_CV CNN Classification Deep_Learning Recognition
author: Qun Liu, Supratik Mukhopadhyay
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Convolutional features extracted from a comprehensive labeled dataset, contain substantial representations which could be effectively used in a new domain. Despite the fact that generic features achieved good results in many visual tasks, fine-tuning is required for pretrained deep CNN models to be more effective and provide state-of-the-art performance. Fine tuning using the backpropagation algorithm in a supervised setting, is a time and resource consuming process. In this paper, we present a new architecture and an approach for unsupervised object recognition that addresses the above mentioned problem with fine tuning associated with pretrained CNN-based supervised deep learning approaches while allowing automated feature extraction. Unlike existing works, our approach is applicable to general object recognition tasks. It uses a pretrained (on a related domain) CNN model for automated feature extraction pipelined with a Hopfield network based associative memory bank for storing patterns for classification purposes. The use of associative memory bank in our framework allows eliminating backpropagation while providing competitive performance on an unseen dataset.

##### Abstract (translated by Google)
从全面标记的数据集提取的深度卷积特征包含可在新域中有效使用的实质性表示。尽管通用特征在许多视觉任务中取得了良好的效果，但对于预训练的深CNN模型来说，微调需要更有效并提供最先进的性能。在监督设置中使用反向传播算法进行微调是耗费时间和资源的过程。在本文中，我们提出了一种新的架构和一种无监督对象识别方法，通过与预训练的基于CNN的有监督深度学习方法相关联的微调来解决上述问题，同时允许自动特征提取。与现有的作品不同，我们的方法适用于一般物体识别任务。它使用预训练（在相关领域）CNN模型进行自动特征提取，流水线化为基于Hopfield网络的联想记忆库，用于存储用于分类目的的模式。在我们的框架中使用联想记忆库可以消除反向传播，同时在不可见的数据集上提供有竞争力的表现。

##### URL
[http://arxiv.org/abs/1805.01033](http://arxiv.org/abs/1805.01033)

##### PDF
[http://arxiv.org/pdf/1805.01033](http://arxiv.org/pdf/1805.01033)

