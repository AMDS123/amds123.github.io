---
layout: post
title: "Full-Network Embedding in a Multimodal Embedding Pipeline"
date: 2017-08-09 13:11:42
categories: arXiv_CL
tags: arXiv_CL Image_Caption Image_Retrieval Embedding
author: Armand Vilalta, Dario Garcia-Gasulla, Ferran Parés, Eduard Ayguadé, Jesus Labarta, Ulises Cortés, Toyotaro Suzumura
mathjax: true
---

* content
{:toc}

##### Abstract
The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.

##### Abstract (translated by Google)
当前最先进的图像标注和图像检索任务是通过深层神经网络，将图像表示和文本表示组合到一个共享的嵌入空间中获得的。在本文中，我们评估在此设置中使用全网络嵌入的影响，在竞争多模式嵌入生成方案中替换原始图像表示。与大多数方法通常使用的单层图像嵌入不同，全网嵌入提供了图像的多尺度表示，这导致了更丰富的表征。为了测量全网嵌入的影响，我们评估了它在三个不同数据集上的性能，并且将结果与使用单层图像嵌入时的原始多模式嵌入生成方案进行比较，并且与其余的状态 - 艺术。图像标注和图像检索任务的结果表明，全网嵌入一直优于单层嵌入。这些结果激发了全网嵌入到任何多模式嵌入生成方案的整合，由于该方法的灵活性，这是可行的。

##### URL
[https://arxiv.org/abs/1707.09872](https://arxiv.org/abs/1707.09872)

##### PDF
[https://arxiv.org/pdf/1707.09872](https://arxiv.org/pdf/1707.09872)

