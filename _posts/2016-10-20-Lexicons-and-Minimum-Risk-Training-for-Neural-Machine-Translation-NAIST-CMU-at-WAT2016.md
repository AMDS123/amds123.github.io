---
layout: post
title: "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016"
date: 2016-10-20 19:10:09
categories: arXiv_CL
tags: arXiv_CL NMT
author: Graham Neubig
mathjax: true
---

* content
{:toc}

##### Abstract
This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task.

##### Abstract (translated by Google)
今年，奈良科学技术研究院（NAIST）/卡内基梅隆大学（CMU）提交的2016年亚洲翻译研讨会的日英翻译版本是基于注意神经机器翻译（NMT）模型。除了标准的NMT模型外，我们做了一些改进，特别是使用离散翻译词典来提高概率估计，以及使用最小风险训练来优化MT系统的BLEU评分。因此，我们的系统达到了最高的翻译评估分数的任务。

##### URL
[https://arxiv.org/abs/1610.06542](https://arxiv.org/abs/1610.06542)

