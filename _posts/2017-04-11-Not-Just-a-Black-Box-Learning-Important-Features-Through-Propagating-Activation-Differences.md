---
layout: post
title: "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences"
date: 2017-04-11 15:58:48
categories: arXiv_CV
tags: arXiv_CV
author: Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje
mathjax: true
---

* content
{:toc}

##### Abstract
Note: This paper describes an older version of DeepLIFT. See this https URL for the newer version. Original abstract follows: The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.

##### Abstract (translated by Google)
注意：本文介绍了较旧版本的DeepLIFT。请参阅此https URL以获取更新的版本。原始摘要如下：神经网络的“黑匣子”本质是在可解释性至关重要的应用中被采用的障碍。在这里，我们介绍DeepLIFT（学习重要特性），一种高效和有效的方法来计算神经网络中的重要性分数。 DeepLIFT将每个神经元的激活与其“参考激活”进行比较，并根据差异分配贡献分数。我们将DeepLIFT应用于训练自然图像和基因组数据的模型，并且显示了基于梯度的方法的显着优势。

##### URL
[https://arxiv.org/abs/1605.01713](https://arxiv.org/abs/1605.01713)

##### PDF
[https://arxiv.org/pdf/1605.01713](https://arxiv.org/pdf/1605.01713)

