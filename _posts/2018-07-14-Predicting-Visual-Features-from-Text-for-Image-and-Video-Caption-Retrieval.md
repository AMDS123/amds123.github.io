---
layout: post
title: "Predicting Visual Features from Text for Image and Video Caption Retrieval"
date: 2018-07-14 12:01:20
categories: arXiv_CV
tags: arXiv_CV Video_Caption Caption Embedding CNN
author: Jianfeng Dong, Xirong Li, Cees G. M. Snoek
mathjax: true
---

* content
{:toc}

##### Abstract
This paper strives to find amidst a set of sentences the one best describing the content of a given image or video. Different from existing works, which rely on a joint subspace for their image and video caption retrieval, we propose to do so in a visual space exclusively. Apart from this conceptual novelty, we contribute \emph{Word2VisualVec}, a deep neural network architecture that learns to predict a visual feature representation from textual input. Example captions are encoded into a textual embedding based on multi-scale sentence vectorization and further transferred into a deep visual feature of choice via a simple multi-layer perceptron. We further generalize Word2VisualVec for video caption retrieval, by predicting from text both 3-D convolutional neural network features as well as a visual-audio representation. Experiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset and the very recent NIST TrecVid challenge for video caption retrieval detail Word2VisualVec's properties, its benefit over textual embeddings, the potential for multimodal query composition and its state-of-the-art results.

##### Abstract (translated by Google)
本文力图在一组句子中找到最能描述给定图像或视频内容的句子。与现有作品不同，现有作品依赖于联合子空间进行图像和视频字幕检索，我们建议在视觉空间中专门进行。除了这个概念上的新颖性，我们贡献\ emph {Word2VisualVec}，一个深度神经网络架构，学习从文本输入预测视觉特征表示。示例字幕被编码为基于多尺度句子矢量化的文本嵌入，并且通过简单的多层感知器进一步转换为选择的深度视觉特征。我们进一步推广Word2VisualVec用于视频字幕检索，通过从文本预测3-D卷积神经网络特征以及视觉 - 音频表示。 Flickr8k，Flickr30k，Microsoft视频描述数据集以及最近NIST TrecVid视频字幕检索挑战的实验详细介绍了Word2VisualVec的属性，它对文本嵌入的好处，多模式查询组合的潜力及其最先进的结果。

##### URL
[https://arxiv.org/abs/1709.01362](https://arxiv.org/abs/1709.01362)

##### PDF
[https://arxiv.org/pdf/1709.01362](https://arxiv.org/pdf/1709.01362)

