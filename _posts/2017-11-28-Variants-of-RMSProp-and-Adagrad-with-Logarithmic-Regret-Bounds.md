---
layout: post
title: "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds"
date: 2017-11-28 18:47:37
categories: arXiv_CV
tags: arXiv_CV Optimization Gradient_Descent
author: Mahesh Chandra Mukkamala, Matthias Hein
mathjax: true
---

* content
{:toc}

##### Abstract
Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.

##### Abstract (translated by Google)
自适应梯度方法近来已经变得非常流行，特别是因为它们已经被证明在深度神经网络的训练中是有用的。在本文中，我们分析了最初提出用于深度神经网络训练的RMSProp，在线凸优化和显示$ \ sqrt {T} $类型的遗憾边界。此外，我们提出了两个变体SC-Adagrad和SC-RMSProp，我们给出了强凸函数的对数后值界。最后，我们在实验中证明这些新的变体在强凸函数的优化以及在深度神经网络的训练中优于其他自适应梯度技术或随机梯度下降。

##### URL
[https://arxiv.org/abs/1706.05507](https://arxiv.org/abs/1706.05507)

##### PDF
[https://arxiv.org/pdf/1706.05507](https://arxiv.org/pdf/1706.05507)

