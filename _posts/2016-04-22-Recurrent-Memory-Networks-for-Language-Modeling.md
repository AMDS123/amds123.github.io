---
layout: post
title: "Recurrent Memory Networks for Language Modeling"
date: 2016-04-22 11:13:11
categories: arXiv_CV
tags: arXiv_CV RNN Language_Model Memory_Networks
author: Ke Tran, Arianna Bisazza, Christof Monz
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state-of-the-art by a large margin.

##### Abstract (translated by Google)
递归神经网络（RNN）在许多自然语言处理（NLP）任务中取得了很好的效果。然而，理解和解释这一成功的根源仍然是一个挑战。在本文中，我们提出了一种新型的RNN架构 - 循环记忆网络（Recurrent Memory Network，RMN），它不仅可以放大RNN的功能，而且可以帮助我们理解其内部功能，并且可以发现数据中的底层模式。我们展示了RMN在语言建模和句子完成任务上的能力。在语言建模上，RMN比三个大的德语，意大利语和英语数据集优于长期短期记忆（LSTM）网络。另外，我们对RMN捕获的各种语言维度进行深入分析。在“句子完成挑战”中，我们的RMN获得了69.2％的准确性，并且大大超过了先前的技术水平。

##### URL
[https://arxiv.org/abs/1601.01272](https://arxiv.org/abs/1601.01272)

##### PDF
[https://arxiv.org/pdf/1601.01272](https://arxiv.org/pdf/1601.01272)

