---
layout: post
title: "Exploiting Inherent Error-Resiliency of Neuromorphic Computing to achieve Extreme Energy-Efficiency through Mixed-Signal Neurons"
date: 2018-06-13 16:43:05
categories: arXiv_AI
tags: arXiv_AI CNN Classification Recognition
author: Baibhab Chatterjee, Priyadarshini Panda, Shovan Maity, Ayan Biswas, Kaushik Roy, Shreyas Sen
mathjax: true
---

* content
{:toc}

##### Abstract
Neuromorphic computing, inspired by the brain, promises extreme efficiency for certain classes of learning tasks, such as classification and pattern recognition. The performance and power consumption of neuromorphic computing depends heavily on the choice of the neuron architecture. Digital neurons (Dig-N) are conventionally known to be accurate and efficient at high speed, while suffering from high leakage currents from a large number of transistors in a large design. On the other hand, analog/mixed-signal neurons are prone to noise, variability and mismatch, but can lead to extremely low-power designs. In this work, we will analyze, compare and contrast existing neuron architectures with a proposed mixed-signal neuron (MS-N) in terms of performance, power and noise, thereby demonstrating the applicability of the proposed mixed-signal neuron for achieving extreme energy-efficiency in neuromorphic computing. The proposed MS-N is implemented in 65 nm CMOS technology and exhibits &gt; 100X better energy-efficiency across all frequencies over two traditional digital neurons synthesized in the same technology node. We also demonstrate that the inherent error-resiliency of a fully connected or even convolutional neural network (CNN) can handle the noise as well as the manufacturing non-idealities of the MS-N up to certain degrees. Notably, a system-level implementation on MNIST datasets exhibits a worst-case increase in classification error by 2.1% when the integrated noise power in the bandwidth is ~ 0.1 uV2, along with +-3{\sigma} amount of variation and mismatch introduced in the transistor parameters for the proposed neuron with 8-bit precision.

##### Abstract (translated by Google)
受大脑启发的神经形态计算为某些类别的学习任务（如分类和模式识别）提供了极高的效率。神经形态计算的性能和功耗在很大程度上取决于神经元结构的选择。数字神经元（Dig-N）通常被认为是高速精确和高效的，同时在大型设计中遭受来自大量晶体管的高漏电流。另一方面，模拟/混合信号神经元容易出现噪声，变化和不匹配，但可导致极低功耗的设计。在这项工作中，我们将分析，比较和比较现有的神经元架构与建议的混合信号神经元（MS-N）的性能，功率和噪声，从而表明提出的混合信号神经元获得极端能量的适用性神经形态计算的效率。所提出的MS-N采用65nm CMOS技术实现，在相同技术节点中合成的两个传统数字神经元上，所有频率上的能量效率提高了100倍。我们还证明，完全连接或甚至卷积神经网络（CNN）的固有错误恢复能力可以处理噪声以及MS-N的制造非理想性达到某种程度。值得注意的是，当MNIST数据集上的系统级实现在带宽中的集成噪声功率为〜0.1uV2时引入了分类错误的最差情况增加了2.1％，同时导入了±3σ变化量和失配在所提出的神经元的晶体管参数中具有8位精度。

##### URL
[http://arxiv.org/abs/1806.05141](http://arxiv.org/abs/1806.05141)

##### PDF
[http://arxiv.org/pdf/1806.05141](http://arxiv.org/pdf/1806.05141)

