---
layout: post
title: "Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs"
date: 2016-09-22 17:16:43
categories: arXiv_CL
tags: arXiv_CL Knowledge_Graph Knowledge Attention Represenation_Learning Classification Prediction
author: Jiawei Wu, Ruobing Xie, Zhiyuan Liu, Maosong Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Textual information is considered as significant supplement to knowledge representation learning (KRL). There are two main challenges for constructing knowledge representations from plain texts: (1) How to take full advantages of sequential contexts of entities in plain texts for KRL. (2) How to dynamically select those informative sentences of the corresponding entities for KRL. In this paper, we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences. Given each reference sentence of an entity, we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity. Then we further design an attention model to measure the informativeness of each sentence, and build text-based representations of entities. We evaluate our method on two tasks, including triple classification and link prediction. Experimental results demonstrate that our method outperforms other baselines on both tasks, which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations.

##### Abstract (translated by Google)
文本信息被认为是知识表示学习（KRL）的重要补充。从纯文本构建知识表示有两个主要挑战：（1）如何充分利用纯文本实体的顺序上下文对于KRL。 （2）如何动态选择KRL对应实体的信息句子。在本文中，我们提出了序列文本体现的知识表示学习，从多个句子建立知识表示。给定一个实体的每个参考句，我们首先利用具有汇集或长期短期记忆网络的递归神经网络来对该实体的语义信息进行编码。然后我们进一步设计一个关注模型来衡量每个句子的信息性，并建立实体的文本表示。我们评估我们的方法在两个任务，包括三重分类和链接预测。实验结果表明，我们的方法在两个任务上都优于其他基线，这表明我们的方法能够选择信息句子，并将文本信息很好地编码为知识表示。

##### URL
[https://arxiv.org/abs/1609.07075](https://arxiv.org/abs/1609.07075)

##### PDF
[https://arxiv.org/pdf/1609.07075](https://arxiv.org/pdf/1609.07075)

