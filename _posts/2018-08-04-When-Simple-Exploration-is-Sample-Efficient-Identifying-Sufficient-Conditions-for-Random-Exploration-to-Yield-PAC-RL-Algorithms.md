---
layout: post
title: "When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms"
date: 2018-08-04 01:13:03
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Yao Liu, Emma Brunskill
mathjax: true
---

* content
{:toc}

##### Abstract
Efficient exploration is one of the key challenges for reinforcement learning (RL) algorithms. Most traditional sample efficiency bounds require strategic exploration. Recently many deep RL algorithms with simple heuristic exploration strategies that have few formal guarantees, achieve surprising success in many domains. These results pose an important question about understanding these exploration strategies such as $e$-greedy, as well as understanding what characterize the difficulty of exploration in MDPs. In this work we propose problem specific sample complexity bounds of $Q$ learning with random walk exploration that rely on several structural properties. We also link our theoretical results to some empirical benchmark domains, to illustrate if our bound gives polynomial sample complexity in these domains and how that is related with the empirical performance.

##### Abstract (translated by Google)
有效的探索是强化学习（RL）算法的关键挑战之一。大多数传统的样本效率限制需要战略探索。最近，许多具有简单启发式探索策略的深度RL算法几乎没有正式保证，在许多领域取得了惊人的成功。这些结果提出了一个重要的问题，即理解这些探索策略，例如$ e $ -greedy，以及了解MDP中探索难度的特征。在这项工作中，我们提出了问题特定的样本复杂性界限$ Q $学习与随机游走探索依赖于几个结构属性。我们还将我们的理论结果与一些经验基准域相关联，以说明我们的约束是否给出了这些域中的多项式样本复杂性以及它与经验性能的关系。

##### URL
[http://arxiv.org/abs/1805.09045](http://arxiv.org/abs/1805.09045)

##### PDF
[http://arxiv.org/pdf/1805.09045](http://arxiv.org/pdf/1805.09045)

