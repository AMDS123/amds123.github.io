---
layout: post
title: "Auto-Encoding Variational Neural Machine Translation"
date: 2018-07-27 13:03:06
categories: arXiv_CL
tags: arXiv_CL Inference Prediction
author: Bryan Eikema, Wilker Aziz
mathjax: true
---

* content
{:toc}

##### Abstract
We present a deep generative model of bilingual sentence pairs. The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks. Efficient training is done by amortised variational inference and reparameterised gradients. Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a posteriori decoding for fast test-time predictions. We demonstrate the effectiveness of our model in three scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data. Our experiments show consistently that our joint formulation outperforms conditional modelling in all such scenarios.

##### Abstract (translated by Google)
我们提出了双语句子对的深层生成模型。该模型从共享的潜在表示联合生成源和目标句子，并由神经网络参数化。通过摊销的变分推理和重新参数化的梯度进行有效的训练。此外，我们讨论了联合建模的统计含义，并提出了对最大后验解码的有效近似，以实现快速的测试时间预测。我们在三种情景中证明了我们模型的有效性：域内培训，混合域培训以及从金标准和合成数据的混合中学习。我们的实验一致表明，我们的联合公式在所有这些情景中都优于条件建模。

##### URL
[http://arxiv.org/abs/1807.10564](http://arxiv.org/abs/1807.10564)

##### PDF
[http://arxiv.org/pdf/1807.10564](http://arxiv.org/pdf/1807.10564)

