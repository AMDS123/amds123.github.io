---
layout: post
title: "Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions"
date: 2018-02-28 21:28:28
categories: arXiv_AI
tags: arXiv_AI Adversarial Deep_Learning Relation
author: Quynh Nguyen, Mahesh Mukkamala, Matthias Hein
mathjax: true
---

* content
{:toc}

##### Abstract
In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide layer is necessary to produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.

##### Abstract (translated by Google)
在最近的文献中，强调深度学习在深度学习中的重要作用。在本文中，我们认为前馈网络的足够宽度通过回答关于神经网络决策区域连接条件的简单问题同等重要。事实证明，对于一类激活函数，包括泄漏的ReLU，具有金字塔结构的神经网络，没有层具有比输入维数更多的隐藏单元，产生必然连接的判定区域。这意味着需要足够宽的层来产生断开的决策区域。我们讨论这个结果对神经网络构建的影响，特别是与分类器对抗操纵问题的关系。

##### URL
[http://arxiv.org/abs/1803.00094](http://arxiv.org/abs/1803.00094)

##### PDF
[http://arxiv.org/pdf/1803.00094](http://arxiv.org/pdf/1803.00094)

