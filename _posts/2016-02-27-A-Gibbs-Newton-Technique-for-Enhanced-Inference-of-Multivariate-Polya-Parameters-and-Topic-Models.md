---
layout: post
title: "A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya Parameters and Topic Models"
date: 2016-02-27 12:29:43
categories: arXiv_CL
tags: arXiv_CL Inference Classification
author: Osama Khalifa, David Wolfe Corne, Mike Chantler
mathjax: true
---

* content
{:toc}

##### Abstract
Hyper-parameters play a major role in the learning and inference process of latent Dirichlet allocation (LDA). In order to begin the LDA latent variables learning process, these hyper-parameters values need to be pre-determined. We propose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs Newton' (LDA-GN), which places non-informative priors over these hyper-parameters and uses Gibbs sampling to learn appropriate values for them. At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new technique for learning the parameters of multivariate Polya distributions. We report Gibbs-Newton performance results compared with two prominent existing approaches to the latter task: Minka's fixed-point iteration method and the Moments method. We then evaluate LDA-GN in two ways: (i) by comparing it with standard LDA in terms of the ability of the resulting topic models to generalize to unseen documents; (ii) by comparing it with standard LDA in its performance on a binary classification task.

##### Abstract (translated by Google)
超参数在潜在狄利克雷分配（LDA）的学习和推理过程中起着重要的作用。为了开始LDA潜变量学习过程，这些超参数值需要预先确定。我们提出了LDA的扩展，我们称之为“潜在Dirichlet分配Gibbs牛顿”（LDA-GN），它将非信息性的先验置于这些超参数上，并使用吉布斯采样来为它们学习适当的值。 LDA-GN的核心是我们提出的“Gibbs-Newton”算法，它是一种学习多元Polya分布参数的新技术。我们报告吉布斯牛顿性能结果比较两个突出的现有方法对后者的任务：敏卡的定点迭代法和矩量法。然后，我们通过两种方式评估LDA-GN：（i）通过将其与标准LDA进行比较，从而将所得主题模型推广到看不见的文档的能力; （ii）通过将其与标准LDA在二进制分类任务上的性能进行比较。

##### URL
[https://arxiv.org/abs/1510.06646](https://arxiv.org/abs/1510.06646)

##### PDF
[https://arxiv.org/pdf/1510.06646](https://arxiv.org/pdf/1510.06646)

