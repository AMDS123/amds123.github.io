---
layout: post
title: "Smaller Text Classifiers with Discriminative Cluster Embeddings"
date: 2019-06-23 02:00:40
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Mingda Chen, Kevin Gimpel
mathjax: true
---

* content
{:toc}

##### Abstract
Word embedding parameters often dominate overall model sizes in neural methods for natural language processing. We reduce deployed model sizes of text classifiers by learning a hard word clustering in an end-to-end manner. We use the Gumbel-Softmax distribution to maximize over the latent clustering while minimizing the task loss. We propose variations that selectively assign additional parameters to words, which further improves accuracy while still remaining parameter-efficient.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.09532](http://arxiv.org/abs/1906.09532)

##### PDF
[http://arxiv.org/pdf/1906.09532](http://arxiv.org/pdf/1906.09532)

