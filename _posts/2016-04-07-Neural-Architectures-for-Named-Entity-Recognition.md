---
layout: post
title: "Neural Architectures for Named Entity Recognition"
date: 2016-04-07 15:09:36
categories: arXiv_SD
tags: arXiv_SD Knowledge RNN Recognition
author: Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer
mathjax: true
---

* content
{:toc}

##### Abstract
State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.

##### Abstract (translated by Google)
最先进的命名实体识别系统在很大程度上依赖于手工制作的特征和特定领域的知识，以便从可用的小监督训练语料库中有效地学习。在本文中，我们介绍两种新的神经架构 - 一种基于双向LSTM和条件随机场，另一种使用基于移位 - 缩减解析器的基于过渡的方法构造和标记片段。我们的模型依赖于关于单词的两种信息来源：从监督语料库学习的基于字符的单词表示和从未注释语料库中学习的无监督词表示。我们的模型以四种语言获得了NER的最新性能，而无需使用任何特定语言的知识或地名词典等资源。

##### URL
[https://arxiv.org/abs/1603.01360](https://arxiv.org/abs/1603.01360)

##### PDF
[https://arxiv.org/pdf/1603.01360](https://arxiv.org/pdf/1603.01360)

