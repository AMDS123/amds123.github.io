---
layout: post
title: "Jointly Learning to Align and Convert Graphemes to Phonemes with Neural Attention Models"
date: 2016-10-20 19:00:48
categories: arXiv_CL
tags: arXiv_CL Attention
author: Shubham Toshniwal, Karen Livescu
mathjax: true
---

* content
{:toc}

##### Abstract
We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).

##### Abstract (translated by Google)
我们提出了一个注重启用的编解码器模型来解决字素到音素转换的问题。大多数以前的工作已经通过联合序列模型解决了这个问题，需要明确的对齐进行训练。相反，启用注意力的编码器 - 解码器模型允许联合学习对齐并将字符转换为音素。我们探索不同类型的关注模型，包括全球和本地关注，我们的最佳模型在三个标准数据集（CMUDict，Pronlex和NetTalk）上实现了最新的结果。

##### URL
[https://arxiv.org/abs/1610.06540](https://arxiv.org/abs/1610.06540)

##### PDF
[https://arxiv.org/pdf/1610.06540](https://arxiv.org/pdf/1610.06540)

