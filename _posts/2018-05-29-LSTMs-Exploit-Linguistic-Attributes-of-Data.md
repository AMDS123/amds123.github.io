---
layout: post
title: "LSTMs Exploit Linguistic Attributes of Data"
date: 2018-05-29 18:44:31
categories: arXiv_CL
tags: arXiv_CL RNN
author: Nelson F. Liu, Omer Levy, Roy Schwartz, Chenhao Tan, Noah A. Smith
mathjax: true
---

* content
{:toc}

##### Abstract
While recurrent neural networks have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of natural language data affect an LSTM's ability to learn a nonlinguistic task: recalling elements from its input. We find that models trained on natural language data are able to recall tokens from much longer sequences than models trained on non-language sequential data. Furthermore, we show that the LSTM learns to solve the memorization task by explicitly using a subset of its neurons to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different training data on the learnability of LSTMs remains an open question.

##### Abstract (translated by Google)
虽然循环神经网络已经在各种自然语言处理应用中取得成功，但它们是顺序数据的一般模型。我们调查自然语言数据的属性如何影响LSTM学习非语言任务的能力：回忆其输入中的元素。我们发现训练自然语言数据的模型能够从比非语言序列数据训练的模型更长的序列中回想到令牌。此外，我们显示LSTM通过明确使用其神经元的子集来计算输入中的时间步长来学习解决记忆任务。我们假设自然语言数据中的模式和结构使得LSTM通过提供减少损失的近似方式来学习，但了解不同训练数据对LSTM可学习性的影响仍然是一个悬而未决的问题。

##### URL
[http://arxiv.org/abs/1805.11653](http://arxiv.org/abs/1805.11653)

##### PDF
[http://arxiv.org/pdf/1805.11653](http://arxiv.org/pdf/1805.11653)

