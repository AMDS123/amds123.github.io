---
layout: post
title: "The Importance of Being Recurrent for Modeling Hierarchical Structure"
date: 2018-03-09 16:13:02
categories: arXiv_CL
tags: arXiv_CL Attention RNN Language_Model
author: Ke Tran, Arianna Bisazza, Christof Monz
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks such as language modeling (Linzen et al., 2016) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures---recurrent versus non-recurrent---with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose.

##### Abstract (translated by Google)
最近的研究表明，递归神经网络（RNN）可以在训练时隐式捕获和利用分层信息，以解决常见的自然语言处理任务，如语言建模（Linzen等，2016）和神经机器翻译（Shi等，2016 ）。相反，使用非递归神经网络建模结构化数据的能力虽然在许多NLP任务中取得成功，但几乎没有受到关注（Gehring等，2017; Vaswani等，2017）。在这项工作中，我们比较了两种架构 - 循环与非循环 - 就模型层次结构的能力以及发现循环对于此目的而言的确非常重要。

##### URL
[http://arxiv.org/abs/1803.03585](http://arxiv.org/abs/1803.03585)

##### PDF
[http://arxiv.org/pdf/1803.03585](http://arxiv.org/pdf/1803.03585)

