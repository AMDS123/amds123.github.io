---
layout: post
title: "Colorization as a Proxy Task for Visual Understanding"
date: 2017-08-13 17:40:29
categories: arXiv_CV
tags: arXiv_CV Segmentation Classification
author: Gustav Larsson, Michael Maire, Gregory Shakhnarovich
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate and improve self-supervision as a drop-in replacement for ImageNet pretraining, focusing on automatic colorization as the proxy task. Self-supervised training has been shown to be more promising for utilizing unlabeled data than other, traditional unsupervised learning methods. We build on this success and evaluate the ability of our self-supervised network in several contexts. On VOC segmentation and classification tasks, we present results that are state-of-the-art among methods not using ImageNet labels for pretraining representations. Moreover, we present the first in-depth analysis of self-supervision via colorization, concluding that formulation of the loss, training details and network architecture play important roles in its effectiveness. This investigation is further expanded by revisiting the ImageNet pretraining paradigm, asking questions such as: How much training data is needed? How many labels are needed? How much do features change when fine-tuned? We relate these questions back to self-supervision by showing that colorization provides a similarly powerful supervisory signal as various flavors of ImageNet pretraining.

##### Abstract (translated by Google)
我们调查和改进自我监督，作为ImageNet预训练的直接替代品，将自动着色作为代理任务。与其他传统的无监督学习方法相比，自监督训练对于利用未标记数据更有希望。我们建立在这个成功的基础上，评估我们的自我监督网络在多种情况下的能力。在挥发性有机化合物分割和分类任务，我们目前的结果是最先进的方法不使用ImageNet标签预训练表示。此外，我们首先通过彩色化进行了自我监督的深入分析，得出损失的形成，培训细节和网络架构对其有效性起着重要作用。通过重新研究ImageNet预训练范式，这一调查进一步得到扩展，并提出如下问题：需要多少训练数据？需要多少标签？微调时功能会改变多少？我们将这些问题归结为自我监督，通过显示彩色化提供了与各种类型的ImageNet预训练类似的强大的监督信号。

##### URL
[https://arxiv.org/abs/1703.04044](https://arxiv.org/abs/1703.04044)

##### PDF
[https://arxiv.org/pdf/1703.04044](https://arxiv.org/pdf/1703.04044)

