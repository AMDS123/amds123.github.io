---
layout: post
title: "Run, skeleton, run: skeletal model in a physics-based simulation"
date: 2018-01-28 09:29:07
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Mikhail Pavlov, Sergey Kolesnikov, Sergey M. Plis
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we present our approach to solve a physics-based reinforcement learning challenge "Learning to Run" with objective to train physiologically-based human model to navigate a complex obstacle course as quickly as possible. The environment is computationally expensive, has a high-dimensional continuous action space and is stochastic. We benchmark state of the art policy-gradient methods and test several improvements, such as layer normalization, parameter noise, action and state reflecting, to stabilize training and improve its sample-efficiency. We found that the Deep Deterministic Policy Gradient method is the most efficient method for this environment and the improvements we have introduced help to stabilize training. Learned models are able to generalize to new physical scenarios, e.g. different obstacle courses.

##### Abstract (translated by Google)
在本文中，我们提出了我们的方法来解决基于物理的强化学习挑战“学习运行”，目标是培养基于生理学的人体模型以尽可能快地驾驶复杂的障碍物。环境计算昂贵，具有高维连续作用空间并且是随机的。我们对最先进的策略梯度方法进行了基准测试，并测试了层次归一化，参数噪声，行为和状态反映等几项改进，以稳定训练并提高其抽样效率。我们发现深度确定性策略梯度法是这种环境中最有效的方法，我们引入的改进有助于稳定培训。学习的模型能够推广到新的物理场景，例如不同的障碍课程。

##### URL
[http://arxiv.org/abs/1711.06922](http://arxiv.org/abs/1711.06922)

##### PDF
[http://arxiv.org/pdf/1711.06922](http://arxiv.org/pdf/1711.06922)

