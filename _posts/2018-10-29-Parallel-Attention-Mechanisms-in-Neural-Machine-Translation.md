---
layout: post
title: "Parallel Attention Mechanisms in Neural Machine Translation"
date: 2018-10-29 21:58:13
categories: arXiv_AI
tags: arXiv_AI Attention CNN RNN
author: Julian Richard Medina, Jugal Kalita
mathjax: true
---

* content
{:toc}

##### Abstract
Recent papers in neural machine translation have proposed the strict use of attention mechanisms over previous standards such as recurrent and convolutional neural networks (RNNs and CNNs). We propose that by running traditionally stacked encoding branches from encoder-decoder attention- focused architectures in parallel, that even more sequential operations can be removed from the model and thereby decrease training time. In particular, we modify the recently published attention-based architecture called Transformer by Google, by replacing sequential attention modules with parallel ones, reducing the amount of training time and substantially improving BLEU scores at the same time. Experiments over the English to German and English to French translation tasks show that our model establishes a new state of the art.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1810.12427](http://arxiv.org/abs/1810.12427)

##### PDF
[http://arxiv.org/pdf/1810.12427](http://arxiv.org/pdf/1810.12427)

