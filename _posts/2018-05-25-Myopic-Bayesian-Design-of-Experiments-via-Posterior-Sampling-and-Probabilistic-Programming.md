---
layout: post
title: "Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming"
date: 2018-05-25 03:36:09
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Inference
author: Kirthevasan Kandasamy, Willie Neiswanger, Reed Zhang, Akshay Krishnamurthy, Jeff Schneider, Barnabas Poczos
mathjax: true
---

* content
{:toc}

##### Abstract
We design a new myopic strategy for a wide class of sequential design of experiment (DOE) problems, where the goal is to collect data in order to to fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling (MPS), is inspired by the classical posterior (Thompson) sampling algorithm for multi-armed bandits and leverages the flexibility of probabilistic programming and approximate Bayesian inference to address a broad set of problems. Empirically, this general-purpose strategy is competitive with more specialised methods in a wide array of DOE tasks, and more importantly, enables addressing complex DOE goals where no existing method seems applicable. On the theoretical side, we leverage ideas from adaptive submodularity and reinforcement learning to derive conditions under which MPS achieves sublinear regret against natural benchmark policies.

##### Abstract (translated by Google)
我们设计了一个新的近视策略，用于大量实验（DOE）问题的顺序设计，其目标是收集数据以实现特定问题的具体目标。我们的方法，近视后抽样（MPS），受到多臂土匪的经典后验（汤普森）采样算法的启发，并利用概率编程和近似贝叶斯推理的灵活性来解决一系列广泛的问题。从经验上讲，这种通用策略在多种DOE任务中与更专业化的方法相比具有竞争力，更重要的是，在不存在现有方法的情况下，可以解决复杂的DOE目标。在理论方面，我们利用自适应子模块和强化学习的想法来推导出MPS对自然基准策略实现次线性后悔的条件。

##### URL
[http://arxiv.org/abs/1805.09964](http://arxiv.org/abs/1805.09964)

##### PDF
[http://arxiv.org/pdf/1805.09964](http://arxiv.org/pdf/1805.09964)

