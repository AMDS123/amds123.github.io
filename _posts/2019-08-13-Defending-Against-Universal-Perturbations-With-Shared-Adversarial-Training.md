---
layout: post
title: "Defending Against Universal Perturbations With Shared Adversarial Training"
date: 2019-08-13 11:58:27
categories: arXiv_CV
tags: arXiv_CV Adversarial Segmentation Image_Classification Semantic_Segmentation Classification
author: Chaithanya Kumar Mummadi, Thomas Brox, Jan Hendrik Metzen
mathjax: true
---

* content
{:toc}

##### Abstract
Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of image classifiers against such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible fraction of the inputs. In this work, we show that adversarial training is more effective in preventing universal perturbations, where the same perturbation needs to fool a classifier on many inputs. Moreover, we investigate the trade-off between robustness against universal perturbations and performance on unperturbed data and propose an extension of adversarial training that handles this trade-off more gracefully. We present results for image classification and semantic segmentation to showcase that universal perturbations that fool a model hardened with adversarial training become clearly perceptible and show patterns of the target scene.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1812.03705](http://arxiv.org/abs/1812.03705)

##### PDF
[http://arxiv.org/pdf/1812.03705](http://arxiv.org/pdf/1812.03705)

