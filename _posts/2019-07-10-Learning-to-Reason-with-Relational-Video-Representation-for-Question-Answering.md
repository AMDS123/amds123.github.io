---
layout: post
title: "Learning to Reason with Relational Video Representation for Question Answering"
date: 2019-07-10 07:53:17
categories: arXiv_AI
tags: arXiv_AI QA Relation VQA
author: Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran
mathjax: true
---

* content
{:toc}

##### Abstract
How does machine learn to reason about the content of a video in answering a question? A Video QA system must simultaneously understand language, represent visual content over space-time, and iteratively transform these representations in response to lingual content in the query, and finally arriving at a sensible answer. While recent advances in textual and visual question answering have come up with sophisticated visual representation and neural reasoning mechanisms, major challenges in Video QA remain on dynamic grounding of concepts, relations and actions to support the reasoning process. We present a new end-to-end layered architecture for Video QA, which is composed of a question-guided video representation layer and a generic reasoning layer to produce answer. The video is represented using a hierarchical model that encodes visual information about objects, actions and relations in space-time given the textual cues from the question. The encoded representation is then passed to a reasoning module, which in this paper, is implemented as a MAC net. The system is evaluated on the SVQA (synthetic) and TGIF-QA datasets (real), demonstrating state-of-the-art results, with a large margin in the case of multi-step reasoning.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.04553](http://arxiv.org/abs/1907.04553)

##### PDF
[http://arxiv.org/pdf/1907.04553](http://arxiv.org/pdf/1907.04553)

