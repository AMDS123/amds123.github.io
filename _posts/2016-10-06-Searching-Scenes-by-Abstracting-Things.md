---
layout: post
title: "Searching Scenes by Abstracting Things"
date: 2016-10-06 10:03:45
categories: arXiv_CV
tags: arXiv_CV
author: Svetlana Kordumova, Jan C. van Gemert, Cees G. M. Snoek, Arnold W. M. Smeulders
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we propose to represent a scene as an abstraction of 'things'. We start from 'things' as generated by modern object proposals, and we investigate their immediately observable properties: position, size, aspect ratio and color, and those only. Where the recent successes and excitement of the field lie in object identification, we represent the scene composition independent of object identities. We make three contributions in this work. First, we study simple observable properties of 'things', and call it things syntax. Second, we propose translating the things syntax in linguistic abstract statements and study their descriptive effect to retrieve scenes. Thirdly, we propose querying of scenes with abstract block illustrations and study their effectiveness to discriminate among different types of scenes. The benefit of abstract statements and block illustrations is that we generate them directly from the images, without any learning beforehand as in the standard attribute learning. Surprisingly, we show that even though we use the simplest of features from 'things' layout and no learning at all, we can still retrieve scenes reasonably well.

##### Abstract (translated by Google)
在本文中，我们提出将场景表示为“事物”的抽象。我们从现代物体提案产生的“事物”开始，我们研究它们立即可观察到的性质：位置，大小，纵横比和颜色，以及只有这些。最近的领域的成功和兴奋在于对象识别，我们代表与对象身份无关的场景组合。我们在这项工作中做出三个贡献。首先，我们研究“事物”的简单的可观测属性，并将其称为事物语法。其次，我们提出在语言抽象语句中翻译事物语法，并研究其描述效果来检索场景。第三，我们提出用抽象的块状插图来查询场景，研究它们在不同类型的场景之间进行区分的有效性。抽象陈述和块插图的好处是，我们直接从图像中生成它们，而没有像标准属性学习那样的事先学习。令人惊讶的是，即使我们使用“事物”布局中最简单的特征，根本不学习，我们仍然可以很好地检索场景。

##### URL
[https://arxiv.org/abs/1610.01801](https://arxiv.org/abs/1610.01801)

##### PDF
[https://arxiv.org/pdf/1610.01801](https://arxiv.org/pdf/1610.01801)

