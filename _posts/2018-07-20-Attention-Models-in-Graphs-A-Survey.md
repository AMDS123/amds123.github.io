---
layout: post
title: "Attention Models in Graphs: A Survey"
date: 2018-07-20 18:11:07
categories: arXiv_AI
tags: arXiv_AI Attention Survey Classification Prediction Relation
author: John Boaz Lee, Ryan A. Rossi, Sungchul Kim, Nesreen K. Ahmed, Eunyee Koh
mathjax: true
---

* content
{:toc}

##### Abstract
Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large - with many complex patterns - and noisy which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate "attention" into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction, etc.). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work.

##### Abstract (translated by Google)
图形结构数据自然地出现在许多不同的应用领域中。通过将数据表示为图形，我们可以相互捕获实体（即，节点）以及它们的关系（即，边缘）。许多有用的见解可以从图形结构数据中获得，正如不断增长的专注于图挖掘的工作所证明的那样。但是，在现实世界中，图形可能很大 - 具有许多复杂模式 - 并且噪声可能会对有效图形挖掘造成问题。处理这个问题的有效方法是将“注意力”纳入图挖掘解决方案。注意机制允许方法专注于图的任务相关部分，帮助它做出更好的决策。在这项工作中，我们对图形注意力模型的新兴领域的文献进行了全面而有针对性的调查。我们引入了三种直观的分类法来对现有工作进行分组。这些基于问题设置（输入和输出的类型），所使用的关注机制的类型和任务（例如，图分类，链接预测等）。我们通过详细的示例激励我们的分类法，并从独特的角度使用每个分类法来调查竞争方法。最后，我们强调该领域的一些挑战，并讨论未来工作的有希望的方向。

##### URL
[http://arxiv.org/abs/1807.07984](http://arxiv.org/abs/1807.07984)

##### PDF
[http://arxiv.org/pdf/1807.07984](http://arxiv.org/pdf/1807.07984)

