---
layout: post
title: "Suffix Bidirectional Long Short-Term Memory"
date: 2018-05-18 17:46:25
categories: arXiv_AI
tags: arXiv_AI Sentiment Sentiment_Classification Text_Classification RNN Classification Recognition
author: Siddhartha Brahma
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks have become ubiquitous in computing representations of sequential data, especially textual data in natural language processing. In particular, Bidirectional LSTMs are at the heart of several neural models achieving state-of-the-art performance in a wide variety of tasks in NLP. We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions. We call our model Suffix BiLSTM or SuBiLSTM. Using an extensive set of experiments, we demonstrate that using SuBiLSTM instead of a BiLSTM in existing base models leads to improvements in performance in learning general sentence representations, text classification, textual entailment and named entity recognition. We achieve new state-of-the-art results for fine-grained sentiment classification and question classification using SuBiLSTM.

##### Abstract (translated by Google)
递归神经网络在计算连续数据表示，尤其是在自然语言处理中的文本数据时，已经变得无处不在。特别是，双向LSTM是几种神经模型的核心，在NLP的各种任务中实现了最先进的性能。我们提出了对BiLSTM模型的一般和有效的改进，该模型对正向和反向两个令牌序列的每个后缀和前缀进行编码。我们称之为后缀BiLSTM或SuBiLSTM。使用一系列广泛的实验，我们证明在现有基础模型中使用SuBiLSTM而不是BiLSTM可以提高学习一般句子表示，文本分类，文本包含和命名实体识别的性能。我们使用SuBiLSTM为细粒度的情感分类和问题分类实现了最新的最新结果。

##### URL
[http://arxiv.org/abs/1805.07340](http://arxiv.org/abs/1805.07340)

##### PDF
[http://arxiv.org/pdf/1805.07340](http://arxiv.org/pdf/1805.07340)

