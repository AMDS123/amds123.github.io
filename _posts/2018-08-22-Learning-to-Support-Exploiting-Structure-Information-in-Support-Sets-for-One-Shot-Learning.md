---
layout: post
title: "Learning to Support: Exploiting Structure Information in Support Sets for One-Shot Learning"
date: 2018-08-22 08:29:16
categories: arXiv_AI
tags: arXiv_AI Attention Embedding Optimization Classification Deep_Learning
author: Jinchao Liu, Stuart J. Gibson, Margarita Osadchy
mathjax: true
---

* content
{:toc}

##### Abstract
Deep Learning shows very good performance when trained on large labeled data sets. The problem of training a deep net on a few or one sample per class requires a different learning approach which can generalize to unseen classes using only a few representatives of these classes. This problem has previously been approached by meta-learning. Here we propose a novel meta-learner which shows state-of-the-art performance on common benchmarks for one/few shot classification. Our model features three novel components: First is a feed-forward embedding that takes random class support samples (after a customary CNN embedding) and transfers them to a better class representation in terms of a classification problem. Second is a novel attention mechanism, inspired by competitive learning, which causes class representatives to compete with each other to become a temporary class prototype with respect to the query point. This mechanism allows switching between representatives depending on the position of the query point. Once a prototype is chosen for each class, the predicated label is computed using a simple attention mechanism over prototypes of all considered classes. The third feature is the ability of our meta-learner to incorporate deeper CNN embedding, enabling larger capacity. Finally, to ease the training procedure and reduce overfitting, we averages the top $t$ models (evaluated on the validation) over the optimization trajectory. We show that this approach can be viewed as an approximation to an ensemble, which saves the factor of $t$ in training and test times and the factor of of $t$ in the storage of the final model.

##### Abstract (translated by Google)
深度学习在大型标记数据集上训练时表现出非常好的性能。在每个类的几个或一个样本上训练深度网的问题需要一种不同的学习方法，这种方法可以仅使用这些类的少数代表来推广到看不见的类。以前通过元学习来解决这个问题。在这里，我们提出了一种新颖的元学习器，它在一个/几个镜头分类的共同基准上显示了最先进的性能。我们的模型具有三个新颖的组件：第一个是前馈嵌入，它采用随机类支持样本（在习惯性CNN嵌入之后）并根据分类问题将它们转换为更好的类表示。其次是一种新颖的注意机制，受到竞争性学习的启发，使得阶级代表能够相互竞争，成为关于查询点的临时类原型。此机制允许根据查询点的位置在代表之间切换。一旦为每个类选择了原型，就使用简单的注意机制来计算所有考虑类的原型的谓词标签。第三个特征是我们的元学习者能够整合更深入的CNN嵌入，从而实现更大的容量。最后，为了简化训练程序并减少过度拟合，我们对优化轨迹中的顶级$ t $模型（在验证中评估）进行平均。我们证明这种方法可以看作是一个集合的近似值，它可以节省训练和测试时间中$ t $的因子以及最终模型存储中$ t $的因子。

##### URL
[http://arxiv.org/abs/1808.07270](http://arxiv.org/abs/1808.07270)

##### PDF
[http://arxiv.org/pdf/1808.07270](http://arxiv.org/pdf/1808.07270)

