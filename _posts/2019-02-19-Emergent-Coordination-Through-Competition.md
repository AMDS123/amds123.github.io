---
layout: post
title: "Emergent Coordination Through Competition"
date: 2019-02-19 17:18:14
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Optimization
author: Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, Thore Graepel
mathjax: true
---

* content
{:toc}

##### Abstract
We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics. We demonstrate that decentralized, population-based training with co-play can lead to a progression in agents' behaviors: from random, to simple ball chasing, and finally showing evidence of cooperation. Our study highlights several of the challenges encountered in large scale multi-agent training in continuous control. In particular, we demonstrate that the automatic optimization of simple shaping rewards, not themselves conducive to co-operative behavior, can lead to long-horizon team behavior. We further apply an evaluation scheme, grounded by game theoretic principals, that can assess agent performance in the absence of pre-defined evaluation tasks or human baselines.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1902.07151](http://arxiv.org/abs/1902.07151)

##### PDF
[http://arxiv.org/pdf/1902.07151](http://arxiv.org/pdf/1902.07151)

