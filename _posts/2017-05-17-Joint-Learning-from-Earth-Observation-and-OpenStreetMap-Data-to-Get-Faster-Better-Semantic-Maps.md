---
layout: post
title: "Joint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps"
date: 2017-05-17 09:07:08
categories: arXiv_CV
tags: arXiv_CV Segmentation CNN Classification Deep_Learning
author: Nicolas Audebert (Palaiseau, OBELIX), Bertrand Le Saux (Palaiseau), Sébastien Lefèvre (OBELIX)
mathjax: true
---

* content
{:toc}

##### Abstract
In this work, we investigate the use of OpenStreetMap data for semantic labeling of Earth Observation images. Deep neural networks have been used in the past for remote sensing data classification from various sensors, including multispectral, hyperspectral, SAR and LiDAR data. While OpenStreetMap has already been used as ground truth data for training such networks, this abundant data source remains rarely exploited as an input information layer. In this paper, we study different use cases and deep network architectures to leverage OpenStreetMap data for semantic labeling of aerial and satellite images. Especially , we look into fusion based architectures and coarse-to-fine segmentation to include the OpenStreetMap layer into multispectral-based deep fully convolutional networks. We illustrate how these methods can be successfully used on two public datasets: ISPRS Potsdam and DFC2017. We show that OpenStreetMap data can efficiently be integrated into the vision-based deep learning models and that it significantly improves both the accuracy performance and the convergence speed of the networks.

##### Abstract (translated by Google)
在这项工作中，我们调查了OpenStreetMap数据用于对地观测图像的语义标记。过去，深度神经网络已经用于各种传感器的遥感数据分类，包括多光谱，高光谱，SAR和LiDAR数据。虽然OpenStreetMap已经被用作训练这种网络的地面实况数据，但是这个丰富的数据源很少被用作输入信息层。在本文中，我们研究不同的使用案例和深度网络架构，以利用OpenStreetMap数据进行航空和卫星图像的语义标记。特别是我们研究基于融合的体系结构和从粗到精的分割，将OpenStreetMap层包含到基于多光谱的深度完全卷积网络中。我们举例说明如何将这些方法成功应用于两个公共数据集：ISPRS Potsdam和DFC2017。我们展示OpenStreetMap数据可以有效地集成到基于视觉的深度学习模型中，并且它显着提高了网络的准确性和收敛速度。

##### URL
[https://arxiv.org/abs/1705.06057](https://arxiv.org/abs/1705.06057)

##### PDF
[https://arxiv.org/pdf/1705.06057](https://arxiv.org/pdf/1705.06057)

