---
layout: post
title: "Latent Attention For If-Then Program Synthesis"
date: 2016-11-07 00:56:19
categories: arXiv_CL
tags: arXiv_CL Attention
author: Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Automatic translation from natural language descriptions into programs is a longstanding challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data.

##### Abstract (translated by Google)
将自然语言描述自动翻译成程序是一个长期存在的挑战性问题。在这项工作中，我们考虑一个简单而重要的子问题：从文本描述到If-Then程序的翻译。我们为这个任务设计了一个新的神经网络架构，我们进行端到端的训练。具体而言，我们引入了潜在的注意力，它在两阶段过程中为描述中的词汇计算乘法权重，目标是更好地利用指示相关部分预测程序要素的自然语言结构。与现有技术相比，我们的架构将错误率降低了28.57％。我们还提出了If-Then程序综合的一次性学习场景，并用我们现有的数据集进行模拟。我们在这个场景的训练过程中演示了一个变化，这个变化胜过了原来的过程，显着缩小了与所有数据训练过的模型之间的差距。

##### URL
[https://arxiv.org/abs/1611.01867](https://arxiv.org/abs/1611.01867)

##### PDF
[https://arxiv.org/pdf/1611.01867](https://arxiv.org/pdf/1611.01867)

