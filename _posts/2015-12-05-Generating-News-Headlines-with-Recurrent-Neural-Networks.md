---
layout: post
title: "Generating News Headlines with Recurrent Neural Networks"
date: 2015-12-05 23:41:22
categories: arXiv_CL
tags: arXiv_CL Attention RNN
author: Konstantin Lopyrev
mathjax: true
---

* content
{:toc}

##### Abstract
We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.

##### Abstract (translated by Google)
我们描述了一个带有LSTM单元的编码器 - 解码器递归神经网络的应用，并且关注从新闻文章的文本产生标题。我们发现这个模型在简洁地解释新闻文章方面非常有效。此外，我们研究神经网络如何决定哪些输入词要注意，特别是我们在简化的注意机制中识别不同神经元的功能。有趣的是，我们简化的关注机制在一组文章中执行得更好，更复杂的关注机制。

##### URL
[https://arxiv.org/abs/1512.01712](https://arxiv.org/abs/1512.01712)

##### PDF
[https://arxiv.org/pdf/1512.01712](https://arxiv.org/pdf/1512.01712)

