---
layout: post
title: "Multilingual Universal Sentence Encoder for Semantic Retrieval"
date: 2019-07-09 17:46:17
categories: arXiv_CL
tags: arXiv_CL QA Embedding Transfer_Learning
author: Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce two pre-trained retrieval focused multilingual sentence encoding models, respectively based on the Transformer and CNN model architectures. The models embed text from 16 languages into a single semantic space using a multi-task trained dual-encoder that learns tied representations using translation based bridge tasks (Chidambaram al., 2018). The models provide performance that is competitive with the state-of-the-art on: semantic retrieval (SR), translation pair bitext retrieval (BR) and retrieval question answering (ReQA). On English transfer learning tasks, our sentence-level embeddings approach, and in some cases exceed, the performance of monolingual, English only, sentence embedding models. Our models are made available for download on TensorFlow Hub.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.04307](http://arxiv.org/abs/1907.04307)

##### PDF
[http://arxiv.org/pdf/1907.04307](http://arxiv.org/pdf/1907.04307)

