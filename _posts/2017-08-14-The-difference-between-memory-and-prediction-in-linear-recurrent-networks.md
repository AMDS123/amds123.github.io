---
layout: post
title: "The difference between memory and prediction in linear recurrent networks"
date: 2017-08-14 15:54:00
categories: arXiv_CV
tags: arXiv_CV Prediction
author: Sarah Marzen
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent networks are trained to memorize their input better, often in the hopes that such training will increase the ability of the network to predict. We show that networks designed to memorize input can be arbitrarily bad at prediction. We also find, for several types of inputs, that one-node networks optimized for prediction are nearly at upper bounds on predictive capacity given by Wiener filters, and are roughly equivalent in performance to randomly generated five-node networks. Our results suggest that maximizing memory capacity leads to very different networks than maximizing predictive capacity, and that optimizing recurrent weights can decrease reservoir size by half an order of magnitude.

##### Abstract (translated by Google)
经常性网络经过培训能够更好地记忆他们的输入，通常希望这种训练能够提高网络的预测能力。我们表明，设计来记忆输入的网络在预测时可以是任意的。对于多种输入类型，我们还发现，为预测而优化的单节点网络几乎处于由维纳滤波器给出的预测容量的上界，并且在性能上大致与随机生成的五节点网络相当。我们的研究结果表明，最大化记忆容量导致了非常不同的网络，而不是最大化预测能力，并且优化经常性的权重可以使储集层的大小减少一个数量级。

##### URL
[https://arxiv.org/abs/1706.09382](https://arxiv.org/abs/1706.09382)

##### PDF
[https://arxiv.org/pdf/1706.09382](https://arxiv.org/pdf/1706.09382)

