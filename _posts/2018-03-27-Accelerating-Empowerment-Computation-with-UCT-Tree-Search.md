---
layout: post
title: "Accelerating Empowerment Computation with UCT Tree Search"
date: 2018-03-27 03:05:35
categories: arXiv_AI
tags: arXiv_AI Quantitative
author: Christoph Salge, Christian Guckelsberger, Rodrigo Canaan, Tobias Mahlmann
mathjax: true
---

* content
{:toc}

##### Abstract
Models of intrinsic motivation present an important means to produce sensible behaviour in the absence of extrinsic rewards. Applications in video games are varied, and range from intrinsically motivated general game-playing agents to non-player characters such as companions and enemies. The information-theoretic quantity of Empowerment is a particularly promising candidate motivation to produce believable, generic and robust behaviour. However, while it can be used in the absence of external reward functions that would need to be crafted and learned, empowerment is computationally expensive. In this paper, we propose a modified UCT tree search method to mitigate empowerment's computational complexity in discrete and deterministic scenarios. We demonstrate how to modify a Monte-Carlo Search Tree with UCT to realise empowerment maximisation, and discuss three additional modifications that facilitate better sampling. We evaluate the approach both quantitatively, by analysing how close our approach gets to the baseline of exhaustive empowerment computation with varying amounts of computational resources, and qualitatively, by analysing the resulting behaviour in a Minecraft-like scenario.

##### Abstract (translated by Google)
内在动机模型是在没有外在奖励的情况下产生明智行为的重要手段。视频游戏中的应用程序各不相同，其范围从内在动机的一般游戏代理到非玩家角色，如同伴和敌人。信息理论的授权量是产生可信，一般和强大行为的一个特别有前途的候选动机。然而，虽然它可以用于缺乏需要制定和学习的外部奖励功能，但授权在计算上花费很大。在本文中，我们提出了一种改进的UCT树搜索方法，以减轻离散和确定性场景中授权的计算复杂度。我们演示了如何使用UCT修改蒙特卡洛搜索树以实现授权最大化，并讨论了三个便于更好采样的附加修改。我们通过分析我们的方法与具有不同数量的计算资源的穷尽授权计算的基线之间的距离，并通过分析类似Minecraft的场景中的结果行为来定性地评估该方法。

##### URL
[https://arxiv.org/abs/1803.09866](https://arxiv.org/abs/1803.09866)

##### PDF
[https://arxiv.org/pdf/1803.09866](https://arxiv.org/pdf/1803.09866)

