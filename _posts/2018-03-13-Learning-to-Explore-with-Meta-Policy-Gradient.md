---
layout: post
title: "Learning to Explore with Meta-Policy Gradient"
date: 2018-03-13 21:04:17
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Tianbing Xu, Qiang Liu, Liang Zhao, Wei Xu, Jian Peng
mathjax: true
---

* content
{:toc}

##### Abstract
The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration methods are mostly based on adding noise to the on-going actor policy and can only explore \emph{local} regions close to what the actor policy dictates. In this work, we develop a simple meta-policy gradient algorithm that allows us to adaptively learn the exploration policy in DDPG. Our algorithm allows us to train flexible exploration behaviors that are independent of the actor policy, yielding a \emph{global exploration} that significantly speeds up the learning process. With an extensive study, we show that our method significantly improves the sample-efficiency of DDPG on a variety of reinforcement learning tasks.

##### Abstract (translated by Google)
非政策学习的表现，包括深度Q学习和深度确定性政策梯度（DDPG），关键取决于勘探政策的选择。现有的探索方法主要基于为正在进行的演员策略添加噪音，并且只能探索接近演员策略规定的\ emph {local}区域。在这项工作中，我们开发了一个简单的元策略梯度算法，使我们能够自适应地学习DDPG中的勘探策略。我们的算法允许我们训练独立于演员策略的灵活探索行为，产生显着加速学习过程的\ emph {全球探索}。通过广泛的研究，我们证明了我们的方法在各种强化学习任务中显着提高了DDPG的采样效率。

##### URL
[https://arxiv.org/abs/1803.05044](https://arxiv.org/abs/1803.05044)

##### PDF
[https://arxiv.org/pdf/1803.05044](https://arxiv.org/pdf/1803.05044)

