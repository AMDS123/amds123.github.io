---
layout: post
title: "A Novel Framework for Recurrent Neural Networks with Enhancing Information Processing and Transmission between Units"
date: 2018-06-02 12:59:18
categories: arXiv_CL
tags: arXiv_CL Text_Classification Image_Classification RNN Classification Language_Model
author: Xi Chen, Zhihong Deng, Gehui Shen, Ting Huang
mathjax: true
---

* content
{:toc}

##### Abstract
This paper proposes a novel framework for recurrent neural networks (RNNs) inspired by the human memory models in the field of cognitive neuroscience to enhance information processing and transmission between adjacent RNNs' units. The proposed framework for RNNs consists of three stages that is working memory, forget, and long-term store. The first stage includes taking input data into sensory memory and transferring it to working memory for preliminary treatment. And the second stage mainly focuses on proactively forgetting the secondary information rather than the primary in the working memory. And finally, we get the long-term store normally using some kind of RNN's unit. Our framework, which is generalized and simple, is evaluated on 6 datasets which fall into 3 different tasks, corresponding to text classification, image classification and language modelling. Experiments reveal that our framework can obviously improve the performance of traditional recurrent neural networks. And exploratory task shows the ability of our framework of correctly forgetting the secondary information.

##### Abstract (translated by Google)
本文提出了一种新颖的循环神经网络框架（RNNs），该框架受到认知神经科学领域的人类记忆模型的启发，以加强相邻RNNs单元之间的信息处理和传输。拟议的RNN框架由三个阶段组成，即工作记忆，遗忘和长期存储。第一阶段包括将输入数据输入到感官记忆并将其传输到工作记忆中进行初步处理。第二阶段主要集中在积极忘记辅助信息而不是工作记忆中的主要信息。最后，我们通常使用某种RNN的单位获得长期存储。我们的框架是广义和简单的，在6个数据集上进行评估，分为3个不同的任务，对应于文本分类，图像分类和语言建模。实验表明，我们的框架可以明显提高传统递归神经网络的性能。探索性任务展示了我们正确遗忘辅助信息框架的能力。

##### URL
[http://arxiv.org/abs/1806.00628](http://arxiv.org/abs/1806.00628)

##### PDF
[http://arxiv.org/pdf/1806.00628](http://arxiv.org/pdf/1806.00628)

