---
layout: post
title: "Adversarial Examples that Fool both Computer Vision and Time-Limited Humans"
date: 2018-05-22 03:02:41
categories: arXiv_CV
tags: arXiv_CV Adversarial Classification
author: Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein
mathjax: true
---

* content
{:toc}

##### Abstract
Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.

##### Abstract (translated by Google)
机器学习模型容易受到对抗性例子的影响：对图像的微小改变会导致计算机视觉模型出现错误，例如将校车标识为鸵鸟。然而，人类是否容易出现类似的错误仍然是一个悬而未决的问题。在这里，我们通过利用最近的技术来解决这个问题，这些技术将具有已知参数和架构的计算机视觉模型的敌对示例转换为其他具有未知参数和架构的模型，并且通过匹配人类视觉系统的初始处理。我们发现，在计算机视觉模型之间进行强烈转移的对抗性例子影响了时间有限的人类观察者所做的分类。

##### URL
[http://arxiv.org/abs/1802.08195](http://arxiv.org/abs/1802.08195)

##### PDF
[http://arxiv.org/pdf/1802.08195](http://arxiv.org/pdf/1802.08195)

