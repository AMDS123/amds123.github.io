---
layout: post
title: "Understanding Memory Modules on Learning Simple Algorithms"
date: 2019-07-01 14:27:03
categories: arXiv_AI
tags: arXiv_AI
author: Kexin Wang, Yu Zhou, Shaonan Wang, Jiajun Zhang, Chengqing Zong
mathjax: true
---

* content
{:toc}

##### Abstract
Recent work has shown that memory modules are crucial for the generalization ability of neural networks on learning simple algorithms. However, we still have little understanding of the working mechanism of memory modules. To alleviate this problem, we apply a two-step analysis pipeline consisting of first inferring hypothesis about what strategy the model has learned according to visualization and then verify it by a novel proposed qualitative analysis method based on dimension reduction. Using this method, we have analyzed two popular memory-augmented neural networks, neural Turing machine and stack-augmented neural network on two simple algorithm tasks including reversing a random sequence and evaluation of arithmetic expressions. Results have shown that on the former task both models can learn to generalize and on the latter task only the stack-augmented model can do so. We show that different strategies are learned by the models, in which specific categories of input are monitored and different policies are made based on that to change the memory.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.00820](http://arxiv.org/abs/1907.00820)

##### PDF
[http://arxiv.org/pdf/1907.00820](http://arxiv.org/pdf/1907.00820)

