---
layout: post
title: "Generalization in Deep Learning"
date: 2017-12-24 19:44:43
categories: arXiv_AI
tags: arXiv_AI Regularization Deep_Learning
author: Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
This paper explains why deep learning can generalize well, despite large capacity and possible algorithmic instability, nonrobustness, and sharp minima, effectively addressing an open problem in the literature. Based on our theoretical insight, this paper also proposes a family of new regularization methods. Its simplest member was empirically shown to improve base models and achieve competitive performance on MNIST and CIFAR-10 benchmarks. Moreover, this paper presents both data-dependent and data-independent generalization guarantees with improved convergence rates. Our results suggest several new open areas of research.

##### Abstract (translated by Google)
本文解释了为什么深度学习能够很好地推广，尽管容量大，可能算法不稳定，非鲁棒性和尖锐极小，有效地解决了文献中的一个公开问题。基于我们的理论洞察力，本文还提出了一套新的正则化方法。其最简单的成员经验性地显示，以改善基础模型，并在MNIST和CIFAR-10基准测试中获得有竞争力的表现。此外，本文提出了数据依赖和数据无关的广义保证和改进的收敛速度。我们的研究结果表明几个新的开放研究领域。

##### URL
[http://arxiv.org/abs/1710.05468](http://arxiv.org/abs/1710.05468)

##### PDF
[http://arxiv.org/pdf/1710.05468](http://arxiv.org/pdf/1710.05468)

