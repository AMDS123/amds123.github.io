---
layout: post
title: "Boosting Supervision with Self-Supervision for Few-shot Learning"
date: 2019-06-17 15:17:40
categories: arXiv_AI
tags: arXiv_AI Classification
author: Jong-Chyi Su, Subhransu Maji, Bharath Hariharan
mathjax: true
---

* content
{:toc}

##### Abstract
We present a technique to improve the transferability of deep representations learned on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. While recent approaches for self-supervised learning have shown the benefits of training on large unlabeled datasets, we find improvements in generalization even on small datasets and when combined with strong supervision. Learning representations with self-supervised losses reduces the relative error rate of a state-of-the-art meta-learner by 5-25% on several few-shot learning benchmarks, as well as off-the-shelf deep networks on standard classification tasks when training from scratch. We find the benefits of self-supervision increase with the difficulty of the task. Our approach utilizes the images within the dataset to construct self-supervised losses and hence is an effective way of learning transferable representations without relying on any external training data.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1906.07079](http://arxiv.org/abs/1906.07079)

##### PDF
[http://arxiv.org/pdf/1906.07079](http://arxiv.org/pdf/1906.07079)

