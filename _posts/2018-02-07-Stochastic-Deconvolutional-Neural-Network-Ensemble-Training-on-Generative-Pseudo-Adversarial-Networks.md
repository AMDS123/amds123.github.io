---
layout: post
title: "Stochastic Deconvolutional Neural Network Ensemble Training on Generative Pseudo-Adversarial Networks"
date: 2018-02-07 14:36:15
categories: arXiv_CV
tags: arXiv_CV Adversarial GAN CNN Relation
author: Alexey Chaplygin, Joshua Chacksfield
mathjax: true
---

* content
{:toc}

##### Abstract
The training of Generative Adversarial Networks is a difficult task mainly due to the nature of the networks. One such issue is when the generator and discriminator start oscillating, rather than converging to a fixed point. Another case can be when one agent becomes more adept than the other which results in the decrease of the other agent's ability to learn, reducing the learning capacity of the system as a whole. Additionally, there exists the problem of Mode Collapse which involves the generators output collapsing to a single sample or a small set of similar samples. To train GANs a careful selection of the architecture that is used along with a variety of other methods to improve training. Even when applying these methods there is low stability of training in relation to the parameters that are chosen. Stochastic ensembling is suggested as a method for improving the stability while training GANs.

##### Abstract (translated by Google)
生成敌对网络的培训是一个艰巨的任务，主要是由于网络的性质。一个这样的问题是当发生器和鉴别器开始振荡时，而不是收敛到一个固定点。另一种情况可能是一方代理人比另一方更擅长其他代理人的学习能力下降，从而降低整个系统的学习能力。此外，还存在模式崩溃的问题，其中发生器输出崩溃为单个样本或一组相似的样本。为了训练GAN仔细选择与其他各种方法一起使用以改善训练的架构。即使应用这些方法，与选择的参数相关的训练稳定性也较低。随机集成被认为是提高训练GANs稳定性的一种方法。

##### URL
[https://arxiv.org/abs/1802.02436](https://arxiv.org/abs/1802.02436)

##### PDF
[https://arxiv.org/pdf/1802.02436](https://arxiv.org/pdf/1802.02436)

