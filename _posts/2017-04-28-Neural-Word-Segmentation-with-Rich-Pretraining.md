---
layout: post
title: "Neural Word Segmentation with Rich Pretraining"
date: 2017-04-28 14:46:25
categories: arXiv_CL
tags: arXiv_CL Segmentation Embedding
author: Jie Yang, Yue Zhang, Fei Dong
mathjax: true
---

* content
{:toc}

##### Abstract
Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.

##### Abstract (translated by Google)
神经分词研究已经从大规模的原始文本中受益，利用它们来预处理字符和单词嵌入。另一方面，统计分割研究利用了更丰富的外部信息来源，如标点符号，自动分割和POS等。我们通过构建模块化分割模型，使用丰富的外部资源预训练最重要的子模块，调查一系列外部训练来源对神经分词的有效性。结果表明，这种预训练显着改善模型，导致准确性在六个基准上与最好的方法竞争。

##### URL
[https://arxiv.org/abs/1704.08960](https://arxiv.org/abs/1704.08960)

##### PDF
[https://arxiv.org/pdf/1704.08960](https://arxiv.org/pdf/1704.08960)

