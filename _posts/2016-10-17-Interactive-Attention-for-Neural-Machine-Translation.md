---
layout: post
title: "Interactive Attention for Neural Machine Translation"
date: 2016-10-17 08:33:20
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1610.05011](https://arxiv.org/abs/1610.05011)

##### PDF
[https://arxiv.org/pdf/1610.05011](https://arxiv.org/pdf/1610.05011)

