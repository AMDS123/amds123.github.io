---
layout: post
title: "Interactive Attention for Neural Machine Translation"
date: 2016-10-17 08:33:20
categories: arXiv_CL
tags: arXiv_CL NMT
author: Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu
mathjax: true
---

* content
{:toc}

##### Abstract
Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.

##### Abstract (translated by Google)
传统的基于注意的神经机器翻译（NMT）在生成目标句子时进行动态调整。通过反复阅读编码器（Bahdanau et al。，2015）生成的源语句的表示，注意机制大大增强了最先进的NMT。在本文中，我们提出了一种新的注意机制，称为交互注意（INTERACTIVE ATTENTION），它通过读写操作来模拟翻译过程中译码器与源语句表示之间的相互作用。交互式注意能跟踪交互历史，从而提高翻译表现。对NIST中英文翻译任务的实验表明，交互式注意能够比以前的基于注意力的NMT基线和一些最先进的注意型NMT变体（即覆盖模型（Tu et al。 ，2016））。与我们的INTERACTIVE ATTENTION相比，神经机器翻译器可以在多个测试集上平均比4.22 BLEU点开源注意的NMT系统Groundhog和3.94 BLEU点的开源短语系统Moses更胜一筹。

##### URL
[https://arxiv.org/abs/1610.05011](https://arxiv.org/abs/1610.05011)

