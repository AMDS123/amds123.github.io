---
layout: post
title: "Clinical Concept Embeddings Learned from Massive Sources of Multimodal Medical Data"
date: 2018-05-18 19:25:31
categories: arXiv_AI
tags: arXiv_AI Embedding Relation
author: Andrew L. Beam, Benjamin Kompa, Inbar Fried, Nathan P. Palmer, Xu Shi, Tianxi Cai, Isaac S. Kohane
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings are a popular approach to unsupervised learning of word relationships that are widely used in natural language processing. In this article, we present a new set of embeddings for medical concepts learned using an extremely large collection of multimodal medical data. Leaning on recent theoretical insights, we demonstrate how an insurance claims database of 60 million members, a collection of 20 million clinical notes, and 1.7 million full text biomedical journal articles can be combined to embed concepts into a common space, resulting in the largest ever set of embeddings for 108,477 medical concepts. To evaluate our approach, we present a new benchmark methodology based on statistical power specifically designed to test embeddings of medical concepts. Our approach, called cui2vec, attains state of the art performance relative to previous methods in most instances. Finally, we provide a downloadable set of pre-trained embeddings for other researchers to use, as well as an online tool for interactive exploration of the cui2vec embeddings.

##### Abstract (translated by Google)
词嵌入是广泛用于自然语言处理的无监督学习单词关系的流行方法。在这篇文章中，我们提出了一套新的用于医学概念的嵌入，这些概念是使用极其庞大的多模式医学数据集合学习的。依靠最近的理论见解，我们演示了6000万会员的保险索赔数据库，2000万份临床记录集和170万份全文生物医学期刊文章如何结合起来，将概念嵌入共同空间，从而创造了有史以来最大的108,477个医学概念的一组嵌入。为了评估我们的方法，我们提出了一种基于专门设计用于测试医学概念嵌入的统计功效的新基准方法。我们的方法，称为cui2vec，在大多数情况下相对于以前的方法达到了最先进的性能。最后，我们为其他研究人员提供一组可供下载的预先训练的嵌入，以及一个用于交互式探索cui2vec嵌入的在线工具。

##### URL
[http://arxiv.org/abs/1804.01486](http://arxiv.org/abs/1804.01486)

##### PDF
[http://arxiv.org/pdf/1804.01486](http://arxiv.org/pdf/1804.01486)

