---
layout: post
title: "Improving Text-to-SQL Evaluation Methodology"
date: 2018-06-23 20:02:55
categories: arXiv_AI
tags: arXiv_AI
author: Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev
mathjax: true
---

* content
{:toc}

##### Abstract
To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.

##### Abstract (translated by Google)
为了提供信息，评估必须衡量系统对现实的不可见数据的概括。我们确定当前文本到SQL系统的评估的局限性并提出改进建议。首先，我们比较人为生成和自动生成的问题，表征实际应用程序所需查询的属性。为了便于评估多个数据集，我们发布了七个现有数据集的标准化和改进版本以及一个新的文本到SQL数据集。其次，我们表明，目前将数据划分为训练和测试集的措施对测试问题方式的变化具有鲁棒性，但仅部分测试了系统对新查询的适用性;因此，我们提出一个补充数据集拆分评估未来的工作。最后，我们演示了在评估过程中匿名变量的通用做法如何消除了任务的重要挑战。我们的观察强调了关键困难，我们的方法能够有效衡量未来的发展。

##### URL
[http://arxiv.org/abs/1806.09029](http://arxiv.org/abs/1806.09029)

##### PDF
[http://arxiv.org/pdf/1806.09029](http://arxiv.org/pdf/1806.09029)

