---
layout: post
title: "Multimodal Residual Learning for Visual QA"
date: 2016-08-31 08:28:38
categories: arXiv_CV
tags: arXiv_CV QA Attention VQA Recognition
author: Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.

##### Abstract (translated by Google)
深度神经网络以各种方法继续推进图像识别任务的最新技术。但是，这些方法在多式联运方面的应用仍然有限。提出了多模态残差网络（MRN）用于视觉问答多模式残差学习，拓展了残差深度学习的思想。与深度残差学习不同，MRN有效地从视觉和语言信息中学习联合表示。其主要思想是利用最近的研究中利用注意模型的残差学习的联合残差映射的元素乘法。基于我们的研究探索了多模式引入的各种替代模型。我们在Visual QA数据集中实现了开放式和多项选择任务的最新成果。此外，我们引入了一种新的方法，即使没有空间信息的视觉特征被折叠，使用反向传播算法使每个学习块的联合表示的关注效果可视化。

##### URL
[https://arxiv.org/abs/1606.01455](https://arxiv.org/abs/1606.01455)

##### PDF
[https://arxiv.org/pdf/1606.01455](https://arxiv.org/pdf/1606.01455)

