---
layout: post
title: "Variational Inference for Policy Gradient"
date: 2018-02-21 22:18:20
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning Inference
author: Tianbing Xu
mathjax: true
---

* content
{:toc}

##### Abstract
Inspired by the seminal work on Stein Variational Inference and Stein Variational Policy Gradient, we derived a method to generate samples from the posterior variational parameter distribution by \textit{explicitly} minimizing the KL divergence to match the target distribution in an amortize fashion. Consequently, we applied this varational inference technique into vanilla policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations for reinforcement learning problems.

##### Abstract (translated by Google)
受Stein Variational Inference和Stein Variational Policy Gradient开创性工作的启发，我们推导出一种方法，通过\ textit {显式}最小化KL散度来按照摊销方式匹配目标分布，从后变分参数分布生成样本。因此，我们将这种变分推理技术应用于香草政策梯度，TRPO和PPO以及用于强化学习问题的贝叶斯神经网络参数化。

##### URL
[http://arxiv.org/abs/1802.07833](http://arxiv.org/abs/1802.07833)

##### PDF
[http://arxiv.org/pdf/1802.07833](http://arxiv.org/pdf/1802.07833)

