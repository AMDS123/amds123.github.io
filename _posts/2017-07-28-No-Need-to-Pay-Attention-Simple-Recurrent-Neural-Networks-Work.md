---
layout: post
title: "No Need to Pay Attention: Simple Recurrent Neural Networks Work!"
date: 2017-07-28 15:28:01
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention RNN Relation
author: Ferhan Ture, Oliver Jojic
mathjax: true
---

* content
{:toc}

##### Abstract
First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 65%-76% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. We train a recurrent neural network to solve each problem. On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results --- even neural networks based on much more complex architectures. The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting. In fact, we present a preliminary analysis of the performance of our model on real queries from Comcast's X1 entertainment platform with millions of users every day.

##### Abstract (translated by Google)
一阶真题回答假定这个问题可以通过知识库（KB）中的一个事实来回答。虽然这似乎不是一个具有挑战性的任务，但是最近许多应用复杂语言推理或深度神经网络的尝试对基准集合的准确度达到了65％-76％。我们的方法将任务定义为两个机器学习问题：检测问题中的实体，并将问题分类为知识库中的一种关系类型。我们训练一个循环神经网络来解决每个问题。在SimpleQuestions数据集中，我们的方法比以前发布的结果产生了实质性的改进 - 即使是基于更复杂架构的神经网络。我们方法的简单性也具有实际的优点，如效率和模块性，这在工业环境中是非常有价值的。事实上，我们初步分析了我们的模型对Comcast X1娱乐平台每天有数百万用户的真实查询的性能表现。

##### URL
[https://arxiv.org/abs/1606.05029](https://arxiv.org/abs/1606.05029)

##### PDF
[https://arxiv.org/pdf/1606.05029](https://arxiv.org/pdf/1606.05029)

