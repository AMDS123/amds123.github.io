---
layout: post
title: "MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning"
date: 2018-06-12 07:07:44
categories: arXiv_CV
tags: arXiv_CV Regularization Sparse Embedding
author: Bo Zhao, Xinwei Sun, Yanwei Fu, Yuan Yao, Yizhou Wang
mathjax: true
---

* content
{:toc}

##### Abstract
It is one typical and general topic of learning a good embedding model to efficiently learn the representation coefficients between two spaces/subspaces. To solve this task, $L_{1}$ regularization is widely used for the pursuit of feature selection and avoiding overfitting, and yet the sparse estimation of features in $L_{1}$ regularization may cause the underfitting of training data. $L_{2}$ regularization is also frequently used, but it is a biased estimator. In this paper, we propose the idea that the features consist of three orthogonal parts, \emph{namely} sparse strong signals, dense weak signals and random noise, in which both strong and weak signals contribute to the fitting of data. To facilitate such novel decomposition, \emph{MSplit} LBI is for the first time proposed to realize feature selection and dense estimation simultaneously. We provide theoretical and simulational verification that our method exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental results show that our method achieves state-of-the-art performance in the few-shot and zero-shot learning.

##### Abstract (translated by Google)
学习一个好的嵌入模型以有效地学习两个空间/子空间之间的表示系数是一个典型和普遍的话题。为了解决这个问题，$ L_ {1} $正则化被广泛用于追求特征选择和避免过拟合，而$ L_ {1} $正则化中特征的稀疏估计可能导致训练数据的不足。 $ L_ {2} $正则化也经常使用，但它是一个有偏差的估计量。在本文中，我们提出这样的想法，即特征由三个正交部分组成，即稀疏强信号，密集弱信号和随机噪声，其中强信号和弱信号都有助于数据的拟合。为了促进这种新颖的分解，首次提出了\ emph {MSplit} LBI来同时实现特征选择和密集估计。我们提供了理论和仿真验证，证明了我们的方法超过$ L_ {1} $和$ L_ {2} $正则化，并且广泛的实验结果表明，我们的方法在少数和零点处理中实现了最先进的性能，开枪学习。

##### URL
[http://arxiv.org/abs/1806.04360](http://arxiv.org/abs/1806.04360)

##### PDF
[http://arxiv.org/pdf/1806.04360](http://arxiv.org/pdf/1806.04360)

