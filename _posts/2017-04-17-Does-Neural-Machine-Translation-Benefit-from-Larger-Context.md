---
layout: post
title: "Does Neural Machine Translation Benefit from Larger Context?"
date: 2017-04-17 21:42:19
categories: arXiv_CL
tags: arXiv_CL Attention Prediction
author: Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task.

##### Abstract (translated by Google)
我们提出了一个神经机器翻译体系结构，除了源语句之外，还可以模拟周围的文本。这些模型在通常的翻译质量和代词预测方面导致更好的表现，当在小语料库上进行训练时，尽管当用更大的语料库训练时，这种改进大部分消失。我们还发现，基于注意力的神经机器翻译非常适合用于代词预测，并与专门为此任务设计的其他方法相比较。

##### URL
[https://arxiv.org/abs/1704.05135](https://arxiv.org/abs/1704.05135)

##### PDF
[https://arxiv.org/pdf/1704.05135](https://arxiv.org/pdf/1704.05135)

