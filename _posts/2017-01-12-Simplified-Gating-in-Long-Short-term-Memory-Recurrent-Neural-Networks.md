---
layout: post
title: "Simplified Gating in Long Short-term Memory Recurrent Neural Networks"
date: 2017-01-12 18:12:05
categories: arXiv_CV
tags: arXiv_CV RNN
author: Yuzhen Lu, Fathi M. Salem
mathjax: true
---

* content
{:toc}

##### Abstract
The standard LSTM recurrent neural networks while very powerful in long-range dependency sequence applications have highly complex structure and relatively large (adaptive) parameters. In this work, we present empirical comparison between the standard LSTM recurrent neural network architecture and three new parameter-reduced variants obtained by eliminating combinations of the input signal, bias, and hidden unit signals from individual gating signals. The experiments on two sequence datasets show that the three new variants, called simply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the standard LSTM model with less (adaptive) parameters.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1701.03441](https://arxiv.org/abs/1701.03441)

##### PDF
[https://arxiv.org/pdf/1701.03441](https://arxiv.org/pdf/1701.03441)

