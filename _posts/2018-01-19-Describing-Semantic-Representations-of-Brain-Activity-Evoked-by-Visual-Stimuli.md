---
layout: post
title: "Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli"
date: 2018-01-19 05:12:59
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption Deep_Learning Quantitative Relation
author: Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida, Hideki Asoh
mathjax: true
---

* content
{:toc}

##### Abstract
Quantitative modeling of human brain activity based on language representations has been actively studied in systems neuroscience. However, previous studies examined word-level representation, and little is known about whether we could recover structured sentences from brain activity. This study attempts to generate natural language descriptions of semantic contents from human brain activity evoked by visual stimuli. To effectively use a small amount of available brain activity data, our proposed method employs a pre-trained image-captioning network model using a deep learning framework. To apply brain activity to the image-captioning network, we train regression models that learn the relationship between brain activity and deep-layer image features. The results demonstrate that the proposed model can decode brain activity and generate descriptions using natural language sentences. We also conducted several experiments with data from different subsets of brain regions known to process visual stimuli. The results suggest that semantic information for sentence generations is widespread across the entire cortex.

##### Abstract (translated by Google)
已经在系统神经科学中积极地研究了基于语言表示的人类大脑活动的定量建模。然而，以前的研究考察了单词级别的表示，关于我们是否可以从大脑活动中恢复结构化的句子知之甚少。本研究试图从视觉刺激诱发的人脑活动中产生语义内容的自然语言描述。为了有效地利用少量的大脑活动数据，我们提出的方法采用了一个预先训练的图像字幕网络模型，使用深度学习框架。为了将大脑活动应用于图像字幕网络，我们训练回归模型，以了解大脑活动与深层图像特征之间的关系。结果表明，该模型可以解码大脑活动，并使用自然语言句子生成描述。我们还用已知处理视觉刺激的不同脑区域的数据进行了数个实验。结果表明，句子生成的语义信息遍布整个皮层。

##### URL
[https://arxiv.org/abs/1802.02210](https://arxiv.org/abs/1802.02210)

##### PDF
[https://arxiv.org/pdf/1802.02210](https://arxiv.org/pdf/1802.02210)

