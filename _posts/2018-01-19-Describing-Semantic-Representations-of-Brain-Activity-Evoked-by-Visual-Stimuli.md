---
layout: post
title: "Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli"
date: 2018-01-19 05:12:59
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption Deep_Learning Quantitative Relation
author: Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida, Hideki Asoh
mathjax: true
---

* content
{:toc}

##### Abstract
Quantitative modeling of human brain activity based on language representations has been actively studied in systems neuroscience. However, previous studies examined word-level representation, and little is known about whether we could recover structured sentences from brain activity. This study attempts to generate natural language descriptions of semantic contents from human brain activity evoked by visual stimuli. To effectively use a small amount of available brain activity data, our proposed method employs a pre-trained image-captioning network model using a deep learning framework. To apply brain activity to the image-captioning network, we train regression models that learn the relationship between brain activity and deep-layer image features. The results demonstrate that the proposed model can decode brain activity and generate descriptions using natural language sentences. We also conducted several experiments with data from different subsets of brain regions known to process visual stimuli. The results suggest that semantic information for sentence generations is widespread across the entire cortex.

##### Abstract (translated by Google)
基于语言表征的人类大脑活动的定量建模已在系统神经科学中得到积极研究。然而，之前的研究检查了词级表示，对于我们是否能从大脑活动中恢复结构化句子知之甚少。本研究试图从视觉刺激引起的人类大脑活动中产生语义内容的自然语言描述。为了有效地使用少量可用的大脑活动数据，我们提出的方法采用了使用深度学习框架的预训练的图像字幕网络模型。为了将大脑活动应用于图像字幕网络，我们训练回归模型，以了解大脑活动和深层图像特征之间的关系。结果表明，所提出的模型可以解码大脑活动并使用自然语言句子生成描述。我们还对来自已知处理视觉刺激的不同脑区域子集的数据进行了几次实验。结果表明，句子世代的语义信息在整个皮质中都很普遍。

##### URL
[https://arxiv.org/abs/1802.02210](https://arxiv.org/abs/1802.02210)

##### PDF
[https://arxiv.org/pdf/1802.02210](https://arxiv.org/pdf/1802.02210)

