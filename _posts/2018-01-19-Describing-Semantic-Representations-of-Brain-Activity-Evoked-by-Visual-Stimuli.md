---
layout: post
title: "Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli"
date: 2018-01-19 05:12:59
categories: arXiv_CV
tags: arXiv_CV Image_Caption Caption Deep_Learning Quantitative Relation
author: Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida, Hideki Asoh
mathjax: true
---

* content
{:toc}

##### Abstract
Quantitative modeling of human brain activity based on language representations has been actively studied in systems neuroscience. However, previous studies examined word-level representation, and little is known about whether we could recover structured sentences from brain activity. This study attempts to generate natural language descriptions of semantic contents from human brain activity evoked by visual stimuli. To effectively use a small amount of available brain activity data, our proposed method employs a pre-trained image-captioning network model using a deep learning framework. To apply brain activity to the image-captioning network, we train regression models that learn the relationship between brain activity and deep-layer image features. The results demonstrate that the proposed model can decode brain activity and generate descriptions using natural language sentences. We also conducted several experiments with data from different subsets of brain regions known to process visual stimuli. The results suggest that semantic information for sentence generations is widespread across the entire cortex.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1802.02210](https://arxiv.org/abs/1802.02210)

##### PDF
[https://arxiv.org/pdf/1802.02210](https://arxiv.org/pdf/1802.02210)

