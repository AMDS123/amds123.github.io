---
layout: post
title: "Joint Dictionaries for Zero-Shot Learning"
date: 2017-09-12 04:41:33
categories: arXiv_CV
tags: arXiv_CV Sparse
author: Soheil Kolouri, Mohammad Rostami, Yuri Owechko, Kyungnam Kim
mathjax: true
---

* content
{:toc}

##### Abstract
A classic approach toward zero-shot learning (ZSL) is to map the input domain to a set of semantically meaningful attributes that could be used later on to classify unseen classes of data (e.g. visual data). In this paper, we propose to learn a visual feature dictionary that has semantically meaningful atoms. Such dictionary is learned via joint dictionary learning for the visual domain and the attribute domain, while enforcing the same sparse coding for both dictionaries. Our novel attribute aware formulation provides an algorithmic solution to the domain shift/hubness problem in ZSL. Upon learning the joint dictionaries, images from unseen classes can be mapped into the attribute space by finding the attribute aware joint sparse representation using solely the visual data. We demonstrate that our approach provides superior or comparable performance to that of the state of the art on benchmark datasets.

##### Abstract (translated by Google)
一个经典的零点学习（ZSL）方法是将输入域映射到一组语义上有意义的属性，以后可以用来分类看不见的数据类（例如可视数据）。在本文中，我们建议学习一个具有语义上有意义的原子的视觉特征字典。这样的字典是通过对可视域和属性域的联合字典学习来学习的，同时对这两个字典执行相同的稀疏编码。我们新颖的属性感知公式提供了一个算法解决ZSL的域转移/集线器问题。在学习联合词典时，通过仅使用视觉数据找到属性感知联合稀疏表示，可以将来自不可见类的图像映射到属性空间中。我们证明了我们的方法在基准数据集上提供了与现有技术水平相当的性能。

##### URL
[https://arxiv.org/abs/1709.03688](https://arxiv.org/abs/1709.03688)

##### PDF
[https://arxiv.org/pdf/1709.03688](https://arxiv.org/pdf/1709.03688)

