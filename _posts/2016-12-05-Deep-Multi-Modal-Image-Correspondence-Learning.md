---
layout: post
title: "Deep Multi-Modal Image Correspondence Learning"
date: 2016-12-05 02:16:09
categories: arXiv_CV
tags: arXiv_CV Inference
author: Chen Liu, Jiajun Wu, Pushmeet Kohli, Yasutaka Furukawa
mathjax: true
---

* content
{:toc}

##### Abstract
Inference of correspondences between images from different modalities is an extremely important perceptual ability that enables humans to understand and recognize cross-modal concepts. In this paper, we consider an instance of this problem that involves matching photographs of building interiors with their corresponding floorplan. This is a particularly challenging problem because a floorplan, as a stylized architectural drawing, is very different in appearance from a color photograph. Furthermore, individual photographs by themselves depict only a part of a floorplan (e.g., kitchen, bathroom, and living room). We propose the use of a number of different neural network architectures for this task, which are trained and evaluated on a novel large-scale dataset of 5 million floorplan images and 80 million associated photographs. Experimental evaluation reveals that our neural network architectures are able to identify visual cues that result in reliable matches across these two quite different modalities. In fact, the trained networks are able to even outperform human subjects in several challenging image matching problems. Our result implies that neural networks are effective at perceptual tasks that require long periods of reasoning even for humans to solve.

##### Abstract (translated by Google)
推断不同形式的图像之间的对应关系是一个非常重要的感知能力，使人类能够理解和认识跨模态的概念。在本文中，我们考虑这个问题的一个实例，涉及将建筑物内部的照片与其相应的平面图匹配。这是一个特别具有挑战性的问题，因为平面图，作为一个程式化的建筑图，与彩色照片的外观非常不同。此外，单独的照片本身仅描绘了平面图的一部分（例如，厨房，浴室和起居室）。我们提出使用许多不同的神经网络体系结构来完成这个任务，这些体系结构在500万个平面布置图和8000万张相关照片的新型大规模数据集上进行了训练和评估。实验评估表明，我们的神经网络架构能够识别视觉线索，导致这两种完全不同的方式的可靠匹配。事实上，训练有素的网络甚至可以在几个具有挑战性的图像匹配问题上超越人体对象。我们的结果意味着神经网络在需要长时间推理甚至人类解决的感知任务上是有效的。

##### URL
[https://arxiv.org/abs/1612.01225](https://arxiv.org/abs/1612.01225)

##### PDF
[https://arxiv.org/pdf/1612.01225](https://arxiv.org/pdf/1612.01225)

