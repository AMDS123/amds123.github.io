---
layout: post
title: "Flexible Rectified Linear Units for Improving Convolutional Neural Networks"
date: 2017-06-25 13:28:27
categories: arXiv_CV
tags: arXiv_CV CNN Image_Classification Classification
author: Suo Qiu, Bolun Cai
mathjax: true
---

* content
{:toc}

##### Abstract
Rectified linear unit (ReLU) is a widely used activation function for deep convolutional neural networks. In this paper, we propose a novel activation function called flexible rectified linear unit (FReLU). FReLU improves the flexibility of ReLU by a learnable rectified point. FReLU achieves a faster convergence and higher performance. Furthermore, FReLU does not rely on strict assumptions by self-adaption. FReLU is also simple and effective without using exponential function. We evaluate FReLU on two standard image classification dataset, including CIFAR-10 and CIFAR-100. Experimental results show the strengths of the proposed method.

##### Abstract (translated by Google)
整流线性单元（ReLU）是深度卷积神经网络广泛使用的激活函数。在本文中，我们提出了一种新的激活功能，称为柔性整流线性单元（FReLU）。 FReLU通过可学习的整改点提高了ReLU的灵活性。 FReLU实现了更快的收敛性和更高的性能。此外，FReLU不依赖于自适应的严格假设。 FReLU也不使用指数函数，简单而有效。我们评估FReLU在两个标准的图像分类数据集，包括CIFAR-10和CIFAR-100。实验结果表明了该方法的优点。

##### URL
[https://arxiv.org/abs/1706.08098](https://arxiv.org/abs/1706.08098)

##### PDF
[https://arxiv.org/pdf/1706.08098](https://arxiv.org/pdf/1706.08098)

