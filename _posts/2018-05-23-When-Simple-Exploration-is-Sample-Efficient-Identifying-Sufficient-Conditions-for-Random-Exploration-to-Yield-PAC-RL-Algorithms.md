---
layout: post
title: "When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms"
date: 2018-05-23 10:43:56
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Yao Liu, Emma Brunskill
mathjax: true
---

* content
{:toc}

##### Abstract
Efficient exploration is one of the key challenges for reinforcement learning (RL) algorithms. Most traditional sample efficiency bounds require strategic exploration. Recently many deep RL algorithm with simple heuristic exploration strategies that have few formal guarantees, achieve surprising success in many domains. These results pose an important question about understanding these exploration strategies such as $e$-greedy, as well as understanding what characterize the difficulty of exploration in MDPs. In this work we propose problem specific sample complexity bounds of $Q$ learning with random walk exploration that rely on several structural properties. We also link our theoretical results to some empirical benchmark domains, to illustrate if our bound gives polynomial sample complexity or not in these domains and how that is related with the empirical performance in these domains.

##### Abstract (translated by Google)
高效探索是强化学习（RL）算法的关键挑战之一。大多数传统的样本效率界限都需要进行战略探索最近，许多具有简单的启发式探索策略的深度RL算法几乎没有形式化的保证，在许多领域取得惊人的成功。这些结果对理解这些勘探策略提出了一个重要问题，例如$ e $ -greedy，以及理解什么是MDP勘探困难的特征。在这项工作中，我们根据几个结构性质提出了随机游走探索的问题特定问题样本复杂性边界。我们还将我们的理论结果与一些经验性基准域相关联，以说明我们的界限是否给出了这些域中的多项式样本复杂性，以及这些域与这些域中的经验性表现如何相关。

##### URL
[http://arxiv.org/abs/1805.09045](http://arxiv.org/abs/1805.09045)

##### PDF
[http://arxiv.org/pdf/1805.09045](http://arxiv.org/pdf/1805.09045)

