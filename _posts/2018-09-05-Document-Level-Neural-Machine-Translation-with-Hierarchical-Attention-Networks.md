---
layout: post
title: "Document-Level Neural Machine Translation with Hierarchical Attention Networks"
date: 2018-09-05 15:27:16
categories: arXiv_CL
tags: arXiv_CL Attention NMT
author: Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson
mathjax: true
---

* content
{:toc}

##### Abstract
Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.

##### Abstract (translated by Google)
可以通过包括文档级上下文信息来改进神经机器翻译（NMT）。为此，我们提出了一种分层注意模型，以结构化和动态的方式捕获上下文。该模型作为另一个抽象层次集成在原始NMT架构中，调整了NMT模型自己以前的隐藏状态。实验表明，分层注意力在强大的NMT基线上显着提高了BLEU得分，采用了最新的上下文感知方法，并且编码器和解码器都以互补的方式从上下文中获益。

##### URL
[http://arxiv.org/abs/1809.01576](http://arxiv.org/abs/1809.01576)

##### PDF
[http://arxiv.org/pdf/1809.01576](http://arxiv.org/pdf/1809.01576)

