---
layout: post
title: "Videos as Space-Time Region Graphs"
date: 2018-06-05 16:58:59
categories: arXiv_CV
tags: arXiv_CV CNN Relation
author: Xiaolong Wang, Abhinav Gupta
mathjax: true
---

* content
{:toc}

##### Abstract
How do humans recognize the action "opening a book" ? We argue that there are two important cues: modeling temporal shape dynamics and modeling functional relationships between humans and objects. In this paper, we propose to represent videos as space-time region graphs which capture these two important cues. Our graph nodes are defined by the object region proposals from different frames in a long range video. These nodes are connected by two types of relations: (i) similarity relations capturing the long range dependencies between correlated objects and (ii) spatial-temporal relations capturing the interactions between nearby objects. We perform reasoning on this graph representation via Graph Convolutional Networks. We achieve state-of-the-art results on both Charades and Something-Something datasets. Especially for Charades, we obtain a huge 4.4% gain when our model is applied in complex environments.

##### Abstract (translated by Google)
人类如何认识“打开书”的行为？我们认为有两个重要线索：建模时间形状动态和建模人与物体之间的功能关系。在本文中，我们建议将视频表示为捕获这两个重要线索的时空区域图。我们的图形节点由长距离视频中来自不同帧的对象区域提案定义。这些节点通过两种关系相连：（i）捕获相关对象之间的长距离依赖关系的相似关系和（ii）捕获附近对象之间相互作用的时空关系。我们通过图形卷积网络对这个图形表示进行推理。我们在Charades和Something-Something数据集上实现了最先进的结果。特别是对于Charades，当我们的模型应用于复杂环境时，我们获得了4.4％的巨大收益。

##### URL
[http://arxiv.org/abs/1806.01810](http://arxiv.org/abs/1806.01810)

##### PDF
[http://arxiv.org/pdf/1806.01810](http://arxiv.org/pdf/1806.01810)

