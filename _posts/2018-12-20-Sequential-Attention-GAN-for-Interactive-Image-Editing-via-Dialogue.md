---
layout: post
title: "Sequential Attention GAN for Interactive Image Editing via Dialogue"
date: 2018-12-20 03:55:33
categories: arXiv_AI
tags: arXiv_AI Adversarial QA Attention GAN Quantitative
author: Yu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, Jianfeng Gao
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we introduce a new task - interactive image editing via conversational language, where users can guide an agent to edit images via multi-turn dialogue in natural language. In each dialogue turn, the agent takes a source image and a natural language description from the user as the input, and generates a target image following the textual description. Two new datasets are created for this task,Zap-Seq and DeepFashion-Seq, collected via crowdsourcing. For this task, we propose a new Sequential Attention Genrative Adversarial Network (SeqAttnGAN) framework, which applies a neural state tracker to encode both source image and textual descriptions, and generates high quality images in each dialogue turn. To achieve better region specific text-to-image generation, we also introducean attention mechanism into the model. Experiments on the two datasets, including quantitative evaluation and user study, show that our model outperforms state-of-the-art ap-proaches in both image quality and text-to-image consistency.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1812.08352](https://arxiv.org/abs/1812.08352)

##### PDF
[https://arxiv.org/pdf/1812.08352](https://arxiv.org/pdf/1812.08352)

