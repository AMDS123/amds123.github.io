---
layout: post
title: "Universal Language Model Fine-tuning for Text Classification"
date: 2018-05-14 13:57:04
categories: arXiv_CL
tags: arXiv_CL Text_Classification Transfer_Learning Classification Language_Model
author: Jeremy Howard, Sebastian Ruder
mathjax: true
---

* content
{:toc}

##### Abstract
Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.

##### Abstract (translated by Google)
归纳转移学习对计算机视觉有很大影响，但现有的NLP方法仍然需要从头开始进行特定于任务的修改和培训。我们提出通用语言模型微调（ULMFiT），这是一种有效的转移学习方法，可以应用于NLP中的任何任务，并引入对微调语言模型至关重要的技术。我们的方法明显优于六种文本分类任务的最新技术，可将大多数数据集的错误减少18-24％。此外，只有100个标记示例，它匹配100倍以上数据从头开始的培训性能。我们开源我们的预训练模型和代码。

##### URL
[http://arxiv.org/abs/1801.06146](http://arxiv.org/abs/1801.06146)

##### PDF
[http://arxiv.org/pdf/1801.06146](http://arxiv.org/pdf/1801.06146)

