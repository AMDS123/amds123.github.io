---
layout: post
title: "Reciprocal Attention Fusion for Visual Question Answering"
date: 2018-05-11 06:13:56
categories: arXiv_AI
tags: arXiv_AI QA Attention Relation VQA
author: Moshiur R Farazi, Salman Khan
mathjax: true
---

* content
{:toc}

##### Abstract
Existing attention mechanisms either attend to local image grid or object level features for Visual Question Answering (VQA). Motivated by the observation that questions can relate to both object instances and their parts, we propose a novel attention mechanism that jointly considers reciprocal relationships between the two levels of visual details. The bottom-up attention thus generated is further coalesced with the top-down information to only focus on the scene elements that are most relevant to a given question. Our design hierarchically fuses multi-modal information i.e., language, object- and gird-level features, through an efficient tensor decomposition scheme. The proposed model improves the state-of-the-art single model performances from 67.9% to 68.2% on VQAv1 and from 65.3% to 67.4% on VQAv2, demonstrating a significant boost.

##### Abstract (translated by Google)
现有的注意机制要么参加本地图像网格，要么参与视觉问答（VQA）的对象级特征。受到观察结果的启发，问题可能与对象实例及其部分有关，我们提出了一种新颖的关注机制，共同考虑两个视觉细节层次之间的相互关系。由此产生的自下而上的注意力进一步与自上而下的信息相结合，仅聚焦于与给定问题最相关的场景元素。我们的设计通过高效的张量分解方案分层融合了多模态信息，即语言，对象和网格级特征。该模型将VQAv1中最先进的单模型性能从67.9％提高到68.2％，VQAv2将模型性能从65.3％提高到67.4％，显示出显着的提升。

##### URL
[http://arxiv.org/abs/1805.04247](http://arxiv.org/abs/1805.04247)

##### PDF
[http://arxiv.org/pdf/1805.04247](http://arxiv.org/pdf/1805.04247)

