---
layout: post
title: "Deep Semantic Role Labeling with Self-Attention"
date: 2017-12-05 11:48:51
categories: arXiv_CL
tags: arXiv_CL RNN
author: Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, Xiaodong Shi
mathjax: true
---

* content
{:toc}

##### Abstract
Semantic Role Labeling (SRL) is believed to be a crucial step towards natural language understanding and has been widely studied. Recent years, end-to-end SRL with recurrent neural networks (RNN) has gained increasing attention. However, it remains a major challenge for RNNs to handle structural information and long range dependencies. In this paper, we present a simple and effective architecture for SRL which aims to address these problems. Our model is based on self-attention which can directly capture the relationships between two tokens regardless of their distance. Our single model achieves F$_1=83.4$ on the CoNLL-2005 shared task dataset and F$_1=82.7$ on the CoNLL-2012 shared task dataset, which outperforms the previous state-of-the-art results by $1.8$ and $1.0$ F$_1$ score respectively. Besides, our model is computationally efficient, and the parsing speed is 50K tokens per second on a single Titan X GPU.

##### Abstract (translated by Google)
语义角色标注（SRL）被认为是自然语言理解的一个关键步骤，并被广泛研究。近年来，带有递归神经网络（RNN）的端到端SRL得到越来越多的关注。然而，RNNs处理结构信息和长程依赖性仍然是一个重大挑战。在本文中，我们提出了一个简单而有效的SRL架构，旨在解决这些问题。我们的模型是基于自我注意的，可以直接捕捉两个令牌之间的关系，而不管他们的距离。我们的单一模型在CoNLL-2005共享任务数据集上达到F $ _1 = 83.4 $，在CoNLL-2012共享任务数据集上达到F $ _1 = 82.7 $，比之前的最新结果高出1.8美元和$ 1.0 $ F $ _1 $分别。此外，我们的模型在计算上是高效的，在单个Titan X GPU上解析速度为每秒50K令牌。

##### URL
[http://arxiv.org/abs/1712.01586](http://arxiv.org/abs/1712.01586)

##### PDF
[http://arxiv.org/pdf/1712.01586](http://arxiv.org/pdf/1712.01586)

