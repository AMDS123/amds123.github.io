---
layout: post
title: "Second-Order Word Embeddings from Nearest Neighbor Topological Features"
date: 2017-05-23 19:12:05
categories: arXiv_CL
tags: arXiv_CL Embedding Recognition
author: Denis Newman-Griffis, Eric Fosler-Lussier
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.

##### Abstract (translated by Google)
我们在预训练的上下文词嵌入中引入从最近邻居拓扑特征引起的词的二阶向量表示。然后分析了二阶嵌入作为输入特征在两种深层自然语言处理模型中的作用，即命名实体识别和文本蕴涵识别，以及释义识别的线性模型。令人惊讶的是，我们发现单独的最近邻居信息足以捕获使用预先训练的单词嵌入所带来的大部分性能好处。此外，二阶嵌入能够比一阶表示更好地处理高度异构的数据，尽管以一些特定的代价为代价。另外，在某些情况下，利用二阶信息增强上下文嵌入可进一步提高模型的性能。由于字嵌入的随机初始化的变化，利用来自多个一阶嵌入采样的最近邻特征也可以有助于下行性能增益。最后，为了进一步的研究，我们确定了二阶嵌入空间的有趣特征，包括更高的密度和不同的余弦相似度的语义解释。

##### URL
[https://arxiv.org/abs/1705.08488](https://arxiv.org/abs/1705.08488)

##### PDF
[https://arxiv.org/pdf/1705.08488](https://arxiv.org/pdf/1705.08488)

