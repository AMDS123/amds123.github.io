---
layout: post
title: "Classical Structured Prediction Losses for Sequence to Sequence Learning"
date: 2018-03-07 16:43:21
categories: arXiv_CL
tags: arXiv_CL Attention Summarization Reinforcement_Learning Survey Optimization Prediction
author: Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc&#x27;Aurelio Ranzato
mathjax: true
---

* content
{:toc}

##### Abstract
There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT'14 German-English translation as well as Gigaword abstractive summarization. On the larger WMT'14 English-French translation task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.

##### Abstract (translated by Google)
最近有许多工作是使用强化学习式方法或通过优化光束来在序列级别上训练神经关注模型。在本文中，我们调查了一系列经典的目标函数，这些函数已被广泛用于训练结构预测的线性模型，并将它们应用于神经序列以模拟序列。我们的实验表明，这些损失可以通过略微超越波束搜索优化来达到惊人的效果，就像类似的设置一样。我们还在IWSLT'14德英翻译以及Gigaword抽象概述中报告了最新的最新成果。在较大的WMT'14英法翻译任务中，顺序级别训练达到了41.5 BLEU，这与现有技术水平相当。

##### URL
[http://arxiv.org/abs/1711.04956](http://arxiv.org/abs/1711.04956)

##### PDF
[http://arxiv.org/pdf/1711.04956](http://arxiv.org/pdf/1711.04956)

