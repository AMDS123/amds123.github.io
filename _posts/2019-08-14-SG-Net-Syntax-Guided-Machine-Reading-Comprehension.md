---
layout: post
title: "SG-Net: Syntax-Guided Machine Reading Comprehension"
date: 2019-08-14 14:28:07
categories: arXiv_CL
tags: arXiv_CL Knowledge Attention
author: Zhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng Duan, Hai Zhao
mathjax: true
---

* content
{:toc}

##### Abstract
For machine reading comprehension, how to effectively model the linguistic knowledge from the detail-riddled and lengthy passages and get ride of the noises is essential to improve its performance. In this work, we propose using syntax to guide the text modeling of both passages and questions by incorporating syntactic clues into multi-head attention mechanism to fully fuse information from both global and attended representations. Accordingly, we present a novel syntax-guided network (SG-Net) for challenging reading comprehension tasks. Extensive experiments on popular benchmarks including SQuAD 2.0 and RACE validate the effectiveness of the proposed method with substantial improvements over fine-tuned BERT. This work empirically discloses the effectiveness of syntactic structural information for text modeling. The proposed attention mechanism also verifies the practicability of using linguistic information to guide attention learning and can be easily adapted with other tree-structured annotations.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.05147](http://arxiv.org/abs/1908.05147)

##### PDF
[http://arxiv.org/pdf/1908.05147](http://arxiv.org/pdf/1908.05147)

