---
layout: post
title: "Grounding Spatio-Semantic Referring Expressions for Human-Robot Interaction"
date: 2017-07-18 16:02:05
categories: arXiv_CL
tags: arXiv_CL Face Relation
author: Mohit Shridhar, David Hsu
mathjax: true
---

* content
{:toc}

##### Abstract
The human language is one of the most natural interfaces for humans to interact with robots. This paper presents a robot system that retrieves everyday objects with unconstrained natural language descriptions. A core issue for the system is semantic and spatial grounding, which is to infer objects and their spatial relationships from images and natural language expressions. We introduce a two-stage neural-network grounding pipeline that maps natural language referring expressions directly to objects in the images. The first stage uses visual descriptions in the referring expressions to generate a candidate set of relevant objects. The second stage examines all pairwise relationships between the candidates and predicts the most likely referred object according to the spatial descriptions in the referring expressions. A key feature of our system is that by leveraging a large dataset of images labeled with text descriptions, it allows unrestricted object types and natural language referring expressions. Preliminary results indicate that our system outperforms a near state-of-the-art object comprehension system on standard benchmark datasets. We also present a robot system that follows voice commands to pick and place previously unseen objects.

##### Abstract (translated by Google)
人类语言是人类与机器人互动最自然的接口之一。本文提出了一个机器人系统，用无拘束的自然语言描述检索日常物体。该系统的核心问题是语义和空间基础，即从图像和自然语言表达中推断出对象及其空间关系。我们引入了一个两级神经网络接地流水线，它将自然语言引用表达式直接映射到图像中的对象。第一阶段在引用表达式中使用视觉描述来生成候选的相关对象集合。第二阶段检查候选人之间的所有配对关系，并根据引用表达式中的空间描述来预测最可能被引用的对象。我们系统的一个主要特点是，通过利用标有文本描述的大型图像数据集，它允许不受限制的对象类型和自然语言引用表达式。初步的结果表明，我们的系统比标准基准数据集上近乎最先进的对象理解系统更胜一筹。我们还提供了一个机器人系统，遵循语音命令挑选和放置以前看不见的物体。

##### URL
[https://arxiv.org/abs/1707.05720](https://arxiv.org/abs/1707.05720)

##### PDF
[https://arxiv.org/pdf/1707.05720](https://arxiv.org/pdf/1707.05720)

