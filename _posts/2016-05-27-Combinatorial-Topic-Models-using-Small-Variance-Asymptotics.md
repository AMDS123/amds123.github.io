---
layout: post
title: "Combinatorial Topic Models using Small-Variance Asymptotics"
date: 2016-05-27 03:11:02
categories: arXiv_CL
tags: arXiv_CL Optimization Inference
author: Ke Jiang, Suvrit Sra, Brian Kulis
mathjax: true
---

* content
{:toc}

##### Abstract
Topic models have emerged as fundamental tools in unsupervised machine learning. Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from LDA by passing to the small-variance limit. We minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are competitive with popular LDA-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts.

##### Abstract (translated by Google)
主题模型已经成为无监督机器学习的基本工具。大多数现代主题建模算法采用概率论的观点，推导出基于潜在狄利克雷分布（LDA）或其变体的推理算法。相比之下，我们将话题建模作为一个组合优化问题进行研究，并提出了一个新的由LDA引入的小目标函数。我们通过使用组合优化的思想来最小化派生的目标，这导致了一种新的，快速和高质量的主题建模算法。特别是，我们展示了我们的结果与基于LDA的流行主题建模方法相比具有竞争性，并且还讨论了我们的方法和概率对应方法之间的（非）相似性。

##### URL
[https://arxiv.org/abs/1604.02027](https://arxiv.org/abs/1604.02027)

##### PDF
[https://arxiv.org/pdf/1604.02027](https://arxiv.org/pdf/1604.02027)

