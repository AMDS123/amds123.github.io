---
layout: post
title: "Predicting Expressive Speaking Style From Text In End-To-End Speech Synthesis"
date: 2018-08-04 02:21:07
categories: arXiv_CL
tags: arXiv_CL Embedding Inference
author: Daisy Stanton, Yuxuan Wang, RJ Skerry-Ryan
mathjax: true
---

* content
{:toc}

##### Abstract
Global Style Tokens (GSTs) are a recently-proposed method to learn latent disentangled representations of high-dimensional data. GSTs can be used within Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to uncover expressive factors of variation in speaking style. In this work, we introduce the Text-Predicted Global Style Token (TP-GST) architecture, which treats GST combination weights or style embeddings as "virtual" speaking style labels within Tacotron. TP-GST learns to predict stylistic renderings from text alone, requiring neither explicit labels during training nor auxiliary inputs for inference. We show that, when trained on a dataset of expressive speech, our system generates audio with more pitch and energy variation than two state-of-the-art baseline models. We further demonstrate that TP-GSTs can synthesize speech with background noise removed, and corroborate these analyses with positive results on human-rated listener preference audiobook tasks. Finally, we demonstrate that multi-speaker TP-GST models successfully factorize speaker identity and speaking style. We provide a website with audio samples for each of our findings.

##### Abstract (translated by Google)
全局样式标记（GST）是最近提出的用于学习高维数据的潜在解缠表示的方法。 GST可以在Tacotron中使用，Tacotron是一种先进的端到端文本到语音合成系统，用于发现说话风格变化的表达因素。在这项工作中，我们引入了Text-Predicted Global Style Token（TP-GST）架构，该架构将GST组合权重或样式嵌入视为Tacotron中的“虚拟”说话风格标签。 TP-GST学会单独从文本中预测文体效果图，在训练期间既不需要明确的标签，也不需要辅助输入来进行推理。我们表明，当训练表达语音的数据集时，我们的系统产生的音频具有比两个最先进的基线模型更多的音调和能量变化。我们进一步证明TP-GST可以合成语音并消除背景噪声，并证实这些分析对人类听众偏好有声读物任务的积极结果。最后，我们证明了多音箱TP-GST模型成功地将演讲者身份和演讲风格分解。我们为每个研究结果提供了一个包含音频样本的网站。

##### URL
[http://arxiv.org/abs/1808.01410](http://arxiv.org/abs/1808.01410)

##### PDF
[http://arxiv.org/pdf/1808.01410](http://arxiv.org/pdf/1808.01410)

