---
layout: post
title: "MaskGAN: Better Text Generation via Filling in the ______"
date: 2018-01-23 19:22:21
categories: arXiv_AI
tags: arXiv_AI Adversarial GAN Summarization Text_Generation Language_Model Quantitative
author: William Fedus, Ian Goodfellow, Andrew M. Dai
mathjax: true
---

* content
{:toc}

##### Abstract
Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.

##### Abstract (translated by Google)
神经文本生成模型通常是自回归语言模型或seq2seq模型。这些模型通过依次采样单词来生成文本，每个单词以前一个单词为条件，并且是几种机器翻译和汇总基准的最新技术。这些基准通常由验证困惑定义，尽管这不是对生成文本质量的直接测量。另外，这些模型通常是通过最大可能性和教师强迫训练的。这些方法非常适合优化混淆，但是由于生成文本需要调整在训练时间从未观察到的单词序列，因此可能导致样本质量差。我们提出使用生成对抗网络（GAN）来提高样本质量，这种网络明确训练发生器产生高质量的样本，并在图像生成方面取得了很大的成功。 GAN最初设计用于输出可区分的值，所以离散的语言生成对他们来说是具有挑战性的。我们声称，验证困惑本身并不代表模型生成的文本的质量。我们引入了一个演员评论家条件GAN，它填补了周围环境条件的缺失文本。我们用定性和定量的方式表明，与最大似然训练模型相比，这会产生更真实的有条件和无条件的文本样本。

##### URL
[http://arxiv.org/abs/1801.07736](http://arxiv.org/abs/1801.07736)

##### PDF
[http://arxiv.org/pdf/1801.07736](http://arxiv.org/pdf/1801.07736)

