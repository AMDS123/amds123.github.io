---
layout: post
title: "Memory Visualization for Gated Recurrent Neural Networks in Speech Recognition"
date: 2017-02-27 02:07:34
categories: arXiv_CV
tags: arXiv_CV Speech_Recognition RNN Recognition
author: Zhiyuan Tang, Ying Shi, Dong Wang, Yang Feng, Shiyue Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.

##### Abstract (translated by Google)
递归神经网络（Recursive Neural Network，RNNs）在序列建模中表现出明显的优势，尤其是门控单元，如长时间短记忆（LSTM）和门控循环单元（GRU）。然而，在许多应用（例如，自动语音识别（ASR））中，显着性能背后的动态特性仍不清楚。本文采用可视化技术来研究LSTM和GRU在执行语音识别任务时的行为。我们的实验在门控内存中显示了一些有趣的模式，其中一些启发了简单而有效的网络结构修改。我们报告了两个这样的修改：（1）LSTM中的懒惰单元更新，以及（2）用于残差学习的快捷连接。这两个修改导致更容易理解和强大的网络。

##### URL
[https://arxiv.org/abs/1609.08789](https://arxiv.org/abs/1609.08789)

##### PDF
[https://arxiv.org/pdf/1609.08789](https://arxiv.org/pdf/1609.08789)

