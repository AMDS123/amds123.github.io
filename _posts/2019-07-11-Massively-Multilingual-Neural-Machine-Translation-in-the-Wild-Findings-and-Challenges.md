---
layout: post
title: "Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges"
date: 2019-07-11 06:47:30
categories: arXiv_AI
tags: arXiv_AI Transfer_Learning NMT
author: Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, Yonghui Wu
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce our efforts towards building a universal neural machine translation (NMT) system capable of translating between any language pair. We set a milestone towards this goal by building a single massively multilingual NMT model handling 103 languages trained on over 25 billion examples. Our system demonstrates effective transfer learning ability, significantly improving translation quality of low-resource languages, while keeping high-resource language translation quality on-par with competitive bilingual baselines. We provide in-depth analysis of various aspects of model building that are crucial to achieving quality and practicality in universal NMT. While we prototype a high-quality universal translation system, our extensive empirical analysis exposes issues that need to be further addressed, and we suggest directions for future research.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.05019](http://arxiv.org/abs/1907.05019)

##### PDF
[http://arxiv.org/pdf/1907.05019](http://arxiv.org/pdf/1907.05019)

