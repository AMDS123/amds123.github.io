---
layout: post
title: "All-but-the-Top: Simple and Effective Postprocessing for Word Representations"
date: 2017-02-05 15:43:07
categories: arXiv_SD
tags: arXiv_SD Language_Model Quantitative
author: Jiaqi Mu, Suma Bhat, Pramod Viswanath
mathjax: true
---

* content
{:toc}

##### Abstract
Real-valued word representations have transformed NLP applications, popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a very simple, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations even stronger. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level extrinsic tasks (semantic textual similarity) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages, in each case, the processed representations are consistently better than the original ones. Furthermore, we demonstrate quantitatively in downstream applications that neural network architectures "automatically learn" the postprocessing operation.

##### Abstract (translated by Google)
实值的词表达已经改变了NLP的应用，流行的例子是word2vec和GloVe，它们捕捉语言规律的能力得到了认可。在本文中，我们展示了一个非常简单而又反直观的后处理技术 - 从单词向量中消除了常见的均值向量和几个顶级的主导方向 - 这使得现成的表示更加强大。后处理是对多个数据集上的各种词汇层次内在任务（词语相似性，概念分类，词类推）和句子级外在性任务（语义文本相似性）进行实证验证，并以多种表示方法和多参数语言，在每种情况下，处理后的表示总是比原来的更好。此外，我们在下游应用中定量展示神经网络架构“自动学习”后处理操作。

##### URL
[https://arxiv.org/abs/1702.01417](https://arxiv.org/abs/1702.01417)

##### PDF
[https://arxiv.org/pdf/1702.01417](https://arxiv.org/pdf/1702.01417)

