---
layout: post
title: "Text Classification based on Word Subspace with Term-Frequency"
date: 2018-06-08 12:55:37
categories: arXiv_CL
tags: arXiv_CL Text_Classification Classification Language_Model
author: Erica K. Shimomoto, Lincon S. Souza, Bernardo B. Gatto, Kazuhiro Fukui
mathjax: true
---

* content
{:toc}

##### Abstract
Text classification has become indispensable due to the rapid increase of text in digital form. Over the past three decades, efforts have been made to approach this task using various learning algorithms and statistical models based on bag-of-words (BOW) features. Despite its simple implementation, BOW features lack semantic meaning representation. To solve this problem, neural networks started to be employed to learn word vectors, such as the word2vec. Word2vec embeds word semantic structure into vectors, where the angle between vectors indicates the meaningful similarity between words. To measure the similarity between texts, we propose the novel concept of word subspace, which can represent the intrinsic variability of features in a set of word vectors. Through this concept, it is possible to model text from word vectors while holding semantic information. To incorporate the word frequency directly in the subspace model, we further extend the word subspace to the term-frequency (TF) weighted word subspace. Based on these new concepts, text classification can be performed under the mutual subspace method (MSM) framework. The validity of our modeling is shown through experiments on the Reuters text database, comparing the results to various state-of-art algorithms.

##### Abstract (translated by Google)
由于数字形式的文本快速增长，文本分类变得不可或缺。在过去的三十年中，已经努力使用各种学习算法和基于袋装词（BOW）特征的统计模型来处理此任务。尽管它的实现简单，BOW特性缺乏语义表示。为了解决这个问题，神经网络开始被用来学习单词向量，如word2vec。 Word2vec将单词语义结构嵌入到向量中，其中向量之间的角度表示单词之间的有意义的相似性。为了度量文本之间的相似度，我们提出了词语子空间的新概念，它可以表示一组词向量中特征的固有变异性。通过这个概念，可以在保持语义信息的同时对来自单词向量的文本进行建模。为了将词频直接结合到子空间模型中，我们进一步将词子空间扩展到词频（TF）加权词子空间。基于这些新概念，文本分类可以在相互子空间方法（MSM）框架下执行。我们建模的有效性通过路透社文本数据库上的实验显示，并将结果与​​各种最先进的算法进行比较。

##### URL
[http://arxiv.org/abs/1806.03125](http://arxiv.org/abs/1806.03125)

##### PDF
[http://arxiv.org/pdf/1806.03125](http://arxiv.org/pdf/1806.03125)

