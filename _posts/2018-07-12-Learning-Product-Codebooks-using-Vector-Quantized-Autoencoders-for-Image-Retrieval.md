---
layout: post
title: "Learning Product Codebooks using Vector Quantized Autoencoders for Image Retrieval"
date: 2018-07-12 15:08:31
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval
author: Hanwei Wu, Markus Flierl
mathjax: true
---

* content
{:toc}

##### Abstract
The Vector Quantized-Variational Autoencoder (VQ-VAE) provides an unsupervised model for learning discrete representations by combining vector quantization and autoencoders. The VQ-VAE can avoid the issue of "posterior collapse" so that its learned discrete representation is meaningful. In this paper, we incorporate the product quantization into the bottleneck stage of VQ-VAE and propose an end-to-end unsupervised learning model for the image retrieval tasks. Compared to the classic vector quantization, product quantization has the advantage of generating large size codebook and fast retrieval can be achieved by using the lookup tables that store the distance between every two sub-codewords. In our proposed model, the product codebook is jointly learned with the encoders and decoders of the autoencoders. The encodings of query and database images can be generated by feeding the images into the trained encoder and learned product codebook. The experiment shows that our proposed model outperforms other state-of-the-art hashing and quantization methods for image retrieval.

##### Abstract (translated by Google)
矢量量化变分自动编码器（VQ-VAE）通过组合矢量量化和自动编码器提供用于学习离散表示的无监督模型。 VQ-VAE可以避免“后塌陷”的问题，因此其学习的离散表示是有意义的。在本文中，我们将产品量化纳入VQ-VAE的瓶颈阶段，并为图像检索任务提出端到端的无监督学习模型。与经典矢量量化相比，产品量化具有生成大尺寸码本的优点，并且可以通过使用存储每两个子码字之间的距离的查找表来实现快速检索。在我们提出的模型中，产品码本与自动编码器的编码器和解码器共同学习。可以通过将图像馈送到训练的编码器和学习的产品码本来生成查询和数据库图像的编码。实验表明，我们提出的模型优于其他最先进的用于图像检索的散列和量化方法。

##### URL
[http://arxiv.org/abs/1807.04629](http://arxiv.org/abs/1807.04629)

##### PDF
[http://arxiv.org/pdf/1807.04629](http://arxiv.org/pdf/1807.04629)

