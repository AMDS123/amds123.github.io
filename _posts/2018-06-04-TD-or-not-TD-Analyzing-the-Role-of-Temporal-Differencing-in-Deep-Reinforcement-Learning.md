---
layout: post
title: "TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning"
date: 2018-06-04 16:16:51
categories: arXiv_AI
tags: arXiv_AI Sparse Reinforcement_Learning
author: Artemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, Thomas Brox
mathjax: true
---

* content
{:toc}

##### Abstract
Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.

##### Abstract (translated by Google)
我们对强化学习（RL）的理解已经通过几十年前使用表格表示和线性函数逼近器获得的理论和实证结果形成。这些结果表明，使用时间差分（TD）的RL方法优于直接蒙特卡洛估计（MC）。这些结果如何在深度RL中保持不变，这涉及感知复杂的环境和深度非线性模型？在本文中，我们重新审视TD在现代深度RL中的作用，使用特殊设计的环境来控制影响性能的特定因素，如奖励稀疏性，奖励延迟​​和任务的感知复杂性。将TD与无限时域MC进行比较时，我们能够在现代设置中重现经典结果。然而我们也发现，即使奖励稀少或延迟，有限时间MC也不逊色于TD。这使得MC在深RL中可以替代TD。

##### URL
[http://arxiv.org/abs/1806.01175](http://arxiv.org/abs/1806.01175)

##### PDF
[http://arxiv.org/pdf/1806.01175](http://arxiv.org/pdf/1806.01175)

