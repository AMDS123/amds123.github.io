---
layout: post
title: "Joint Text Embedding for Personalized Content-based Recommendation"
date: 2017-06-23 21:55:56
categories: arXiv_CL
tags: arXiv_CL Embedding Recommendation
author: Ting Chen, Liangjie Hong, Yue Shi, Yizhou Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned effectively from user interaction data, in many cases, such data is not available, especially for newly emerged items. In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. The text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with little or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can significantly improve the effectiveness of recommendation systems on real-world datasets.

##### Abstract (translated by Google)
学习良好的文本表示是很多推荐应用程序的关键。例子包括新闻推荐，每天不断出版推荐的文本。然而，大多数现有的推荐技术，如基于矩阵分解的方法，主要依靠交互历史来学习项目的表示。虽然可以从用户交互数据中有效地学习项目的潜在因素，但在许多情况下，这些数据是不可用的，特别是对于新出现的项目。在这项工作中，我们的目标是解决个性化推荐问题，提供全新的文本信息。我们将这个问题作为一个个性化的文本排序问题，提出了一个将文本嵌入与个性化推荐相结合的通用框架。用户和文本内容被嵌入到潜在的特征空间中。通过预测用户与项目的交互，文本嵌入功能可以被端对端地学习。为了缓解交互数据中的稀疏性，并且利用大量的文本数据而不需要用户交互，我们进一步提出了联合文本嵌入模型，其将无监督文本嵌入与组合模块相结合。实验结果表明，该模型可以显着提高推荐系统对实际数据集的有效性。

##### URL
[https://arxiv.org/abs/1706.01084](https://arxiv.org/abs/1706.01084)

##### PDF
[https://arxiv.org/pdf/1706.01084](https://arxiv.org/pdf/1706.01084)

