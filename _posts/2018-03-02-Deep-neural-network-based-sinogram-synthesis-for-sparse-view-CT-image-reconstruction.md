---
layout: post
title: "Deep-neural-network based sinogram synthesis for sparse-view CT image reconstruction"
date: 2018-03-02 03:38:47
categories: arXiv_CV
tags: arXiv_CV Sparse
author: Hoyeon Lee, Jongha Lee, Hyeongseok Kim, Byungchul Cho, Seungryong Cho
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, a number of approaches to low-dose computed tomography (CT) have been developed and deployed in commercialized CT scanners. Tube current reduction is perhaps the most actively explored technology with advanced image reconstruction algorithms. Sparse data sampling is another viable option to the low-dose CT, and sparse-view CT has been particularly of interest among the researchers in CT community. Since analytic image reconstruction algorithms would lead to severe image artifacts, various iterative algorithms have been developed for reconstructing images from sparsely view-sampled projection data. However, iterative algorithms take much longer computation time than the analytic algorithms, and images are usually prone to different types of image artifacts that heavily depend on the reconstruction parameters. Interpolation methods have also been utilized to fill the missing data in the sinogram of sparse-view CT thus providing synthetically full data for analytic image reconstruction. In this work, we introduce a deep-neural-network-enabled sinogram synthesis method for sparse-view CT, and show its outperformance to the existing interpolation methods and also to the iterative image reconstruction approach.

##### Abstract (translated by Google)
最近，许多低剂量计算机断层扫描（CT）的方法已经开发出来并部署在商业化的CT扫描仪中。采用先进的图像重建算法，缩小管电流可能是最积极探索的技术。稀疏数据采样是低剂量CT的另一种可行选择，稀疏CT在CT社区的研究人员中特别受到关注。由于分析图像重建算法会导致严重的图像伪影，因此已经开发了各种迭代算法用于从稀疏视图样本投影数据重建图像。然而，迭代算法比分析算法需要更长的计算时间，并且图像通常倾向于不同类型的严重依赖于重建参数的图像伪影。插值法也被用于填补稀疏CT正弦图中缺失的数据，从而为分析图像重建提供综合全面的数据。在这项工作中，我们介绍了一种基于深度神经网络的正弦图合成方法用于稀疏CT图像，并且显示了它与现有插值方法以及迭代图像重建方法的优越性。

##### URL
[http://arxiv.org/abs/1803.00694](http://arxiv.org/abs/1803.00694)

##### PDF
[http://arxiv.org/pdf/1803.00694](http://arxiv.org/pdf/1803.00694)

