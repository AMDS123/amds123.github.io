---
layout: post
title: "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?"
date: 2018-04-18 07:03:57
categories: arXiv_CL
tags: arXiv_CL Embedding NMT
author: Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Janani Padmanabhan, Graham Neubig
mathjax: true
---

* content
{:toc}

##### Abstract
The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 BLEU points in the most favorable setting.

##### Abstract (translated by Google)
神经机器翻译（NMT）系统的性能在低资源情况下经常受到影响，其中不能获得足够大规模的平行语料库。预先训练的单词嵌入对于提高自然语言分析任务的性能已经证明是非常宝贵的，因为自然语言分析任务经常遭受数据缺乏的困扰。然而，他们对NMT的效用还没有被广泛探讨。在这项工作中，我们执行五组实验，分析何时我们可以期望预先训练的词嵌入来帮助NMT任务。我们表明，在某些情况下，这种嵌入可能会出人意料 - 在最有利的环境下提供高达20 BLEU点的增益。

##### URL
[https://arxiv.org/abs/1804.06323](https://arxiv.org/abs/1804.06323)

##### PDF
[https://arxiv.org/pdf/1804.06323](https://arxiv.org/pdf/1804.06323)

