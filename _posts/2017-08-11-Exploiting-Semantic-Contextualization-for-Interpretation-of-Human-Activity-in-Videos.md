---
layout: post
title: "Exploiting Semantic Contextualization for Interpretation of Human Activity in Videos"
date: 2017-08-11 22:49:47
categories: arXiv_CV
tags: arXiv_CV Knowledge Inference Deep_Learning Relation
author: Sathyanarayanan N. Aakur, Fillipe DM de Souza, Sudeep Sarkar
mathjax: true
---

* content
{:toc}

##### Abstract
We use large-scale commonsense knowledge bases, e.g. ConceptNet, to provide context cues to establish semantic relationships among entities directly hypothesized from video signal, such as putative object and actions labels, and infer a deeper interpretation of events than what is directly sensed. One approach is to learn semantic relationships between objects and actions from training annotations of videos and as such, depend largely on statistics of the vocabulary in these annotations. However, the use of prior encoded commonsense knowledge sources alleviates this dependence on large annotated training datasets. We represent interpretations using a connected structure of basic detected (grounded) concepts, such as objects and actions, that are bound by semantics with other background concepts not directly observed, i.e. contextualization cues. We mathematically express this using the language of Grenander's pattern generator theory. Concepts are basic generators and the bonds are defined by the semantic relationships between concepts. We formulate an inference engine based on energy minimization using an efficient Markov Chain Monte Carlo that uses the ConceptNet in its move proposals to find these structures. Using three different publicly available datasets, Breakfast, CMU Kitchen and MSVD, whose distribution of possible interpretations span more than 150000 possible solutions for over 5000 videos, we show that the proposed model can generate video interpretations whose quality are comparable or better than those reported by approaches such as discriminative approaches, hidden Markov models, context free grammars, deep learning models, and prior pattern theory approaches, all of whom rely on learning from domain-specific training data.

##### Abstract (translated by Google)
我们使用大规模的常识性知识库，例如ConceptNet，提供上下文线索来建立直接从视频信号假设的实体之间的语义关系，比如推定的对象和动作标签，推断事件的深层解释，而不是直接感知的事件。一种方法是通过训练视频的注释来学习对象和动作之间的语义关系，并且因此在很大程度上取决于这些注释中词汇的统计。然而，使用先前编码的常识性知识源减轻了对大型注释训练数据集的依赖。我们使用基本检测（基础）概念（如对象和行为）的连接结构来表示解释，这些概念被语义绑定到不直接观察的其他背景概念，即情境化提示。我们用Grenander的模式生成器理论的语言在数学上表达了这一点。概念是基本的生成器，键是由概念之间的语义关系来定义的。我们制定了一个基于能量最小化的推理引擎，使用一个高效的马尔可夫链蒙特卡罗，在其移动提议中使用ConceptNet来找到这些结构。使用三种不同的公开可用的数据集，早餐，CMU厨房和MSVD，其可能的解释分布跨越超过150000个可能的解决方案超过5000个视频，我们表明，该模型可以生成视频解释质量可比或比方法，如歧视性方法，隐马尔可夫模型，上下文无关语法，深度学习模型，以及模式理论的方法，他们都依赖于领域特定的训练数据的学习。

##### URL
[https://arxiv.org/abs/1708.03725](https://arxiv.org/abs/1708.03725)

##### PDF
[https://arxiv.org/pdf/1708.03725](https://arxiv.org/pdf/1708.03725)

