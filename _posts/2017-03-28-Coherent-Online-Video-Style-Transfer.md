---
layout: post
title: "Coherent Online Video Style Transfer"
date: 2017-03-28 09:04:15
categories: arXiv_CV
tags: arXiv_CV Style_Transfer Optimization Quantitative
author: Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, Gang Hua
mathjax: true
---

* content
{:toc}

##### Abstract
Training a feed-forward network for fast neural style transfer of images is proven to be successful. However, the naive extension to process video frame by frame is prone to producing flickering results. We propose the first end-to-end network for online video style transfer, which generates temporally coherent stylized video sequences in near real-time. Two key ideas include an efficient network by incorporating short-term coherence, and propagating short-term coherence to long-term, which ensures the consistency over larger period of time. Our network can incorporate different image stylization networks. We show that the proposed method clearly outperforms the per-frame baseline both qualitatively and quantitatively. Moreover, it can achieve visually comparable coherence to optimization-based video style transfer, but is three orders of magnitudes faster in runtime.

##### Abstract (translated by Google)
训练前馈网络以实现图像的快速神经风格传输被证明是成功的。然而，逐帧处理视频的幼稚扩展容易产生闪烁的结果。我们提出了第一个用于在线视频风格转换的端到端网络，它近乎实时地产生时间上一致的风格化的视频序列。两个关键思想包括一个有效的网络，通过融合短期的连贯性，将短期的连贯性传播到长期，从而确保在较长一段时间内的一致性。我们的网络可以包含不同的图像风格化网络。我们表明，提出的方法明显优于每帧基线定性和定量。而且，它可以在视觉上实现与基于优化的视频风格转换相当的连贯性，但在运行时速度要快三个数量级。

##### URL
[https://arxiv.org/abs/1703.09211](https://arxiv.org/abs/1703.09211)

##### PDF
[https://arxiv.org/pdf/1703.09211](https://arxiv.org/pdf/1703.09211)

