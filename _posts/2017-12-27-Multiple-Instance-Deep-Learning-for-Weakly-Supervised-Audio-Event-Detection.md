---
layout: post
title: "Multiple Instance Deep Learning for Weakly Supervised Audio Event Detection"
date: 2017-12-27 20:30:45
categories: arXiv_SD
tags: arXiv_SD Weakly_Supervised Embedding CNN RNN Deep_Learning Detection
author: Shao-Yen Tseng, Juncheng Li, Yun Wang, Joseph Szurley, Florian Metze, Samarjit Das
mathjax: true
---

* content
{:toc}

##### Abstract
State-of-the-art audio event detection (AED) systems rely on supervised learning using strongly labeled data. However, this dependence severely limits scalability to large-scale datasets where fine resolution annotations are too expensive to obtain. In this paper, we propose a multiple instance learning (MIL) framework for multi-class AED using weakly annotated labels. The proposed MIL framework uses audio embeddings extracted from a pre-trained convolutional neural network as input features. We show that by using audio embeddings the MIL framework can be implemented using a simple DNN with performance comparable to recurrent neural networks. We evaluate our approach by training an audio tagging system using a subset of AudioSet, which is a large collection of weakly labeled YouTube video excerpts. Combined with a late-fusion approach, we improve the F1 score of a baseline audio tagging system by 17\%. We show that audio embeddings extracted by the convolutional neural networks significantly boost the performance of all MIL models. This framework reduces the model complexity of the AED system and is suitable for applications where computational resources are limited.

##### Abstract (translated by Google)
最先进的音频事件检测（AED）系统依靠使用强标记数据的监督式学习。但是，这种依赖性严重限制了高分辨率注释过于昂贵的大规模数据集的可扩展性。在本文中，我们提出了一种使用弱标注标签的多级AED的多实例学习（MIL）框架。所提出的MIL框架使用从预先训练的卷积神经网络提取的音频嵌入作为输入特征。我们表明，通过使用音频嵌入，MIL框架可以使用简单的DNN来实现，其性能可与递归神经网络相媲美。我们通过使用音频集的一个子集来训练音频标记系统来评估我们的方法，所述音频集是弱标记的YouTube视频摘录的大量集合。结合后期融合方法，我们将基线音频标签系统的F1分数提高了17％。我们展示了由卷积神经网络提取的音频嵌入显着地提高了所有MIL模型的性能。该框架降低了AED系统的模型复杂度，适用于计算资源有限的应用。

##### URL
[https://arxiv.org/abs/1712.09673](https://arxiv.org/abs/1712.09673)

##### PDF
[https://arxiv.org/pdf/1712.09673](https://arxiv.org/pdf/1712.09673)

