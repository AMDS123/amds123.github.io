---
layout: post
title: "Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for Transferable Chemical Property Prediction"
date: 2018-03-18 13:50:02
categories: arXiv_AI
tags: arXiv_AI Knowledge Speech_Recognition Transfer_Learning RNN Prediction Recognition
author: Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas
mathjax: true
---

* content
{:toc}

##### Abstract
With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry, data is inherently small and fragmented. In this work, we develop an approach of using rule-based knowledge for training ChemNet, a transferable and generalizable deep neural network for chemical property prediction that learns in a weak-supervised manner from large unlabeled chemical databases. When coupled with transfer learning approaches to predict other smaller datasets for chemical properties that it was not originally trained on, we show that ChemNet's accuracy outperforms contemporary DNN models that were trained using conventional supervised learning. Furthermore, we demonstrate that the ChemNet pre-training approach is equally effective on both CNN (Chemception) and RNN (SMILES2vec) models, indicating that this approach is network architecture agnostic and is effective across multiple data modalities. Our results indicate a pre-trained ChemNet that incorporates chemistry domain knowledge, enables the development of generalizable neural networks for more accurate prediction of novel chemical properties.

##### Abstract (translated by Google)
通过访问大型数据集，深度神经网络（DNN）在图像和语音识别任务中实现了人类级精度。然而，在化学中，数据固有地小而且分散。在这项工作中，我们开发了一种使用基于规则的知识来培训ChemNet的方法，ChemNet是一种可转移且可推广的化学性质预测深度神经网络，可以从大型未标记的化学数据库中以弱监督的方式进行学习。当结合转移学习方法来预测其他较小的化学特性数据集时，我们证明ChemNet的准确性优于使用传统监督学习训练的当代DNN模型。此外，我们证明了ChemNet预训练方法在CNN（Chemception）和RNN（SMILES2vec）模型上同样有效，表明这种方法与网络架构无关，并且在多种数据模式中都是有效的。我们的研究结果表明，预先培训的ChemNet结合了化学领域的知识，可以开发通用的神经网络，以更准确地预测新的化学特性。

##### URL
[http://arxiv.org/abs/1712.02734](http://arxiv.org/abs/1712.02734)

##### PDF
[http://arxiv.org/pdf/1712.02734](http://arxiv.org/pdf/1712.02734)

