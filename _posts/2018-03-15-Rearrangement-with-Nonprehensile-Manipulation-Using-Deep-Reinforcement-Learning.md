---
layout: post
title: "Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning"
date: 2018-03-15 14:00:24
categories: arXiv_AI
tags: arXiv_AI Face Reinforcement_Learning
author: Weihao Yuan, Johannes A. Stork, Danica Kragic, Michael Y. Wang, Kaiyu Hang
mathjax: true
---

* content
{:toc}

##### Abstract
Rearranging objects on a tabletop surface by means of nonprehensile manipulation is a task which requires skillful interaction with the physical world. Usually, this is achieved by precisely modeling physical properties of the objects, robot, and the environment for explicit planning. In contrast, as explicitly modeling the physical environment is not always feasible and involves various uncertainties, we learn a nonprehensile rearrangement strategy with deep reinforcement learning based on only visual feedback. For this, we model the task with rewards and train a deep Q-network. Our potential field-based heuristic exploration strategy reduces the amount of collisions which lead to suboptimal outcomes and we actively balance the training set to avoid bias towards poor examples. Our training process leads to quicker learning and better performance on the task as compared to uniform exploration and standard experience replay. We demonstrate empirical evidence from simulation that our method leads to a success rate of 85%, show that our system can cope with sudden changes of the environment, and compare our performance with human level performance.

##### Abstract (translated by Google)
通过非易懂的操作重新排列桌面上的物体是一项需要与物理世界进行巧妙交互的任务。通常，这是通过精确建模物体的物理属性，机器人和明确计划的环境来实现的。相反，由于明确地建模物理环境并不总是可行的并且涉及各种不确定性，所以我们学习了基于仅仅视觉反馈的深度强化学习的非易失性重排策略。为此，我们用奖励来模拟任务并训练一个深度的Q网络。我们潜在的基于领域的启发式探索策略减少了导致次优结果的冲突量，并且我们主动平衡了训练集以避免对不良事例的偏见。与统一探索和标准体验重播相比，我们的培训过程可以加快学习速度，提高任务性能。我们通过仿真证明经验证据表明，我们的方法导致85％的成功率，表明我们的系统能够应对环境的突然变化，并将我们的表现与人类的表现进行比较。

##### URL
[https://arxiv.org/abs/1803.05752](https://arxiv.org/abs/1803.05752)

##### PDF
[https://arxiv.org/pdf/1803.05752](https://arxiv.org/pdf/1803.05752)

