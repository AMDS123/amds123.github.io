---
layout: post
title: "Modelling Temporal Information Using Discrete Fourier Transform for Video Classification"
date: 2016-08-17 00:48:55
categories: arXiv_CV
tags: arXiv_CV Action_Recognition CNN Video_Classification Classification Recognition
author: Haimin Zhang
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, video classification attracts intensive research efforts. However, most existing works are based on framelevel visual features, which might fail to model the temporal information, e.g. characteristics accumulated along time. In order to capture video temporal information, we propose to analyse features in frequency domain transformed by discrete Fourier transform (DFT features). Frame-level features are firstly extract by a pre-trained deep convolutional neural network (CNN). Then, time domain features are transformed and interpolated into DFT features. CNN and DFT features are further encoded by using different pooling methods and fused for video classification. In this way, static image features extracted from a pre-trained deep CNN and temporal information represented by DFT features are jointly considered for video classification. We test our method for video emotion classification and action recognition. Experimental results demonstrate that combining DFT features can effectively capture temporal information and therefore improve the performance of both video emotion classification and action recognition. Our approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset) and competitive results on UCF-101.

##### Abstract (translated by Google)
最近，视频分类吸引了大量的研究工作。然而，大多数现有的作品是基于框架级的视觉特征，这可能不能模拟时间信息，例如，随时间积累的特征。为了捕获视频时间信息，我们建议分析通过离散傅里叶变换（DFT特征）变换的频域中的特征。帧级特征首先被预先训练的深度卷积神经网络（CNN）提取。然后，时域特征被转换并插入到DFT特征中。 CNN和DFT特征通过使用不同的汇聚方法被进一步编码并且被融合用于视频分类。这样，从预先训练的深度CNN中提取的静态图像特征和由DFT特征表示的时间信息被联合考虑用于视频分类。我们测试我们的视频情感分类和动作识别的方法。实验结果表明，结合DFT特征可以有效捕捉时间信息，从而提高视频情感分类和动作识别的性能。我们的方法已经在最大的视频情绪数据集（VideoEmotion-8数据集）和UCF-101上的竞争结果上取得了最先进的性能。

##### URL
[https://arxiv.org/abs/1603.06182](https://arxiv.org/abs/1603.06182)

##### PDF
[https://arxiv.org/pdf/1603.06182](https://arxiv.org/pdf/1603.06182)

