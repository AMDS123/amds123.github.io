---
layout: post
title: "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling"
date: 2018-07-09 00:15:14
categories: arXiv_CV
tags: arXiv_CV Adversarial Face Reinforcement_Learning Caption
author: Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.

##### Abstract (translated by Google)
尽管在视觉字幕中已经取得了令人瞩目的成果，但是从照片流生成抽象故事的任务仍然是一个小问题。与字幕不同，故事具有更具表现力的语言风格，并包含许多未出现在图像中的虚构概念。因此，它对行为克隆算法提出了挑战。此外，由于评估故事质量的自动指标的局限性，具有手工制作奖励的强化学习方法在获得整体性能提升方面也面临困难。因此，我们提出了一种对抗性奖励学习（AREL）框架，用于从人类示范中学习隐含奖励函数，然后利用学习到的奖励函数优化策略搜索。虽然自动评估表明，在克隆专家行为方面，最先进（SOTA）方法的性能略有提升，但人类评估表明，与SOTA系统相比，我们的方法在生成更像人类的故事方面取得了显着的进步。

##### URL
[https://arxiv.org/abs/1804.09160](https://arxiv.org/abs/1804.09160)

##### PDF
[https://arxiv.org/pdf/1804.09160](https://arxiv.org/pdf/1804.09160)

