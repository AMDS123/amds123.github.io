---
layout: post
title: "Code-Switching Language Modeling using Syntax-Aware Multi-Task Learning"
date: 2018-05-30 16:37:43
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model Prediction
author: Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, Pascale Fung
mathjax: true
---

* content
{:toc}

##### Abstract
Lack of text data has been the major issue on code-switching language modeling. In this paper, we introduce multi-task learning based language model which shares syntax representation of languages to leverage linguistic information and tackle the low resource data issue. Our model jointly learns both language modeling and Part-of-Speech tagging on code-switched utterances. In this way, the model is able to identify the location of code-switching points and improves the prediction of next word. Our approach outperforms standard LSTM based language model, with an improvement of 9.7% and 7.4% in perplexity on SEAME Phase I and Phase II dataset respectively.

##### Abstract (translated by Google)
缺少文本数据一直是代码切换语言建模的主要问题。在本文中，我们介绍了基于多任务学习的语言模型，它共享语言的语法表示以利用语言信息并解决低资源数据问题。我们的模型共同学习语言建模和代码交换话语的词性标注。通过这种方式，该模型能够识别码转换点的位置并改进下一个词的预测。我们的方法优于基于标准LSTM的语言模型，分别在SEAME阶段I和阶段II数据集中困惑性提高了9.7％和7.4％。

##### URL
[http://arxiv.org/abs/1805.12070](http://arxiv.org/abs/1805.12070)

##### PDF
[http://arxiv.org/pdf/1805.12070](http://arxiv.org/pdf/1805.12070)

