---
layout: post
title: "Building a Web-Scale Dependency-Parsed Corpus from CommonCrawl"
date: 2018-02-28 18:14:30
categories: arXiv_CL
tags: arXiv_CL Embedding
author: Alexander Panchenko, Eugen Ruppert, Stefano Faralli, Simone Paolo Ponzetto, Chris Biemann
mathjax: true
---

* content
{:toc}

##### Abstract
We present DepCC, the largest-to-date linguistically analyzed corpus in English including 365 million documents, composed of 252 billion tokens and 7.5 billion of named entity occurrences in 14.3 billion sentences from a web-scale crawl of the \textsc{Common Crawl} project. The sentences are processed with a dependency parser and with a named entity tagger and contain provenance information, enabling various applications ranging from training syntax-based word embeddings to open information extraction and question answering. We built an index of all sentences and their linguistic meta-data enabling quick search across the corpus. We demonstrate the utility of this corpus on the verb similarity task by showing that a distributional model trained on our corpus yields better results than models trained on smaller corpora, like Wikipedia. This distributional model outperforms the state of art models of verb similarity trained on smaller corpora on the SimVerb3500 dataset.

##### Abstract (translated by Google)
我们使用英语最大的语言分析语料库（包括36500万个文档，包括2550亿个令牌和75亿个命名实体出现在143亿个句子中的DepCC，从\ textsc {Common Crawl}项目。这些句子通过一个依赖解析器和一个命名实体标记器进行处理，并包含起源信息，从而实现从基于语法的单词嵌入到开放式信息抽取和问题回答等各种应用。我们建立了所有句子及其语言元数据的索引，以便在整个语料库中快速搜索。我们通过展示在我们的语料库上训练的分布式模型产生比在较小的语料库上训练的模型（如维基百科）更好的结果来证明该语料库对动词相似性任务的效用。该分布模型胜过SimVerb3500数据集上较小语料库上训练的动词相似性的艺术模型状态。

##### URL
[http://arxiv.org/abs/1710.01779](http://arxiv.org/abs/1710.01779)

##### PDF
[http://arxiv.org/pdf/1710.01779](http://arxiv.org/pdf/1710.01779)

