---
layout: post
title: "Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration"
date: 2018-07-10 04:55:45
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles
mathjax: true
---

* content
{:toc}

##### Abstract
Our goal is for a robot to execute a previously unseen task based on a single video demonstration of the task. The success of our approach relies on the principle of transferring knowledge from seen tasks to unseen ones with similar semantics. More importantly, we hypothesize that to successfully execute a complex task from a single video demonstration, it is necessary to explicitly incorporate compositionality to the model. To test our hypothesis, we propose Neural Task Graph (NTG) Networks, which use task graph as the intermediate representation to modularize the representations of both the video demonstration and the derived policy. We show this formulation achieves strong inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. We further show that the same principle is applicable to real-world videos. We show that NTG can improve data efficiency of few-shot activity understanding in the Breakfast Dataset.

##### Abstract (translated by Google)
我们的目标是让机器人根据任务的单个视频演示执行以前看不见的任务。我们的方法的成功依赖于将知识从看到的任务转移到具有类似语义的看不见的任务的原则。更重要的是，我们假设要从单个视频演示成功执行复杂任务，有必要明确地将组合性合并到模型中。为了验证我们的假设，我们提出了神经任务图（NTG）网络，它使用任务图作为中间表示来模块化视频演示和派生策略的表示。我们证明了这个公式在两个复杂的任务上实现了强大的任务间泛化：BulletPhysics中的块堆叠和AI2-THOR中的对象收集。我们进一步表明，相同的原则适用于真实世界的视频。我们证明NTG可以提高早餐数据集中少数活动理解的数据效率。

##### URL
[http://arxiv.org/abs/1807.03480](http://arxiv.org/abs/1807.03480)

##### PDF
[http://arxiv.org/pdf/1807.03480](http://arxiv.org/pdf/1807.03480)

