---
layout: post
title: "Stochastic Gradient Descent with Hyperbolic-Tangent Decay"
date: 2018-06-05 10:14:01
categories: arXiv_CV
tags: arXiv_CV RNN Gradient_Descent
author: Bo Yang Hsueh, Wei Li, I-Chen Wu
mathjax: true
---

* content
{:toc}

##### Abstract
Learning rate scheduler has been a critical issue in the deep neural network training. Several schedulers and methods have been proposed, including step decay scheduler, adaptive method, cosine scheduler and cyclical scheduler. This paper proposes a new scheduling method, named hyperbolic-tangent decay (HTD). We run experiments on several benchmarks such as: ResNet, Wide ResNet and DenseNet for CIFAR-10 and CIFAR-100 datasets, LSTM for PAMAP2 dataset, ResNet on ImageNet and Fashion-MNIST datasets. In our experiments, HTD outperforms step decay and cosine scheduler in nearly all cases, while requiring less hyperparameters than step decay, and more flexible than cosine scheduler.

##### Abstract (translated by Google)
学习速率调度器一直是深度神经网络训练中的关键问题。已经提出了几种调度器和方法，包括步骤衰减调度器，自适应方法，余弦调度器和循环调度器。本文提出了一种新的调度方法，称为双曲正切衰减（HTD）。我们在几个基准上运行实验，例如：ResNet，Wide ResNet和DenseNet，用于CIFAR-10和CIFAR-100数据集，LSTM用于PAMAP2数据集，ResNet on ImageNet和Fashion-MNIST数据集。在我们的实验中，几乎在所有情况下，HTD比逐步衰减和余弦调度器都要优越，同时比阶跃衰减需要更少的超参数，并且比余弦调度器更灵活。

##### URL
[http://arxiv.org/abs/1806.01593](http://arxiv.org/abs/1806.01593)

##### PDF
[http://arxiv.org/pdf/1806.01593](http://arxiv.org/pdf/1806.01593)

