---
layout: post
title: "Adversarial Vulnerability of Neural Networks Increases With Input Dimension"
date: 2018-02-05 14:36:44
categories: arXiv_CV
tags: arXiv_CV Regularization Adversarial Face Prediction
author: Carl-Johann Simon-Gabriel, Yann Ollivier, Bernhard Sch&#xf6;lkopf, L&#xe9;on Bottou, David Lopez-Paz
mathjax: true
---

* content
{:toc}

##### Abstract
Over the past four years, neural networks have proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when seen as a function of the inputs. For most current network architectures, we prove that the $\ell_1$-norm of these gradients grows as the square root of the input-size. These nets therefore become increasingly vulnerable with growing image size. Over the course of our analysis we rediscover and generalize double-backpropagation, a technique that penalizes large gradients in the loss surface to reduce adversarial vulnerability and increase generalization performance. We show that this regularization-scheme is equivalent at first order to training with adversarial noise. Finally, we demonstrate that replacing strided by average-pooling layers decreases adversarial vulnerability. Our proofs rely on the network's weight-distribution at initialization, but extensive experiments confirm their conclusions after training.

##### Abstract (translated by Google)
在过去的四年中，神经网络已经被证明容易受到对抗性图像的攻击：有针对性但不可察觉的图像扰动导致截然不同的预测。我们表明，敌对的脆弱性随着训练目标的梯度而增加，当被视为输入的函数时。对于大多数当前的网络体系结构，我们证明这些渐变的$ \ ell_1 $ -norm成长为输入大小的平方根。因此，随着图像尺寸的增加，这些网络变得越来越脆弱在我们的分析过程中，我们重新发现并推广了双反向传播技术，该技术惩罚损失表面中的大梯度，以减少对抗性漏洞并提高泛化性能。我们表明，这个正则化方案是相当于一开始对敌人的噪音训练。最后，我们证明替换平均池图层会降低对抗性漏洞。我们的证明依赖于网络在初始化时的权重分布，但是广泛的实验在训练之后证实了他们的结论。

##### URL
[http://arxiv.org/abs/1802.01421](http://arxiv.org/abs/1802.01421)

##### PDF
[http://arxiv.org/pdf/1802.01421](http://arxiv.org/pdf/1802.01421)

