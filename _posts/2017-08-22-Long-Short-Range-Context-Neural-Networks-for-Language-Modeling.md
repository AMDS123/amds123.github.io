---
layout: post
title: "Long-Short Range Context Neural Networks for Language Modeling"
date: 2017-08-22 10:26:41
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow
mathjax: true
---

* content
{:toc}

##### Abstract
The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.

##### Abstract (translated by Google)
语言建模技术的目标是从训练语料中捕捉自然语言的统计和结构特性。这个任务通常涉及短距离依赖性的学习，短距离依赖性通常对语言的语法属性和/或长程依赖性进行建模，这些语义本质上是语义的。我们在本文中提出了一种新的多跨度架构，它将短期和长期的上下文信息分别建模，同时将其动态合并，以执行语言建模任务。这是通过一个新颖的经常性长 - 短距离上下文（LSRC）网络来​​完成的，该网络使用随时间发展的两个单独的隐藏状态来明确地模拟本地（短）和全局（长）上下文。这种新的架构是对长短期记忆网络（LSTM）的改编，以考虑到语言特性。在Penn Treebank（PTB）和大文本压缩基准（LTCB）语料库上进行的大量实验显示，与最先进的语言建模技术相比，困惑性显着减少。

##### URL
[https://arxiv.org/abs/1708.06555](https://arxiv.org/abs/1708.06555)

##### PDF
[https://arxiv.org/pdf/1708.06555](https://arxiv.org/pdf/1708.06555)

