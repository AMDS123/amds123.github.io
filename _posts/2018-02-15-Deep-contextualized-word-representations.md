---
layout: post
title: "Deep contextualized word representations"
date: 2018-02-15 00:05:11
categories: arXiv_CL
tags: arXiv_CL Sentiment Language_Model
author: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer
mathjax: true
---

* content
{:toc}

##### Abstract
We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.

##### Abstract (translated by Google)
我们引入一种新的类型的深层上下文化的词表示，它们模拟（1）词使用的复杂特征（例如语法和语义）和（2）这些使用如何在语言上下文中变化（即模拟多义性）。我们的词向量是深度双向语言模型（biLM）的内部状态的学习函数，其在大型文本语料库上被预先训练。我们表明，这些表示可以很容易地添加到现有模型，并显着改善六个具有挑战性的NLP问题的艺术状态，包括问题解答，文本包含和情感分析。我们还提供了一个分析，显示暴露预训练网络的深层内部是至关重要的，允许下游模型混合不同类型的半监督信号。

##### URL
[https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365)

##### PDF
[https://arxiv.org/pdf/1802.05365](https://arxiv.org/pdf/1802.05365)

