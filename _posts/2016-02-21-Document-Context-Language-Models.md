---
layout: post
title: "Document Context Language Models"
date: 2016-02-21 23:46:44
categories: arXiv_CL
tags: arXiv_CL Language_Model
author: Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, Jacob Eisenstein
mathjax: true
---

* content
{:toc}

##### Abstract
Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence.

##### Abstract (translated by Google)
文本文档是按照多层次的细节来构建的：单个词语是通过语法相关联的，而较大的单位文本则通过语篇结构相关联。现有的语言模型通常不能解释话语结构，但是如果我们有语言模型来奖励连贯性并且产生连贯的文本，这是至关重要的。我们提出并经验性地评估一套多层递归的神经网络语言模型，称为文档 - 语境语言模型（DCLM），它包含句子内外的语境信息。与词级递归神经网络语言模型相比，DCLM模型可以获得稍好的预测可能性，并且可以更好地评估文档的一致性。

##### URL
[https://arxiv.org/abs/1511.03962](https://arxiv.org/abs/1511.03962)

##### PDF
[https://arxiv.org/pdf/1511.03962](https://arxiv.org/pdf/1511.03962)

