---
layout: post
title: "Binary Constrained Deep Hashing Network for Image Retrieval without Human Intervention"
date: 2018-02-21 06:20:59
categories: arXiv_CV
tags: arXiv_CV Image_Retrieval Knowledge Attention
author: Thanh-Toan Do, Dang-Khoa Le Tan, Trung Pham, Tuan Hoang, Ngai-Man Cheung
mathjax: true
---

* content
{:toc}

##### Abstract
Learning compact binary codes for image retrieval problem using deep neural networks has attracted increasing attention recently. However, training deep hashing networks is challenging due to the binary constraints on the hash codes, the similarity preserving properties, and the requirement for a vast amount of labelled images. To the best of our knowledge, none of the existing methods has tackled all of these challenges completely in a unified framework. In this work, we propose a novel end-toend deep hashing approach, which is trained to produce binary codes directly from image pixels without human intervention. In particular, our main contribution is to propose a novel pairwise loss function, which simultaneously encodes the distances between pairs of binary codes, and the binary quantization error. We propose an efficient parameter learning algorithm for this loss function. In addition, to provide similar/dissimilar images for our pairwise loss function, we exploit 3D models reconstructed from unlabeled images for automatic generation of enormous similar/dissimilar pairs. Extensive experiments on three image retrieval benchmark datasets demonstrate the superior performance of the proposed method.

##### Abstract (translated by Google)
学习使用深度神经网络的图像检索紧缩二进制编码问题近来引起越来越多的关注。然而，由于哈希码的二进制约束，相似性保持属性以及对大量标记图像的需求，训练深度哈希网络具有挑战性。据我们所知，现有的方法都没有在统一的框架中完全解决所有这些挑战。在这项工作中，我们提出了一种新颖的端到端深度哈希方法，该方法经过训练，无需人工干预即可直接从图像像素生成二进制代码。特别是，我们的主要贡献是提出一种新的成对损失函数，它同时编码二进制码对之间的距离以及二进制量化误差。我们提出了一个有效的参数学习算法来处理这个损失函数此外，为了为我们的成对损失函数提供相似/不相似的图像，我们利用从未标记图像重建的3D模型来自动生成巨大的相似/不相似对。在三个图像检索基准数据集上的大量实验证明了所提出方法的优越性能。

##### URL
[http://arxiv.org/abs/1802.07437](http://arxiv.org/abs/1802.07437)

##### PDF
[http://arxiv.org/pdf/1802.07437](http://arxiv.org/pdf/1802.07437)

