---
layout: post
title: "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"
date: 2018-01-02 05:22:06
categories: arXiv_CV
tags: arXiv_CV Review Adversarial Survey Deep_Learning
author: Naveed Akhtar, Ajmal Mian
mathjax: true
---

* content
{:toc}

##### Abstract
Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.

##### Abstract (translated by Google)
深度学习是当前机器学习和人工智能兴起的核心。在计算机视觉领域，它已经成为从自驾车到监视和安全等应用的主力。尽管深度神经网络在解决复杂问题方面已经取得了惊人的成就（通常超出了人类的能力），但最近的研究表明，它们容易受到对手输入的细微扰动形式的敌对攻击，导致模型预测错误的输出。对于图像来说，这样的扰动通常太小而不能被察觉，但它们完全欺骗了深度学习模型。敌对攻击对实践中深度学习的成功构成严重威胁。这个事实导致了这方面的大量捐款。本文介绍了计算机视觉领域深入学习对抗攻击的第一次综合性调查。我们回顾设计对抗性攻击的作品，分析这些攻击的存在并提出防御。为了强调在实际情况下敌对攻击是可能的，我们分别回顾在现实世界情景下评估敌对攻击的贡献。最后，我们借鉴文献来提供一个更广阔的研究方向的展望。

##### URL
[http://arxiv.org/abs/1801.00553](http://arxiv.org/abs/1801.00553)

##### PDF
[http://arxiv.org/pdf/1801.00553](http://arxiv.org/pdf/1801.00553)

