---
layout: post
title: "A Bimodal Learning Approach to Assist Multi-sensory Effects Synchronization"
date: 2018-04-28 15:37:41
categories: arXiv_AI
tags: arXiv_AI Prediction
author: Raphael Abreu, Joel dos Santos, Eduardo Bezerra
mathjax: true
---

* content
{:toc}

##### Abstract
In mulsemedia applications, traditional media content (text, image, audio, video, etc.) can be related to media objects that target other human senses (e.g., smell, haptics, taste). Such applications aim at bridging the virtual and real worlds through sensors and actuators. Actuators are responsible for the execution of sensory effects (e.g., wind, heat, light), which produce sensory stimulations on the users. In these applications sensory stimulation must happen in a timely manner regarding the other traditional media content being presented. For example, at the moment in which an explosion is presented in the audiovisual content, it may be adequate to activate actuators that produce heat and light. It is common to use some declarative multimedia authoring language to relate the timestamp in which each media object is to be presented to the execution of some sensory effect. One problem in this setting is that the synchronization of media objects and sensory effects is done manually by the author(s) of the application, a process which is time-consuming and error prone. In this paper, we present a bimodal neural network architecture to assist the synchronization task in mulsemedia applications. Our approach is based on the idea that audio and video signals can be used simultaneously to identify the timestamps in which some sensory effect should be executed. Our learning architecture combines audio and video signals for the prediction of scene components. For evaluation purposes, we construct a dataset based on Google's AudioSet. We provide experiments to validate our bimodal architecture. Our results show that the bimodal approach produces better results when compared to several variants of unimodal architectures.

##### Abstract (translated by Google)
在mulsemedia应用程序中，传统媒体内容（文本，图像，音频，视频等）可以与针对其他人类感官（例如，气味，触觉，味觉）的媒体对象相关联。这种应用旨在通过传感器和执行器来弥合虚拟世界和现实世界。执行器负责执行对用户产生感官刺激的感官效果（例如，风，热，光）。在这些应用中，感官刺激必须及时地对其他传统媒体内容进行呈现。例如，在视听内容中出现爆炸的时刻，可能足以激活产生热量和光线的致动器。通常使用一些声明性的多媒体创作语言来将每个媒体对象将要呈现的时间戳与某种感官效果的执行关联起来。此设置中的一个问题是媒体对象和感官效果的同步是由应用程序的作者手动完成的，这是一个耗时且易出错的过程。在本文中，我们提出了一种双模神经网络架构来协助mulsemedia应用中的同步任务。我们的方法基于这样的想法：可以同时使用音频和视频信号来识别应该执行一些感官效果的时间戳。我们的学习架构结合了音频和视频信号来预测场景组件。出于评估目的，我们构建基于Google AudioSet的数据集。我们提供实验来验证我们的双峰体系结构。我们的研究结果表明，与单峰架构的几种变体相比，双峰法可以产生更好的结果。

##### URL
[https://arxiv.org/abs/1804.10822](https://arxiv.org/abs/1804.10822)

##### PDF
[https://arxiv.org/pdf/1804.10822](https://arxiv.org/pdf/1804.10822)

