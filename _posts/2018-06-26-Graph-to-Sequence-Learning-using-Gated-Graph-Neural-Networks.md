---
layout: post
title: "Graph-to-Sequence Learning using Gated Graph Neural Networks"
date: 2018-06-26 08:08:30
categories: arXiv_CL
tags: arXiv_CL
author: Daniel Beck, Gholamreza Haffari, Trevor Cohn
mathjax: true
---

* content
{:toc}

##### Abstract
Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on this setting obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work, we propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results show that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.

##### Abstract (translated by Google)
许多NLP应用程序可以被定义为图到序列学习问题。与以语法为基础的方法相比，以前在这一环境中提出神经架构的工作取得了令人满意的结果，但仍依赖于线性化启发式和/或标准循环网络来实现最佳性能。在这项工作中，我们提出了一个新的模型，对图中包含的完整结构信息进行编码。我们的架构将最近提出的门控图神经网络与输入转换结合起来，该转换允许节点和边具有自己的隐藏表示，同时解决了以前工作中存在的参数爆炸问题。实验结果表明，我们的模型优于AMR图和基于语法的神经机器翻译生成的强基线。

##### URL
[http://arxiv.org/abs/1806.09835](http://arxiv.org/abs/1806.09835)

##### PDF
[http://arxiv.org/pdf/1806.09835](http://arxiv.org/pdf/1806.09835)

