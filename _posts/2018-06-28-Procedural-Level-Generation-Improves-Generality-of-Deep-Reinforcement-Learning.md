---
layout: post
title: "Procedural Level Generation Improves Generality of Deep Reinforcement Learning"
date: 2018-06-28 01:16:11
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, Sebastian Risi
mathjax: true
---

* content
{:toc}

##### Abstract
Over the last few years, deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when networks are trained in a fixed environment, such as a single level in a video game, it will usually overfit and fail to generalize to new levels. When RL agents overfit, even slight modifications to the environment can result in poor agent performance. In this paper, we present an approach to prevent overfitting by generating more general agent controllers, through training the agent on a completely new and procedurally generated level each episode. The level generator generate levels whose difficulty slowly increases in response to the observed performance of the agent. Our results show that this approach can learn policies that generalize better to other procedurally generated levels, compared to policies trained on fixed levels.

##### Abstract (translated by Google)
在过去的几年中，深层强化学习（RL）在各种领域显示出令人印象深刻的结果，直接从高维感官流学习。但是，当网络在固定的环境下进行培训时，例如视频游戏中的单一级别，它通常会过度适应并且无法推广到新的水平。当RL代理过度配备时，即使对环境稍作修改也会导致代理性能不佳。在本文中，我们提出了一种通过生成更全面的代理控制器来防止过度拟合的方法，通过在每个情节中以全新的和程序化的方式对代理进行培训。级别生成器生成难度缓慢增加以响应代理观察到的性能的级别。我们的研究结果表明，与在固定水平上培训的政策相比，这种方法可以学习更好地推广到其他程序化生成水平的政策。

##### URL
[http://arxiv.org/abs/1806.10729](http://arxiv.org/abs/1806.10729)

##### PDF
[http://arxiv.org/pdf/1806.10729](http://arxiv.org/pdf/1806.10729)

