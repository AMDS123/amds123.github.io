---
layout: post
title: "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models"
date: 2016-09-30 22:20:39
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition RNN Prediction Recognition
author: Viktoriya Krakovna, Finale Doshi-Velez
mathjax: true
---

* content
{:toc}

##### Abstract
As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks (RNNs), state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.

##### Abstract (translated by Google)
随着深度神经网络不断革新各种应用领域，人们越来越有兴趣使这些强大的模型更易于理解和解释，并缩小好的和坏的预测的原因。我们专注于循环神经网络（RNN），语音识别和翻译的最新模型。我们提高可解释性的方法是将RNN与隐马尔可夫模型（HMM）相结合，HMM是一个更简单，更透明的模型。我们探索RNN和HMM的各种组合：在LSTM状态上训练的HMM;首先训练HMM的混合模型，然后给小的LSTM提供HMM状态分布并训练填补HMM性能的差距;和一个联合训练的混合模型。我们发现LSTM和HMM学习有关文本特征的补充信息。

##### URL
[https://arxiv.org/abs/1606.05320](https://arxiv.org/abs/1606.05320)

##### PDF
[https://arxiv.org/pdf/1606.05320](https://arxiv.org/pdf/1606.05320)

