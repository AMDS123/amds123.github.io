---
layout: post
title: "ExpandNets: Exploiting Linear Redundancy to Train Small Networks"
date: 2018-11-26 16:40:24
categories: arXiv_CV
tags: arXiv_CV Knowledge
author: Shuxuan Guo, Jose M. Alvarez, Mathieu Salzmann
mathjax: true
---

* content
{:toc}

##### Abstract
While very deep networks can achieve great performance, they are ill-suited to applications in resource-constrained environments. Knowledge transfer, which leverages a deep teacher network to train a given small network, has emerged as one of the most popular strategies to address this problem. In this paper, we introduce an alternative approach to training a given small network, based on the intuition that parameter redundancy facilitates learning. We propose to expand each linear layer of a small network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can be compressed back to the small one algebraically, but, as evidenced by our experiments, consistently outperforms training the small network from scratch. This strategy is orthogonal to knowledge transfer. We therefore further show on several standard benchmarks that, for any knowledge transfer technique, using our expanded network as student systematically improves over using the small network.

##### Abstract (translated by Google)


##### URL
[https://arxiv.org/abs/1811.10495](https://arxiv.org/abs/1811.10495)

##### PDF
[https://arxiv.org/pdf/1811.10495](https://arxiv.org/pdf/1811.10495)

