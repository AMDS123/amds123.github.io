---
layout: post
title: "Recurrent Multimodal Interaction for Referring Image Segmentation"
date: 2017-08-04 21:53:15
categories: arXiv_CV
tags: arXiv_CV Segmentation CNN RNN
author: Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Alan Yuille
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-to-image interaction.

##### Abstract (translated by Google)
在本文中，我们感兴趣的是给出自然语言描述的图像分割问题，即引用表达式。现有的作品通过首先对图像和句子独立建模，然后通过组合这两种表示来分割图像来解决这个问题。我们认为学习图像交互在图像分割任务的两种模式联合建模意义上是更原生的，我们提出卷积多模式LSTM编码单个词，视觉信息和空间信息之间的顺序相互作用。我们表明，我们提出的模型胜过基准数据集上的基准模型。另外，我们分析了提出的多模态LSTM方法的中间输出，并且用实证的方法解释了这种方法如何实现更有效的字 - 图交互。

##### URL
[https://arxiv.org/abs/1703.07939](https://arxiv.org/abs/1703.07939)

##### PDF
[https://arxiv.org/pdf/1703.07939](https://arxiv.org/pdf/1703.07939)

