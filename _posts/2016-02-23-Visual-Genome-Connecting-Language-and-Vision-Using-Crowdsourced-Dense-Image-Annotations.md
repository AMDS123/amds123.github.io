---
layout: post
title: "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"
date: 2016-02-23 22:00:40
categories: arXiv_CV
tags: arXiv_CV Image_Caption Image_Classification Classification Relation
author: Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li
mathjax: true
---

* content
{:toc}

##### Abstract
Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage". In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.

##### Abstract (translated by Google)
尽管在图像分类等感知任务方面取得了进展，但计算机在图像描述和问题回答等认知任务上仍然表现欠佳。认知是任务不可或缺的核心，这些任务不仅包括对视觉世界的认识，而且还对其进行推理。然而，用于处理认知任务图像中丰富内容的模型仍在使用为感知任务设计的相同数据集进行训练。为了在认知任务中取得成功，模型需要了解图像中物体之间的相互作用和关系。当被问及“人骑车是什么车”时，计算机将需要识别图像中的物体以及骑马（人，马车）和拉（马，马车）之间的关系以便正确地回答“人是乘坐马车“。在本文中，我们提出了Visual Genome数据集来启用这种关系的建模。我们收集每个图像中的对象，属性和关系的密集注释以学习这些模型。具体而言，我们的数据集包含超过100K的图像，其中每个图像平均具有21个对象，18个属性和18个对象之间的配对关系。我们将区域描述中的对象，属性，关系和名词短语进行规范化，并将问题答案对与WordNet同义词对进行对比。这些注释一起表示图像描述，对象，属性，关系和问题答案的最密集和最大的数据集。

##### URL
[https://arxiv.org/abs/1602.07332](https://arxiv.org/abs/1602.07332)

##### PDF
[https://arxiv.org/pdf/1602.07332](https://arxiv.org/pdf/1602.07332)

