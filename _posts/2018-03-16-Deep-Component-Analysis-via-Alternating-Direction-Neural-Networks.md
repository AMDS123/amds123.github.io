---
layout: post
title: "Deep Component Analysis via Alternating Direction Neural Networks"
date: 2018-03-16 21:40:02
categories: arXiv_CV
tags: arXiv_CV Sparse Knowledge Represenation_Learning Optimization Inference Prediction
author: Calvin Murdock, Ming-Fang Chang, Simon Lucey
mathjax: true
---

* content
{:toc}

##### Abstract
Despite a lack of theoretical understanding, deep neural networks have achieved unparalleled performance in a wide range of applications. On the other hand, shallow representation learning with component analysis is associated with rich intuition and theory, but smaller capacity often limits its usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA), an expressive multilayer model formulation that enforces hierarchical structure through constraints on latent variables in each layer. For inference, we propose a differentiable optimization algorithm implemented using recurrent Alternating Direction Neural Networks (ADNNs) that enable parameter learning using standard backpropagation. By interpreting feed-forward networks as single-iteration approximations of inference in our model, we provide both a novel theoretical perspective for understanding them and a practical technique for constraining predictions with prior knowledge. Experimentally, we demonstrate performance improvements on a variety of tasks, including single-image depth prediction with sparse output constraints.

##### Abstract (translated by Google)
尽管缺乏理论上的理解，深度神经网络已经在广泛的应用中取得了无与伦比的性能。另一方面，使用成分分析的浅层表示学习与丰富的直觉和理论联系在一起，但容量较小常常限制了其有用性。为弥补这一差距，我们引入了深层组件分析（DeepCA），这是一种表达多层模型的公式，通过对每层潜变量的约束来强化层次结构。为了推论，我们提出了一种使用递归交替方向神经网络（ADNN）实现的可微分优化算法，其使用标准反向传播来实现参数学习。通过将前馈网络解释为我们模型中推理的单迭代逼近，我们提供了理解它们的新理论视角以及用先验知识约束预测的实用技术。在实验中，我们演示了各种任务的性能改进，包括具有稀疏输出约束的单图像深度预测。

##### URL
[https://arxiv.org/abs/1803.06407](https://arxiv.org/abs/1803.06407)

##### PDF
[https://arxiv.org/pdf/1803.06407](https://arxiv.org/pdf/1803.06407)

