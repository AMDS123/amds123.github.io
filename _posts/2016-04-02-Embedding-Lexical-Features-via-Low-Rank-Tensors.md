---
layout: post
title: "Embedding Lexical Features via Low-Rank Tensors"
date: 2016-04-02 04:59:21
categories: arXiv_SD
tags: arXiv_SD Relation_Extraction Embedding Prediction Relation
author: Mo Yu, Mark Dredze, Raman Arora, Matthew Gormley
mathjax: true
---

* content
{:toc}

##### Abstract
Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to over-fitting. We present a new model that represents complex lexical features---comprised of parts for words, contextual information and labels---in a tensor that captures conjunction information among these parts. We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include $n$-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation.

##### Abstract (translated by Google)
现代NLP模型严重依赖于设计的特征，这些特征往往将单词和上下文信息结合成复杂的词汇特征。这种组合导致大量的特征，这可能导致过度拟合。我们提出了一个新的模型，代表复杂的词汇特征---由词，上下文信息和标签组成的部分---在一个张量中捕获这些部分之间的连接信息。我们对相应的参数张量应用低秩张量逼近来减小参数空间，提高预测速度。此外，我们调查了两种处理包含$ n $  - 混合长度的特征的方法。我们的模型在关系提取，PP附件和介词歧义化方面取得了最先进的成果。

##### URL
[https://arxiv.org/abs/1604.00461](https://arxiv.org/abs/1604.00461)

##### PDF
[https://arxiv.org/pdf/1604.00461](https://arxiv.org/pdf/1604.00461)

