---
layout: post
title: "Attention-based Audio-Visual Fusion for Robust Automatic Speech Recognition"
date: 2018-09-05 20:38:48
categories: arXiv_SD
tags: arXiv_SD Attention Speech_Recognition Recognition
author: George Sterpu, Christian Saam, Naomi Harte
mathjax: true
---

* content
{:toc}

##### Abstract
Automatic speech recognition can potentially benefit from the lip motion patterns, complementing acoustic speech to improve the overall recognition performance, particularly in noise. In this paper we propose an audio-visual fusion strategy that goes beyond simple feature concatenation and learns to automatically align the two modalities, leading to enhanced representations which increase the recognition accuracy in both clean and noisy conditions. We test our strategy on the TCD-TIMIT and LRS2 datasets, designed for large vocabulary continuous speech recognition, applying three types of noise at different power ratios. We also exploit state of the art Sequence-to-Sequence architectures, showing that our method can be easily integrated. Results show relative improvements from 7% up to 30% on TCD-TIMIT over the acoustic modality alone, depending on the acoustic noise level. We anticipate that the fusion strategy can easily generalise to many other multimodal tasks which involve correlated modalities.

##### Abstract (translated by Google)
自动语音识别可以潜在地受益于唇部运动模式，补充声学语音以改善整体识别性能，尤其是噪声。在本文中，我们提出了一种视听融合策略，该策略超越了简单的特征连接，并学习自动对齐两种模态，从而增强了表示，从而提高了干净和嘈杂条件下的识别准确性。我们在TCD-TIMIT和LRS2数据集上测试我们的策略，该数据集专为大词汇量连续语音识别而设计，在不同功率比下应用三种类型的噪声。我们还利用最先进的序列到序列架构，表明我们的方法可以轻松集成。结果显示TCD-TIMIT相对于单独的声学模态从7％到30％的相对改善，取决于声学噪声水平。我们预计融合策略可以很容易地推广到涉及相关模态的许多其他多模式任务。

##### URL
[http://arxiv.org/abs/1809.01728](http://arxiv.org/abs/1809.01728)

##### PDF
[http://arxiv.org/pdf/1809.01728](http://arxiv.org/pdf/1809.01728)

