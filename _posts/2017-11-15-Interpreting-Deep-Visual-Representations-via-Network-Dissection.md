---
layout: post
title: "Interpreting Deep Visual Representations via Network Dissection"
date: 2017-11-15 15:05:25
categories: arXiv_CV
tags: arXiv_CV Regularization CNN Prediction
author: Bolei Zhou, David Bau, Aude Oliva, Antonio Torralba
mathjax: true
---

* content
{:toc}

##### Abstract
The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. However, CNNs often criticized as being black boxes that lack interpretability, since they have millions of unexplained model parameters. In this work, we describe Network Dissection, a method that interprets networks by providing labels for the units of their deep visual representations. The proposed method quantifies the interpretability of CNN representations by evaluating the alignment between individual hidden units and a set of visual semantic concepts. By identifying the best alignments, units are given human interpretable labels across a range of objects, parts, scenes, textures, materials, and colors. The method reveals that deep representations are more transparent and interpretable than expected: we find that representations are significantly more interpretable than they would be under a random equivalently powerful basis. We apply the method to interpret and compare the latent representations of various network architectures trained to solve different supervised and self-supervised training tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initializations, and the network depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a prediction given by a CNN for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure.

##### Abstract (translated by Google)
最近的深度卷积神经网络（CNNs）的成功取决于学习隐藏的表示，可以总结数据背后变化的重要因素。然而，CNN经常被批评为缺乏解释性的黑盒，因为它们有数以百万计的不明原因的模型参数。在这项工作中，我们描述网络解剖，一种解释网络的方法，通过提供深度视觉表示的单位标签。所提出的方法通过评估单个隐藏单元与一组视觉语义概念之间的对齐来量化CNN表示的可解释性。通过识别最佳路线，单元被赋予了对象，部件，场景，材质，材质和颜色范围内的人类可解释的标签。这个方法揭示了深层的表示比预期更加透明和可解释：我们发现表达式比它们在随机等价强大的基础上更易于解释。我们应用这种方法来解释和比较各种网络架构的潜在表示，这些网络架构被训练来解决不同的监督和自我监督的训练任务。然后，我们研究影响网络可解释性的因素，如训练迭代次数，正则化，不同的初始化，以及网络的深度和宽度。最后我们展示解释的单位可以用来提供CNN为图像给出的预测的明确解释。我们的研究结果强调，可解释性是深层神经网络的一个重要特性，为其层次结构提供了新的见解。

##### URL
[https://arxiv.org/abs/1711.05611](https://arxiv.org/abs/1711.05611)

##### PDF
[https://arxiv.org/pdf/1711.05611](https://arxiv.org/pdf/1711.05611)

