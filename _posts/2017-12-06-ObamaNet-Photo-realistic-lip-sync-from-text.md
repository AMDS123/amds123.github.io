---
layout: post
title: "ObamaNet: Photo-realistic lip-sync from text"
date: 2017-12-06 16:18:31
categories: arXiv_CV
tags: arXiv_CV RNN
author: Rithesh Kumar, Jose Sotelo, Kundan Kumar, Alexandre de Brebisson, Yoshua Bengio
mathjax: true
---

* content
{:toc}

##### Abstract
We present ObamaNet, the first architecture that generates both audio and synchronized photo-realistic lip-sync videos from any new text. Contrary to other published lip-sync approaches, ours is only composed of fully trainable neural modules and does not rely on any traditional computer graphics methods. More precisely, we use three main modules: a text-to-speech network based on Char2Wav, a time-delayed LSTM to generate mouth-keypoints synced to the audio, and a network based on Pix2Pix to generate the video frames conditioned on the keypoints.

##### Abstract (translated by Google)
我们提供ObamaNet，第一个架构，从任何新的文本生成音频和同步逼真的口形同步视频。与其他已公布的口型同步方法相反，我们只是由完全可训练的神经模块组成，并不依赖于任何传统的计算机图形方法。更精确地说，我们使用三个主要模块：基于Char2Wav的文本到语音网络，时间延迟的LSTM以生成与音频同步的嘴巴关键点，以及基于Pix2Pix的网络来生成关键点。

##### URL
[http://arxiv.org/abs/1801.01442](http://arxiv.org/abs/1801.01442)

##### PDF
[http://arxiv.org/pdf/1801.01442](http://arxiv.org/pdf/1801.01442)

