---
layout: post
title: "A Theoretical Framework for Robustness of Classifiers against Adversarial Examples"
date: 2017-09-27 16:02:48
categories: arXiv_CV
tags: arXiv_CV Adversarial Represenation_Learning Relation
author: Beilun Wang, Ji Gao, Yanjun Qi
mathjax: true
---

* content
{:toc}

##### Abstract
Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.

##### Abstract (translated by Google)
大多数机器学习分类器，包括深度神经网络，都容易受到敌对的例子。这样的输入通常是通过添加小而有目的的修改而产生的，这些修改导致错误的输出而人眼不可察觉。本文的目的不是介绍一个单一的方法，而是为了充分理解敌对的例子而做出理论上的步骤。通过使用拓扑的概念，我们的理论分析提出了一个对抗性例子可以欺骗一个分类器（$ f_1 $）并在其分析中添加它的oracle（$ f_2 $，就像人眼）的关键原因。通过调查与预测变量$ f_1 $和oracle $ f_2 $相对应的两个（伪）度量空间之间的拓扑关系，我们开发了足够的条件来确定$ f_1 $是否总是对强对抗$ F_2 $。有趣的是，我们的定理表明，只有一个不必要的特征可以使得$ f_1 $不是强健的，正确的特征表示学习是获得准确和强健的分类器的关键。

##### URL
[https://arxiv.org/abs/1612.00334](https://arxiv.org/abs/1612.00334)

##### PDF
[https://arxiv.org/pdf/1612.00334](https://arxiv.org/pdf/1612.00334)

