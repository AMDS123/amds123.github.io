---
layout: post
title: "A Neural Attention Model for Abstractive Sentence Summarization"
date: 2015-09-03 19:55:45
categories: arXiv_CL
tags: arXiv_CL Attention Summarization
author: Alexander M. Rush, Sumit Chopra, Jason Weston
mathjax: true
---

* content
{:toc}

##### Abstract
Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.

##### Abstract (translated by Google)
基于文本提取的摘要本质上是有限的，但是生成式抽象方法已被证明是有挑战性的。在这项工作中，我们提出了一个完全数据驱动的方法来抽象化句子总结。我们的方法利用了一个局部基于注意力的模型，生成输入句子的条件摘要的每个单词。虽然模型结构简单，但它可以很容易地进行端到端的培训，并可以扩展到大量的培训数据。与几个强大的基线相比，该模型在DUC-2004共享任务上显示出显着的性能提升。

##### URL
[https://arxiv.org/abs/1509.00685](https://arxiv.org/abs/1509.00685)

##### PDF
[https://arxiv.org/pdf/1509.00685](https://arxiv.org/pdf/1509.00685)

