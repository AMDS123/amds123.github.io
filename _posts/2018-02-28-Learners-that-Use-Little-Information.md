---
layout: post
title: "Learners that Use Little Information"
date: 2018-02-28 03:14:32
categories: arXiv_AI
tags: arXiv_AI
author: Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, Amir Yehudayoff
mathjax: true
---

* content
{:toc}

##### Abstract
We study learning algorithms that are restricted to using a small amount of information from their input sample. We introduce a category of learning algorithms we term $d$-bit information learners, which are algorithms whose output conveys at most $d$ bits of information of their input. A central theme in this work is that such algorithms generalize. 
 We focus on the learning capacity of these algorithms, and prove sample complexity bounds with tight dependencies on the confidence and error parameters. We also observe connections with well studied notions such as sample compression schemes, Occam's razor, PAC-Bayes and differential privacy. 
 We discuss an approach that allows us to prove upper bounds on the amount of information that algorithms reveal about their inputs, and also provide a lower bound by showing a simple concept class for which every (possibly randomized) empirical risk minimizer must reveal a lot of information. On the other hand, we show that in the distribution-dependent setting every VC class has empirical risk minimizers that do not reveal a lot of information.

##### Abstract (translated by Google)
我们研究的学习算法限于使用输入样本中的少量信息。我们介绍一类学习算法，我们称之为$ d $ -bit信息学习器，它是一种算法，其输出传达的输入信息最多为$ d $比特。这项工作的核心主题是这种算法的推广。
 我们专注于这些算法的学习能力，并证明样本复杂性边界与置信度和误差参数之间的紧密依赖关系。我们还观察了诸如样本压缩方案，奥卡姆剃刀，PAC-Bayes和差分隐私等良好研究概念的联系。
 我们讨论了一种方法，使我们能够证明算法揭示的关于其输入的信息量的上限，并且通过展示一个简单的概念类来提供一个下界，每个（可能是随机的）经验风险最小化者必须揭示大量的信息。另一方面，我们表明，在依赖分布的设置中，每个VC类都有经验风险最小化，但不会显示大量信息。

##### URL
[http://arxiv.org/abs/1710.05233](http://arxiv.org/abs/1710.05233)

##### PDF
[http://arxiv.org/pdf/1710.05233](http://arxiv.org/pdf/1710.05233)

