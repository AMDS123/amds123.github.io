---
layout: post
title: "Incorporating Loose-Structured Knowledge into Conversation Modeling via Recall-Gate LSTM"
date: 2017-02-06 03:43:17
categories: arXiv_CL
tags: arXiv_CL Knowledge RNN Relation
author: Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, Xiaolong Wang
mathjax: true
---

* content
{:toc}

##### Abstract
Modeling human conversations is the essence for building satisfying chat-bots with multi-turn dialog ability. Conversation modeling will notably benefit from domain knowledge since the relationships between sentences can be clarified due to semantic hints introduced by knowledge. In this paper, a deep neural network is proposed to incorporate background knowledge for conversation modeling. Through a specially designed Recall gate, domain knowledge can be transformed into the extra global memory of Long Short-Term Memory (LSTM), so as to enhance LSTM by cooperating with its local memory to capture the implicit semantic relevance between sentences within conversations. In addition, this paper introduces the loose structured domain knowledge base, which can be built with slight amount of manual work and easily adopted by the Recall gate. Our model is evaluated on the context-oriented response selecting task, and experimental results on both two datasets have shown that our approach is promising for modeling human conversations and building key components of automatic chatting systems.

##### Abstract (translated by Google)
人类对话建模是构建满足多回合对话能力的聊天机器人的要素。会话建模将显着受益于领域知识，因为由于知识引入的语义提示，语句之间的关系可以得到澄清。本文提出了一个深度神经网络来结合会话建模的背景知识。通过专门设计的Recall门，可以将领域知识转化为长时短暂记忆（LSTM）的额外全局记忆，从而通过与本地存储器协作来增强LSTM，以捕捉对话内句子之间隐含的语义关联。此外，本文还介绍了松散结构化的领域知识库，它可以用少量的手工工作建立起来，容易被Recall门采用。我们的模型是在面向上下文的响应选择任务上进行评估的，两个数据集上的实验结果都表明，我们的方法对建模人类对话和建立自动聊天系统的关键组件非常有前途。

##### URL
[https://arxiv.org/abs/1605.05110](https://arxiv.org/abs/1605.05110)

##### PDF
[https://arxiv.org/pdf/1605.05110](https://arxiv.org/pdf/1605.05110)

