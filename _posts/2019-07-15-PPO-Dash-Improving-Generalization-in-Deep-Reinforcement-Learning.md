---
layout: post
title: "PPO Dash: Improving Generalization in Deep Reinforcement Learning"
date: 2019-07-15 19:15:17
categories: arXiv_AI
tags: arXiv_AI Reinforcement_Learning
author: Joe Booth
mathjax: true
---

* content
{:toc}

##### Abstract
Deep reinforcement learning is prone to overfitting, and traditional benchmarks such as Atari 2600 benchmark can exacerbate this problem. The Obstacle Tower Challenge addresses this by using randomized environments and separate seeds for training, validation, and test runs. This paper examines various improvements and best practices to the PPO algorithm using the Obstacle Tower Challenge to empirically study their impact with regards to generalization. Our experiments show that the combination provides state-of-the-art performance on the Obstacle Tower Challenge.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1907.06704](http://arxiv.org/abs/1907.06704)

##### PDF
[http://arxiv.org/pdf/1907.06704](http://arxiv.org/pdf/1907.06704)

