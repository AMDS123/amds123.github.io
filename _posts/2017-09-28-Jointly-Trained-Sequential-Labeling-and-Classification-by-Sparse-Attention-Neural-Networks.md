---
layout: post
title: "Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks"
date: 2017-09-28 22:40:07
categories: arXiv_CL
tags: arXiv_CL Sparse Attention RNN Classification Relation Recognition
author: Mingbo Ma, Kai Zhao, Liang Huang, Bing Xiang, Bowen Zhou
mathjax: true
---

* content
{:toc}

##### Abstract
Sentence-level classification and sequential labeling are two fundamental tasks in language understanding. While these two tasks are usually modeled separately, in reality, they are often correlated, for example in intent classification and slot filling, or in topic classification and named-entity recognition. In order to utilize the potential benefits from their correlations, we propose a jointly trained model for learning the two tasks simultaneously via Long Short-Term Memory (LSTM) networks. This model predicts the sentence-level category and the word-level label sequence from the stepwise output hidden representations of LSTM. We also introduce a novel mechanism of "sparse attention" to weigh words differently based on their semantic relevance to sentence-level classification. The proposed method outperforms baseline models on ATIS and TREC datasets.

##### Abstract (translated by Google)
句级分类和顺序标注是语言理解的两个基本任务。虽然这两个任务通常是分开建模的，但实际上它们通常是相关的，例如意图分类和时隙填充，或者主题分类和命名实体识别。为了利用其相关性的潜在收益，我们提出了一个联合训练模型，通过长期短期记忆（LSTM）网络同时学习这两个任务。该模型从LSTM的逐步输出隐藏表示中预测句子级别类别和词级标签序列。我们还引入了一种新颖的“稀疏注意”机制，根据与句子级别分类的语义相关性，对单词加以不同的权重。所提出的方法优于ATIS和TREC数据集的基线模型。

##### URL
[https://arxiv.org/abs/1709.10191](https://arxiv.org/abs/1709.10191)

##### PDF
[https://arxiv.org/pdf/1709.10191](https://arxiv.org/pdf/1709.10191)

