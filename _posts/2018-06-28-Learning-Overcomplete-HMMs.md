---
layout: post
title: "Learning Overcomplete HMMs"
date: 2018-06-28 01:49:33
categories: arXiv_AI
tags: arXiv_AI Sparse
author: Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant
mathjax: true
---

* content
{:toc}

##### Abstract
We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.

##### Abstract (translated by Google)
我们研究学习超完备隐马尔可夫模型的问题---那些隐藏状态很多但输出字母表很小的问题。尽管具有重要的实际重要性，但对于高效学习，这种HMM尚不清楚，但没有已知的积极或消极结果。在本文中，我们提出了几个新的结果 - 正面和负面 - 这有助于定义易处理和难处理设置之间的界限。具体来说，我们展示了HMM的一个大的子类的正面结果，它们的转移矩阵是稀疏的，良好的，并且在短周期上具有小概率质量。另一方面，我们表明，只给出一个输出字母表小于HMM的样本的多项式数目，并且其转移矩阵是随机正则图具有较大程度的学习是不可能的。我们也在学习能够捕获长期依赖关系的HMMs的背景下讨论这些结果。

##### URL
[http://arxiv.org/abs/1711.02309](http://arxiv.org/abs/1711.02309)

##### PDF
[http://arxiv.org/pdf/1711.02309](http://arxiv.org/pdf/1711.02309)

