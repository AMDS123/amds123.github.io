---
layout: post
title: "Boosting Dictionary Learning with Error Codes"
date: 2017-01-15 10:12:36
categories: arXiv_CV
tags: arXiv_CV Sparse
author: Yigit Oktar, Mehmet Turkan
mathjax: true
---

* content
{:toc}

##### Abstract
In conventional sparse representations based dictionary learning algorithms, initial dictionaries are generally assumed to be proper representatives of the system at hand. However, this may not be the case, especially in some systems restricted to random initializations. Therefore, a supposedly optimal state-update based on such an improper model might lead to undesired effects that will be conveyed to successive iterations. In this paper, we propose a dictionary learning method which includes a general feedback process that codes the intermediate error left over from a less intensive initial learning attempt, and then adjusts sparse codes accordingly. Experimental observations show that such an additional step vastly improves rates of convergence in high-dimensional cases, also results in better converged states in the case of random initializations. Improvements also scale up with more lenient sparsity constraints.

##### Abstract (translated by Google)
在传统的基于稀疏表示的字典学习算法中，初始字典通常被认为是系统的恰当代表。但是，情况可能并非如此，特别是在一些限制于随机初始化的系统中。因此，基于这种不正确的模型的假定最佳的状态更新可能导致不期望的效果，将被传达给连续的迭代。在本文中，我们提出了一个字典学习方法，其中包括一个一般的反馈过程，编码从一个不太密集的初始学习尝试遗留的中间错误，然后相应地调整稀疏代码。实验观察表明，这种额外的步骤极大地提高了高维情况下的收敛速度，在随机初始化的情况下也导致更好的收敛状态。改进也随着更为宽松的稀疏性限制而扩大。

##### URL
[https://arxiv.org/abs/1701.04018](https://arxiv.org/abs/1701.04018)

##### PDF
[https://arxiv.org/pdf/1701.04018](https://arxiv.org/pdf/1701.04018)

