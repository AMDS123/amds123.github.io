---
layout: post
title: "Attend in groups: a weakly-supervised deep learning framework for learning from web data"
date: 2016-11-30 01:23:43
categories: arXiv_CV
tags: arXiv_CV Attention Deep_Learning Recognition
author: Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, Ian Reid
mathjax: true
---

* content
{:toc}

##### Abstract
Large-scale datasets have driven the rapid development of deep neural networks for visual recognition. However, annotating a massive dataset is expensive and time-consuming. Web images and their labels are, in comparison, much easier to obtain, but direct training on such automatically harvested images can lead to unsatisfactory performance, because the noisy labels of Web images adversely affect the learned recognition models. To address this drawback we propose an end-to-end weakly-supervised deep learning framework which is robust to the label noise in Web images. The proposed framework relies on two unified strategies -- random grouping and attention -- to effectively reduce the negative impact of noisy web image annotations. Specifically, random grouping stacks multiple images into a single training instance and thus increases the labeling accuracy at the instance level. Attention, on the other hand, suppresses the noisy signals from both incorrectly labeled images and less discriminative image regions. By conducting intensive experiments on two challenging datasets, including a newly collected fine-grained dataset with Web images of different car models, the superior performance of the proposed methods over competitive baselines is clearly demonstrated.

##### Abstract (translated by Google)
大规模数据集驱动深层神经网络的快速发展，以进行视觉识别。但是，注释大量的数据集是昂贵和耗时的。 Web图像及其标签比较容易获得，但是对这种自动获取的图像的直接训练可能导致不令人满意的性能，因为Web图像的嘈杂标签对学习的识别模型产生不利影响。为了解决这个缺点，我们提出了一个端到端的弱监督深度学习框架，它对于Web图像中的标签噪声是鲁棒的。所提出的框架依赖于两个统一的策略 - 随机分组和注意 - 有效地减少噪声网页图像注释的负面影响。具体而言，随机分组将多个图像叠加到一个训练实例中，从而提高实例级别的标注精度。另一方面，注意力抑制来自不正确标记的图像和较少识别图像区域的噪声信号。通过对两个具有挑战性的数据集（包括新收集的具有不同车型的Web图像的细粒度数据集）进行深入实验，可以清楚地展示所提出的方法在竞争基线上的优越性能。

##### URL
[https://arxiv.org/abs/1611.09960](https://arxiv.org/abs/1611.09960)

##### PDF
[https://arxiv.org/pdf/1611.09960](https://arxiv.org/pdf/1611.09960)

