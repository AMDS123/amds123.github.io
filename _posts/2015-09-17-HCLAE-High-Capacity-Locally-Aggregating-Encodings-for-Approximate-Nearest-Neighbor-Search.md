---
layout: post
title: "HCLAE: High Capacity Locally Aggregating Encodings for Approximate Nearest Neighbor Search"
date: 2015-09-17 10:18:05
categories: arXiv_CV
tags: arXiv_CV
author: Shicong Liu, Junru Shao, Hongtao Lu
mathjax: true
---

* content
{:toc}

##### Abstract
Vector quantization-based approaches are successful to solve Approximate Nearest Neighbor (ANN) problems which are critical to many applications. The idea is to generate effective encodings to allow fast distance approximation. We propose quantization-based methods should partition the data space finely and exhibit locality of the dataset to allow efficient non-exhaustive search. In this paper, we introduce the concept of High Capacity Locality Aggregating Encodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learn HCLAE by a simulated annealing procedure. The quantization error is lower than other state-of-the-art. The algorithms of DA can be easily extended to an online learning scheme, allowing effective handle of large scale data. Further, we propose Aggregating-Tree (A-Tree), a non-exhaustive search method using HCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-up on ANN-Search tasks, compared to the state-of-the-art.

##### Abstract (translated by Google)
基于矢量量化的方法成功解决了对于许多应用至关重要的近似最近邻（ANN）问题。这个想法是生成有效的编码，以允许快速距离近似。我们提出的基于量化的方法应该精细地划分数据空间并展现数据集的局部性以允许高效的非穷举搜索。为此，本文介绍了高容量局部性聚合编码（HCLAE）的概念，并提出了字典退火（DA）来模拟退火过程来学习HCLAE。量化误差比其他技术水平低。 DA的算法可以很容易地扩展到在线学习方案，使大规模数据的有效处理。此外，我们提出聚合树（A树），一个非穷举的搜索方法使用HCLAE执行有效的神经网络搜索。与最先进的技术相比，A-Tree在人工神经网络搜索任务上实现了大幅度的加速。

##### URL
[https://arxiv.org/abs/1509.05194](https://arxiv.org/abs/1509.05194)

##### PDF
[https://arxiv.org/pdf/1509.05194](https://arxiv.org/pdf/1509.05194)

