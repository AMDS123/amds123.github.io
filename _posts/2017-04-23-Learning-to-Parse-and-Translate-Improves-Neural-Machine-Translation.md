---
layout: post
title: "Learning to Parse and Translate Improves Neural Machine Translation"
date: 2017-04-23 16:52:03
categories: arXiv_SD
tags: arXiv_SD Attention NMT RNN
author: Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho
mathjax: true
---

* content
{:toc}

##### Abstract
There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.

##### Abstract (translated by Google)
在神经机器翻译之前，并没有把注意力集中在语言上。以前的大部分工作进一步受限于从源头上考虑语言。在本文中，我们提出了一种名为NMT + RNNG的混合模型，通过将递归神经网络语法结合到基于注意力的神经机器翻译中来学习解析和翻译。我们的方法鼓励神经机器翻译模型在训练过程中融合语言，然后让它自行翻译。使用四种语言对的大量实验显示了所提出的NMT + RNNG的有效性。

##### URL
[https://arxiv.org/abs/1702.03525](https://arxiv.org/abs/1702.03525)

##### PDF
[https://arxiv.org/pdf/1702.03525](https://arxiv.org/pdf/1702.03525)

