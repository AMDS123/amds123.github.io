---
layout: post
title: "Self-Normalization Properties of Language Modeling"
date: 2018-06-04 01:20:59
categories: arXiv_CL
tags: arXiv_CL Regularization Language_Model Relation
author: Jacob Goldberger, Oren Melamud
mathjax: true
---

* content
{:toc}

##### Abstract
Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function. In the context of language modeling, this property is particularly appealing as it may significantly reduce run-times due to large word vocabularies. In this study, we provide a comprehensive investigation of language modeling self-normalization. First, we theoretically analyze the inherent self-normalization properties of Noise Contrastive Estimation (NCE) language models. Then, we compare them empirically to softmax-based approaches, which are self-normalized using explicit regularization, and suggest a hybrid model with compelling properties. Finally, we uncover a surprising negative correlation between self-normalization and perplexity across the board, as well as some regularity in the observed errors, which may potentially be used for improving self-normalization algorithms in the future.

##### Abstract (translated by Google)
自我规范化判别模型近似于类的标准化概率，而不必计算分区函数。在语言建模的背景下，这个属性特别吸引人，因为它可能由于大词汇词汇表而显着减少运行时间。在这项研究中，我们提供了对语言建模自我规范化的全面调查。首先，我们从理论上分析了噪声对比估计（NCE）语言模型固有的自我归一化特性。然后，我们将它们与基于softmax的方法进行经验比较，这些方法是使用显式正则化进行自我标准化的，并且提出了具有令人信服的属性的混合模型。最后，我们揭示了自我规范化和全面困惑之间的令人惊讶的负相关，以及观察到的错误中的一些规律性，这些错误可能潜在地用于改进未来的自我规范化算法。

##### URL
[http://arxiv.org/abs/1806.00913](http://arxiv.org/abs/1806.00913)

##### PDF
[http://arxiv.org/pdf/1806.00913](http://arxiv.org/pdf/1806.00913)

