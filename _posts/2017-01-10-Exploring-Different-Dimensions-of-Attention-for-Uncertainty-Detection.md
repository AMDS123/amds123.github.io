---
layout: post
title: "Exploring Different Dimensions of Attention for Uncertainty Detection"
date: 2017-01-10 14:56:03
categories: arXiv_CL
tags: arXiv_CL Attention Detection
author: Heike Adel, Hinrich Schütze
mathjax: true
---

* content
{:toc}

##### Abstract
Neural networks with attention have proven effective for many natural language processing tasks. In this paper, we develop attention mechanisms for uncertainty detection. In particular, we generalize standardly used attention mechanisms by introducing external attention and sequence-preserving attention. These novel architectures differ from standard approaches in that they use external resources to compute attention weights and preserve sequence information. We compare them to other configurations along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features.

##### Abstract (translated by Google)
注意力的神经网络已被证明是有效的许多自然语言处理任务。在本文中，我们开发了不确定性检测的注意机制。具体而言，我们通过引入外部注意力和序列保留注意力来推广标准使用的关注机制。这些新颖的体系结构不同于标准方法，因为它们使用外部资源来计算注意力权重并保留序列信息。我们将它们与其他配置沿着不同的关注维度进行比较。我们的新颖架构在维基百科基准数据集上设置了最新的技术水平，并且在生物医学基准测试中使用了大量的语言特征，与现有技术的模型类似。

##### URL
[https://arxiv.org/abs/1612.06549](https://arxiv.org/abs/1612.06549)

##### PDF
[https://arxiv.org/pdf/1612.06549](https://arxiv.org/pdf/1612.06549)

