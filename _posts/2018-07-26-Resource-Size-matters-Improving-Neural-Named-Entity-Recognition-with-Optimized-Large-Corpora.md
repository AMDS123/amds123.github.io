---
layout: post
title: "Resource-Size matters: Improving Neural Named Entity Recognition with Optimized Large Corpora"
date: 2018-07-26 17:05:20
categories: arXiv_CL
tags: arXiv_CL Embedding Optimization Recognition
author: Sajawel Ahmed, Alexander Mehler
mathjax: true
---

* content
{:toc}

##### Abstract
This study improves the performance of neural named entity recognition by a margin of up to 11% in F-score on the example of a low-resource language like German, thereby outperforming existing baselines and establishing a new state-of-the-art on each single open-source dataset. Rather than designing deeper and wider hybrid neural architectures, we gather all available resources and perform a detailed optimization and grammar-dependent morphological processing consisting of lemmatization and part-of-speech tagging prior to exposing the raw data to any training process. We test our approach in a threefold monolingual experimental setup of a) single, b) joint, and c) optimized training and shed light on the dependency of downstream-tasks on the size of corpora used to compute word embeddings.

##### Abstract (translated by Google)
这项研究以德语等低资源语言为例，将F-score中神经命名实体识别的性能提高了11％，从而超越现有基线并建立了新的最新技术水平。每个单一的开源数据集。我们不是设计更深更广的混合神经架构，而是在将原始数据暴露给任何培训过程之前，收集所有可用资源并执行详细的优化和依赖于语法的形态处理，包括词形还原和词性标注。我们在a）单一，b）联合和c）优化训练的三重单语实验设置中测试我们的方法，并阐明下游任务对用于计算单词嵌入的语料库大小的依赖性。

##### URL
[http://arxiv.org/abs/1807.10675](http://arxiv.org/abs/1807.10675)

##### PDF
[http://arxiv.org/pdf/1807.10675](http://arxiv.org/pdf/1807.10675)

