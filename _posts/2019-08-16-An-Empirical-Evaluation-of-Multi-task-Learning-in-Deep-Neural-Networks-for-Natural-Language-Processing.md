---
layout: post
title: "An Empirical Evaluation of Multi-task Learning in Deep Neural Networks for Natural Language Processing"
date: 2019-08-16 03:16:40
categories: arXiv_CL
tags: arXiv_CL
author: Jianquan Li, Xiaokang Liu, Wenpeng Yin, Min Yang, Liqun Ma
mathjax: true
---

* content
{:toc}

##### Abstract
Multi-Task Learning (MTL) aims at boosting the overall performance of each individual task by leveraging useful information contained in multiple related tasks. It has shown great success in natural language processing (NLP). Currently, a number of MLT architectures and learning mechanisms have been proposed for various NLP tasks. However, there is no systematic exploration and comparison of different MLT architectures and learning mechanisms for their strong performance in-depth. In this paper, we conduct a thorough examination of typical MTL methods on a broad range of representative NLP tasks. Our primary goal is to understand the merits and demerits of existing MTL methods in NLP tasks, thus devising new hybrid architectures intended to combine their strengths.

##### Abstract (translated by Google)


##### URL
[http://arxiv.org/abs/1908.07820](http://arxiv.org/abs/1908.07820)

##### PDF
[http://arxiv.org/pdf/1908.07820](http://arxiv.org/pdf/1908.07820)

