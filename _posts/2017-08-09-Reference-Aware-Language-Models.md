---
layout: post
title: "Reference-Aware Language Models"
date: 2017-08-09 00:39:51
categories: arXiv_CL
tags: arXiv_CL Attention Language_Model
author: Zichao Yang, Phil Blunsom, Chris Dyer, Wang Ling
mathjax: true
---

* content
{:toc}

##### Abstract
We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or discourse context, even when the targets of the reference may be rare words. Experiments on three tasks shows our model variants based on deterministic attention.

##### Abstract (translated by Google)
我们提出了一个通用的语言模型，把参考作为一个显式的随机潜变量。该体系结构允许模型通过访问外部数据库（例如对话生成和配方生成所需的）和内部状态（例如，知晓共同参照的语言模型所需的）来创建实体及其属性的提及。这有助于在数据库或话语背景中的可预测位置上可以访问的信息，即使参考的目标可能是罕见的词语。三个任务的实验显示了基于确定性关注的模型变体。

##### URL
[https://arxiv.org/abs/1611.01628](https://arxiv.org/abs/1611.01628)

##### PDF
[https://arxiv.org/pdf/1611.01628](https://arxiv.org/pdf/1611.01628)

