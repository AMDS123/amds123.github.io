---
layout: post
title: "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality for Increased Model Capacity and Performance With No Computational Overhead"
date: 2017-04-15 02:59:14
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model
author: Alexandre Salle, Aline Villavicencio
mathjax: true
---

* content
{:toc}

##### Abstract
Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, resulting in a significant increase of computational cost. An alternative is the recurrent neural tensor network (RNTN), which increases capacity by employing distinct hidden layer weights for each vocabulary word. However, memory usage scales linearly with vocabulary size, which can reach millions for word-level language models. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that r-RNTNs improve language model performance over standard RNNs using only a small fraction of the parameters of unrestricted RNTNs.

##### Abstract (translated by Google)
递增递归神经网络（RNN）的容量通常涉及增大隐藏层的大小，导致计算成本的显着增加。另一种方法是递归神经张量网络（RNTN），它通过为每个词汇单词使用不同的隐藏层权重来增加容量。但是，内存使用量与词汇大小呈线性关系，对于单词级语言模型，内存使用量可能达到数百万。在本文中，我们引入限制递归神经张量网络（r-RNTN），为频繁的词汇词汇保留不同的隐藏层权重，同时为不频繁的词汇分配一组权重。困惑评估显示，r-RNTNs仅使用非限制性RNTN的一小部分参数在标准RNN上提高语言模型性能。

##### URL
[https://arxiv.org/abs/1704.00774](https://arxiv.org/abs/1704.00774)

##### PDF
[https://arxiv.org/pdf/1704.00774](https://arxiv.org/pdf/1704.00774)

