---
layout: post
title: "Improving Context Aware Language Models"
date: 2017-04-21 02:27:26
categories: arXiv_CL
tags: arXiv_CL Embedding RNN Classification Language_Model Prediction
author: Aaron Jaech, Mari Ostendorf
mathjax: true
---

* content
{:toc}

##### Abstract
Increased adaptability of RNN language models leads to improved predictions that benefit many applications. However, current methods do not take full advantage of the RNN structure. We show that the most widely-used approach to adaptation (concatenating the context with the word embedding at the input to the recurrent layer) is outperformed by a model that has some low-cost improvements: adaptation of both the hidden and output layers. and a feature hashing bias term to capture context idiosyncrasies. Experiments on language modeling and classification tasks using three different corpora demonstrate the advantages of the proposed techniques.

##### Abstract (translated by Google)
RNN语言模型的适应性增加导致改进的预测有益于许多应用。但是，目前的方法并没有充分利用RNN结构。我们展示了最广泛使用的适应方法（将上下文与输入到复现层的词嵌入在一起）优于具有一些低成本改进的模型：适应隐藏层和输出层。和一个功能散列偏差项来捕捉上下文的特质。语言建模和分类任务使用三个不同的语料库的实验证明了所提出的技术的优点。

##### URL
[https://arxiv.org/abs/1704.06380](https://arxiv.org/abs/1704.06380)

##### PDF
[https://arxiv.org/pdf/1704.06380](https://arxiv.org/pdf/1704.06380)

