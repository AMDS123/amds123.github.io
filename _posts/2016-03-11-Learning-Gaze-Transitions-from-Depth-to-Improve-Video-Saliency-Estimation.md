---
layout: post
title: "Learning Gaze Transitions from Depth to Improve Video Saliency Estimation"
date: 2016-03-11 15:53:58
categories: arXiv_CV
tags: arXiv_CV Salient Attention CNN
author: G. Leifman, D. Rudoy, T. Swedish, E. Bayro-Corrochano, R. Raskar
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper we introduce a novel Depth-Aware Video Saliency approach to predict human focus of attention when viewing RGBD videos on regular 2D screens. We train a generative convolutional neural network which predicts a saliency map for a frame, given the fixation map of the previous frame. Saliency estimation in this scenario is highly important since in the near future 3D video content will be easily acquired and yet hard to display. This can be explained, on the one hand, by the dramatic improvement of 3D-capable acquisition equipment. On the other hand, despite the considerable progress in 3D display technologies, most of the 3D displays are still expensive and require wearing special glasses. To evaluate the performance of our approach, we present a new comprehensive database of eye-fixation ground-truth for RGBD videos. Our experiments indicate that integrating depth into video saliency calculation is beneficial. We demonstrate that our approach outperforms state-of-the-art methods for video saliency, achieving 15% relative improvement.

##### Abstract (translated by Google)
在本文中，我们介绍了一种新颖的深度感知视频显着性方法，用于在常规2D屏幕上观看RGBD视频时预测人的注意力。我们训练一个生成卷积神经网络，预测一个帧的显着图，给定的前一帧的固定图。在这种情况下的显着性估计是非常重要的，因为在不久的将来3D视频内容将很容易获得，但很难显示。这一方面可以解释为三维采集设备的显着改进。另一方面，尽管3D显示技术取得了长足的进步，但大部分的3D显示器仍然昂贵，需要佩戴特殊的眼镜。为了评估我们的方法的性能，我们提出了一个新的综合数据库的RGBD视频眼固视地面真相。我们的实验表明，将深度集成到视频显着性计算中是有益的。我们证明，我们的方法胜过了最先进的视频显示方法，实现了15％的相对提高。

##### URL
[https://arxiv.org/abs/1603.03669](https://arxiv.org/abs/1603.03669)

##### PDF
[https://arxiv.org/pdf/1603.03669](https://arxiv.org/pdf/1603.03669)

