---
layout: post
title: "Autoencoder Node Saliency: Selecting Relevant Latent Representations"
date: 2017-11-21 16:17:14
categories: arXiv_CV
tags: arXiv_CV Salient
author: Ya Ju Fan
mathjax: true
---

* content
{:toc}

##### Abstract
The autoencoder is an artificial neural network model that learns hidden representations of unlabeled data. With a linear transfer function it is similar to the principal component analysis (PCA). While both methods use weight vectors for linear transformations, the autoencoder does not come with any indication similar to the eigenvalues in PCA that are paired with the eigenvectors. We propose a novel supervised node saliency (SNS) method that ranks the hidden nodes by comparing class distributions of latent representations against a fixed reference distribution. The latent representations of a hidden node can be described using a one-dimensional histogram. We apply normalized entropy difference (NED) to measure the "interestingness" of the histograms, and conclude a property for NED values to identify a good classifying node. By applying our methods to real data sets, we demonstrate the ability of SNS to explain what the trained autoencoders have learned.

##### Abstract (translated by Google)
自编码器是一种人工神经网络模型，用于学习未标记数据的隐藏表示。线性传递函数与主成分分析（PCA）相似。虽然这两种方法都使用权向量来进行线性变换，但是自编码器没有任何类似于与特征向量配对的PCA中的特征值的指示。我们提出了一种新的监督节点显着性（SNS）方法，通过比较潜在表示的类分布与固定的参考分布来对隐藏节点进行排序。隐藏节点的潜在表示可以使用一维直方图来描述。我们应用归一化熵差（NED）来度量直方图的“兴趣度”，并得出NED值的一个属性来确定一个好的分类节点。通过将我们的方法应用于实际的数据集，我们展示了SNS解释受过训练的自动编码器所学到的内容的能力。

##### URL
[https://arxiv.org/abs/1711.07871](https://arxiv.org/abs/1711.07871)

##### PDF
[https://arxiv.org/pdf/1711.07871](https://arxiv.org/pdf/1711.07871)

