---
layout: post
title: "Using accumulation to optimize deep residual neural nets"
date: 2018-01-14 07:17:00
categories: arXiv_CV
tags: arXiv_CV
author: Yatin Saraiya
mathjax: true
---

* content
{:toc}

##### Abstract
Residual Neural Networks [1] won first place in all five main tracks of the ImageNet and COCO 2015 competitions. This kind of network involves the creation of pluggable modules such that the output contains a residual from the input. The residual in that paper is the identity function. We propose to include residuals from all lower layers, suitably normalized, to create the residual. This way, all previous layers contribute equally to the output of a layer. We show that our approach is an improvement on [1] for the CIFAR-10 dataset.

##### Abstract (translated by Google)
残留神经网络[1]在ImageNet和COCO 2015比赛的所有五条主要赛道中均获得第一名。这种网络涉及创建可插拔模块，使得输出包含来自输​​入的残差。该论文中的残差是身份函数。我们建议包括来自所有较低层的残差，适当标准化以产生残差。这样，所有以前的图层对层的输出作出同样的贡献。我们表明，我们的方法对CIFAR-10数据集[1]是一个改进。

##### URL
[https://arxiv.org/abs/1803.05778](https://arxiv.org/abs/1803.05778)

##### PDF
[https://arxiv.org/pdf/1803.05778](https://arxiv.org/pdf/1803.05778)

