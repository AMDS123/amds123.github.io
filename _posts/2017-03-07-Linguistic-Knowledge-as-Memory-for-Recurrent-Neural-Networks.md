---
layout: post
title: "Linguistic Knowledge as Memory for Recurrent Neural Networks"
date: 2017-03-07 22:13:17
categories: arXiv_CV
tags: arXiv_CV Knowledge QA RNN Relation
author: Bhuwan Dhingra, Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov
mathjax: true
---

* content
{:toc}

##### Abstract
Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.

##### Abstract (translated by Google)
训练递归神经网络来模拟长期相关性是困难的。因此，我们建议使用外部的语言知识作为一个明确的信号来通知模型记忆它应该利用。具体来说，外部知识被用来增加具有在任意相距的元素之间的类型边的序列，并且所得的图被分解成有向无环子图。我们引入一个模型，将这些图形编码为循环神经网络中的显式记忆，并用它来对文本中的关联关系进行建模。我们将我们的模型应用于几个文本理解任务，并在所有考虑的基准（包括CNN，bAbi和LAMBADA）上实现新的最新成果。在bAbi QA任务中，我们的模型在20个任务中解决了15个任务，每个任务只有1000个训练样例。对学习表示的分析进一步证明了我们的模型在文档中编码细粒度实体信息的能力。

##### URL
[https://arxiv.org/abs/1703.02620](https://arxiv.org/abs/1703.02620)

##### PDF
[https://arxiv.org/pdf/1703.02620](https://arxiv.org/pdf/1703.02620)

