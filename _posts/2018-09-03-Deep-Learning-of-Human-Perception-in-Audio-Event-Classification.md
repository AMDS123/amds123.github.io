---
layout: post
title: "Deep Learning of Human Perception in Audio Event Classification"
date: 2018-09-03 08:47:32
categories: arXiv_SD
tags: arXiv_SD Classification Deep_Learning Relation
author: Yi Yu, Samuel Beuret, Donghuo Zeng, Keizo Oyama
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we introduce our recent studies on human perception in audio event classification by different deep learning models. In particular, the pre-trained model VGGish is used as feature extractor to process audio data, and DenseNet is trained by and used as feature extractor for our electroencephalography (EEG) data. The correlation between audio stimuli and EEG is learned in a shared space. In the experiments, we record brain activities (EEG signals) of several subjects while they are listening to music events of 8 audio categories selected from Google AudioSet, using a 16-channel EEG headset with active electrodes. Our experimental results demonstrate that i) audio event classification can be improved by exploiting the power of human perception, and ii) the correlation between audio stimuli and EEG can be learned to complement audio event understanding.

##### Abstract (translated by Google)
在本文中，我们介绍了我们最近对不同深度学习模型的音频事件分类中人类感知的研究。特别是，预训练模型VGGish用作特征提取器来处理音频数据，DenseNet由我们的脑电图（EEG）数据训练并用作特征提取器。音频刺激和EEG之间的相关性是在共享空间中学习的。在实验中，我们使用具有有源电极的16通道EEG耳机，在他们正在收听从Google AudioSet中选择的8个音频类别的音乐事件时，记录几个主体的大脑活动（EEG信号）。我们的实验结果表明，i）通过利用人类感知的力量可以改善音频事件分类，以及ii）可以学习音频刺激和EEG之间的相关性以补充音频事件理解。

##### URL
[http://arxiv.org/abs/1809.00502](http://arxiv.org/abs/1809.00502)

##### PDF
[http://arxiv.org/pdf/1809.00502](http://arxiv.org/pdf/1809.00502)

