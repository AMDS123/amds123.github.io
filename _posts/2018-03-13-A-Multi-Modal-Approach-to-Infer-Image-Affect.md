---
layout: post
title: "A Multi-Modal Approach to Infer Image Affect"
date: 2018-03-13 23:07:45
categories: arXiv_CV
tags: arXiv_CV Knowledge Prediction
author: Ashok Sundaresan, Sugumar Murugesan, Sean Davis, Karthik Kappaganthu, ZhongYi Jin, Divya Jain, Anurag Maunder
mathjax: true
---

* content
{:toc}

##### Abstract
The group affect or emotion in an image of people can be inferred by extracting features about both the people in the picture and the overall makeup of the scene. The state-of-the-art on this problem investigates a combination of facial features, scene extraction and even audio tonality. This paper combines three additional modalities, namely, human pose, text-based tagging and CNN extracted features / predictions. To the best of our knowledge, this is the first time all of the modalities were extracted using deep neural networks. We evaluate the performance of our approach against baselines and identify insights throughout this paper.

##### Abstract (translated by Google)
通过提取关于图片中的人物和场景的整体构成的特征，可以推断人物形象中的群体情感或情感。这个问题的最新技术研究面部特征，场景提取，甚至音频调性的组合。本文结合三种附加模式，即人体姿势，基于文本的标记和CNN提取的特征/预测。就我们所知，这是第一次使用深度神经网络提取所有模态。我们评估我们针对基线的方法的表现，并在本文中找出见解。

##### URL
[https://arxiv.org/abs/1803.05070](https://arxiv.org/abs/1803.05070)

##### PDF
[https://arxiv.org/pdf/1803.05070](https://arxiv.org/pdf/1803.05070)

