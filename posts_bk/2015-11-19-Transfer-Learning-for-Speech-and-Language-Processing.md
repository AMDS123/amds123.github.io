---
layout: post
title: "Transfer Learning for Speech and Language Processing"
date: 2015-11-19 05:54:45
categories: arXiv_CL
tags: arXiv_CL Review Speech_Recognition Transfer_Learning Deep_Learning Recognition
author: Dong Wang, Thomas Fang Zheng
mathjax: true
---

* content
{:toc}

##### Abstract
Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field.

##### Abstract (translated by Google)
转移学习是一种重要的技术，将为一项设置或任务培训的模型推广到其他设置或任务。例如，在语音识别中，针对一种语言训练的声学模型可以用于识别另一种语言的语音，而很少或不需要重新训练数据。转移学习与多任务学习（跨语言与多语言）密切相关，传统上以“模式适应”为名进行研究。深度学习的最新进展表明，通过深层模型学习的高层次抽象特征，转移学习变得更容易和更有效，“转移”不仅可以在数据分布和数据类型之间进行，也可以在模型结构之间进行，浅网和深层网）甚至模型类型（如贝叶斯模型和神经模型）。这篇评论文章总结了一些最近对这个方向的突出研究，尤其是语言和语言处理。我们也报告了我们小组的一些结果，并强调了这个非常有趣的研究领域的潜力。

##### URL
[https://arxiv.org/abs/1511.06066](https://arxiv.org/abs/1511.06066)

##### PDF
[https://arxiv.org/pdf/1511.06066](https://arxiv.org/pdf/1511.06066)

