---
layout: post
title: "Recurrent Neural Networks with External Memory for Language Understanding"
date: 2015-05-31 05:10:03
categories: arXiv_CV
tags: arXiv_CV RNN Prediction
author: Baolin Peng, Kaisheng Yao
mathjax: true
---

* content
{:toc}

##### Abstract
Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.

##### Abstract (translated by Google)
递归神经网络（RNN）已经越来越受到语言理解任务的欢迎。在这个任务中，部署语义标记器以将语义标签与输入序列中的每个词相关联。 RNN的成功可能归因于其能够记住将当前时间语义标签预测与许多时间实例相关的长期依赖关系。然而，由于梯度消失和爆炸问题，简单RNN的存储容量有限。我们建议使用外部存储器来提高RNN的记忆能力。我们对ATIS数据集进行了实验，并观察到所提出的模型能够达到最新的结果。我们比较我们提出的模型与替代模型和报告分析结果，可能为未来的研究提供见解。

##### URL
[https://arxiv.org/abs/1506.00195](https://arxiv.org/abs/1506.00195)

##### PDF
[https://arxiv.org/pdf/1506.00195](https://arxiv.org/pdf/1506.00195)

