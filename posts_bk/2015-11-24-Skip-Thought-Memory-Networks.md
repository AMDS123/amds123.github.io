---
layout: post
title: "Skip-Thought Memory Networks"
date: 2015-11-24 02:30:16
categories: arXiv_CV
tags: arXiv_CV QA Weakly_Supervised Face Inference Relation Memory_Networks
author: Ethan Caballero
mathjax: true
---

* content
{:toc}

##### Abstract
Question Answering (QA) is fundamental to natural language processing in that most nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly supervised memory network models that have been proposed so far struggle at answering questions that involve relations among multiple entities (such as facebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address this problem of learning multi-argument multi-hop semantic relations for the purpose of QA, we propose a method that combines the jointly learned long-term read-write memory and attentive inference components of end-to-end memory networks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector representations encoded by a Skip-Thought model (Kiros et al., 2015). This choice to append Skip-Thought Vectors to the existing MemN2N framework is motivated by the fact that Skip-Thought Vectors have been shown to accurately model multi-argument semantic relations (Kiros et al., 2015).

##### Abstract (translated by Google)
问答（QA）是自然语言处理的基础，因为大多数问题可以用QA来表达（Kumar et al。，2015）。当前已经提出的弱监督记忆网络模型在回答涉及多个实体之间关系的问题（例如（Weston等人，2015）中的Facebook的bAbi qa5-三 -  arg关系）方面挣扎着。为了解决QA的多参数多跳语义关系学习问题，本文提出了一种将联合学习的长期读写内存和端到端内存网络（MemN2N ）（Sukhbaatar et al。，2015），由跳跃思想模型（Kiros et al。，2015）编码的分布式语句向量表示。将Skip-Thought Vectors附加到现有的MemN2N框架中的这一选择是由跳跃思想向量已经被证明可以精确建模多参数语义关系（Kiros et al。，2015）的事实所驱动的。

##### URL
[https://arxiv.org/abs/1511.06420](https://arxiv.org/abs/1511.06420)

##### PDF
[https://arxiv.org/e-print/1511.06420](https://arxiv.org/e-print/1511.06420)

