---
layout: post
title: "Online Representation Learning in Recurrent Neural Language Models"
date: 2015-08-16 18:27:25
categories: arXiv_CL
tags: arXiv_CL Represenation_Learning Language_Model Prediction
author: Marek Rei
mathjax: true
---

* content
{:toc}

##### Abstract
We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step.

##### Abstract (translated by Google)
我们研究了递归神经网络语言模型中连续在线学习的扩展。该模型保持正在处理的文本的当前单元的单独矢量表示，并在每次预测之后自适应地调整它。最初的实验给出了有希望的结果，表明该方法能够提高语言建模的准确性，同时也减少了存储模型所需的参数以及在每一步所需的计算。

##### URL
[https://arxiv.org/abs/1508.03854](https://arxiv.org/abs/1508.03854)

##### PDF
[https://arxiv.org/pdf/1508.03854](https://arxiv.org/pdf/1508.03854)

