---
layout: post
title: "Ordering-sensitive and Semantic-aware Topic Modeling"
date: 2015-02-12 12:32:39
categories: arXiv_CL
tags: arXiv_CL Embedding Classification Quantitative
author: Min Yang, Tianyi Cui, Wenting Tu
mathjax: true
---

* content
{:toc}

##### Abstract
Topic modeling of textual corpora is an important and challenging problem. In most previous work, the "bag-of-words" assumption is usually made which ignores the ordering of words. This assumption simplifies the computation, but it unrealistically loses the ordering information and the semantic of words in the context. In this paper, we present a Gaussian Mixture Neural Topic Model (GMNTM) which incorporates both the ordering of words and the semantic meaning of sentences into topic modeling. Specifically, we represent each topic as a cluster of multi-dimensional vectors and embed the corpus into a collection of vectors generated by the Gaussian mixture model. Each word is affected not only by its topic, but also by the embedding vector of its surrounding words and the context. The Gaussian mixture components and the topic of documents, sentences and words can be learnt jointly. Extensive experiments show that our model can learn better topics and more accurate word distributions for each topic. Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM obtains significantly better performance in terms of perplexity, retrieval accuracy and classification accuracy.

##### Abstract (translated by Google)
文本语料库的主题建模是一个重要而又具有挑战性的问题。在大多数以前的工作中，通常会做出“袋装词”的假设，忽略词语的顺序。这个假设简化了计算，但是它不切实际地丢失了上下文中的排序信息和单词的语义。在本文中，我们提出了一个高斯混合神经主题模型（GMNTM），它将单词的排序和句子的语义意义合并到主题建模中。具体而言，我们将每个主题表示为多维向量的集合，并将语料库嵌入到由高斯混合模型生成的向量的集合中。每个单词不仅受到其主题的影响，而且受其周围单词和背景的嵌入矢量的影响。高斯混合分量和文档，句子和单词的话题可以共同学习。大量的实验表明，我们的模型可以为每个主题学习更好的主题和更准确的单词分布。在数量上，与先进的主题建模方法相比，GMNTM在困惑性，检索准确性和分类准确性方面获得显着更好的性能。

##### URL
[https://arxiv.org/abs/1502.03630](https://arxiv.org/abs/1502.03630)

##### PDF
[https://arxiv.org/pdf/1502.03630](https://arxiv.org/pdf/1502.03630)

