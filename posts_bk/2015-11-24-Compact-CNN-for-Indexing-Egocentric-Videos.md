---
layout: post
title: "Compact CNN for Indexing Egocentric Videos"
date: 2015-11-24 21:13:18
categories: arXiv_CV
tags: arXiv_CV Sparse CNN Classification Recognition
author: Yair Poleg, Ariel Ephrat, Shmuel Peleg, Chetan Arora
mathjax: true
---

* content
{:toc}

##### Abstract
While egocentric video is becoming increasingly popular, browsing it is very difficult. In this paper we present a compact 3D Convolutional Neural Network (CNN) architecture for long-term activity recognition in egocentric videos. Recognizing long-term activities enables us to temporally segment (index) long and unstructured egocentric videos. Existing methods for this task are based on hand tuned features derived from visible objects, location of hands, as well as optical flow. Given a sparse optical flow volume as input, our CNN classifies the camera wearer's activity. We obtain classification accuracy of 89%, which outperforms the current state-of-the-art by 19%. Additional evaluation is performed on an extended egocentric video dataset, classifying twice the amount of categories than current state-of-the-art. Furthermore, our CNN is able to recognize whether a video is egocentric or not with 99.2% accuracy, up by 24% from current state-of-the-art. To better understand what the network actually learns, we propose a novel visualization of CNN kernels as flow fields.

##### Abstract (translated by Google)
以自我为中心的视频越来越流行，浏览起来非常困难。在本文中，我们提出了一个紧凑的三维卷积神经网络（CNN）架构，以自我为中心的视频中的长期活动识别。认识到长期的活动使我们能够暂时分割（索引）长期和非结构化的以自我为中心的视频。这项任务的现有方法是基于从可见物体得到的手调特征，手的位置以及光流。给定一个稀疏的光流量作为输入，我们的CNN分类相机佩戴者的活动。我们获得了89％的分类准确率，比目前的最新技术水平高19％。对扩展的以自我为中心的视频数据集进行额外的评估，将类别数量分类为当前最先进的两倍。此外，我们的CNN能够以99.2％的准确率来识别视频是否以自我为中心，比目前的最新水平高出24％。为了更好地理解网络实际学到的东西，我们提出了CNN内核作为流场的新颖可视化。

##### URL
[https://arxiv.org/abs/1504.07469](https://arxiv.org/abs/1504.07469)

##### PDF
[https://arxiv.org/pdf/1504.07469](https://arxiv.org/pdf/1504.07469)

