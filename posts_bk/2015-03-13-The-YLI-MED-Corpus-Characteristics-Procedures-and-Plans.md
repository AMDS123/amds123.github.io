---
layout: post
title: "The YLI-MED Corpus: Characteristics, Procedures, and Plans"
date: 2015-03-13 23:36:42
categories: arXiv_SD
tags: arXiv_SD Detection
author: Julia Bernd, Damian Borth, Benjamin Elizalde, Gerald Friedland, Heather Gallagher, Luke Gottlieb, Adam Janin, Sara Karabashlieva, Jocelyn Takahashi, Jennifer Won
mathjax: true
---

* content
{:toc}

##### Abstract
The YLI Multimedia Event Detection corpus is a public-domain index of videos with annotations and computed features, specialized for research in multimedia event detection (MED), i.e., automatically identifying what's happening in a video by analyzing the audio and visual content. The videos indexed in the YLI-MED corpus are a subset of the larger YLI feature corpus, which is being developed by the International Computer Science Institute and Lawrence Livermore National Laboratory based on the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting one of ten target events, or no target event, and are annotated for additional attributes like language spoken and whether the video has a musical score. The annotations also include degree of annotator agreement and average annotator confidence scores for the event categorization of each video. Version 1.0 of YLI-MED includes 1823 "positive" videos that depict the target events and 48,138 "negative" videos, as well as 177 supplementary videos that are similar to event videos but are not positive examples. Our goal in producing YLI-MED is to be as open about our data and procedures as possible. This report describes the procedures used to collect the corpus; gives detailed descriptive statistics about the corpus makeup (and how video attributes affected annotators' judgments); discusses possible biases in the corpus introduced by our procedural choices and compares it with the most similar existing dataset, TRECVID MED's HAVIC corpus; and gives an overview of our future plans for expanding the annotation effort.

##### Abstract (translated by Google)
YLI多媒体事件检测语料库是具有注释和计算特征的视频的公共领域索引，专门用于多媒体事件检测（MED）的研究，即通过分析音频和视频内容来自动识别视频中发生的事情。在YLI-MED语料库索引的视频是YLI特征语料库的一个子集，由国际计算机科学研究所和劳伦斯利弗莫尔国家实验室根据雅虎Flickr创意共享1亿（YFCC100M）数据集开发。 YLI-MED中的视频被分类为描述十个目标事件之一或者没有目标事件，并且被标注用于附加属性，诸如所说的语言以及该视频是否具有乐谱。注释还包括每个视频的事件分类的注释器协议程度和平均注释器置信度分数。 YLI-MED 1.0版包括1823个描述目标事件的“正面”视频和48138个“负面”视频，以及177个与事件视频相似但不是正面例子的补充视频。我们生产YLI-MED的目标是尽可能公开我们的数据和程序。本报告描述了收集语料库的程序;给出了关于语料构成的详细描述性统计（以及视频属性如何影响注释者的判断）。讨论了程序选择所引入的语料库中的可能偏差，并将其与现有最相似的数据集TRECVID MED的HAVIC语料库进行比较;并概述了我们今后扩大注释工作的计划。

##### URL
[https://arxiv.org/abs/1503.04250](https://arxiv.org/abs/1503.04250)

##### PDF
[https://arxiv.org/pdf/1503.04250](https://arxiv.org/pdf/1503.04250)

