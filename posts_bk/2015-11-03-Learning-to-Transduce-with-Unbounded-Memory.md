---
layout: post
title: "Learning to Transduce with Unbounded Memory"
date: 2015-11-03 14:07:29
categories: arXiv_CL
tags: arXiv_CL RNN
author: Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.

##### Abstract (translated by Google)
最近，Deep Recurrent Neural Networks在自然语言转换问题上已经证明了强大的结果。在本文中，我们探索这些模型的代表性的力量，使用合成语法来设计表现出类似于实际转换问题中发现的现象，例如机器翻译。这些实验使我们提出了新的基于内存的循环网络，实现了传统数据结构（如Stacks，Queue和DeQues）的连续可微分类。我们表明，这些架构展现出优越的深度RNN的泛化性能，并且经常能够在我们的转导实验中学习潜在的生成算法。

##### URL
[https://arxiv.org/abs/1506.02516](https://arxiv.org/abs/1506.02516)

##### PDF
[https://arxiv.org/pdf/1506.02516](https://arxiv.org/pdf/1506.02516)

