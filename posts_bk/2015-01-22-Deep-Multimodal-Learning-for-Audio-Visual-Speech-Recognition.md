---
layout: post
title: "Deep Multimodal Learning for Audio-Visual Speech Recognition"
date: 2015-01-22 05:25:33
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition Classification Relation Recognition
author: Youssef Mroueh, Etienne Marcheret, Vaibhava Goel
mathjax: true
---

* content
{:toc}

##### Abstract
In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of $41\%$ under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of $35.83\%$ demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of $34.03\%$.

##### Abstract (translated by Google)
在本文中，我们提出深度多模式学习的方法，用于融合视听自动语音识别（AV-ASR）的语音和视觉形式。首先，我们研究一种单独训练单模深度网络的方法，并且将它们的最终隐藏层融合以获得另一个深度网络所建立的联合特征空间。尽管在IBM大型词汇视听演播室数据集中，音频网络在干净的条件下单独实现了$ 41 \％$的电话错误率（PER），该融合模型实现了$ 35.83 \％$的PER，显示了视觉的巨大价值即使在高信噪比的音频中，也可以在电话分类中使用频道。其次，我们提出了一种新的深层网络架构，它使用双线性最大层来说明模态之间的类特定相关性。我们表明，将双线性网络的后验与上述融合模型的后验相结合会导致更严重的电话差错率降低，产生34.03％的最终市盈率。

##### URL
[https://arxiv.org/abs/1501.05396](https://arxiv.org/abs/1501.05396)

##### PDF
[https://arxiv.org/pdf/1501.05396](https://arxiv.org/pdf/1501.05396)

