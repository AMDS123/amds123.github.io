---
layout: post
title: "Data-selective Transfer Learning for Multi-Domain Speech Recognition"
date: 2015-09-08 15:20:12
categories: arXiv_CL
tags: arXiv_CL Speech_Recognition Transfer_Learning Recognition
author: Mortaza Doulaty, Oscar Saz, Thomas Hain
mathjax: true
---

* content
{:toc}

##### Abstract
Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6--hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.

##### Abstract (translated by Google)
已经报道了在诸如域变化或说话者特征的几种情况下，用于自动语音识别的声学模型的训练中的负迁移。本文提出了一种新的技术，通过有效的语音数据选择来克服声学模型训练中的负迁移。这里的数据是根据特定目标的相关性来选择的。使用基于似然比的子模块函数来确定每个训练话语在目标测试集上的声学相似程度。该方法在广泛的数据集上进行评估，包括广播和电视广播的讲话，电话交谈，会议，讲座和朗读演讲。实验证明，所提出的技术既能找到相关的数据，又能限制负面的转移。 6小时测试集的结果显示，使用基于PLP的模型中的所有数据的数据选择相对提高了4％，使用DNN特征的结果为2％。

##### URL
[https://arxiv.org/abs/1509.02409](https://arxiv.org/abs/1509.02409)

##### PDF
[https://arxiv.org/pdf/1509.02409](https://arxiv.org/pdf/1509.02409)

