---
layout: post
title: "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
date: 2015-02-06 10:44:00
categories: arXiv_CV
tags: arXiv_CV Knowledge Image_Classification Classification Recognition
author: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
mathjax: true
---

* content
{:toc}

##### Abstract
Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.

##### Abstract (translated by Google)
整流活化单元（整流器）对于最先进的神经网络是必不可少的。在这项工作中，我们从两个方面研究整流器神经网络的图像分类。首先，我们提出一个推广传统整流装置的参数整流线性单元（PReLU）。 PRULU提高了模型拟合几乎为零的额外的计算成本和过度拟合的风险。其次，我们推导出一个特别考虑整流器非线性的鲁棒初始化方法。这种方法使我们能够直接从头开始训练非常深入的整流模型，并研究更深入或更广泛的网络架构。基于我们的PReLU网络（PReLU-nets），我们在ImageNet 2012分类数据集中实现了4.94％的前5个测试错误。这比2014年ILSVRC冠军（GoogLeNet，6.66％）有26％的改善。就我们所知，我们的结果是第一个超越人类表现（5.1％，Russakovsky等人）在这个视觉识别挑战。

##### URL
[https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)

##### PDF
[https://arxiv.org/pdf/1502.01852](https://arxiv.org/pdf/1502.01852)

