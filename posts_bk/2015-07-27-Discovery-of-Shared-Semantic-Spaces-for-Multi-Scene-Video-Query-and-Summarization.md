---
layout: post
title: "Discovery of Shared Semantic Spaces for Multi-Scene Video Query and Summarization"
date: 2015-07-27 15:50:03
categories: arXiv_CV
tags: arXiv_CV Summarization Classification
author: Xun Xu, Timothy Hospedales, Shaogang Gong
mathjax: true
---

* content
{:toc}

##### Abstract
The growing rate of public space CCTV installations has generated a need for automated methods for exploiting video surveillance data including scene understanding, query, behaviour annotation and summarization. For this reason, extensive research has been performed on surveillance scene understanding and analysis. However, most studies have considered single scenes, or groups of adjacent scenes. The semantic similarity between different but related scenes (e.g., many different traffic scenes of similar layout) is not generally exploited to improve any automated surveillance tasks and reduce manual effort. Exploiting commonality, and sharing any supervised annotations, between different scenes is however challenging due to: Some scenes are totally un-related -- and thus any information sharing between them would be detrimental; while others may only share a subset of common activities -- and thus information sharing is only useful if it is selective. Moreover, semantically similar activities which should be modelled together and shared across scenes may have quite different pixel-level appearance in each scene. To address these issues we develop a new framework for distributed multiple-scene global understanding that clusters surveillance scenes by their ability to explain each other's behaviours; and further discovers which subset of activities are shared versus scene-specific within each cluster. We show how to use this structured representation of multiple scenes to improve common surveillance tasks including scene activity understanding, cross-scene query-by-example, behaviour classification with reduced supervised labelling requirements, and video summarization. In each case we demonstrate how our multi-scene model improves on a collection of standard single scene models and a flat model of all scenes.

##### Abstract (translated by Google)
公共空间闭路电视安装的发展速度已经产生了对视频监控数据的自动化方法的需求，包括场景理解，查询，行为标注和汇总。出于这个原因，已经对监控现场的理解和分析进行了广泛的研究。然而，大多数研究已经考虑了单个场景或相邻场景的组。不同但相关的场景（例如，具有相似布局的许多不同的交通场景）之间的语义相似性通常不被用来改善任何自动化监视任务并减少手动工作。然而，在不同场景之间利用通用性和共享任何监督的注释是具有挑战性的，因为：有些场景是完全不相关的，因此它们之间的任何信息共享将是有害的;而其他人可能只共享一部分共同的活动 - 因此，信息共享只有在选择性的时候才有用。而且，应该在场景中共同模拟和共享的语义上相似的活动在每个场景中可能具有完全不同的像素级外观。为了解决这些问题，我们开发了一个分布式多场景全局理解的新框架，通过它们解释彼此行为的能力来聚集监视场景;并进一步发现在每个群集内共享活动的哪个子集与特定场景。我们展示了如何使用这种多场景的结构化表示来改善常见的监视任务，包括场景活动理解，跨场景查询的示例，减少监督标签要求的行为分类和视频摘要。在每种情况下，我们都演示了我们的多场景模型如何改进标准单场景模型集合以及所有场景的平面模型。

##### URL
[https://arxiv.org/abs/1507.07458](https://arxiv.org/abs/1507.07458)

##### PDF
[https://arxiv.org/pdf/1507.07458](https://arxiv.org/pdf/1507.07458)

