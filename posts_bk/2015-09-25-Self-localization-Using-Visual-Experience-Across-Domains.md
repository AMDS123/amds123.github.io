---
layout: post
title: "Self-localization Using Visual Experience Across Domains"
date: 2015-09-25 08:07:10
categories: arXiv_CV
tags: arXiv_CV
author: Taisho Tsukamoto, Kanji Tanaka
mathjax: true
---

* content
{:toc}

##### Abstract
In this study, we aim to solve the single-view robot self-localization problem by using visual experience across domains. Although the bag-of-words method constitutes a popular approach to single-view localization, it fails badly when it's visual vocabulary is learned and tested in different domains. Further, we are interested in using a cross-domain setting, in which the visual vocabulary is learned in different seasons and routes from the input query/database scenes. Our strategy is to mine a cross-domain visual experience, a library of raw visual images collected in different domains, to discover the relevant visual patterns that effectively explain the input scene, and use them for scene retrieval. In particular, we show that the appearance and the pose of the mined visual patterns of a query scene can be efficiently and discriminatively matched against those of the database scenes by employing image-to-class distance and spatial pyramid matching. Experimental results obtained using a novel cross-domain dataset show that our system achieves promising results despite our visual vocabulary being learned and tested in different domains.

##### Abstract (translated by Google)
在这项研究中，我们的目标是通过跨领域的视觉体验来解决单视图机器人的自定位问题。尽管单词的方法构成了单一视图本地化的流行方法，但是当视觉词汇在不同的领域被学习和测试时，它会失败。此外，我们感兴趣的是使用跨域设置，其中视觉词汇在不同的季节和来自输入查询/数据库场景的路线中学习。我们的策略是挖掘跨领域的视觉体验，即在不同领域收集的原始视觉图像库，以发现有效解释输入场景的相关视觉模式，并将其用于场景检索。特别地，我们通过采用图像到类别的距离和空间金字塔匹配来显示查询场景的开采视觉模式的外观和姿态可以与数据库场景的外观和姿态有效和有区别地匹配。使用新颖的跨域数据集获得的实验结果表明，尽管我们的视觉词汇学习和测试在不同的领域，我们的系统取得了有希望的结果。

##### URL
[https://arxiv.org/abs/1509.07618](https://arxiv.org/abs/1509.07618)

##### PDF
[https://arxiv.org/pdf/1509.07618](https://arxiv.org/pdf/1509.07618)

