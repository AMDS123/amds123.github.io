---
layout: post
title: "Simultaneous Deep Transfer Across Domains and Tasks"
date: 2015-10-08 03:42:45
categories: arXiv_CV
tags: arXiv_CV Sparse
author: Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko
mathjax: true
---

* content
{:toc}

##### Abstract
Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.

##### Abstract (translated by Google)
最近的报告表明，在大规模数据集上训练的通用监督型深度CNN模型可以减少但不会消除数据集偏差。微调新域中的深层模型可能需要大量的标记数据，这对于许多应用程序来说是不可用的。我们提出了一种新的CNN架构来利用未标记和稀疏标记的目标域数据。我们的方法同时优化域的不变性，以促进域名转移，并使用软标签分配匹配损失在任务之间传递信息。我们提出的适应方法提供的经验表现超过了以前公布的两个标准基准视觉领域适应任务的结果，在监督和半监督适应设置评估。

##### URL
[https://arxiv.org/abs/1510.02192](https://arxiv.org/abs/1510.02192)

##### PDF
[https://arxiv.org/pdf/1510.02192](https://arxiv.org/pdf/1510.02192)

