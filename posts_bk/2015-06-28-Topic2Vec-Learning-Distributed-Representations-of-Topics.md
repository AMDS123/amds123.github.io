---
layout: post
title: "Topic2Vec: Learning Distributed Representations of Topics"
date: 2015-06-28 16:17:40
categories: arXiv_CL
tags: arXiv_CL Embedding Language_Model Relation
author: Li-Qiang Niu, Xin-Yu Dai
mathjax: true
---

* content
{:toc}

##### Abstract
Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.

##### Abstract (translated by Google)
潜在狄利克雷分配（LDA）挖掘文档的主题结构在自然语言处理和机器学习领域起着重要的作用。然而，LDA的概率分布只描述了语料库中出现的统计关系，通常在实践中，概率并不是特征表示的最佳选择。最近，已经提出了通过学习基本概念和表示（例如Word2Vec和Doc2Vec）来表示单词和文档的嵌入方法。在许多任务中，嵌入表示比LDA表示更有效。在本文中，我们提出了Topic2Vec方法，可以在同一个语义向量空间中用单词学习主题表示，作为概率的替代。实验结果表明，Topic2Vec实现了有趣而有意义的结果。

##### URL
[https://arxiv.org/abs/1506.08422](https://arxiv.org/abs/1506.08422)

##### PDF
[https://arxiv.org/pdf/1506.08422](https://arxiv.org/pdf/1506.08422)

