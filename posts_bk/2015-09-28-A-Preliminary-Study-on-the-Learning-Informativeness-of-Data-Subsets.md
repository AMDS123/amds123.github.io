---
layout: post
title: "A Preliminary Study on the Learning Informativeness of Data Subsets"
date: 2015-09-28 15:21:00
categories: arXiv_CL
tags: arXiv_CL Knowledge Relation
author: Simon Kaltenbacher, Nicholas H. Kirk, Dongheui Lee
mathjax: true
---

* content
{:toc}

##### Abstract
Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios, a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention. Such problems need to be addressed to ensure timely and meaningful human-robot interaction. Our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources.

##### Abstract (translated by Google)
估算机器人系统的内部状态是复杂的：这是从多个异构传感器输入和知识源进行的。这种投入的离散化是为了捕捉显着的象征性信息，而这往往表现出结构和复发。由于这些序列被用来推理复杂的情况，所以更紧凑的表示将有助于技术认知推理能力的正确性，而这种能力现在受到计算复杂性问题的限制，并回归到代表性启发式或人为干预。这些问题需要解决，以确保及时和有意义的人机交互。我们的工作是在对给定输入数据集的子集进行训练时理解学习信息性的可变性。这是为了减少培训规模，同时保留大部分象征性的学习潜力。我们证明了人写文本的概念，并且猜想这个工作在收集来自大的远程源的信息时，将会减少顺序指令的训练数据量，同时保留语义关系。

##### URL
[https://arxiv.org/abs/1510.04104](https://arxiv.org/abs/1510.04104)

##### PDF
[https://arxiv.org/pdf/1510.04104](https://arxiv.org/pdf/1510.04104)

