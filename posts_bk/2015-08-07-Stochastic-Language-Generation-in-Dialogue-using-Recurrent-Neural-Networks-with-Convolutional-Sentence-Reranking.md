---
layout: post
title: "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking"
date: 2015-08-07 16:34:11
categories: arXiv_CL
tags: arXiv_CL CNN RNN
author: Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young
mathjax: true
---

* content
{:toc}

##### Abstract
The natural language generation (NLG) component of a spoken dialogue system (SDS) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add significantly to development costs and make cross-domain, multi-lingual dialogue systems intractable. Moreover, human languages are context-aware. The most natural response should be directly learned from data rather than depending on predefined syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experimental conditions. Results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems.

##### Abstract (translated by Google)
口语对话系统（SDS）中的自然语言生成（NLG）组件通常需要大量的手工操作或标记良好的数据集来进行训练。这些限制大大增加了开发成本，使跨领域，多语言的对话系统变得棘手。而且，人类语言是上下文感知的。最自然的反应应该直接从数据中学习，而不是依赖于预定义的语法或规则。本文提出了一种基于联合递归和卷积神经网络结构的统计语言生成器，该结构可以在没有任何语义对齐或预定义语法树的情况下在对话行为 - 话语对上训练。客观指标表明，这个新模型在相同的实验条件下胜过以前的方法。人类法官的评估结果表明，它不仅产生了高质量，而且语言上各不相同的话语，与n-gram和基于规则的系统相比，它们更受欢迎。

##### URL
[https://arxiv.org/abs/1508.01755](https://arxiv.org/abs/1508.01755)

##### PDF
[https://arxiv.org/pdf/1508.01755](https://arxiv.org/pdf/1508.01755)

