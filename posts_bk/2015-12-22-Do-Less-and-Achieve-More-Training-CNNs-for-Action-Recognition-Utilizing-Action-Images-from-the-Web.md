---
layout: post
title: "Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web"
date: 2015-12-22 16:52:19
categories: arXiv_CV
tags: arXiv_CV Action_Recognition Recognition
author: Shugao Ma, Sarah Adel Bargal, Jianming Zhang, Leonid Sigal, Stan Sclaroff
mathjax: true
---

* content
{:toc}

##### Abstract
Recently, attempts have been made to collect millions of videos to train CNN models for action recognition in videos. However, curating such large-scale video datasets requires immense human labor, and training CNNs on millions of videos demands huge computational resources. In contrast, collecting action images from the Web is much easier and training on images requires much less computation. In addition, labeled web images tend to contain discriminative action poses, which highlight discriminative portions of a video's temporal progression. We explore the question of whether we can utilize web action images to train better CNN models for action recognition in videos. We collect 23.8K manually filtered images from the Web that depict the 101 actions in the UCF101 action video dataset. We show that by utilizing web action images along with videos in training, significant performance boosts of CNN models can be achieved. We then investigate the scalability of the process by leveraging crawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M video frames by 393K unfiltered images and get comparable performance.

##### Abstract (translated by Google)
最近，已经尝试收集数百万个视频来训练CNN模型以便在视频中识别动作。然而，策划这样的大规模视频数据集需要巨大的人力，在数百万视频上训练CNN需要巨大的计算资源。相比之下，从Web收集动作图像要容易得多，对图像的训练需要更少的计算。另外，标记的网页图像倾向于包含区分性的动作姿势，其强调了视频的时间进展的区分性部分。我们探讨是否可以利用网络动作图像来训练更好的视频行动识别CNN模型。我们从Web上收集了23.8K个手动过滤的图像，描绘了UCF101动作视频数据集中的101个动作。我们表明，通过在训练中利用网络动作图像和视频，CNN模型可以实现显着的性能提升。然后，我们通过利用UCF101和ActivityNet的爬网图像（未过滤）来调查过程的可伸缩性。我们用393K未经过滤的图像替换了16.2M的视频帧，并获得了可比的性能。

##### URL
[https://arxiv.org/abs/1512.07155](https://arxiv.org/abs/1512.07155)

##### PDF
[https://arxiv.org/pdf/1512.07155](https://arxiv.org/pdf/1512.07155)

