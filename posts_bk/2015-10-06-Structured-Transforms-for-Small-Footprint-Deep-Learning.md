---
layout: post
title: "Structured Transforms for Small-Footprint Deep Learning"
date: 2015-10-06 19:42:22
categories: arXiv_CV
tags: arXiv_CV Speech_Recognition Inference Deep_Learning Recognition
author: Vikas Sindhwani, Tara N. Sainath, Sanjiv Kumar
mathjax: true
---

* content
{:toc}

##### Abstract
We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.

##### Abstract (translated by Google)
我们认为构建紧凑的深度学习管道的任务适合部署在存储和功耗受限的移动设备上。我们提出了一个统一的框架来学习一个广泛的结构化参数矩阵族，其特征是低位移秩的概念。我们的结构化变换允许快速的功能和梯度评估，并且跨越了丰富的参数共享配置，其统计建模能力可以从结构化到非结构化的连续统一调整。实验结果表明，这些变换在训练过程中可以显着加速推理和前进/后退过程，与现有技术相比，提供了更高的精度 - 压缩速度的优劣。在移动语音识别的关键词识别应用中，我们的方法比标准的线性低秩瓶颈层更有效，几乎保持了最先进的模型的性能，同时提供超过3.5倍的压缩。

##### URL
[https://arxiv.org/abs/1510.01722](https://arxiv.org/abs/1510.01722)

##### PDF
[https://arxiv.org/pdf/1510.01722](https://arxiv.org/pdf/1510.01722)

