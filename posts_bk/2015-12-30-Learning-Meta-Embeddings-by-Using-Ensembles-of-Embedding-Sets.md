---
layout: post
title: "Learning Meta-Embeddings by Using Ensembles of Embedding Sets"
date: 2015-12-30 08:29:54
categories: arXiv_CL
tags: arXiv_CL Embedding Deep_Learning
author: Wenpeng Yin, Hinrich Schütze
mathjax: true
---

* content
{:toc}

##### Abstract
Word embeddings -- distributed representations of words -- in deep learning are beneficial for many tasks in natural language processing (NLP). However, different embedding sets vary greatly in quality and characteristics of the captured semantics. Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning meta-embeddings. Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of meta-embeddings compared to individual embedding sets. One advantage of meta-embeddings is the increased vocabulary coverage. We will release our meta-embeddings publicly.

##### Abstract (translated by Google)
词嵌入 - 词的分布式表示 - 深度学习对于自然语言处理（NLP）中的许多任务是有益的。然而，不同的嵌入集合在捕获的语义的质量和特性方面差异很大。本文没有依赖更先进的嵌入学习算法，而是提出了一种结合不同公共嵌入集合的方法，以学习元嵌入为目标。对单词相似性和类比任务以及词性标注的实验表明，与单独的嵌入集相比，元嵌入的性能更好。元嵌入的一个优点是增加了词汇覆盖率。我们将公开发布我们的meta嵌入。

##### URL
[https://arxiv.org/abs/1508.04257](https://arxiv.org/abs/1508.04257)

##### PDF
[https://arxiv.org/pdf/1508.04257](https://arxiv.org/pdf/1508.04257)

