---
layout: post
title: "Scaling Recurrent Neural Network Language Models"
date: 2015-02-02 15:27:37
categories: arXiv_CL
tags: arXiv_CL RNN Language_Model Prediction
author: Will Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson
mathjax: true
---

* content
{:toc}

##### Abstract
This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction.

##### Abstract (translated by Google)
本文研究递归神经网络语言模型（RNNLMs）的缩放特性。我们讨论如何在GPU上训练非常大的RNN，并解决RNNLM如何在模型大小，训练集大小，计算成本和内存方面进行扩展的问题。我们的分析表明，尽管培训费用较高，但RNNLM在标准基准测试中比n-gram模型要困难得多。我们训练最大的已知RNN，并在ASR任务中提出18％的相对误字率增益。我们还在最近发布的十亿字语言建模基准测试中提出了新的最低难度，在机器翻译方面获得了1个BLEU点数增长，在单词预测方面获得了17％的相对命中率增长。

##### URL
[https://arxiv.org/abs/1502.00512](https://arxiv.org/abs/1502.00512)

##### PDF
[https://arxiv.org/pdf/1502.00512](https://arxiv.org/pdf/1502.00512)

