---
layout: post
title: "Adapting Resilient Propagation for Deep Learning"
date: 2015-09-16 11:45:48
categories: arXiv_CV
tags: arXiv_CV Deep_Learning
author: Alan Mosca, George D. Magoulas
mathjax: true
---

* content
{:toc}

##### Abstract
The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method for training Deep Neural Networks as standalone components and in ensemble formulations. Results on the MNIST dataset show that the proposed modification alleviates standard Rprop's problems demonstrating improved learning speed and accuracy.

##### Abstract (translated by Google)
弹性传播（Rprop）算法在各种应用中用于多层前向神经网络的反向传播训练已经非常流行。然而，标准的Rprop在深度神经网络的情况下遇到了困难，这通常发生在基于梯度的学习算法中。在本文中，我们提出了Rprop的一个修改，它将标准Rprop步骤与一个特殊的退出技术相结合。我们将该方法用于训练深度神经网络作为独立的组件和集合配方。 MNIST数据集上的结果表明，所提出的修改减轻了标准Rprop的问题，显示出提高的学习速度和准确性。

##### URL
[https://arxiv.org/abs/1509.04612](https://arxiv.org/abs/1509.04612)

##### PDF
[https://arxiv.org/pdf/1509.04612](https://arxiv.org/pdf/1509.04612)

