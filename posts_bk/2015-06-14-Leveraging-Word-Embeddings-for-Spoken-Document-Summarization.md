---
layout: post
title: "Leveraging Word Embeddings for Spoken Document Summarization"
date: 2015-06-14 09:18:36
categories: arXiv_CL
tags: arXiv_CL Summarization Embedding
author: Kuan-Yu Chen, Shih-Hung Liu, Hsin-Min Wang, Berlin Chen, Hsin-Hsi Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.

##### Abstract (translated by Google)
由于互联网上迅速增长的多媒体内容，为了从口头文件中自动选择一组有代表性的句子，简洁地表达文件中最重要的主题，抽取式口头文件摘要一直是一个活跃的研究领域和实验。另一方面，由于在许多与自然语言处理（NLP）有关的任务中表现出色，词嵌入已经成为一个新兴的研究课题。然而，据我们所知，调查在抽取性文本或语音摘要中使用的研究相对较少。在汇总过程中利用单词嵌入的一个常见方法是通过平均文档（或句子）中出现的单词的单词嵌入来表示文档（或句子）。然后，直观地，可以使用余弦相似性度量来确定一对表示之间的相关度。除了继续努力改进词语的表示外，本文的重点是基于一般词语嵌入方法提取新颖有效的排序模型。实验结果证明了我们所提出的方法与现有的最新方法相比的有效性。

##### URL
[https://arxiv.org/abs/1506.04365](https://arxiv.org/abs/1506.04365)

##### PDF
[https://arxiv.org/pdf/1506.04365](https://arxiv.org/pdf/1506.04365)

