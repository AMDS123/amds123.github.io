---
layout: post
title: "SceneNet: Understanding Real World Indoor Scenes With Synthetic Data"
date: 2015-11-26 22:09:09
categories: arXiv_CV
tags: arXiv_CV Segmentation Attention
author: Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla
mathjax: true
---

* content
{:toc}

##### Abstract
Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data --- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset. Additionally, we offer a route to generating synthesized frame or video data, and understanding of different factors influencing performance gains.

##### Abstract (translated by Google)
对于在真实环境中运行的任何自动化智能机器来说，场景理解是许多高级任务的先决条件。最近有监督学习的尝试在这个方向上显示出了希望，但是也强调了需要大量的监督数据---性能与使用的数据量成比例地增加。然而，当考虑收集这些数据所需的体力劳动时，这很快就变得过分了。在这项工作中，我们将注意力放在基于深度的语义每像素标记上作为场景理解问题，并展示计算机图形从合成3D场景中生成几乎无限制的标记数据的潜力。通过仔细综合训练数据和适当的噪声模型，尽管只使用深度数据作为输入，并且在SUN RGB-D数据集上设置了基于深度的分割的基准，但是我们展示了与NYUv2数据集上的最先进的RGBD系统相当的性能。另外，我们提供了一个生成合成帧或视频数据的途径，并了解影响性能增益的不同因素。

##### URL
[https://arxiv.org/abs/1511.07041](https://arxiv.org/abs/1511.07041)

##### PDF
[https://arxiv.org/pdf/1511.07041](https://arxiv.org/pdf/1511.07041)

