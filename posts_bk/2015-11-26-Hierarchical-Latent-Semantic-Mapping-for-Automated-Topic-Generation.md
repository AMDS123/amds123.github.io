---
layout: post
title: "Hierarchical Latent Semantic Mapping for Automated Topic Generation"
date: 2015-11-26 01:35:58
categories: arXiv_CL
tags: arXiv_CL Detection
author: Guorui Zhou, Guang Chen
mathjax: true
---

* content
{:toc}

##### Abstract
Much of information sits in an unprecedented amount of text data. Managing allocation of these large scale text data is an important problem for many areas. Topic modeling performs well in this problem. The traditional generative models (PLSA,LDA) are the state-of-the-art approaches in topic modeling and most recent research on topic generation has been focusing on improving or extending these models. However, results of traditional generative models are sensitive to the number of topics K, which must be specified manually. The problem of generating topics from corpus resembles community detection in networks. Many effective algorithms can automatically detect communities from networks without a manually specified number of the communities. Inspired by these algorithms, in this paper, we propose a novel method named Hierarchical Latent Semantic Mapping (HLSM), which automatically generates topics from corpus. HLSM calculates the association between each pair of words in the latent topic space, then constructs a unipartite network of words with this association and hierarchically generates topics from this network. We apply HLSM to several document collections and the experimental comparisons against several state-of-the-art approaches demonstrate the promising performance.

##### Abstract (translated by Google)
许多信息都是以前所未有的数量存在的。管理这些大规模文本数据的分配是许多领域的重要问题。主题建模在这个问题上表现良好。传统的生成模型（PLSA，LDA）是主题建模中最先进的方法，最近关于主题生成的研究一直侧重于改进或扩展这些模型。然而，传统生成模型的结果对主题K的数量很敏感，必须手动指定。从语料库中产生话题的问题类似于网络中的社区检测。许多有效的算法可以自动检测来自网络的社区，而无需人工指定数量的社区。在这些算法的启发下，本文提出了一种新的分层潜在语义映射（Hierarchical Latent Semantic Mapping，HLSM）方法，该方法从语料库自动生成主题。 HLSM计算潜在主题空间中每对单词之间的关联，然后构造一个具有这种关联的单词的单一网络，并从这个网络分层生成主题。我们将HLSM应用于多个文档集合，并且针对几种最先进的方法进行的实验性比较展示了有前途的性能。

##### URL
[https://arxiv.org/abs/1511.03546](https://arxiv.org/abs/1511.03546)

##### PDF
[https://arxiv.org/pdf/1511.03546](https://arxiv.org/pdf/1511.03546)

