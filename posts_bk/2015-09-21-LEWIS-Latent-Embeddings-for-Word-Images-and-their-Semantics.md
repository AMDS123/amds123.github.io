---
layout: post
title: "LEWIS: Latent Embeddings for Word Images and their Semantics"
date: 2015-09-21 14:32:43
categories: arXiv_CV
tags: arXiv_CV Attention Embedding CNN Recognition
author: Albert Gordo, Jon Almazan, Naila Murray, Florent Perronnin
mathjax: true
---

* content
{:toc}

##### Abstract
The goal of this work is to bring semantics into the tasks of text recognition and retrieval in natural images. Although text recognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the semantics into consideration. In this paper, we ask the following question: \emph{can we predict semantic concepts directly from a word image, without explicitly trying to transcribe the word image or its characters at any point?} For this goal we propose a convolutional neural network (CNN) with a weighted ranking loss objective that ensures that the concepts relevant to the query image are ranked ahead of those that are not relevant. This can also be interpreted as learning a Euclidean space where word images and concepts are jointly embedded. This model is learned in an end-to-end manner, from image pixels to semantic concepts, using a dataset of synthetically generated word images and concepts mined from a lexical database (WordNet). Our results show that, despite the complexity of the task, word images and concepts can indeed be associated with a high degree of accuracy

##### Abstract (translated by Google)
这项工作的目标是将语义学纳入自然图像中的文本识别和检索任务。尽管近年来文本识别和检索已经引起了人们的广泛关注，但以前的研究集中在识别或检索完全相同的用作查询的单词，而没有考虑语义。在本文中，我们提出以下问题：\ emph {我们是否可以直接从一个单词图像预测语义概念，而不是明确地试图在任何时候抄写单词图像或其字符？为此目的，我们提出了一个卷积神经网络CNN）的加权排名损失目标，确保与查询图像相关的概念排在那些不相关的排名前面。这也可以被解释为学习欧几里得空间，在这个空间中图像和概念被联合嵌入。使用从词汇数据库（WordNet）挖掘的合成生成的单词图像和概念的数据集，从端点到端点地学习这个模型，从图像像素到语义概念。我们的研究结果表明，尽管任务的复杂性，文字图像和概念确实可以高度准确地联系起来

##### URL
[https://arxiv.org/abs/1509.06243](https://arxiv.org/abs/1509.06243)

##### PDF
[https://arxiv.org/pdf/1509.06243](https://arxiv.org/pdf/1509.06243)

