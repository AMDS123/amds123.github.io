---
layout: post
title: "Structured Training for Neural Network Transition-Based Parsing"
date: 2015-06-19 21:05:01
categories: arXiv_CL
tags: arXiv_CL Knowledge
author: David Weiss, Chris Alberti, Michael Collins, Slav Petrov
mathjax: true
---

* content
{:toc}

##### Abstract
We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.

##### Abstract (translated by Google)
我们提出了基于神经网络转换的依赖解析的结构感知器训练。我们学习神经网络表示使用黄金语料库增加了大量的自动分析句子。给定这个固定的网络表示，我们学习最后一层使用结构感知器与波束搜索解码。在Penn Treebank上，我们的解析器达到了94.26％的未标记的和92.41％的标记附件的准确性，据我们所知，这是迄今为止斯坦福大学依赖的最佳准确性。我们还提供深入的烧蚀分析，以确定我们的模型的哪些方面提供了最大的准确性。

##### URL
[https://arxiv.org/abs/1506.06158](https://arxiv.org/abs/1506.06158)

##### PDF
[https://arxiv.org/pdf/1506.06158](https://arxiv.org/pdf/1506.06158)

