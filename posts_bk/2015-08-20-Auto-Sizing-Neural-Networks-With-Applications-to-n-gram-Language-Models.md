---
layout: post
title: "Auto-Sizing Neural Networks: With Applications to n-gram Language Models"
date: 2015-08-20 17:21:50
categories: arXiv_CL
tags: arXiv_CL Regularization Language_Model
author: Kenton Murray, David Chiang
mathjax: true
---

* content
{:toc}

##### Abstract
Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through $\ell_{\infty,1}$ and $\ell_{2,1}$ regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.

##### Abstract (translated by Google)
已经显示，神经网络能够提高一系列自然语言任务的性能。但是，设计和培训起来可能很复杂。通常，研究人员通过反复试验来选择最佳设置。在本文中，我们解决了在隐藏层中选择正确数量单元的问题。我们介绍一种通过$ \ ell _ {\ infty，1} $和$ \ ell_ {2,1} $正则化修剪隐藏单元来自动调整网络大小的方法。我们将这种方法应用于语言建模，并展示了在保持困惑的同时正确选择隐藏单元的数量的能力。我们还将这些模型包含在机器翻译解码器中，并显示这些较小的神经模型保留了未翻版版本的重大改进。

##### URL
[https://arxiv.org/abs/1508.05051](https://arxiv.org/abs/1508.05051)

##### PDF
[https://arxiv.org/pdf/1508.05051](https://arxiv.org/pdf/1508.05051)

